{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNa1+TmCATQz/vo2YCPBPdy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"afGcMYXd6fQ1"},"outputs":[],"source":["!pip install -qU accelerate liger-kernel galore-torch apollo-torch lomo-optim grokadamw schedulefree"]},{"cell_type":"markdown","source":["# Trainer"],"metadata":{"id":"GgxUdiXS6ju6"}},{"cell_type":"markdown","source":["The `Trainer` is a complete training and evaluation loop for PyTorch models implemented in the Transformers library."],"metadata":{"id":"kJrRTHMZPAm0"}},{"cell_type":"markdown","source":["## Basic usage"],"metadata":{"id":"QTGjsUqEQtdg"}},{"cell_type":"markdown","source":["`Trainer` includes all the code in a basic training loop:\n","* performing a training step to calculate the loss\n","* calculating the gradients with the `backward` method\n","* updating the weights based on the gradients\n","* repeating this process until we have reached a predetermined number of epochs\n","\n","If we want to specify any training options or hyperparameters, we can find them in the `TrainingArguments` class:"],"metadata":{"id":"UMqmRI0XQ0Bj"}},{"cell_type":"code","source":["from transformers import TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir='our-model',\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=2,\n","    weight_decay=0.01,\n","    eval_strategy='epoch',\n","    save_strategy='epoch',\n","    load_best_model_at_end=True,\n","    push_to_hub=False,\n",")"],"metadata":{"id":"UAc3yBtd6kbi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then we can pass `training_args` to the `Trainer` along with a model, dataset, preprocessor for the dataset, a data collator, and a function to compute the metrics we want to track during training. Finally, we call `train()` method to start training:"],"metadata":{"id":"2pxhVUJ0ReWT"}},{"cell_type":"code","source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset['train'],\n","    eval_dataset=dataset['test'],\n","    processing_class=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","trainer.train()"],"metadata":{"id":"sgvfAjl5Rxml"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Checkpoints"],"metadata":{"id":"qf_3WXAHR-DV"}},{"cell_type":"markdown","source":["Our trained model checkpoints are saved in the `output_dir` directory, and saved in a `checkpoint-000` subfolder where the numbers at the end corresponding to the training step."],"metadata":{"id":"qLjf59nTTWUN"}},{"cell_type":"code","source":["# resume from latest checkpoint\n","trainer.train(resume_from_checkpoint=True)\n","\n","# resume from specific checkpoint saved in the output directory\n","trainer.train(resume_from_checkpoint='our-model/checkpoint-1000')"],"metadata":{"id":"obZAjFmkR_QX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Customize the Trainer"],"metadata":{"id":"LYVMg3QRT4FC"}},{"cell_type":"markdown","source":["Many of the `Trainer`'s method can be subclassed and overridden to support the functionality we want, without having to rewrite the entire training loop from scratch. These methods include:\n","* `get_train_dataloader()`\n","* `get_eval_dataloader()`\n","* `get_test_dataloader()`\n","* `log()`\n","* `create_optimizer_and_scheduler()`\n","* `compute_loss()`\n","* `training_step()`\n","* `prediction_step()`\n","* `evaluate()`\n","* `predict()`\n","\n","For example, if we want to customize the `compute_loss()` method to use a weighted loss instead:"],"metadata":{"id":"_6OrWsWBUgVJ"}},{"cell_type":"code","source":["from torch improt nn\n","from transformers improt Trainer\n","\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n","        labels = inputs.pop('labels')\n","\n","        # forward pass\n","        outputs = model(**inputs)\n","        logits = outputs.get('logits')\n","\n","        # compute custom loss for 3 labels with different weights\n","        loss_fn = nn.CrossEntropyLoss(\n","            weight=torch.tensor([1.0, 2.0, 3.0]),\n","            device=model.device\n","        )\n","        loss = loss_fn(\n","            logits.view(-1, self.model.config.num_labels),\n","            labels.view(-1)\n","        )\n","\n","        return (loss, outputs) if return_outputs else loss"],"metadata":{"id":"p_tMOUYWT5hq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Callbacks"],"metadata":{"id":"C_3wKwt-VmiV"}},{"cell_type":"markdown","source":["**Callbacks** do not change anything in the training loop. They inspect the training loop state and then execute some action (early stopping, logging results, etc) depending on the state. A callback cannot be used to implement something like a custom loss function.\n","\n","For example, if we want to add an early stopping callback to the training loop after 10 steps:"],"metadata":{"id":"xLZIzZV0Vn_v"}},{"cell_type":"code","source":["from transformers import TrainerCallback\n","\n","class EarlyStoppingCallback(TrainerCallback):\n","    def __init__(self, num_steps=10):\n","        self.num_steps = num_steps\n","\n","    def on_step_end(self, args, state, control, **kwargs):\n","        if state.global_step >= self.num_steps:\n","            control.should_training_stop = True"],"metadata":{"id":"1ZLdKsNBVnc_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then we pass it to the `Trainer`'s `callback` parameter:"],"metadata":{"id":"F8mKiJjQWLX-"}},{"cell_type":"code","source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset['train'],\n","    eval_dataset=dataset['test'],\n","    processing_class=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    callbacks=[EarlyStoppingCallback()]\n",")"],"metadata":{"id":"5kHREbPuWOVd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Logging"],"metadata":{"id":"o1_PlCgSWXxI"}},{"cell_type":"markdown","source":["The `Trainer` is set to `logging.INFO` be default which reports errors, warnings, and other basic information. A `Trainer` replica - in distributed environments - is set to `logging.WARNING` which only reports errors and warnings.\n","\n","For example, to set our main code and modules to use the same log level according to each node:\n","```python\n","from transformers.utils import logging\n","import datasets\n","import transformers\n","\n","logger = logging.getLogger(__name__)\n","\n","logging.basicConfig(\n","    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n","    datefmt='%m/%d/%Y %H:%M:%S',\n","    handlers=[logging.StreamHandler(sys.stdout)],\n",")\n","\n","log_level = training_args.get_process_log_level()\n","logger.setLevel(log_level)\n","datasets.utils.logging.set_verbosity(log_level)\n","transformers.utils.logging.set_verbosity(log_level)\n","\n","trainer = Trainer(...)\n","```"],"metadata":{"id":"5MpasPkoXZyi"}},{"cell_type":"markdown","source":["## NEFTune"],"metadata":{"id":"73MOOYY6YiE6"}},{"cell_type":"markdown","source":["**NEFTune** is a technique that can improve performance by adding noise to the embedding vectors during training. To enable it in `Trainer`, set the `neftune_noise_alpha` parameter:\n","```python\n","from transformers import Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","    ...,\n","    neftune_noise_alpha=0.1\n",")\n","trainer = Trainer(\n","    ...,\n","    args=training_args\n",")\n","```"],"metadata":{"id":"F30UjIb1Yj4J"}},{"cell_type":"markdown","source":["## Liger Kernel"],"metadata":{"id":"SgEOSIKsY-nk"}},{"cell_type":"markdown","source":["**Liger-Kernel** kernel is a collection of Triton kernels developed by LinkedIn designed specifically for LLM training. It can effectively increase multi-GPU training throughput by 20% and reduces memory usage by 60%.\n","\n","```python\n","from transformers import TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir=\"your-model\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=2,\n","    weight_decay=0.01,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    push_to_hub=True,\n","    use_liger_kernel=True # add here\n",")\n","```"],"metadata":{"id":"lAJkn1oOZEb9"}},{"cell_type":"markdown","source":["## Optimizers"],"metadata":{"id":"f89td_D3Zdmi"}},{"cell_type":"markdown","source":["To choose a built-in optimizer,\n","```python\n","from transformers import TrainingArguments\n","\n","training_args = TrainingArguments(\n","    ...,\n","    optim='adamw_torch'\n",")\n","```\n","\n","We can also use an arbitrary PyTorch optimizer:\n","```python\n","import torch\n","from transformers import Trainer\n","\n","optimizer_cls = torch.optim.AdamW\n","optimizer_kwargs = {\n","    'lr': 4e-3,\n","    'betas': (0.9, 0.999),\n","    'weight_decay': 0.05,\n","}\n","\n","trainer = Trainer(\n","    ...,\n","    optimizer_cls_and_kwargs=(optimizer_cls, optimizer_kwargs)\n",")\n","```"],"metadata":{"id":"-pTX8sd3bsua"}},{"cell_type":"markdown","source":["### GaLore"],"metadata":{"id":"V08HFibGhIpx"}},{"cell_type":"markdown","source":["**Gradient Low-Rank Projection (GaLore)** is a memory-efficient low-rank training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adapation methods, such as LoRA."],"metadata":{"id":"0eWLDCkIj2HL"}},{"cell_type":"code","source":["import torch\n","import datasets\n","import trl\n","from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, AutoConfig\n","\n","train_dataset = datasets.load_dataset('imdb', split='train')\n","\n","model_id = 'google/gemma-2b'\n","config = AutoConfig.from_pretrained(model_id)\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForCausalLM.from_pretrained(config).to('cuda')"],"metadata":{"id":"zHmgMMVLWYzB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args = TrainingArguments(\n","    output_dir='./text-galore',\n","    max_steps=100,\n","    per_device_train_batch_size=2,\n","    optim='galore_adamw',\n","    optim_target_modules=[r\".*.attn.*\", r\".*.mlp.*\"]\n",")\n","\n","trainer = trl.SFTTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset,\n","    dataset_text_field='text',\n","    max_seq_length=512\n",")\n","\n","trainer.train()"],"metadata":{"id":"kwyXRFm9kpwL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To pass extra arguments supported by GaLore, we should pass correctly `optim_args`:"],"metadata":{"id":"p1Cs13zSlC-h"}},{"cell_type":"code","source":["args = TrainingArguments(\n","    output_dir='./text-galore',\n","    max_steps=100,\n","    per_device_train_batch_size=2,\n","    optim='galore_adamw',\n","    optim_target_modules=[r\".*.attn.*\", r\".*.mlp.*\"],\n","    optim_args=\"rank=64, update_proj_gap=100, scale=0.10\"\n",")\n","\n","trainer = trl.SFTTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset,\n","    dataset_text_field='text',\n","    max_seq_length=512\n",")\n","\n","trainer.train()"],"metadata":{"id":"_zMfEdm7lHXP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also perform layer-wise optimization by post-pending the optimizer name with `layerwise` :"],"metadata":{"id":"O7NnrxtIlV20"}},{"cell_type":"code","source":["args = TrainingArguments(\n","    output_dir='./text-galore',\n","    max_steps=100,\n","    per_device_train_batch_size=2,\n","    optim='galore_adamw_layerwise', # change here\n","    optim_target_modules=[r\".*.attn.*\", r\".*.mlp.*\"]\n",")\n","\n","trainer = trl.SFTTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset,\n","    dataset_text_field='text',\n","    max_seq_length=512\n",")\n","\n","trainer.train()"],"metadata":{"id":"uOogUSUSlaGD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### APOLLO"],"metadata":{"id":"7xMCK2jpliBz"}},{"cell_type":"markdown","source":["**Approximated Gradient Scaling for Memory Efficient LLM Optimization (APOLLO)** is a memory-efficient training strategy that allows full-parameter learning for both pre-training and fine-tuning, while maintaining AdamW-level performance with SGD-like memory efficiency.\n","* **Ultra-low rank efficiency**, requiring much lower rank than GaLore - even rank 1 (APOLLO-Mini) suffices\n","* **No expensive SVD computations**, APOLLO leverages random projection, avoiding training stalls, unlike GaLore"],"metadata":{"id":"0S8D7bzHljiQ"}},{"cell_type":"code","source":["import torch\n","import datasets\n","import trl\n","from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n","\n","train_dataset = datasets.load_dataset('imdb', split='train')\n","\n","model_id = 'google/gemma-2b'\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    low_cpu_mem_usage=True,\n",").to('cuda')"],"metadata":{"id":"a363aTe5ljJW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args = TrainingArguments(\n","    output_dir='./text-apollo',\n","    max_steps=100,\n","    per_device_train_batch_size=2,\n","    optim='apollo_adamw',\n","    optim_target_modules=[r\".*.attn.*\", r\".*.mlp.*\"]\n",")\n","\n","trainer = trl.SFTTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset,\n","    dataset_text_field='text',\n","    max_seq_length=512\n",")\n","\n","trainer.train()"],"metadata":{"id":"CVy6uKPZmXkz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can even enable APOLLO-Mini (rank=1 for extreme memory efficiency) by passing `optim_args`:"],"metadata":{"id":"vgAZxtibmlLX"}},{"cell_type":"code","source":["args = TrainingArguments(\n","    output_dir='./text-apollo',\n","    max_steps=100,\n","    per_device_train_batch_size=2,\n","    optim='apollo_adamw',\n","    optim_target_modules=[r\".*.attn.*\", r\".*.mlp.*\"],\n","    optim_args=\"proj=random, rank=1, scale=128.0, scale_type=tensor, update_proj_gap=200\"\n",")"],"metadata":{"id":"ryFwRZpbmrzZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### LOMO optimizer"],"metadata":{"id":"quT9_JJ3m1ac"}},{"cell_type":"markdown","source":["The **LOMO optimizers** have been introduced in *Full Parameter Fine-Tuning for Large Language Models with Limited Resources* and *AdaLomo: Low-memory Optimization with Adaptive Learning Rate*. They both consists of an efficient full-parameter fine-tuning method. These optimizers fuse the gradient computation and the parameter update in one step to reduce memory usage.\n","\n","According to the authors, it is recommended to use `AdaLomo` without `grad_norm` to get better performance and higher throughput.\n","\n","For example, to fine-tune `google/gemma-2b` on IMDB dataset in full precision:"],"metadata":{"id":"51uZhoYgrMW7"}},{"cell_type":"code","source":["import torch\n","import datasets\n","from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n","import trl\n","\n","train_dataset = datasets.load_dataset('imdb', split='train')\n","\n","model_id = 'google/gemma-2b'\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    low_cpu_mem_usage=True,\n",").to('cuda')"],"metadata":{"id":"RiNwOJxom27F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args = TrainingArguments(\n","    output_dir='./text-lomo',\n","    max_steps=1000,\n","    per_device_train_batch_size=4,\n","    optim='adalomo',\n","    gradient_checkpointing=True,\n","    logging_strategy='steps',\n","    logging_steps=1,\n","    learning_rate=2e-6,\n","    save_strategy='no',\n","    run_name='lomo-imdb'\n",")\n","\n","trainer = trl.SFTTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset,\n","    dataset_text_field='text',\n","    max_seq_length1024\n",")\n","\n","trainer.train()"],"metadata":{"id":"ZGu8Cx_oW_io"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GrokAdamW optimizer"],"metadata":{"id":"B2T50n_1XSD5"}},{"cell_type":"markdown","source":["The **GrokAdamW** optimizer is designed to enhance training performance and stability, particularly for models that benefit from grokking signal functions.\n","\n","GrokAdamW is particularly useful for models that require advanced optimization techniques to achieve better performance and stability.\n","\n","For example, to fine-tune `google/gemma-2b` on the IMDB dataset using the GrokAdamW opimizer:"],"metadata":{"id":"KOSpwKsBXVHX"}},{"cell_type":"code","source":["import torch\n","import datasets\n","from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n","import trl\n","\n","train_dataset = datasets.load_dataset('imdb', split='train')\n","\n","model_id = 'google/gemma-2b'\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    low_cpu_mem_usage=True,\n",").to('cuda')"],"metadata":{"id":"upuxqMEaXUZs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args = TrainingArguments(\n","    output_dir='./text-grokadamw',\n","    max_steps=1000,\n","    per_device_train_batch_size=4,\n","    optim='grokadamw',\n","    logging_strategy='steps',\n","    logging_steps=1,\n","    learning_rate=2e-5,\n","    save_strategy='no',\n","    run_name='grokadamw-imdb'\n",")\n","\n","trainer = trl.SFTTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset,\n","    dataset_text_field='text',\n","    max_seq_length1024\n",")\n","\n","trainer.train()"],"metadata":{"id":"hbRBWScLXrW-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Schedule-free optimizer"],"metadata":{"id":"27Ce_q1MX0i8"}},{"cell_type":"markdown","source":["The **Schedule-free** optimizers have been introduced in *The Road Less Scheduled*.\n","\n","Schedule-free learning replaces the momentum of the base optimizer with a combination of averaging and interpolation, to completely remove the need to anneal the learning rate with a traditional schedule. In addition, neither `warmup_steps` nor `warmup_ratio` are required when using `schedule_free_radam`."],"metadata":{"id":"BxaTgH_2X3iX"}},{"cell_type":"code","source":["import torch\n","import datasets\n","from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n","import trl\n","\n","train_dataset = datasets.load_dataset('imdb', split='train')\n","\n","model_id = 'google/gemma-2b'\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    low_cpu_mem_usage=True,\n",").to('cuda')"],"metadata":{"id":"q-KlHzooX2oN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args = TrainingArguments(\n","    output_dir='./text-schedulefree',\n","    max_steps=1000,\n","    per_device_train_batch_size=4,\n","    optim='schedule_free_radam,\n","    lr_scheduler_type='constant',\n","    gradient_checkpointing=True,\n","    logging_strategy='steps',\n","    logging_steps=1,\n","    learning_rate=2e-6,\n","    save_strategy='no',\n","    run_name='sfo-imdb'\n",")\n","\n","trainer = trl.SFTTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset,\n","    dataset_text_field='text',\n","    max_seq_length1024\n",")\n","\n","trainer.train()"],"metadata":{"id":"D6ULz6JGY0OT"},"execution_count":null,"outputs":[]}]}