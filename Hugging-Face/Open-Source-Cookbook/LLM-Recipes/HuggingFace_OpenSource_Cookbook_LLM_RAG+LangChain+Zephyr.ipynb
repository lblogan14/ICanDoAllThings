{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNP8/ozmUvD+4ny0T+s2qIx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Simple RAG for GitHub Issues using HuggingFace Zephyr and LangChain"],"metadata":{"id":"K9C59BMyyvsS"}},{"cell_type":"markdown","source":["In this example, we will build a RAG for a project's GitHub issues using [`HuggingFaceH4/zephyr-7b-beta`](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) model and LangChain."],"metadata":{"id":"ZKp8Q78_y1t8"}},{"cell_type":"markdown","source":["## RAG introduction"],"metadata":{"id":"x1oQynYizBi0"}},{"cell_type":"markdown","source":["RAG is a popular approach to address the issue of a powerful LLM not being aware of specific content due to said content not being in its training data, or hallucinating even when it has seen it before. Such specific content may be proprietary, sensitive, or, as in this example, recent and updated often.\n","\n","If our data is static and does not change regularly, we may consider fine-tuning a large model. In many cases, however, fine-tuning can be costly, and, when done repeatedly (e.g., to address data drift), leads to \"model shift\".\n","\n","RAG does not require model fine-tuning. Instead, RAG works by providing an LLM with additional context that is retrieved from relevant data so that it can generate a better-informed response.\n","\n","Note that\n","* The external data is converted into embedding vectors with a separate embedding model, and the vector are kept in a database. Embedding models are typically small, so updating the embedding vecotrs on a regular basis is faster, cheaper, and easier than fine-tuning a model.\n","* The fact that fine-tuning is not required gives us the freedom to swap our LLM for a more powerful one when it becomes available, or switch to a smaller distilled version for faster inference."],"metadata":{"id":"EWW7SOI0zD9l"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"e-YL46FNz4Si"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fyMcgAXyylhm"},"outputs":[],"source":["!pip install -qU torch transformers accelerate bitsandbytes transformers sentence-transformers faiss-gpu"]},{"cell_type":"code","source":["# If running in Google Colab, may need to run this cell to make sure to using UTF-8 locale to install LangChain\n","import locale\n","\n","locale.getpreferredencoding = lambda: \"UTF-8\""],"metadata":{"id":"7iwsIUprz87l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -qU langchain langchain-community"],"metadata":{"id":"XikBDIoJ0AnC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prepare the data"],"metadata":{"id":"WyayOSKE0B6W"}},{"cell_type":"markdown","source":["In this example, we will load all of the issues (both open and closed) from [HuggingFace PEFT Github repository](https://github.com/huggingface/peft)."],"metadata":{"id":"EyYPftc0c9Cw"}},{"cell_type":"code","source":["from google.colab import userdata\n","\n","ACCESS_TOKEN = userdata.get('GITHUB_ACCESS_TOKEN')"],"metadata":{"id":"PDtilj1U0C2j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["By default, the pull requests are considered issues as well. In this example, we will exclude them from data by setting `include_prs = False`. We will also set `state = \"all\"` to load both open and closed issues."],"metadata":{"id":"V_LoPgBRdgkX"}},{"cell_type":"code","source":["from langchain.document_loaders import GitHubIssuesLoader\n","\n","loader = GitHubIssuesLoader(\n","    repo='huggingface/peft',\n","    access_token=ACCESS_TOKEN,\n","    include_prs=False,\n","    state='all'\n",")\n","\n","docs = loader.load()"],"metadata":{"id":"LDl_tittdwvh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The content of individual GitHub issues may be longer than what an embedding model can take as input. We need to chunk the documents into appropriately sized pieces.\n","\n","The most common and straightforward approach to chunking is to define a fixed size of chunks and whether there should be any overlap between them."],"metadata":{"id":"yLvVvjJed-BN"}},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=512,\n","    chunk_overlap=30\n",")\n","\n","chunked_docs = splitter.split_documents(docs)"],"metadata":{"id":"gOv8ouPmeOfJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create the embeddings and retriever"],"metadata":{"id":"oFwPhJtReYpo"}},{"cell_type":"markdown","source":["To create document chunk embeddings, we will use the `HuggingFaceEmbeddings` and the [`BAAI/bge-base-en-v1.5`](https://huggingface.co/BAAI/bge-base-en-v1.5) embedding model.\n","\n","To create the vector database, we will use `FAISS`, a library developed by Facebook AI, which offers efficient similarity search and clustering of dense vectors."],"metadata":{"id":"F669UlgWebWo"}},{"cell_type":"code","source":["from langchain.vectorstores import FAISS\n","from langchain.embeddings import HuggingFaceEmbeddings\n","\n","db = FAISS.from_documents(\n","    chunked_docs,\n","    HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",")"],"metadata":{"id":"VolpdN3WearE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To retrieve the documents give nan unstructured query, we will use the `as_retriever` method using the `db` database as a backbone."],"metadata":{"id":"2_maQ2FWe_bn"}},{"cell_type":"code","source":["retriever = db.as_retriever(\n","    search_type='similarity',\n","    search_kwargs={'k':4}\n",")"],"metadata":{"id":"mmI9jL60fLxD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* `search_type='similarity'` - perform similarity search between the query and documents\n","* `search_kwargs={'k':4}` - instruct the retriever to return top 4 results"],"metadata":{"id":"QD7_ZDjlfQqg"}},{"cell_type":"code","source":[],"metadata":{"id":"DDTNdll7fcny"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now the vector database and retriever are set up. Next, we need to set up the model."],"metadata":{"id":"1tSRqULafdb5"}},{"cell_type":"markdown","source":["## Load quantized model"],"metadata":{"id":"JiVY6bQpfiUM"}},{"cell_type":"markdown","source":["To make inference faster, we will load the quantized model:"],"metadata":{"id":"dsfE6zNVflzr"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","import torch\n","\n","model_name = 'HuggingFaceH4/zephyr-7b-beta'\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type='nf4',\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config\n",")"],"metadata":{"id":"JsnWtDOZfh2g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Set up the LLM chain"],"metadata":{"id":"KDpjkIVTf3W5"}},{"cell_type":"markdown","source":["Finally, we have everything in hand and need to set up the LLM chain:\n","1. creating a `text_generation` pipeline using the loaded model and its tokenizer\n","2. creating a `prompt_template`, which should follow the format of the model, so if we substitute the model checkpoint, we will make sure to use the appropriate formatting"],"metadata":{"id":"Rc95kISZf56-"}},{"cell_type":"code","source":["from langchain.llms import HuggingFacePipeline\n","from langchain.prompts improt PromptTemplate\n","from transformers import pipeline\n","from langchain_core.output_parsers import StrOutputParser\n","\n","text_generation_pipeline = pipeline(\n","    model=model,\n","    tokenizer=tokenizer,\n","    task='text-generation',\n","    temperature=0.2,\n","    do_sample=True,\n","    repetition_penalty=1.1,\n","    return_full_text=True,\n","    max_new_tokens=400,\n",")\n","\n","llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"],"metadata":{"id":"Pib7yoqXf4rY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt_template = \"\"\"\n","<|system|>\n","Answer the question based on your knowledge. Use the follwoing context to help:\n","\n","{context}\n","\n","</s>\n","<|user|>\n","{question>\n","</s>\n","<|assistant|>\n","\n","\"\"\"\n","\n","prompt = Prompt_template(\n","    input_variables=['context', 'question'],\n","    template=prompt_template\n",")"],"metadata":{"id":"ocLzysIbglZJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up the entire chain\n","llm_chain = prompt | llm | StrOutputParser()"],"metadata":{"id":"ZgEuU8tZg43-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we need to combine the `llm_chain` with the retriever to create a RAG chain. We will pass the original question through to the final generation step, as well as the retrieved context docs:"],"metadata":{"id":"KfGkUjLYg_6r"}},{"cell_type":"code","source":["from langchain_core.runnables import RunnablePassthrough\n","\n","rag_result = {\n","    'context': retriever,\n","    'question': RunnablePassthrough()\n","}\n","\n","rag_chain = rag_result | llm_chain"],"metadata":{"id":"6q9gVHgWhKkQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Compare the results"],"metadata":{"id":"5AAG69F5hbz9"}},{"cell_type":"code","source":["question = 'How do you combine multiple adapters?'"],"metadata":{"id":"lAGu8w16hdg-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["First, let's see what kind of answer we can get with the model itself without additional context provided:"],"metadata":{"id":"Boqe4ST4hhKh"}},{"cell_type":"code","source":["llm_chain.invoke({'context': \"\", 'question': question})"],"metadata":{"id":"ERJqXJ8vhoAy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's see if adding context from GitHub issues helps the model give a more relevant answer:"],"metadata":{"id":"4klRcZ9BhtER"}},{"cell_type":"code","source":["rag_chain.invoke(question)"],"metadata":{"id":"dtQN8DBYhxZH"},"execution_count":null,"outputs":[]}]}