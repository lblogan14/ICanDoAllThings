{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 19016,
     "status": "ok",
     "timestamp": 1730123983766,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "UReDBKkaZwv2"
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain langchain-openai langgraph>0.2.27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0fBpSRKaDAq"
   },
   "source": [
    "# Build a Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGrMQiOra1hh"
   },
   "source": [
    "In this session, we will implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxSMr2t0bNB3"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1730123987739,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "N9yeYdJ6bOQQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "langchain_api_key = 'your_langchain_api_key_here'  # Replace with your actual LangChain API key\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_API_KEY'] = langchain_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN9trzTPbSY_"
   },
   "source": [
    "## Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4389,
     "status": "ok",
     "timestamp": 1730124587749,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "s6sD4DTqbPn0"
   },
   "outputs": [],
   "source": [
    "openai_api_key = 'your_openai_api_key_here'  # Replace with your actual OpenAI API key\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRebtfNxdjIL"
   },
   "source": [
    "First we use the model directly. `ChatModel`s are instances of LangChain \"Runnables\", which means they expose a standard interface for interacting with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 842,
     "status": "ok",
     "timestamp": 1730124661166,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "mdZ7Z75QdhEj",
    "outputId": "36c63e90-4d97-4f64-a390-4dff777d6e32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Bin! Nice to meet you. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 13, 'total_tokens': 28, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3f6e6ee8-e87a-49c8-8a08-dec366fe6b99-0', usage_metadata={'input_tokens': 13, 'output_tokens': 15, 'total_tokens': 28, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content='Hi! My name is Bin')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrCPOKYYd4YY"
   },
   "source": [
    "In this case, the model on its own does not have any concept of state. If we ask a followup question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1730124706364,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "K1lBwjuedz3r",
    "outputId": "0df22c2c-2c98-4f83-d669-cf2d8741b60c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm sorry, I do not have access to personal information such as your name.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 12, 'total_tokens': 29, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-97bf07a6-c1ce-4729-b21f-1149e849288b-0', usage_metadata={'input_tokens': 12, 'output_tokens': 17, 'total_tokens': 29, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content='What is my name?')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQKrEqy4eUuc"
   },
   "source": [
    "We can see that if does not take the previous conversation turn into context, and cannot answer the question.\n",
    "\n",
    "To get around this, we need to pass the entire conversation history into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 591,
     "status": "ok",
     "timestamp": 1730124885179,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "VH62EN06d-67",
    "outputId": "7ff3449c-328e-4530-8b77-6dbe137b658a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Bin.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 36, 'total_tokens': 41, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-57fb39e5-0e51-4c74-95e9-dad33b45c9ae-0', usage_metadata={'input_tokens': 36, 'output_tokens': 5, 'total_tokens': 41, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content='Hi! My name is Bin'),\n",
    "        AIMessage(content='Hi Bin! How can I help you today?'),\n",
    "        HumanMessage(content='What is my name?'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OrYCyy8evvY"
   },
   "source": [
    "and now we can see that we get a good response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF2A1aP1e0UH"
   },
   "source": [
    "## Message persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ua3PC1Kie9tq"
   },
   "source": [
    "**LangGraph** implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\n",
    "\n",
    "LangGraph comes with a simple in-memory checkpointer, which we use below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 145,
     "status": "ok",
     "timestamp": 1730125151635,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "tBN8F9xReqnl"
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state['messages'])\n",
    "    return {'messages': response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, 'model')\n",
    "workflow.add_node('model', call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thlIeTOYfs4N"
   },
   "source": [
    "We now need to create a `config` that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a `thread_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1730125232328,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "dtG5XI_-frwx"
   },
   "outputs": [],
   "source": [
    "config = {'configurable': {'thread_id': 'abc123'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2tb4B5ZgAFu"
   },
   "source": [
    "This enables us to support multiple conversation threads with a single application, a common requirement when our application has multiple users.\n",
    "\n",
    "Now we can invoke the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 740,
     "status": "ok",
     "timestamp": 1730125344487,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "uIHArJAhf_ey",
    "outputId": "81ec8feb-8b37-4b2d-973d-b8c155508626"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Bin! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! My name is Bin.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = app.invoke({'messages': input_messages}, config)\n",
    "output['messages'][-1].pretty_print() # contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 743,
     "status": "ok",
     "timestamp": 1730125424400,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "1QR6m6BngatC",
    "outputId": "b82f8575-4098-4346-9833-81d8a84901bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bin.\n"
     ]
    }
   ],
   "source": [
    "query2 = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query2)]\n",
    "\n",
    "output = app.invoke({'messages': input_messages}, config)\n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEL2k9dMgvVE"
   },
   "source": [
    "Our chatbot now remembers things about us. If we change the config to reference a different `thread_id`, we can see that it starts the conversation fresh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 905,
     "status": "ok",
     "timestamp": 1730125492931,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "NkmxJFOEguOE",
    "outputId": "f60704cc-1b5b-432f-92f2-5607c762b722"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, I do not have access to personal information such as your name.\n"
     ]
    }
   ],
   "source": [
    "config2 = {'configurable': {'thread_id': 'xyz456'}}\n",
    "\n",
    "input_messages = [HumanMessage(query2)]\n",
    "\n",
    "output = app.invoke({'messages': input_messages}, config2)\n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZRuWXDrhAhf"
   },
   "source": [
    "However, we can always go back to the original conversation (since we are persisting it in a database):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 706,
     "status": "ok",
     "timestamp": 1730125571880,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "Fkuksua_g-60",
    "outputId": "fb18d66b-098f-454c-b99d-b90a06049d67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bin.\n"
     ]
    }
   ],
   "source": [
    "output = app.invoke(\n",
    "    {'messages': input_messages},\n",
    "    config, # swithc back to our first configuration\n",
    ")\n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Slfe3OsFhW-O"
   },
   "source": [
    "For async support, update the `call_model` node to be an async function and use `.ainvoke` when invoking the application:\n",
    "```python\n",
    "# Async function for code\n",
    "async def call_model(state: MessagesState):\n",
    "    response = await model.ainvoke(state['messages'])\n",
    "    return {'messages': reponse}\n",
    "\n",
    "# Define graph as before:\n",
    "workflow = StateGraph(state_scheme=MessagesState)\n",
    "workflow.add_edge(START, 'model')\n",
    "workflow.add_node('model', call_model)\n",
    "app = workflow.complie(checkpointer=MemorySaver())\n",
    "\n",
    "# Async invocation\n",
    "output = await app.ainvoke({'messages': input_messages}, config)\n",
    "output['messages'][-1].pretty_print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SATMjwA4i82w"
   },
   "source": [
    "## Prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpkTky7Ti_6K"
   },
   "source": [
    "Prompt Templates help to turn raw user information into a format that the LLM can work with.\n",
    "\n",
    "To add in a system message, we will create a `ChatPromptTemplate`. We will utilize `MessagesPlaceholder` to pass all the messages in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 121,
     "status": "ok",
     "timestamp": 1730126153904,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "S4pVoCHYi_Ir"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            'You talk like a pirate. Answer all questions to the best of your ability.',\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name='messages')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBiufCiqjhCM"
   },
   "source": [
    "We can now update our application to incorporate this template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 145,
     "status": "ok",
     "timestamp": 1730126261540,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "5n4ggzW4jger"
   },
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    chain = prompt | model\n",
    "    response = chain.invoke(state)\n",
    "    return {'messages': response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, 'model')\n",
    "workflow.add_node('model', call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1730126341045,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "k1OY0Iywj6v9",
    "outputId": "3f981731-d27a-424a-8d5b-7a4f5bf1ba85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ahoy, Bin! What be ye needin' help with today, matey?\n"
     ]
    }
   ],
   "source": [
    "config3 = {'configurable': {'thread_id': 'xyz456'}}\n",
    "\n",
    "query3 = \"Hi! I'm Bin.\"\n",
    "\n",
    "input_messages = [HumanMessage(query3)]\n",
    "\n",
    "output = app.invoke({'messages': input_messages}, config3)\n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 835,
     "status": "ok",
     "timestamp": 1730126372590,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "DcIWGQjnkOBA",
    "outputId": "d887f7ba-0b4d-413f-a8f4-8afb77a33a74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yer name be Bin, me hearty! A fine name for a swashbuckling pirate like yerself. Arrr!\n"
     ]
    }
   ],
   "source": [
    "query4 = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query4)]\n",
    "\n",
    "output = app.invoke({'messages': input_messages}, config3)\n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nV4Q7CWqkYkq"
   },
   "source": [
    "Let's now make our prompt more complicated. Let's assume that the prompt template now looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 118,
     "status": "ok",
     "timestamp": 1730126470173,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "vbTndgR9kVsL"
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\n",
    "         'You are a helpful assistant. Answer all questions to the best of your ability in {language}.'),\n",
    "        MessagesPlaceholder(variable_name='messages'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkSsF8S9kuA1"
   },
   "source": [
    "We have added a new `language` input to the input. Our application has two parameters now -- the input `messages` and `language`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 125,
     "status": "ok",
     "timestamp": 1730126623988,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "A7ReqWWiktsW"
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    chain = prompt | model\n",
    "    response = chain.invoke(state)\n",
    "    return {'messages': [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, 'model')\n",
    "workflow.add_node('model', call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 900,
     "status": "ok",
     "timestamp": 1730126737628,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "IHk2Fb4vlTPm",
    "outputId": "246859ea-d13f-4e20-dbce-59d7b25ea621"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Â¡Hola, Bin! Â¿En quÃ© puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "config = {'configurable': {'thread_id': 'acb456'}}\n",
    "query5 = \"Hi! I'm Bin.\"\n",
    "language = 'Spanish'\n",
    "\n",
    "input_messages = [HumanMessage(query5)]\n",
    "\n",
    "output = app.invoke(\n",
    "    {'messages': input_messages,\n",
    "     'language': language},\n",
    "    config,\n",
    ")\n",
    "\n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8z2E5qilxtj"
   },
   "source": [
    "Note that the entire state is persisted, so we can omit parameters like `language` if no changes are desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 778,
     "status": "ok",
     "timestamp": 1730126780180,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "S43Y8I3vl3P_",
    "outputId": "7cd36d39-06c5-4f65-855a-c795651002c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Tu nombre es Bin.\n"
     ]
    }
   ],
   "source": [
    "query6 = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query6)]\n",
    "\n",
    "output = app.invoke({'messages': input_messages}, config)\n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1Q7oANHl_pu"
   },
   "source": [
    "## Managing conversation history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrW_OiVBmCQH"
   },
   "source": [
    "If we leave the conversation history unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages we are passing in.\n",
    "\n",
    "**Importantly, we will want to do this BEFORE the prompt template but AFTER we load previous messages from Message Histor.**\n",
    "\n",
    "In this case, we will use the `trim_messages` helper to reduce how many messages we are sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1730128664673,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "QYQKgPURmBmq",
    "outputId": "46c1b849-735a-4f7b-f2ba-380b14f488a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy='last',\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on='human',\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKWqwnD-sEmB"
   },
   "source": [
    "To user it in our chain, we just need to run the trimmer before we pass the messages input to our prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1730128669040,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "CfQnPwoJr5Sk"
   },
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "def call_model(state: State):\n",
    "    chain = prompt | model\n",
    "    trimmed_messages = trimmer.invoke(state['messages'])\n",
    "    response = chain.invoke(\n",
    "        {'messages': trimmed_messages,\n",
    "         'language': state['language']},\n",
    "    )\n",
    "    return {'messages': [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, 'model')\n",
    "workflow.add_node('model', call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZLM0bR6sli-"
   },
   "source": [
    "If we try asking the model our name, it will NOT know it since we trimmed that part of the chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 871,
     "status": "ok",
     "timestamp": 1730128600593,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "yyw6Hw5NskKz",
    "outputId": "6470df76-5f20-4f41-ee2f-dab6152294d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, I don't have access to personal information. How may I assist you today?\n"
     ]
    }
   ],
   "source": [
    "config = {'configurable': {'thread_id': 'acb456'}}\n",
    "query6 = \"what is my name?\"\n",
    "language = 'English'\n",
    "\n",
    "input_messages = messages + [HumanMessage(query6)]\n",
    "\n",
    "output = app.invoke(\n",
    "    {'messages': input_messages,\n",
    "     'language': language},\n",
    "    config,\n",
    ")\n",
    "\n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBic2ieRs3C4"
   },
   "source": [
    "But if we ask about information that is within the last few messages, it remembers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 613,
     "status": "ok",
     "timestamp": 1730128676770,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "ugMpa8KEs6hy",
    "outputId": "44855fc8-23b4-4c0d-c82d-e0747094b028"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You asked \"what's 2 + 2?\"\n"
     ]
    }
   ],
   "source": [
    "config = {'configurable': {'thread_id': 'acb456'}}\n",
    "query6 = \"What math problem did I ask?\"\n",
    "language = 'English'\n",
    "\n",
    "input_messages = messages + [HumanMessage(query6)]\n",
    "\n",
    "output = app.invoke(\n",
    "    {'messages': input_messages,\n",
    "     'language': language},\n",
    "    config,\n",
    ")\n",
    "\n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McmpsNu8tXJL"
   },
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJAreILdtaVS"
   },
   "source": [
    "LLMs can sometimes take a while to respond, and so in order to improve user experience one thing that most applications do is stream back each token as it is generated. This allows the users to see progress.\n",
    "\n",
    "By default, `.stream` in our LangGraph application streams application steps -- in this case, the single step of the model response. Setting `stream_mode=\"messages\"` allows use to stream output tokens instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1003,
     "status": "ok",
     "timestamp": 1730128956942,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "-ybOrJ9CtYff",
    "outputId": "7426c1a8-f2c1-4df3-d94d-68fddab94596"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Sure|,| Bin|!| Here|'s| a| joke| for| you|:\n",
      "\n",
      "|Why| couldn|'t| the| bicycle| find| its| way| home|?\n",
      "\n",
      "|Because| it| lost| its| bearings|!| ðŸ˜„||"
     ]
    }
   ],
   "source": [
    "config = {'configurable': {'thread_id': 'acb789'}}\n",
    "query = \"Hi! I'm Bin. Tell me a joke.\"\n",
    "language = 'English'\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "for chunk, metadata in app.stream(\n",
    "    {'messages': input_messages, 'language': language},\n",
    "    config,\n",
    "    stream_mode='messages',\n",
    "):\n",
    "    if isinstance(chunk, AIMessage): # filter to just model responses\n",
    "        print(chunk.content, end='|')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNdqxip6oXGar6xtnN08j4Q",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
