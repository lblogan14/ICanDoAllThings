{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNMziVJEXpVxWb4gghbX2CV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Getting Started with Chat Templates for Text LLMs"],"metadata":{"id":"hX_ISYPcRLqj"}},{"cell_type":"markdown","source":["**Chat templates** are part of the tokenizer for text-only LLMs or processor for multimodal LLMs. They specify how to convert conversations, represented as lists of messages, into a single tokenizable string in the format that the model expects.\n","\n","To start with, we use `mistralai/Mistral-7B-Instruct-v0.1` model as a example:"],"metadata":{"id":"yfeQizcOjU-A"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xK3EsyOyRID0"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")"]},{"cell_type":"code","source":["# chat template\n","chat = [\n","    {'role': 'user', 'content': 'Hellow, how are you?'},\n","    {'role': 'assistant', 'content': \"I'm doing great. How can I help you today?\"},\n","    {'role': 'user', 'content': \"I'd liek to show off how chat templates work!\"}\n","]\n","\n","tokenizer.apply_chat_template(chat, tokenize=False)"],"metadata":{"id":"J2J2q2vSmvyC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note how the tokenizer has added to the control tokens `[INST]` and `[/INST]` to indicate the start and end of user messages (but not assistant messages!), and the entire chat is condensed into a single string.\n","\n","If we set `tokenize=True`, the string will also be tokenized for us:"],"metadata":{"id":"rFNZZZPrwPEX"}},{"cell_type":"code","source":["tokenizer.apply_chat_template(chat, tokenize=True)"],"metadata":{"id":"X6k2EhpHwmSg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we swap in the `HuggingFaceH4/zephyr-7b-beta` model instead:"],"metadata":{"id":"zriY0gL4wtUu"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")"],"metadata":{"id":"sptIw75rwyy2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# chat template\n","chat = [\n","    {'role': 'user', 'content': 'Hellow, how are you?'},\n","    {'role': 'assistant', 'content': \"I'm doing great. How can I help you today?\"},\n","    {'role': 'user', 'content': \"I'd liek to show off how chat templates work!\"}\n","]\n","\n","tokenizer.apply_chat_template(chat, tokenize=False)"],"metadata":{"id":"ccqIWFV3w27c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Both Zephyr and Mistral-Instruct were fine-tuned from the same base model, `Mistral-7B-v0.1`. However, they were trained with totally diferent chat formats. Without chat templates, we would have to write manual formatting code for each model."],"metadata":{"id":"mdBcxxi9w3bw"}},{"cell_type":"markdown","source":["## Chat templates"],"metadata":{"id":"8S7Shyiu3P7j"}},{"cell_type":"markdown","source":["After building the chat templates, we just need to pass it to the `apply_chat_template()` method. When using chat templates as input for model generation, it is also a good idea to use `add_generation_prompt=True` to add a generation prompt."],"metadata":{"id":"q1XaKwhu3XVe"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","checkpoint = 'HuggingFaceH4/zephyr-7b-beta'\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map='auto')"],"metadata":{"id":"g8IFzAOy3PJx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["messages = [\n","    {\n","        'role': 'system',\n","        'content': 'You are a friendly chatbot who always responds in the style of a pirate'\n","    },\n","    {\n","        'role': 'user',\n","        'content': 'How many helicopters can a human eat in one sitting?'\n","    }\n","]\n","\n","tokenized_chat = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize=True,\n","    add_generation_prompt=True,\n","    return_tensors='pt'\n",")\n","print(tokenizer.decode(tokenized_chat[0]))"],"metadata":{"id":"mNFdd0t95Ef7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that our input is formatted correctly for Zephyr, we can use the model to generate a reponse to the user's question:"],"metadata":{"id":"VrmMA0AX5i0u"}},{"cell_type":"code","source":["outputs = model.generate(tokenized_chat, max_new_tokens=128)\n","print(tokenizer.decode(outputs[0]))"],"metadata":{"id":"xw8mC2Zq5npT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pipeline for Chat templates"],"metadata":{"id":"p5fztgkh5tuK"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","pipe = pipeline('text-generation', 'HuggingFaceH4/zephyr-7b-beta')"],"metadata":{"id":"jhLAIpww5voW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["messages = [\n","    {\n","        \"role\": \"system\",\n","        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n","    },\n","    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n","]\n","\n","# print the assistant's response\n","print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])"],"metadata":{"id":"FxFuJsFY52TD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## \"Generation prompt\" in `add_generation_prompt`"],"metadata":{"id":"j6GhZnxM58Aj"}},{"cell_type":"markdown","source":["The `add_generation_prompt` argument in the `apply_chat_template` method tells the template to add tokens that indicate the start of a bot response."],"metadata":{"id":"LEPASZ6K5-8J"}},{"cell_type":"code","source":["messages = [\n","    {\"role\": \"user\", \"content\": \"Hi there!\"},\n","    {\"role\": \"assistant\", \"content\": \"Nice to meet you!\"},\n","    {\"role\": \"user\", \"content\": \"Can I ask a question?\"}\n","]"],"metadata":{"id":"LSnglKsG596v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# If setting `add_generation_prompt=False`\n","tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)"],"metadata":{"id":"P5v1Ni1L6JA4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# If setting `add_generation_prompt=True`\n","tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"],"metadata":{"id":"N4wUtLL06RRc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This time, we have added he tokens that indicate the start of a bot response. This ensures that when the model generates text, it will write a bot response instead of doing something unexpected, like continuing the user's message.\n","Because chat models are still lanuguage models and they are trained to continue text, that's why we need to guide them with appropriate control tokens.\n","\n","Not all models require generation prompts. Some models, like LLaMA, do not have any special token before bot responses. In these cases, the `add_generation_prompt` argument will have no effect."],"metadata":{"id":"uDtTsxkP6acg"}},{"cell_type":"markdown","source":["## `continue_final_message`"],"metadata":{"id":"zLX8fekr69dL"}},{"cell_type":"markdown","source":["When passing a list of messages, we can choose to format the chat so the model will continue the final message in the chat instead of starting a new one. This is done by removing any end-of-sequence tokens that indicate the end of the final message, so that the model will simply extend the final message when it begins to generate text. This is useful for \"prefilling\" the model's response."],"metadata":{"id":"3EeW6hDc74WO"}},{"cell_type":"code","source":["chat = [\n","    {\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"},\n","    {\"role\": \"assistant\", \"content\": '{\"name\": \"'}, # partial response\n","]\n","\n","formatted_chat = tokenizer.apply_chat_template(\n","    chat,\n","    tokenize=True,\n","    return_dict=True,\n","    continue_final_message=True,\n",")\n","model.generate(**formatted_chat)"],"metadata":{"id":"CN3PIHrd685g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model will generate text that continues the JSON string, rather than starting a new messag, which can be very useful for improving the accuracy of the model's instruction-following.\n","\n","`add_generation_prompt` and `continue_final_message` cannot be used at the same time."],"metadata":{"id":"oGV_g4pI8yiP"}},{"cell_type":"markdown","source":["## Can I use chat templates in training?"],"metadata":{"id":"BoQX1j4P9FhB"}},{"cell_type":"markdown","source":["It is recommended that we apply the chat template as a preprocesing step for our dataset. Then, we can continue language model training task.\n","\n","When training, we shoud set `add_genenration_prompt=False`, because the added tokens to prompt an assistant response will not be helpful during training."],"metadata":{"id":"y_B0GlYk9MVd"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","from datasets import Dataset\n","\n","tokenizer = AutoTokenizer.from_pretrained('HuggingFaceH4/zephyr-7b-beta')"],"metadata":{"id":"riC9Egqs9HSK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chat1 = [\n","    {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n","    {\"role\": \"assistant\", \"content\": \"The sun.\"}\n","]\n","chat2 = [\n","    {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n","    {\"role\": \"assistant\", \"content\": \"A bacterium.\"}\n","]\n","\n","dataset = Dataset.from_dict(\n","    {'chat': [chat1, chat2]}\n",")\n","dataset = dataset.map(\n","    lambda x: {'formatted_chat': tokenizer.apply_chat_template(\n","        x['chat'],\n","        tokenize=False,\n","        add_generation_prompt=False\n","    )}\n",")\n","\n","print(dataset['formatted_chat'][0])"],"metadata":{"id":"WXwBgYrJ95Pm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From here we can continue training like we would with a standard language modeling task, using the `formatted_chat` column.\n","\n","By default, some tokenizers add special tokens like `<bos>` and `<eos>` to text they tokenize. Chat templates should already include all the special tokens they need, and so additional special tokens will often be incorrect or duplicated, which will hurt model performance.\n","\n","Therefore, if we format text with `apply_chat_template(tokenize=False)`, we should set the argument `add_special_tokens=False` when we tokenize that text later. If we use `apply_chat_template(tokenize=True)`, we do not need to worry about this."],"metadata":{"id":"RpvxQI3S-REM"}},{"cell_type":"markdown","source":["# Multimodal Chat Templates for Vision and Audio LLMs"],"metadata":{"id":"l4jdCpVh-04F"}},{"cell_type":"markdown","source":["Multimodal models provide richer, more interactive experiences, and understanding how to effectively combine these inputs with our templates is the key."],"metadata":{"id":"vTi4LMLcA529"}},{"cell_type":"markdown","source":["## Image inputs"],"metadata":{"id":"sWS7_tZqBIbn"}},{"cell_type":"markdown","source":["For models such as **LLaVA**, the prompts can be formatted as below. The `content` now is a list containing either a text or an image `type`."],"metadata":{"id":"hDoOVMM_BLd4"}},{"cell_type":"code","source":["from transformers import AutoProcessor\n","\n","model_id = 'llava-hf/llava-onevision-qwen2-0.5b-ov-hf'\n","processor = AutoProcessor.from_pretrained(model_id)"],"metadata":{"id":"HTn-6I_L-zqI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["messages = [\n","    {\n","        'role': 'system',\n","        'content': [{\n","                'type': 'text',\n","                'text': \"You are a friendly chatbot who always responds in the style of a pirate.\"\n","        }]\n","    },\n","    {\n","        'role': 'user',\n","        'content': [\n","            {'type': 'image'},\n","            {'type': 'text', 'text': \"What are these?\"}\n","        ]\n","    }\n","]\n","\n","formatted_prompt = processor.apply_chat_template(\n","    messages,\n","    add_generation_prompt=True,\n","    tokenize=False\n",")\n","print(formatted_prompt)"],"metadata":{"id":"x36W9X67BhBg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Image paths or URLs"],"metadata":{"id":"Fy_xNEQpCZXG"}},{"cell_type":"markdown","source":["To incorporate images into our chat templates, we can pass them as file paths or URLs."],"metadata":{"id":"W9YGITEQCcIi"}},{"cell_type":"code","source":["from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n","\n","model_id = \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\"\n","model = LlavaOnevisionForConditionalGeneration.from_pretrained(model_id)\n","processor = AutoProcessor.from_pretrained(model_id)"],"metadata":{"id":"oi1n5PJWCbJe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["messages = [\n","    {\n","        'role': 'system',\n","        'content': [{\n","            'type': 'text',\n","            'text': \"You are a friendly chatbot who always responds in the style of a pirate.\"\n","        }]\n","    },\n","    {\n","        'role': 'user',\n","        'content': [\n","            {'type': 'image', 'url': 'http://images.cocodataset.org/val2017/000000039769.jpg'},\n","            {'type': 'text', 'text': 'What are these?'}\n","        ]\n","    }\n","]\n","\n","processed_chat = processor.apply_chat_template(\n","    messages,\n","    add_generation_prompt=True,\n","    tokenize=True,\n","    return_dict='True',\n","    return_tensors='pt'\n",")\n","print(processed_chat.keys())"],"metadata":{"id":"s0dDI4T4CzEU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This dictionary is ready to be further passed into the `model.generate()` to generate text."],"metadata":{"id":"E55kYSLHEGxY"}},{"cell_type":"code","source":["model.generate(**processed_chat)"],"metadata":{"id":"5pHTp_x9EOZf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Video inputs"],"metadata":{"id":"h6ndkx0nERDI"}},{"cell_type":"markdown","source":["#### Sampling with fixed number of frames"],"metadata":{"id":"9-EKm5RFEW0P"}},{"cell_type":"markdown","source":["The `num_frames` parameter is passed to the `apply_chat_template()` method and controls how many frames to sample uniformly from the video.\n","\n","Each model checkpoint has a maximum frame count it was trained with, and exceeding this limit can significantly impact generation quality.\n","\n","We also have the option to choose a specific framework to load the video. In this example, we use `decord`."],"metadata":{"id":"7VjZerIDEbWG"}},{"cell_type":"code","source":["from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n","\n","model_id = \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\"\n","model = LlavaOnevisionForConditionalGeneration.from_pretrained(model_id)\n","processor = AutoProcessor.from_pretrained(model_id)"],"metadata":{"id":"qiRqQqCjESeE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["messages = [\n","    {\n","        \"role\": \"system\",\n","        \"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n","    },\n","    {\n","      \"role\": \"user\",\n","      \"content\": [\n","            {\"type\": \"video\", \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\"},\n","            {\"type\": \"text\", \"text\": \"What do you see in this video?\"},\n","        ],\n","    },\n","]\n","\n","processed_chat = processor.apply_chat_template(\n","    messages,\n","    add_generation_prompt=True,\n","    tokenize=True,\n","    return_dict=True,\n","    return_tensors='pt',\n","    num_frames=32,\n","    video_load_backend='decord',\n",")\n","print(processed_chat.keys())"],"metadata":{"id":"UHs2bOFxE-uZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.generate(**processed_chat)"],"metadata":{"id":"rqfL8eTdFQ8K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sampling with FPS"],"metadata":{"id":"CjA9kmMwFSSO"}},{"cell_type":"markdown","source":["When working with long videos, we want to sample more frames for better representation. Instead of a fixed number of frames, we can specify `video_fps`, which determines how many frames per second to extract. For example, if a video is **10 seconds long** and we set `video_fps=2`, the model will sample **20 frames** (2 per second, uniformly spaced)."],"metadata":{"id":"9V2NKeasFURr"}},{"cell_type":"code","source":["# keep the same chat messages as above\n","processed_chat = processor.apply_chat_template(\n","    messages,\n","    add_generation_prompt=True,\n","    tokenize=True,\n","    return_dict=True,\n","    video_fps=32,\n","    video_load_backend='decord',\n",")\n","print(processed_chat.keys())"],"metadata":{"id":"rZaDtwh7FTox"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.generate(**processed_chat)"],"metadata":{"id":"KkE5M_rCFyHZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Custom frame sampling with a function"],"metadata":{"id":"GaKxTqDMFzs6"}},{"cell_type":"markdown","source":["Not all models sample frames **uniformly** - some require more complex logic to determine which frames to use.\n","\n","We can **customize** frame selection:\n","* use the `sample_indices_fn` to pass a **callable function** for sampling\n","* if provided, this function **overrides** standard `num_frames` and `fps` methods\n","* it receives alll the arguments passed to `load_video` and must return **valid frame indices** to sample.\n","\n","We should use `sample_indices_fn` when\n","* if we need to custom sampling strategy (e.g., **adaptive frame selection** instead of uniform sampling)\n","* if our model prioritizes **key momments** in a video rather than evenly spaced frames"],"metadata":{"id":"73jcpxYIF2bt"}},{"cell_type":"code","source":["# example\n","def sample_indices_fn(metadata, **kwargs):\n","    # samples only the first and the second frame\n","    return [0, 1]\n","\n","processed_chat = processor.apply_chat_template(\n","    messages,\n","    add_generation_prompt=True,\n","    tokenize=True,\n","    return_dict=True,\n","    return_tensors='pt',\n","    sample_indices_fn=sample_indices\n","    video_load_backend='decord',\n",")\n","print(processed_chat.keys())"],"metadata":{"id":"CvkhqBNTF10O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["By using `sample_indices_fn`, we have **full control** over frame selection, making our model **more adaptable** to different video scenarios."],"metadata":{"id":"WzL8qTLlGp7r"}},{"cell_type":"code","source":["model.generate(**processed_chat)"],"metadata":{"id":"G1meH83eGnqX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### List of image frames as video"],"metadata":{"id":"zb5l9XruG1My"}},{"cell_type":"markdown","source":["We can pass a list of image file paths, and the processor will automatically concatenate them into a video. We need to make sure that all images have the same size, as they are assumed to be from the same video."],"metadata":{"id":"PrJ38bMvIrup"}},{"cell_type":"code","source":["frames_paths = [\"/path/to/frame0.png\", \"/path/to/frame5.png\", \"/path/to/frame10.png\"]\n","messages = [\n","    {\n","        \"role\": \"system\",\n","        \"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n","    },\n","    {\n","      \"role\": \"user\",\n","      \"content\": [\n","            {\"type\": \"video\", \"path\": frames_paths},\n","            {\"type\": \"text\", \"text\": \"What do you see in this video?\"},\n","        ],\n","    },\n","]\n","\n","processed_chat = processor.apply_chat_template(\n","    messages,\n","    add_generation_prompt=True,\n","    tokenize=True,\n","    return_dict=True,\n",")\n","print(processed_chat.keys())"],"metadata":{"id":"tjcnuptSG3L2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Multimodal conversational pipeline"],"metadata":{"id":"xKTLY2lXI2lj"}},{"cell_type":"code","source":["# OpenAI conversation format\n","messages = [\n","    {\n","        \"role\": \"user\",\n","        \"content\": [\n","            {\n","                \"type\": \"text\",\n","                \"text\": \"What is in this image?\",\n","            },\n","            {\n","                \"type\": \"image_url\",\n","                \"image_url\": {\"url\": f\"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n","            },\n","        ],\n","    }\n","]"],"metadata":{"id":"gItnt-WuI4kC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Best practices for multimodal template configuration"],"metadata":{"id":"0Wxy2tScI_1n"}},{"cell_type":"markdown","source":["To add a custom chat template for our multimodal LLM, we can create our template using [**Jinja**](https://jinja.palletsprojects.com/en/stable/templates/) and set it with `processor.chat_template`.\n","\n","In some cases, we may want our template to handle a list of content from multiple modalities, while still supporting a plain string gfor text-only inference. Here is an example of how we can achieve that, using the `Llama-Vision` chat template:\n","```python\n","{% for message in messages %}\n","{% if loop.index0 == 0 %}{{ bos_token }}{% endif %}\n","{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' }}\n","{% if message['content'] is string %}\n","{{ message['content'] }}\n","{% else %}\n","{% for content in message['content'] %}\n","{% if content['type'] == 'image' %}\n","{{ '<|image|>' }}\n","{% elif content['type'] == 'text' %}\n","{{ content['text'] }}\n","{% endif %}\n","{% endfor %}\n","{{ '<|eot_id|>' }}\n","{% endfor %}\n","{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\n","```"],"metadata":{"id":"HI8ixdUKJEPf"}},{"cell_type":"markdown","source":["# Expanding Chat Templates with Tools and Documents"],"metadata":{"id":"UEMXIJK4Pxcn"}},{"cell_type":"markdown","source":["In addition to the required `messages` argument we need to pass to `apply_chat_template`, we can pass any keyword argument to `apply_chat_template` and it will be accessible inside the template.\n","\n","There are some common use-cases, such as passing tools for function calling, or documents for retrieval-augmented generation."],"metadata":{"id":"2Qatk7TbqCz9"}},{"cell_type":"markdown","source":["## Tool use / function calling"],"metadata":{"id":"h87n8uH1qgRa"}},{"cell_type":"markdown","source":["\"Tool use\" LLMs can choose to call functions as external tools before generating an answer.. When passing tools to a tool-use model, we can simply pass a list of functions to the `tools` argument:\n","\n","```python\n","import datetime\n","\n","def current_time():\n","    \"\"\"Get the current local time as a string\"\"\"\n","    return str(datetime.now())\n","\n","def multiply(a: float, b: float):\n","    \"\"\"A function that multiplies two numbers\n","    \n","    Args:\n","        a: the first number to multiply\n","        b: the second number to multiply\n","    \"\"\"\n","    return a * b\n","\n","\n","# define tools as a list\n","tools = [current_time, multiply]\n","\n","model_input = tokenizer.apply_chat_template(\n","    messages,\n","    tools=tools\n",")\n","```\n","\n","\n","For the tools to work correctly, we should write our functions in the format above, so that they can be parsed correctly as tool:\n","* the function should have a descriptive name\n","* every argument must have a type hint\n","* the function must have a docstring in the standard **Google style** (in other words, an initial function description followed by an `Args:` block that describes the arguments, unless the function does not have any arguments.)\n","* do not include types in the `Args:` block. Type hints should go in the function header instead.\n","* the function can have a return type and a `Returns:` block in the docstring. However these are optional because most tool-use models ignore them."],"metadata":{"id":"hRtVCK8Rqij5"}},{"cell_type":"markdown","source":["### Complete tool use example"],"metadata":{"id":"RjqpF6kwrSQG"}},{"cell_type":"markdown","source":["We will use 8B `Hermes-2-Pro`. If we have more memory, we can try a larger model like **Command-R** or **Mixtral-8x22B**."],"metadata":{"id":"4ZAdDBSKr8P3"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","checkpoint = 'NousResearch/Hermes-2-Pro-Llama-3-8B'\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForCausalLM.from_pretrained(\n","    checkpoint,\n","    torch_dtype=torch.float16,\n","    device_map='auto',\n",")"],"metadata":{"id":"46DbHFI8JCyC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define some tools:"],"metadata":{"id":"zF8lB2V7sXe8"}},{"cell_type":"code","source":["def get_current_temperature(location: str, unit: str) -> flloat:\n","    \"\"\"Get the current temperature at a location\n","\n","    Args:\n","        location: The location to get the temperature for, in the format \"City, Country\"\n","        unit: The unit to return the temperature in. (Choices: ['celsius', 'fahrenheit'])\n","\n","    Returns:\n","        The current temperature at the specified location in the specified units\n","    \"\"\"\n","    return 22. # dummy returns\n","\n","\n","def get_current_wind_speed(location: str) -> float:\n","    \"\"\"Get the current wind speed in km/hr at a given location.\n","\n","    Args:\n","        location: The location to get the wind speed for, in the format \"City, Country\"\n","\n","    Returns:\n","        The current wind speed in km/hr at the specified location\n","    \"\"\"\n","    return 6. # dummy returns\n","\n","\n","tools = [get_current_temperature, get_current_wind_speed]"],"metadata":{"id":"u9BjnMb2sYhv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can set up our chat template:"],"metadata":{"id":"mdWChzYxtJ8C"}},{"cell_type":"code","source":["messages = [\n","    {\n","        'role': 'system',\n","        'content': \"You are a bot that responds to weather queries. You should reply with the unit used in the queried location.\"\n","    },\n","    {\n","        'role': 'user',\n","        'content': \"Hey, what's the temperature in Paris right now?\"\n","    }\n","]"],"metadata":{"id":"12bKbNhFtM1_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = tokenizer.apply_chat_template(\n","    messages,\n","    tools=tools,\n","    add_generation_prompt=True,\n","    return_dict=True,\n","    return_tensors='pt'\n",")\n","\n","inputs = {k: v.to(model.device) for k, v in inputs.items()}\n","outputs = model.generate(\n","    **inputs,\n","    max_new_tokens=128\n",")\n","print(tokenizer.decode(outputs[0][len(inputs['input_ids'][0]): ]))"],"metadata":{"id":"Z4Xy-OcgtYgM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model has called the function with valid arguments, in the format requested by the function docstring."],"metadata":{"id":"JoP0ghxQt35B"}},{"cell_type":"code","source":["# complete chat history\n","print(tokenizer.decode(outputs[0]))"],"metadata":{"id":"hKgWHLg4tw1T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can append the model's tool call to the conversation:"],"metadata":{"id":"BSJpT0bCuWaq"}},{"cell_type":"code","source":["tool_call = {\n","    \"name\": \"get_current_temperature\",\n","    \"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}\n","}\n","messages.append(\n","    {\n","        \"role\": \"assistant\",\n","        \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]\n","    }\n",")"],"metadata":{"id":"tzdGAYuOuY2R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the OpenAI API, the `tool_call` is a JSON string instead of a dictionary.\n","\n","Now that we have added the tool call to the conversation, we can call the function and append the result to the conversation."],"metadata":{"id":"ep5aBLhYugzQ"}},{"cell_type":"code","source":["messages.append(\n","    {\n","        \"role\": \"tool\",\n","        \"name\": \"get_current_temperature\",\n","        \"content\": \"22.0\" # dummy returns\n","    }\n",")"],"metadata":{"id":"oYo1pPb7u0kA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Some model architectures, notebly Mistral/Mixtral, also require a `tool_call_id`, which i 9 randomly-generated alphanumeric characters, and assigned to the `id` key of the tool call dictionary. The same key should also be assigned to the `tool_call_id` key of the tool response dictionary below, so that tool calls can be matched to tool responses.\n","\n","For Mistral/Mixtral model, the code above should be:"],"metadata":{"id":"E2L9EjVGu6fI"}},{"cell_type":"code","source":["tool_call_id = \"9Ae3bDc2F\"  # Random ID, 9 alphanumeric characters\n","tool_call = {\n","    \"name\": \"get_current_temperature\",\n","    \"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}\n","}\n","messages.append(\n","    {\n","        \"role\": \"assistant\",\n","        \"tool_calls\": [{\"type\": \"function\", \"id\": tool_call_id, \"function\": tool_call}]\n","    }\n",")\n","\n","messages.append(\n","    {\n","        \"role\": \"tool\",\n","        \"tool_call_id\": tool_call_id,\n","        \"name\": \"get_current_temperature\",\n","        \"content\": \"22.0\"\n","    }\n",")"],"metadata":{"id":"MUMva_jYvTXO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, let the assistant read the function outputs and continue chatting with the user:"],"metadata":{"id":"FXRN8znNviOH"}},{"cell_type":"code","source":["inputs = tokenizer.apply_chat_template(\n","    messages,\n","    tools=tools,\n","    add_generation_prompt=True,\n","    return_dict=True,\n","    return_tensors='pt'\n",")\n","inputs = {k: v.to(model.device) for k, v in inputs.items()}\n","\n","outputs = model.generate(**inputs, max_new_tokens=128)\n","print(tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):]))"],"metadata":{"id":"GR_UyAqQvl88"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.decode(out[0]))"],"metadata":{"id":"jLF9kzyOv3Wy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Understanding the schemas"],"metadata":{"id":"tXLjnJgpv6m5"}},{"cell_type":"markdown","source":["Each function we pass to the `tools` argument of `apply_chat_template` is converted into a [**JSON schema**](https://json-schema.org/learn/getting-started-step-by-step).\n","\n","These schemas are passed to the model chat template. The tool-use models do not see our functions directly, and they never see the actual code inside them. What they care about is the function **definitions** and the **arguments** they need to pass to them - they care about what the tools do and how to use them, not how they work!\n","\n","Generating JSON schemas to pass to the template should be automatic and invisible as long as our functions follow the specification above. If we encounter any problems or we want more control over the conversion, we can handle the conversion manually.\n","\n","Example of a manual schema conversion:"],"metadata":{"id":"dLGUS0UlxzZ7"}},{"cell_type":"code","source":["from transformers.utils import get_json_schema\n","\n","def multiply(a: float, b: float):\n","    \"\"\"A function that multiplies two numbers\n","\n","    Args:\n","    a: the first number to multiply\n","    b: the second number to multiply\n","    \"\"\"\n","    return a * b\n","\n","schema = get_json_schema(multiply)\n","schema"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gVLp8bpmv8fj","executionInfo":{"status":"ok","timestamp":1741203184357,"user_tz":360,"elapsed":16,"user":{"displayName":"Bin Liu","userId":"03585165976699804089"}},"outputId":"dd66c961-be57-4d62-c064-cfec9d68efb1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'type': 'function',\n"," 'function': {'name': 'multiply',\n","  'description': 'A function that multiplies two numbers',\n","  'parameters': {'type': 'object',\n","   'properties': {'a': {'type': 'number',\n","     'description': 'the first number to multiply'},\n","    'b': {'type': 'number', 'description': 'the second number to multiply'}},\n","   'required': ['a', 'b']}}}"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["We can edit these schemas, or even write them from scratch ourselves without using `get_json_schema` at all.\n","\n","JSON schemas can be passed directly to the `tools` argument of `apply_chat_template`.\n","\n","The more complex our schemas, the more likely the model is to get confused when dealing wih them!!! We need to have simple function signatures where possible, keeping arguments (and especially complex, nested arguments) to a minimum."],"metadata":{"id":"aXnUtN8o0BJK"}},{"cell_type":"code","source":["# A simple function that takes no arguments\n","current_time = {\n","  \"type\": \"function\",\n","  \"function\": {\n","    \"name\": \"current_time\",\n","    \"description\": \"Get the current local time as a string.\",\n","    \"parameters\": {\n","      'type': 'object',\n","      'properties': {}\n","    }\n","  }\n","}\n","\n","# A more complete function that takes two numerical arguments\n","multiply = {\n","  'type': 'function',\n","  'function': {\n","    'name': 'multiply',\n","    'description': 'A function that multiplies two numbers',\n","    'parameters': {\n","      'type': 'object',\n","      'properties': {\n","        'a': {\n","          'type': 'number',\n","          'description': 'The first number to multiply'\n","        },\n","        'b': {\n","          'type': 'number', 'description': 'The second number to multiply'\n","        }\n","      },\n","      'required': ['a', 'b']\n","    }\n","  }\n","}\n","\n","model_input = tokenizer.apply_chat_template(\n","    messages,\n","    tools = [current_time, multiply]\n",")"],"metadata":{"id":"wAXh9CUoyuSe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Retrieval-augmented generation"],"metadata":{"id":"PqOHjv-i0kCq"}},{"cell_type":"markdown","source":["**\"Retrieval-augmented generation\" (RAG)** LLMs can search a corpus of documents for information before responding to a query. This allows models to vastly expand their knowledge base beyond their limited context size. The template for RAG models should accept a `documents` arguments. This should be a list of documents, where each `\"document\"` is a single dict with `title` and `content` keys, both of which are strings. Because this format is much simpler than the JSON schemas used for tools, no helper functions are needed."],"metadata":{"id":"4ZDxDXVs0mNE"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","model_id = 'CohereForAI/c4ai-command-r-v01-4bit'\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    device_map='auto',\n",")\n","device = model.device"],"metadata":{"id":"vQTWf6Pt0lzI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1a. Define conversation input\n","conversation = [\n","    {'role': 'user', 'content': 'What has Man always dreamed of?'}\n","]\n","\n","# 1b. Define documents for RAG\n","documents = [\n","    {\n","        'title': 'The Moon: Our Age-Old Foe',\n","        'text': 'Man has dreamed of destroying the moon. In this essay, I shall...'\n","    },\n","    {\n","        \"title\": \"The Sun: Our Age-Old Friend\",\n","        \"text\": \"Although often underappreciated, the sun provides several notable benefits...\"\n","    }\n","]\n","\n","# 2. Tokenize conversation and documents using a RAG template, returning PyTorch tensors\n","input_ids = tokenizer.apply_chat_template(\n","    conversation=conversation,\n","    documents=documents,\n","    chat_template='rag',\n","    tokenize=True,\n","    add_generation_prompt=True,\n","    return_tensors='pt'\n",").to(device)\n","\n","# 3. Generate a response\n","gen_tokens = model.generate(\n","    input_ids,\n","    max_new_tokens=100,\n","    do_sample=True,\n","    temperature=0.3\n",")\n","\n","# 4. Decode and print the generated text along with generation prompt\n","gen_text = tokenizer.decode(gen_tokens[0])\n","gen_text"],"metadata":{"id":"Rr8kQB1f1UqY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To verify if a model supports the `documents` input, we can `print(tokenizer.chat_template` to see if the `documents` key is used anywhere."],"metadata":{"id":"AETF9QaH2LDL"}},{"cell_type":"markdown","source":["# Advanced Usage and Constomizing Our Chat Templates"],"metadata":{"id":"RYJ2YaOb2VfL"}},{"cell_type":"markdown","source":["## Mechanism behind chat templates"],"metadata":{"id":"0BTv52OE2cHD"}},{"cell_type":"markdown","source":["The chat template for a model is stored on the `tokenizer.chat_template`.\n","\n","For a simiplified version of `Zephyr` chat template,\n","```python\n","{%- for message in messages %}\n","    {{- '<|' + message['role'] + '|>\\n' }}\n","    {{- message['content'] + eos_token }}\n","{%- endfor %}\n","{%- if add_generation_prompt %}\n","    {{- '<|assistant|>\\n' }}\n","{%- endif %}\n","```\n","This is a [**Jinja template**](https://jinja.palletsprojects.com/en/3.1.x/templates/). Jinja is a templating language that allows use to write simple code that generates text. The code and syntax resembles Python. In pure Python, this template would look like this:\n","```python\n","for message in messages:\n","    print(f'<|{message[\"role\"]}|>')\n","    print(message['content'] + eos_token)\n","if add_generation_prompt:\n","    print('<|assistant|>')\n","```\n","\n","The template does three things:\n","* For each message, print the role enclosed in `<|` and `|>`, like `<|user|>` or `<|assistant|>`.\n","* Next, print the content of the message, followed by the eod-of-squence `eos_token` token.\n","* Finally, if `add_generation_prompt` is set, print the assistant token, so that the model knows to start generating an assistant response."],"metadata":{"id":"yxcZwiUz2iaL"}},{"cell_type":"markdown","source":["Jinja provides more flexible and more complex patterns. The following Jinja template can format inputs similarly to the way LLaMA formats them (note that the real LLaMA template includes handling for default system messages and slightly different system message handling in generate)\n","```python\n","{%- for message in messages %}\n","    {%- if message['role'] == 'user' %}\n","        {{- bos_token + '[INST] ' + message['content'] + ' [/INST]' }}\n","    {%- elif message['role'] == 'system' %}\n","        {{- '<<SYS>>\\\\n' + message['content'] + '\\\\n<</SYS>>\\\\n\\\\n' }}\n","    {%- elif message['role'] == 'assistant' %}\n","        {{- ' '  + message['content'] + ' ' + eos_token }}\n","    {%- endif %}\n","{%- endfor %}\n","```\n","This template adds specific tokens like `[INST]` and `[/INST]` based on the role of each message. User, assistant, and system messages are distinguishable to the model because of the tokens they are wrapped in."],"metadata":{"id":"8ER7h9Rq34cG"}},{"cell_type":"markdown","source":["## Creating chat template"],"metadata":{"id":"QV1GpxPh4jgo"}},{"cell_type":"markdown","source":["To create a chat template, we just write a jinja template and set `tokenizer.chat_template`. For example, we could take the LLaMA template above and add `\"[ASST]\"` and `\"[/ASST]\"` to assistant messages:\n","```python\n","{%- for message in messages %}\n","    {%- if message['role'] == 'user' %}\n","        {{- bos_token + '[INST] ' + message['content'].strip() + ' [/INST]' }}\n","    {%- elif message['role'] == 'system' %}\n","        {{- '<<SYS>>\\\\n' + message['content'].strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}\n","    {%- elif message['role'] == 'assistant' %}\n","        {{- '[ASST] '  + message['content'] + ' [/ASST]' + eos_token }}\n","    {%- endif %}\n","{%- endfor %}\n","```\n","The `tokenizer.chat_template` attribute will be saved in the `tokenizer_config.json` file:\n","```python\n","template = tokenizer.chat_template\n","template = template.replace(\"SYS\", \"SYSTEM\")  # Change the system token\n","tokenizer.chat_template = template  # Set the new template\n","tokenizer.push_to_hub(\"model_name\")  # Upload your new template to the Hub!\n","```"],"metadata":{"id":"4ip8yxsvdMs0"}},{"cell_type":"markdown","source":["## Models with multiple templates"],"metadata":{"id":"dN35BnvQeFTr"}},{"cell_type":"markdown","source":["Some models use different templates for different use cases. For example, they might use one template for normal chat and another for tool-use, or retrieval-augmented generation. In these cases, `tokenizer.chat_template` is a dictionary. This can cause some confusion, and where possible, we needto apply a single template for all use-cases.\n","\n","When a tokenizer has multiple templates, `tokenizer.chat_template` will be a `dict`, where each key is the name of a template. The `apply_chat_template` method has special handling for certain template names."],"metadata":{"id":"UC2bsxWYeLek"}},{"cell_type":"markdown","source":["## Choosing a template"],"metadata":{"id":"oEzvEGOkepWo"}},{"cell_type":"markdown","source":["When setting the template for a model that's already been trained for chat, we should ensure that the template exactly matches the message formatting that the model saw during training, or else we will probably experience performance degradation. This is true even if we are training the model further.\n","\n","If we are training a model from scratch, or fine-tuning a base language model for chat, on the other hand, we have a lot of freedom to choose an appropriate template. LLMs are smart enough to learn to handle lots of different input formats.\n","\n","One popular choice is the `ChatML` format:\n","```python\n","{% for message in messages %}\n","    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n","{% endfor %}\n","```\n","This also includes support for `generation_prompts`.\n","\n","If our model expects BOS or EOS tokens:\n","```python\n","{% if not add_generation_prompt is defined %}\n","    {% set add_generation_prompt = false %}\n","{% endif %}\n","{% for message in messages %}\n","    {{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}\n","{% if add_generation_prompt %}\n","    {{ '<|im_start|>assistant\\n' }}\n","{% endif %}\n","```"],"metadata":{"id":"mojLrg91exOh"}},{"cell_type":"markdown","source":["## Modifying chat templates"],"metadata":{"id":"MxFi_NOlgHRF"}},{"cell_type":"markdown","source":["Jinja templates in transformers are identical to Jinja templates elsewhere. The conversation history is accessible inside our template as a variable called `messages`."],"metadata":{"id":"n39D-ycn4hl7"}},{"cell_type":"markdown","source":["### Trimming whitespace"],"metadata":{"id":"4Yl_HiV95H4Z"}},{"cell_type":"markdown","source":["By default, Jinja will print any whitespace that comes before or after a block. This can be a problem for chat templates, which generally want to be very precise with whitespace. To avoid this, we strongly recommend writing our templates like this:\n","```python\n","{%- for message in messages %}\n","    {{- message['role'] + message['content'] }}\n","{%- endfor %}\n","```\n","rather than like this:\n","```python\n","{% for message in messages %}\n","    {{ message['role'] + message['content'] }}\n","{% endfor %}\n","```\n","Adding `-` will strip any whitespace that comes before the block."],"metadata":{"id":"TFnVOeHZ6FEc"}},{"cell_type":"markdown","source":["### Special variables"],"metadata":{"id":"wHQ-dFv893xy"}},{"cell_type":"markdown","source":["* `messages` contains the chat history as a list of message dicts.\n","* `tools` contains a list of tools in JSON schema format. Will be `None` or undefined if no tools are passed.\n","* `documents` contains a list of documents in the format `{\"title\": \"Title\", \"contents\": \"Contents\"}`, used for retrieval-augmented generation. Will be `None` or undefined if no documents are passed.\n","* `add_generation_prompt` is a `bool` that is `True` if the user has requested a generation prompt, and `False` otherwise. If this is set, our template should add the header for an assistant message to the end of the conversation. If our model does not have a specific header for assistant messages, we can ignore this flag.\n","* Special tokens like `bos_token` and `eos_token`."],"metadata":{"id":"1vRzAP5y97TY"}},{"cell_type":"markdown","source":["### Callable functions"],"metadata":{"id":"ZA5MIddhBkzV"}},{"cell_type":"markdown","source":["Inside our templates, we can call\n","* `raise_exception(msg)`: Raises a `TemplateException`. This is useful for debugging.\n","* `strftime_now(format_str)`: Equivalent to `datetime.now().strfime(format_str)`."],"metadata":{"id":"hkiZNdeEBo8j"}},{"cell_type":"markdown","source":["### Writing generation prompts"],"metadata":{"id":"PHvXvcD0CA8Z"}},{"cell_type":"markdown","source":["If our model expects a header for assistant messages, then our template must support adding the header when `add_generation_prompt` is set.\n","\n","Here is an example of a template that formats messages ChatML-style, with generation prompt support:\n","```python\n","{{- bos_token }}\n","{%- for message in messages %}\n","    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n","{%- endfor %}\n","{%- if add_generation_prompt %}\n","    {{- '<|im_start|>assistant\\n' }}\n","{%- endif %}\n","```\n","\n","The exact content of the assistant header will depend on our specific model, but it should always be the string that represents the start of an assistant message, so that if the user applies your template with `add_generation_prompt=True` and then generates text, the model will write an assistant response."],"metadata":{"id":"Ggf1qfD9CD86"}},{"cell_type":"markdown","source":["### Writing and debugging larger templates"],"metadata":{"id":"fegyrrhuCfMA"}},{"cell_type":"markdown","source":["Templates for new models and features like tool-use and RAG can be really long. We can save them in separate files and extract chat templates to a file:\n","```python\n","open(\"template.jinja\", \"w\").write(tokenizer.chat_template)\n","```\n","Or load the edited template back into the tokenizer:\n","```python\n","tokenizer.chat_template = open(\"template.jinja\").read()\n","```"],"metadata":{"id":"RgV9uqC-Cm92"}},{"cell_type":"markdown","source":["## Writing templates for tools"],"metadata":{"id":"rG_bbw7IC-Rr"}},{"cell_type":"markdown","source":["The whole point of chat templates is to allow code to be transferable across models, so deviating from the standard tools API means users will have to write custom code to use tools with our model.\n","\n","The following elements are the elements of the standard API."],"metadata":{"id":"lBUSNodRFI1q"}},{"cell_type":"markdown","source":["### Tool definitions"],"metadata":{"id":"XeA1dv1wFeDv"}},{"cell_type":"markdown","source":["The template should expect that the variable `tools` will either be null, or is a list of JSON schema dicts.\n","\n","Example of JSON schema:\n","```yaml\n","{\n","  \"type\": \"function\",\n","  \"function\": {\n","    \"name\": \"multiply\",\n","    \"description\": \"A function that multiplies two numbers\",\n","    \"parameters\": {\n","      \"type\": \"object\",\n","      \"properties\": {\n","        \"a\": {\n","          \"type\": \"number\",\n","          \"description\": \"The first number to multiply\"\n","        },\n","        \"b\": {\n","          \"type\": \"number\",\n","          \"description\": \"The second number to multiply\"\n","        }\n","      },\n","      \"required\": [\"a\", \"b\"]\n","    }\n","  }\n","}\n","```\n","and then the following code is used to handle tools in our chat template:\n","```python\n","{%- if tools %}\n","    {%- for tool in tools %}\n","        {{- '<tool>' + tool['function']['name'] + '\\n' }}\n","        {%- for argument in tool['function']['parameters']['properties'] %}\n","            {{- argument + ': ' + tool['function']['parameters']['properties'][argument]['description'] + '\\n' }}\n","        {%- endfor %}\n","        {{- '\\n</tool>' }}\n","    {%- endif %}\n","{%- endif %}\n","```\n","The specific tokens and tool descriptions our template renders should be chosen to match the ones our model was trained with. There is no requirement that our model understands JSON schema input, only that our template can translate JSON schema into our model's format."],"metadata":{"id":"An4UZnjQFuEt"}},{"cell_type":"markdown","source":["### Tool calls"],"metadata":{"id":"Zxlby5IOHYEi"}},{"cell_type":"markdown","source":["Tool calls will be a list attached to a message with the \"assistant\" role. Note that `tool_calls` is always a list, even though most tool-calling models only support single tool calls at a time, which means the list will usually only have a single element\n","\n","Example of tool calls:\n","```yaml\n","{\n","  \"role\": \"assistant\",\n","  \"tool_calls\": [\n","    {\n","      \"type\": \"function\",\n","      \"function\": {\n","        \"name\": \"multiply\",\n","        \"arguments\": {\n","          \"a\": 5,\n","          \"b\": 6\n","        }\n","      }\n","    }\n","  ]\n","}\n","```\n","and a common pattern for handling them:\n","```python\n","{%- if message['role'] == 'assistant' and 'tool_calls' in message %}\n","    {%- for tool_call in message['tool_calls'] %}\n","            {{- '<tool_call>' + tool_call['function']['name'] + '\\n' + tool_call['function']['arguments']|tojson + '\\n</tool_call>' }}\n","        {%- endif %}\n","    {%- endfor %}\n","{%- endif %}\n","```"],"metadata":{"id":"5gwmrz9bHZtq"}},{"cell_type":"markdown","source":["## Tool responses"],"metadata":{"id":"zEvzWvU4H159"}},{"cell_type":"markdown","source":["Tool responses have a simple format: They are a message dict with\n","* the `\"tool\"` role,\n","* a `\"name\"` key giving the name of the called function, and\n","* a `\"content\"` key containing the result of the tool call.\n","\n","Example of tool response:\n","```yaml\n","{\n","  \"role\": \"tool\",\n","  \"name\": \"multiply\",\n","  \"content\": \"30\"\n","}\n","```\n","If our model does not expect the function name to be included in the tool response, then we can render it as:\n","```python\n","{%- if message['role'] == 'tool' %}\n","    {{- \"<tool_result>\" + message['content'] + \"</tool_result>\" }}\n","{%- endif %}\n","```"],"metadata":{"id":"80TWRDsMH5lT"}},{"cell_type":"code","source":[],"metadata":{"id":"i9DCUMjv2UVz"},"execution_count":null,"outputs":[]}]}