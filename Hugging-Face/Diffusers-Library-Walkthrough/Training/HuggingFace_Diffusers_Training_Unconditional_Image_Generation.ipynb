{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMLta19O38eeKaUewFt6P2k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Unconditional image generation"],"metadata":{"id":"ECnW6hYqzLiF"}},{"cell_type":"markdown","source":["*Unconditional image generation models* are not conditioned on text or images during training. It only generates images that resemble its training data distribution.\n","\n","We will use the [`train_unconditional.py`](https://github.com/huggingface/diffusers/blob/main/examples/unconditional_image_generation/train_unconditional.py) script to train the model.\n","\n","Make sure to install the `diffusers` library from source:\n","```bash\n","git clone https://github.com/huggingface/diffusers\n","cd diffusers\n","pip install .\n","```\n","\n","After installing the `diffusers` library, we need to navigate to the `unconditional_image_generation` folder and install the dependencies:\n","```bash\n","cd examples/unconditional_image_generation\n","pip install -r requirements.txt\n","```\n","\n","HuggingFace Accelerate is also helpful to train on multiple GPUs with mixed-precision.\n","```bash\n","pip install accelerate\n","```\n","\n","Now we can initialize a HuggingFace Accelerate environment\n","```bash\n","accelerate config\n","```\n","To set up a default Acclerate environment without choosing any configurations:\n","```bash\n","accelerate config default\n","```\n","Or if our environment does not support an interactive shell like a notebook, we can use:\n","```python\n","from accelerate.utils import write_basic_config\n","write_basic_config()\n","```"],"metadata":{"id":"ZB9ov0DuzPBV"}},{"cell_type":"markdown","source":["## Script parameters"],"metadata":{"id":"L1cZI7pw08hA"}},{"cell_type":"markdown","source":["All of the parameters and their descriptions can be found in the `parse_args()` function.\n","\n","To speedup training with mixed precision using the `bf16` format, we need to add the `--mixed_precision` to the training command:\n","```bash\n","accelerate launch train_unconditional.py --mix_precision=\"bf16\"\n","```\n","\n","Other important parameters:\n","* `--dataset_name`: the name of the dataset on the Hub or a local path to the dataset to train on\n","* `--output_dir`: where to save the trained model\n","* `--push_to_hub`: whether to push the trained model to the hub\n","* `--checkpointing_steps`: frequency of saving a checkpoint as the model trains. We can continue training from the last saved checkpoint by adding `--resume_from_checkpoint` to our trianing command"],"metadata":{"id":"j0nytOLw1BOY"}},{"cell_type":"markdown","source":["## Training script"],"metadata":{"id":"VlaH40zX6CDu"}},{"cell_type":"markdown","source":["The code for preprocessing the dataset and the training loop is found in the `main()` function under the [`train_unconditional.py`](https://github.com/huggingface/diffusers/blob/096f84b05f9514fae9f185cbec0a4d38fbad9919/examples/unconditional_image_generation/train_unconditional.py#L275). We need to adapt the training script in that function if we want to make any changes.\n","\n","If we do not provide a model configuration, the `train_unconditional.py` will initialize a `UNet2DModel` like the following:\n","```python\n","    # Initialize the model\n","    if args.model_config_name_or_path is None:\n","        model = UNet2DModel(\n","            sample_size=args.resolution,\n","            in_channels=3,\n","            out_channels=3,\n","            layers_per_block=2,\n","            block_out_channels=(128, 128, 256, 256, 512, 512),\n","            down_block_types=(\n","                \"DownBlock2D\",\n","                \"DownBlock2D\",\n","                \"DownBlock2D\",\n","                \"DownBlock2D\",\n","                \"AttnDownBlock2D\",\n","                \"DownBlock2D\",\n","            ),\n","            up_block_types=(\n","                \"UpBlock2D\",\n","                \"AttnUpBlock2D\",\n","                \"UpBlock2D\",\n","                \"UpBlock2D\",\n","                \"UpBlock2D\",\n","                \"UpBlock2D\",\n","            ),\n","        )\n","    else:\n","        config = UNet2DModel.load_config(args.model_config_name_or_path)\n","        model = UNet2DModel.from_config(config)\n","```\n","\n","This is also true for the scheduler and optimizer:\n","```python\n","    # Initialize the scheduler\n","    accepts_prediction_type = \"prediction_type\" in set(inspect.signature(DDPMScheduler.__init__).parameters.keys())\n","    if accepts_prediction_type:\n","        noise_scheduler = DDPMScheduler(\n","            num_train_timesteps=args.ddpm_num_steps,\n","            beta_schedule=args.ddpm_beta_schedule,\n","            prediction_type=args.prediction_type,\n","        )\n","    else:\n","        noise_scheduler = DDPMScheduler(num_train_timesteps=args.ddpm_num_steps, beta_schedule=args.ddpm_beta_schedule)\n","\n","    # Initialize the optimizer\n","    optimizer = torch.optim.AdamW(\n","        model.parameters(),\n","        lr=args.learning_rate,\n","        betas=(args.adam_beta1, args.adam_beta2),\n","        weight_decay=args.adam_weight_decay,\n","        eps=args.adam_epsilon,\n","    )\n","```\n","\n","We may also need to custom the dataset and specify how to preprocess it:\n","```python\n","    # Get the datasets: you can either provide your own training and evaluation files (see below)\n","    # or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub).\n","\n","    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n","    # download the dataset.\n","    if args.dataset_name is not None:\n","        dataset = load_dataset(\n","            args.dataset_name,\n","            args.dataset_config_name,\n","            cache_dir=args.cache_dir,\n","            split=\"train\",\n","        )\n","    else:\n","        dataset = load_dataset(\"imagefolder\", data_dir=args.train_data_dir, cache_dir=args.cache_dir, split=\"train\")\n","        # See more about loading custom images at\n","        # https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder\n","\n","    # Preprocessing the datasets and DataLoaders creation.\n","    augmentations = transforms.Compose(\n","        [\n","            transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n","            transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution),\n","            transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.5], [0.5]),\n","        ]\n","    )\n","\n","```\n","\n","The *training loop* in the `main()` function handles everything else such as adding noise to the images, predicting the noise residual, calculating the loss, saving checkpoints at specified steps, and saving and pushing the model to the Hub."],"metadata":{"id":"v9SWeMRo6GuS"}},{"cell_type":"markdown","source":["## Launch the script"],"metadata":{"id":"e9ftkqtN9WfG"}},{"cell_type":"markdown","source":["Once we have made all our changes, we are ready to launch the training script."],"metadata":{"id":"DJId7SDB9ZoV"}},{"cell_type":"markdown","source":["##### single GPU\n","\n","```bash\n","accelerate launch train_unconditional.py \\\n","  --dataset_name=\"huggan/flowers-102-categories\" \\\n","  --output_dir=\"ddpm-ema-flowers-64\" \\\n","  --mixed_precision=\"fp16\" \\\n","  --push_to_hub\n","```"],"metadata":{"id":"MMDjOlZw9ggu"}},{"cell_type":"markdown","source":["##### multi-GPU\n","\n","```bash\n","accelerate launch --multi_gpu train_unconditional.py \\\n","  --dataset_name=\"huggan/flowers-102-categories\" \\\n","  --output_dir=\"ddpm-ema-flowers-64\" \\\n","  --mixed_precision=\"fp16\" \\\n","  --push_to_hub\n","```"],"metadata":{"id":"GxXeHpy49yqV"}},{"cell_type":"markdown","source":["##### inference"],"metadata":{"id":"B_pUDRTU99Ht"}},{"cell_type":"markdown","source":["After training is done, we can use it for inference:"],"metadata":{"id":"kX9IpXhu9_wb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ftCOEX0WzFNn"},"outputs":[],"source":["from diffusers import DiffusionPipeline\n","import torch\n","\n","pipeline = DiffusionPipeline.from_pretrained(\n","    'anton-l/ddpm-butterflies-128'\n",").to('cuda')"]},{"cell_type":"code","source":["image = pipeline().images[0]\n","image"],"metadata":{"id":"RjquFZOK-LcH"},"execution_count":null,"outputs":[]}]}