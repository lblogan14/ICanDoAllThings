{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMFjXddUCqpPTZhS8iWgQDu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Building a RAG System with Gemma, Elasticsearch and HuggingFace Models"],"metadata":{"id":"RHuliGq3J7hO"}},{"cell_type":"markdown","source":["In this example, we will build a RAG powered by Elasticsearch (ES) and HuggingFace models, letting us toggle between ES-vectorizing (our ES cluster vectorizes for us when ingesting and querying) vs self-vectorizing (we vectorize all our data before sending it to ES).\n","\n","ES-vectorizing means our clients do not have to implement it, so that is the default here; however, if we do not have any ML nodes, or our own embedding setup is better/faster, we can set `USE_ELASTICSEARCH_VECTORIZATION = False` in the Section \"Choose data and query vectorization options\" below."],"metadata":{"id":"NDqErbVWKAIF"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"IpvNCAJBKrZw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmgKR_2qJy1o"},"outputs":[],"source":["!pip install elasticsearch sentence_transformers transformers eland==8.12.1 # accelerate # uncomment if using GPU\n","!pip install datasets==2.19.2 # Remove version lock if https://github.com/huggingface/datasets/pull/6978 has been released"]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","notebook_login()"],"metadata":{"id":"qMOHVi6uKyay"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Elasticsearch deployment"],"metadata":{"id":"9l8QHg0nLc7L"}},{"cell_type":"markdown","source":["Make sure we have `CLOUD_ID` and `ELASTIC_DEPL_API_KEY` ready."],"metadata":{"id":"oG-Fo2SgLq1d"}},{"cell_type":"code","source":["from google.colab import userdata\n","\n","# https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#finding-your-cloud-id\n","CLOUD_ID = userdata.get(\"ELASTIC_CLOUD_ID\")  # or \"<YOUR CLOUD_ID>\"\n","\n","# https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#creating-an-api-key\n","ELASTIC_API_KEY = userdata.get(\"ELASTIC_DEPL_API_KEY\")  # or \"<YOUR API KEY>\""],"metadata":{"id":"xx0UYSjULef9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Set up the client and make sure the credentials work."],"metadata":{"id":"TRz-ELn-L1uE"}},{"cell_type":"code","source":["from elasticsearch import Elasticsearch, helpers\n","\n","client = Elasticsearch(cloud_id=CLOUD_ID, api_key=ELASTIC_API_KEY)\n","\n","client.info()"],"metadata":{"id":"Fzl7jaKML4fu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data sourcing and prepration"],"metadata":{"id":"HFZboQahMhIs"}},{"cell_type":"markdown","source":["We will use the [`MongoDB/embedded_movies`](https://huggingface.co/datasets/MongoDB/embedded_movies) dataset sourced from HuggingFace datasets."],"metadata":{"id":"HqYougiVMkZ5"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset('MongoDB/embedded_movies')"],"metadata":{"id":"eIsTK9cmMjEt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we will do two things:\n","1. (Data integrity) we will ensure that each data point's `fullplot` attribute is not empty, as this is the primary data we utilize in the embedding process.\n","2. (Data quality) we will ensure that we remove the `plot_embedding` attribute from all data points as this will be replaced by new embeddings created with a different embedding model, the `gte-large`."],"metadata":{"id":"5hOdk4GE4mUg"}},{"cell_type":"code","source":["# remove data point where plot column is missing\n","dataset = dataset.filter(lambda x: x['fullplot'] is not None)\n","\n","# remove plot_embedding\n","if 'plot_embedding' in sum(dataset.column_names.values(), []):\n","    dataset = dataset.remove_columns('plot_embedding')\n","\n","dataset['train']"],"metadata":{"id":"pmPwTbbG5E20"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset['train'][0]"],"metadata":{"id":"f1dXIwFc5YKV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load Elasticsearch with vectorized data"],"metadata":{"id":"c2FM8WwF5Zlp"}},{"cell_type":"markdown","source":["### Choose data and query vectorization options"],"metadata":{"id":"vmI8ifWG5zey"}},{"cell_type":"markdown","source":["Here we need to make a decision: do we want Elasticsearch to vectorize our data and queries, or do we want to do it ourselves?\n","\n","Setting `USE_ELASTICSEARCH_VECTORIZATION = True` will set up and use ES-hosted-vectorization for our data and our querying, but be aware that this requires our ES deployment to have at least 1 ML node.\n","\n","If setting `USE_ELASTICSEARCH_VECTORIZATION = False`, then it will set up and use the provided model \"locally\" for data and query vectorization.\n","\n","In this example, we picked the [`thenlper/gte-small`](https://huggingface.co/thenlper/gte-small) model for the embedding. Make sure that the `EMBEDDING_DIMENSIONS` is updated accordingly to the model."],"metadata":{"id":"d98n8fS157dE"}},{"cell_type":"code","source":["USE_ELASTICSEARCH_VECTORIZATION = True\n","\n","EMBEDDING_MODEL_ID = 'thenlper/gte-small'\n","# https://huggingface.co/thenlper/gte-small's page shows the dimensions of the model\n","# If you use the `gte-base` or `gte-large` embedding models, the numDimension\n","# value in the vector search index must be set to 768 and 1024, respectively.\n","EMBEDDING_DIMENSIONS = 384"],"metadata":{"id":"a3ITV8RS5cgd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load HuggingFace model into Elasticsearch if needed"],"metadata":{"id":"PPW65m3P7LRG"}},{"cell_type":"markdown","source":["We can load and deploy the HuggingFace model into Elasticsearch using [Eland](https://eland.readthedocs.io/), if `USE_ELASTICSEARCH_VECTORIZATION = True`. This allows Elasticsearch to vectorize our queries and data in later steps."],"metadata":{"id":"4yw0I1ec7OwD"}},{"cell_type":"code","source":["import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\"\n","!(if [ \"True\" == $USE_ELASTICSEARCH_VECTORIZATION ]; then \\\n","  eland_import_hub_model --cloud-id $CLOUD_ID --hub-model-id $EMBEDDING_MODEL_ID --task-type text_embedding --es-api-key $ELASTIC_API_KEY --start --clear-previous; \\\n","fi)"],"metadata":{"id":"GMDv1fKr7WI8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This step adds functions for creating embeddings for text locally, and enriches the dataset with embeddings, so that the data can be ingested into Elasticsearch as vectors. Do not run if `USE_ELASTICSEARCH_VECTORIZATION = True`."],"metadata":{"id":"b2lS6Ol87ysb"}},{"cell_type":"code","source":["from sentence_transformers improt SentenceTransformer\n","\n","if not USE_ELASTICSEARCH_VECTORIZATION:\n","    embedding_model = SentenceTransformer(EMBEDDING_MODEL_ID)\n","\n","\n","def get_embedding(text: str) -> list[float]:\n","    if USE_ELASTICSEARCH_VECTORIZATION:\n","        raise Exception(f\"Disable when USE_ELASTICSEARCH_VECTORIZATION = [{USE_ELASTICSEARCH_VECTORIZATION}]\")\n","    else:\n","        if not text.strip():\n","            print('Attempted to get embedding for empty text.')\n","            return []\n","\n","        embedding = embedding_model.encode(text)\n","        return embedding.tolist()\n","\n","def add_fullplot_embedding(x):\n","    if USE_ELASTICSEARCH_VECTORIZATION:\n","        raise Exception(f\"Disable when USE_ELASTICSEARCH_VECTORIZATION = [{USE_ELASTICSEARCH_VECTORIZATION}]\")\n","    else:\n","        full_plots = x['fullplot']\n","        return {'embedding:' [get_embedding(full_plot) for full_plot in full_plot]}\n","\n","\n","if not USE_ELASTICSEARCH_VECTORIZATION:\n","    dataset = dataset.map(add_fullplot_embedding, batched=True)\n","    dataset['train']"],"metadata":{"id":"AlqeH7DC7Vi1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create a search index with vector search mappings"],"metadata":{"id":"OmYoBUsP8pYU"}},{"cell_type":"markdown","source":["Now, we can create an index in Elasticsearch with the right index mappings to handle vector searches."],"metadata":{"id":"mEsqKVbI8s_V"}},{"cell_type":"code","source":["# Needs to match the id returned from Eland\n","# For HuggingFace models, we just replace the forward slash with double underscore\n","model_id = EMBEDDING_MODEL_ID.replace('/', '__')\n","\n","index_name = 'movies'\n","\n","index_mapping = {\n","    'properties': {\n","        'fullplot': {'type': 'text'},\n","        'plot': {'type': 'text'},\n","        'title': {'type': 'text'},\n","    }\n","}\n","\n","# define index mapping\n","if USE_ELASTICSEARCH_VECTORIZATION:\n","    index_mapping['properties']['embedding'] = {\n","        'properties': {\n","            'is_truncated': {'type': 'boolean'},\n","            'model_id': {\n","                'type': 'text',\n","                'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}\n","            },\n","            'predicted_value': {\n","                'type': 'dense_vector',\n","                'dims': EMBEDDING_DIMENSIONS,\n","                'index': True,\n","                'similarity': 'cosine'\n","            }\n","        }\n","    }\n","else:\n","    index_mapping['properties']['embedding'] = {\n","        'type': 'dense_vector',\n","        'dims': EMBEDDING_DIMENSIONS,\n","        'index': True,\n","        'similarity': 'cosine'\n","    }\n","\n","# flag to check if index has to be deleted before creating\n","should_delete_index = True\n","\n","# check if we want to delete index before creating the index\n","if should_delete_index:\n","    if client.indices.exists(index=index_name):\n","        print(f'Deleting existing index {index_name}...')\n","        client.indices.delete(index=index_name, ignore=[400, 404])\n","\n","print(f\"Creating index {index_name}...\")\n","# ingest pipeline definition\n","if USE_ELASTICSEARCH_VECTORIZATION:\n","    pipeline_id = 'vectorize_fullplots'\n","\n","    client.ingest.put_pipeline(\n","        id=pipeline_id,\n","        processors=[\n","            {\n","                'inference': {\n","                    'model_id': model_id,\n","                    'target_field': 'embedding',\n","                    'field_map': {'fullplot': 'text_field'}\n","                }\n","            }\n","        ]\n","    )\n","\n","    index_settings = {\n","        'index': {\n","            'default_pipeline': pipeline_id\n","        }\n","    }\n","else:\n","    index_settings = {}\n","\n","\n","client.options(ignore_status=[404, 400]).indices.create(\n","    index=index_name,\n","    mappings=index_mapping,\n","    settings=index_settings\n",")"],"metadata":{"id":"Kp9ByNDw7OUE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ingesting data into a Elasticsearch is best done in batches. We can use `helpers` to achieve this."],"metadata":{"id":"vduiz-Oy-jeh"}},{"cell_type":"code","source":["from elasticsearch.helpers improt BulkIndexError\n","\n","\n","def batch_to_bulk_actions(batch):\n","    for record in batch:\n","        action = {\n","            '_index': 'movies',\n","            '_source': {\n","                'title': record['title'],\n","                'plot': record['plot'],\n","                'fullplot': record['fullplot']\n","            }\n","        }\n","        if not USE_ELASTICSEARCH_VECTORIZATION:\n","            action['_source']['embedding'] = record['embedding']\n","        yield action\n","\n","\n","def bulk_index(dataset):\n","    start = 0\n","    end = len(ds)\n","    batch_size = 100\n","\n","    if USE_ELASTICSEARCH_VECTORIZATION:\n","        # If using auto-embedding, bulk requests can take a lot longer,\n","        # so we pass a longer request_timeout here (default to 10s),\n","        # otherwise we could get connection timeouts\n","        batch_client = client.options(request_timeout=600)\n","    else:\n","        batch_client = client\n","\n","    for batch_start in range(start, end, batch_size):\n","        batch_end = min(batch_start + batch_size, end)\n","        print(f\"batch: start [{batch_start}], end [{batch_end}]\")\n","        batch = dataset.select(range(batch_start, batch_end))\n","\n","        actions = batch_to_bulk_actions(batch)\n","        helpers.bulk(batch_client, actions)"],"metadata":{"id":"57juXLIs-rDd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["try:\n","    bulk_index(dataset['train'])\n","except BulkIndexError as e:\n","    print(f\"{e.errors}\")\n","\n","print('Data ingestion into Elasticsearch complete.')"],"metadata":{"id":"OqFyHOnJ_0aM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Perform Vector Search on user queries"],"metadata":{"id":"j9n-lynq_-Bj"}},{"cell_type":"markdown","source":["* If `USE_ELASTICSEARCH_VECTORIZATION = True`, the text query is sent directly to ES where the uploaded model will be used to vectorize it first before doing a vector search.\n","* If `USE_ELASTICSEARCH_VECTORIZATION = False`, we do the vectorization locally before sending a query with the vectorized form of the query."],"metadata":{"id":"bgkmjPudA91u"}},{"cell_type":"code","source":["def vector_search(plot_query):\n","    if USE_ELASTICSEARCH_VECTORIZATION:\n","        knn = {\n","            'field': 'embedding.predicted_value',\n","            'k': 10,\n","            'query_vector_builder': {\n","                'text_embedding': {\n","                    'model_id': model_id,\n","                    'model_text': plot_query\n","                }\n","            },\n","            'num_candidates': 150,\n","        }\n","    else:\n","        question_embedding = get_embedding(plot_query)\n","        knn = {\n","            'field': 'embedding',\n","            'query_vector': question_embedding,\n","            'k': 10,\n","            'num_candidates': 150\n","        }\n","\n","    response = client.search(index='movies', knn=knn, size=5)\n","\n","    results = []\n","    for hit in response['hits']['hits']:\n","        id = hit['_id']\n","        score = hit['_score']\n","        title = hit['_source']['title']\n","        plot = hit['_source']['plot']\n","        full_plot = hit['_source']['fullplot']\n","\n","        result = {\n","            'id': id,\n","            '_score': score,\n","            'title': title,\n","            'plot': plot,\n","            'full_plot': full_plot\n","        }\n","        results.append(result)\n","\n","    return results"],"metadata":{"id":"Dk0lsFqLABMs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pretty_search(query):\n","    get_knowledge = vector_search(query)\n","\n","    search_result = \"\"\n","    for result in get_knowledge:\n","        search_result += f\"Title: {result.get('title', 'N/A')}, Plot: {result.get('fullplot', 'N/A')}\\n\"\n","\n","    return search_result"],"metadata":{"id":"YwQ9zMw6DCCZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Handle user queries and load Gemma"],"metadata":{"id":"UdlLyfUGDZT3"}},{"cell_type":"code","source":["# Conduct query with retrival of sources,\n","# combining results into something we can feed to Gemma\n","def combined_query(query):\n","    source_information = pretty_search(query)\n","    return f\"Query: {query}\\nContinue to answer the query by using these Search Results:\\n{source_information}\""],"metadata":{"id":"3DQUVuTZDcX-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"What is the best romantic movie to watch and why?\"\n","combined_results = combined_query(query)\n","print(combined_results)"],"metadata":{"id":"F5YCZM45DvWx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can load our LLM"],"metadata":{"id":"WfuhSuJ3D12j"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","llm_id = 'google/gemma-2b-it'\n","\n","tokenizer = AutoTokenizer.from_pretrained(llm_id)\n","model = AutoModelForCausalLM.from_pretrained(llm_id, device_map='auto')"],"metadata":{"id":"ySZ-8wDzD3YJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We need to define a method that fetches formatted results from a vectorized search in ES, and then feed it to the LLM to get our results."],"metadata":{"id":"SkRQ3629EH6v"}},{"cell_type":"code","source":["def rag_query(query):\n","    combined_information = combined_query(query)\n","\n","    input_ids = tokenizer(\n","        combined_information,\n","        return_tensors='pt'\n","    ).to('cuda')\n","    response = model.generate(\n","        **input_ids,\n","        max_new_tokens=700\n","    )\n","\n","    return tokenizer.decode(response[0], skip_special_tokens=True)"],"metadata":{"id":"XjVP3Z4vEQX2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(rag_query(query))"],"metadata":{"id":"3QfQdR11EghU"},"execution_count":null,"outputs":[]}]}