{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOcAXTnTMuZCCyVU/GX+oJA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"iIRkbaPNy677"},"outputs":[],"source":["!pip install -qU diffusers transformers accelerate opencv-python controlnet-aux"]},{"cell_type":"markdown","source":["# T2I-Adapter"],"metadata":{"id":"Ye473-GXy_j9"}},{"cell_type":"markdown","source":["**T2I-Adapter** is a lightweight adapter for controlling and providing more accurate structure guidance for text-to-image models. It works by learning an alignment between the internal knowledge of the text-to-image model and an external control signal, such as edge detection or depth estimation.\n","\n","In the T2I-Adapter, the condition is passed to four feature extraction blocks and three downsample blocks. This makes it fast and easy to train different adapters for different conditions which can be plugged into the text-to-image model.\n","\n","T2I-Adapter is similar to ControlNet except it is smaller and faster because it only runs once during the diffusion process. The downside is that performance may be slightly worse than ControlNet."],"metadata":{"id":"cYCtGSPszB69"}},{"cell_type":"markdown","source":["## Text-to-image"],"metadata":{"id":"7R_Yv0g2zpgw"}},{"cell_type":"markdown","source":["Text-to-image models rely on a text prompt to generate an image, but text alone may not be enough to provide more accurate structural guidance.\n","\n","T2I-Adapter allows us to provide an additional control image to guide the generation process."],"metadata":{"id":"FM_1NUDw1uBs"}},{"cell_type":"markdown","source":["##### Stable Diffusion 1.5"],"metadata":{"id":"-rKkgoff1-n0"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from PIL import Image\n","from diffusers.utils import load_image, make_image_grid\n","\n","original_image = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png\")\n","image = np.array(original_image)\n","\n","low_threshold = 100\n","high_threshold = 200\n","\n","image = cv2.Canny(image, low_threshold, high_threshold)\n","image = Image.fromarray(image)"],"metadata":{"id":"W9X6IiTDzBVb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from diffusers import StableDiffusionAdapterPipeline, T2IAdapter\n","import torch\n","\n","adapter = T2IAdapter.from_pretrained(\n","    'TencentARC/t2iadapter_canny_sd15v2',\n","    torch_dtype=torch.float16\n",")\n","\n","pipeline = StableDiffusionAdapterPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    adapter=adapter,\n","    torch_dtype=torch.float16\n",").to('cuda')"],"metadata":{"id":"eunxT_6U2Gqr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt=\"cinematic photo of a plush and soft midcentury style rug on a wooden floor, 35mm photograph, film, professional, 4k, highly detailed\"\n","generator = torch.Generator('cuda').manual_seed(111)\n","\n","image = pipeline(\n","    prompt,\n","    image=image,\n","    generator=generator\n",").images[0]\n","make_image_grid([original_image, image], 1, 2)"],"metadata":{"id":"S5_o_AQ92Z6A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Stable Diffusion XL"],"metadata":{"id":"TxvPK-gC2nK0"}},{"cell_type":"code","source":["from controlnet_aux.canny import CannyDetector\n","from diffusers.utils import load_image, make_image_grid\n","\n","canny_detector = CannyDetector()\n","\n","image = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png\")\n","image = canny_detector(\n","    image,\n","    detect_resolution=384,\n","    image_resolution=1024\n",")"],"metadata":{"id":"WqW8M1Pv2pBZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, EulerAncestralDiscreteScheduler, AutoencoderKL\n","import torch\n","\n","scheduler = EulerAncestralDiscreteScheduler.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    subfolder='scheduler'\n",")\n","\n","vae = AutoencoderKL.from_pretrained(\n","    'madebyollin/sdxl-vae-fp16-fix',\n","    torch_dtype=torch.float16\n",")\n","\n","adapter = T2IAdapter.from_pretrained(\n","    'TencentARC/t2i-adapter-canny-sdxl-1.0',\n","    torch_dtype=torch.float16\n",")\n","\n","pipeline = StableDiffusionXLAdapterPipeline.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    vae=vae,\n","    adapter=adapter,\n","    scheduler=scheduler,\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",").to('cuda')"],"metadata":{"id":"g8GJIQ_k23vQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt=\"cinematic photo of a plush and soft midcentury style rug on a wooden floor, 35mm photograph, film, professional, 4k, highly detailed\"\n","generator = torch.Generator('cuda').manual_seed(111)\n","\n","image = pipeline(\n","    prompt,\n","    image=image,\n","    generator=generator\n",").images[0]\n","make_image_grid([original_image, image], 1, 2)"],"metadata":{"id":"IyKljr1V3bG3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## MultiAdapter"],"metadata":{"id":"r1cOSSa_3jmk"}},{"cell_type":"markdown","source":["T2I-Adapters are also composable, allowing us to use more than one adapter to impose multiple control conditions on an image."],"metadata":{"id":"KvI4SXcl3mDi"}},{"cell_type":"code","source":["from diffusers.utils import load_image, make_image_grid\n","\n","pose_image = load_image(\n","    \"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/keypose_sample_input.png\"\n",")\n","depth_image = load_image(\n","    \"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/depth_sample_input.png\"\n",")\n","cond = [pose_image, depth_image]\n","prompt = [\"Santa Claus walking into an office room with a beautiful city view\"]\n","\n","make_image_grid(cond, 1, 2)"],"metadata":{"id":"hMMF4Qw53k9c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the corresponding pose and depth adapters as a list\n","from diffusers import StableDiffusionAdapterPipelin, MultiAdapter, T2IAdapter\n","import torch\n","\n","adapters = MultiAdapter(\n","    [\n","        T2IAdapter.from_pretrained('TencentARC/t2iadapter_keypose_sd14v1'),\n","        T2IAdapter.from_pretrained('TencentARC/t2iadapter_depth_sd14v1')\n","    ]\n",")\n","adapters = adapters.to(torch.float16)"],"metadata":{"id":"AXvUqlGw3zh8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipeline = StableDiffusionAdapterPipeline.from_pretrained(\n","    'CompVis/stable-diffusion-v1-4',\n","    torch_dtype=torch.float16,\n","    adapter=adapters # pass all adapters here\n",").to('cuda')"],"metadata":{"id":"AvWtwls04H0n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = pipeline(\n","    prompt,\n","    cond,\n","    adapter_conditioning_scale=[0.7, 0.7], # condition scales\n",").images[0]\n","image"],"metadata":{"id":"RT7JEtWS4R8Q"},"execution_count":null,"outputs":[]}]}