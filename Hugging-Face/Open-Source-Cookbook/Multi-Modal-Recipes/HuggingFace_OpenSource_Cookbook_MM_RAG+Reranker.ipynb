{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOfBMbkuw4tnLVxP86UTZf2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Multimodal RAG with ColQwen2, Reranker, and Quantized VLMs on Consumer GPUs"],"metadata":{"id":"XCNodWfrr7Md"}},{"cell_type":"markdown","source":["In this example, we will build a **Multimodal Retrieval-Augmented Generation (RAG)** system by integrating [`ColQwen2`](https://huggingface.co/vidore/colqwen2-v1.0) for document retrieval, [`MonoQwen2-VL-v0.1`](https://huggingface.co/lightonai/MonoQwen2-VL-v0.1) for reranking, and [`Qwen2-VL`](https://qwenlm.github.io/blog/qwen2-vl/) as the vision language model (VLM).\n","\n","These models will form a powerful RAG system that enhances query responses by seamlessly combining text-based documents and visual data.\n","\n","Instead of relying on a complex OCR-based document processing pipeline, we leverage a **Document Retrieval Model** to efficiently retrieve  the most relevant documents based on a user's query, making the system more scalable and efficient."],"metadata":{"id":"WlqErL3It1Zv"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"zjHcgzZJuqc8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vx6DcMT5r2OH"},"outputs":[],"source":["!pip install -qU byaldi pdf2image qwen-vl-utils transformers bitsandbytes peft rerankers[monovlm]\n","# Tested with byaldi==0.0.7, pdf2image==1.17.0, qwen-vl-utils==0.0.8, transformers==4.46.3"]},{"cell_type":"markdown","source":["## Load dataset"],"metadata":{"id":"sPnOcCJww7fR"}},{"cell_type":"markdown","source":["In this example, we will use charts and maps from [Our World in Data](https://ourworldindata.org/), a resource offering open access to a wide range of data and visualizations. We will focus on the [left expectancy](https://ourworldindata.org/life-expectancy) data. There is a curated small subset on HuggingFace called [`ourworldindata_example`](https://huggingface.co/datasets/sergiopaniego/ourworldindata_example)."],"metadata":{"id":"hyIbQV7aw89C"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"sergiopaniego/ourworldindata_example\", split=\"train\")"],"metadata":{"id":"cdj5_uG8w8eh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After downloading the data, we will save it locally to enable the RAG system to index the files later. It allows the document retrieval model (ColQwen2) to efficiently process and manipulate the visual content. We also reduce the image size to 448x448 to further minimize memory consumption and ensure faster processing."],"metadata":{"id":"AwXckuGVx9eK"}},{"cell_type":"code","source":["import os\n","from PIL import Image\n","\n","def save_images_to_local(dataset, output_folder='data/'):\n","    os.makedirs(output_folder, exist_ok=True)\n","\n","    for image_id, image_data in enumerate(dataset):\n","        image = image_data['image']\n","\n","        if isinstance(image, str):\n","            image = Image.open(image)\n","\n","        image = image.resize((448, 448))\n","\n","        output_path = os.path.join(output_folder, f\"image_{image_id}.png\")\n","        image.save(output_path, format='PNG')\n","        print(f\"Image saved to {output_path}\")\n","\n","save_images_to_local(dataset)"],"metadata":{"id":"CHJS9pUCyWIP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can load the images to explore the data"],"metadata":{"id":"jVhulfQ5yuYD"}},{"cell_type":"code","source":["import os\n","from PIL import Image\n","\n","def load_png_images(image_folder):\n","    png_files = [f for f in os.listdir(image_folder) if f.endswith('.png')]\n","    all_images = {}\n","\n","    for image_id, png_file in enumerate(png_files):\n","        image_path = os.path.join(image_folder, png_file)\n","        image = Image.open(image_path)\n","        all_images[image_id] = image\n","\n","    return all_images\n","\n","all_images = load_png_images('/content/data/')"],"metadata":{"id":"BDU7q0Cxywc6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","fig, axes = plt.subplots(1, 5, figsize=(20, 15))\n","\n","for i, ax in enumerate(axes.flat):\n","    img = all_images[i]\n","    ax.imshow(img)\n","    ax.axis('off')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"Dukmpm15zEWE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Initialize the ColQwen2 multimodal document retrieval model"],"metadata":{"id":"qxdZbVHfzPal"}},{"cell_type":"markdown","source":["The document retrieval model will be responsible for extracting relevant information from the raw images and delivering the appropriate documents based on our queries.\n","\n","For this task, we will use the [`byaldi`](https://github.com/AnswerDotAI/byaldi) library, which is a simple wrapper around the ColPali repository to make it easy to use late-interaction multi-modal models such as ColPali with a familiar API. In this example, we will focus on ColQwen2."],"metadata":{"id":"LhArg6445amW"}},{"cell_type":"code","source":["from byaldi import RAGMultiModalModel\n","\n","docs_retrieval_model = RAGMultiModalModel.from_pretrained('vidore/colqwen2-v1.0')"],"metadata":{"id":"pDP04VXdzSPb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can index our documents directly using the document retrieval model by specifying the folder where the images are stored. This enables the model to process and organize the documents for efficient retrieval based on our queries."],"metadata":{"id":"g9A30zyc6EDt"}},{"cell_type":"code","source":["docs_retrieval_model.index(\n","    input_path='data/',\n","    index_name='image_index',\n","    store_collection_with_index=False,\n","    overwrite=True\n",")"],"metadata":{"id":"IwKlXmZJ6OOA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Retrieve documents with the document retrieval model and re-ranking with the Reranker"],"metadata":{"id":"08ty5Pea6UJZ"}},{"cell_type":"code","source":["# test the document retrieval model\n","text_query = \"How does the life expectancy change over time in France and South Africa?\"\n","\n","results = docs_retrieval_model.search(text_query, k=3)\n","results"],"metadata":{"id":"pCd_hAsC6YG2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we need to examine the specific documents (images) the model has retrieved. This will give us insight into the visual content that corresponds to our query and help us understand how the model selects relevant information."],"metadata":{"id":"jEtQhSD26kLy"}},{"cell_type":"code","source":["def get_grouped_images(results, all_images):\n","    grouped_images = []\n","\n","    for result in results:\n","        doc_id = result['doc_id']\n","        page_num = result['page_num']\n","        grouped_images.append(all_images[doc_id])\n","\n","    return grouped_images\n","\n","\n","grouped_images = get_grouped_images(results, all_images)"],"metadata":{"id":"b8WjRAnO6tW_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","fig, axes = plt.subplots(1, 3, figsize=(20, 15))\n","\n","for i, ax in enumerate(axes.flat):\n","    img = grouped_images[i]\n","    ax.imshow(img)\n","    ax.axis('off')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"nbIKny6V67fE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we will initialize our reranker model by using the `reranker` module"],"metadata":{"id":"XglBzkcj7D9z"}},{"cell_type":"code","source":["from rerankers import Reranker\n","\n","ranker = Reranker('monovlm', device='cuda')"],"metadata":{"id":"Nul0skN17Hxs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The reranker requires the images to be in base64 format."],"metadata":{"id":"90jxPoDS7Ltq"}},{"cell_type":"code","source":["import base64\n","from io import BytesIO\n","\n","def images_to_base64(images):\n","    base64_images = []\n","\n","    for img in images:\n","        buffer = BytesIO()\n","        img.save(buffer, format='JPEG')\n","        buffer.seek(0)\n","\n","        img_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n","        base64_images.append(img_base64)\n","\n","    return base64_images\n","\n","\n","base64_list = images_to_base64(grouped_images)"],"metadata":{"id":"d3l4Offy7OgE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we pass the `text_query` and the list of images to the reranker so it can enhance the retrieved context. This time, instead of using the 3 previously retrieved documents, we will return only 1."],"metadata":{"id":"YYH-E0bZ8yTK"}},{"cell_type":"code","source":["results = ranker.rank(text_query, base64_list)"],"metadata":{"id":"RMO8_YPG8-fb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_ranker_results(results, grouped_images, top_k=3, log=False):\n","    new_grouped_images = []\n","\n","    for i, doc in enumerate(results.top_k(top_k)):\n","        if log:\n","            print(f\"Rank {i}:\")\n","            print('Document ID: ', doc.doc_id)\n","            print('Document Score: ', doc.score)\n","            print('Document Base64: ', doc.base64[:30] + '...')\n","            print('Document Path: ', doc_image_path)\n","\n","        new_grouped_images.append(grouped_images[doc.doc_id])\n","    return new_grouped_images\n","\n","\n","new_grouped_images = process_ranker_results(\n","    results,\n","    grouped_images,\n","    top_k=1,\n","    log=True\n",")"],"metadata":{"id":"YWJAKWV39CU9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Initialize the vision language model for question answering"],"metadata":{"id":"D_vQkSXC9jgg"}},{"cell_type":"markdown","source":["We will initialize the vision language model for question answering with Qwen2-VL."],"metadata":{"id":"UrMFDy9g9n0Y"}},{"cell_type":"code","source":["from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor, BitsAndBytesConfig\n","from qwen_vl_utils import process_vision_info\n","import torch\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","vl_model = Qwen2VLForConditionalGeneration.from_pretrained(\n","    'Qwen/Qwen2-VL-7B-Instruct',\n","    device_map='auto',\n","    torch_dtype=torch.bfloat16,\n","    quantization_config=bnb_config\n",")\n","vl_model.eval()\n","\n","min_pixels = 224 * 224\n","max_pixels = 1024 * 1024\n","vl_model_processor = Qwen2VLProcessor.from_pretrained(\n","    'Qwen/Qwen2-VL-7B-Instruct',\n","    min_pixels=min_pixels,\n","    max_pixels=max_pixels\n",")"],"metadata":{"id":"dU090Obs9nS0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We need to specify the minimum and maximum pixel sizes to optimzie how images fit into the GPU memory."],"metadata":{"id":"wxx8AaUy-Ph4"}},{"cell_type":"markdown","source":["## Assemble the VLM model and test the system"],"metadata":{"id":"PgSduXGe-V28"}},{"cell_type":"markdown","source":["We will set up the chat structure by providing the system with the retrieved images and the user's query."],"metadata":{"id":"741ZzP72-Zlw"}},{"cell_type":"code","source":["chat_template = [\n","    {\n","        'role': 'user',\n","        'content': [\n","            {\n","                'type': 'image',\n","                'image': new_grouped_images[0]\n","            },\n","            {\n","                'type': 'text',\n","                'text': text_query\n","            }\n","        ]\n","    }\n","]"],"metadata":{"id":"Ru5sHVy--VSo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can apply this chat template to set up the system for interacting with the model."],"metadata":{"id":"mkL75u4e-otE"}},{"cell_type":"code","source":["text = vl_model_processor.apply_chat_template(\n","    chat_template,\n","    tokenize=False,\n","    add_generation_prompt=True\n",")"],"metadata":{"id":"pQfEzx17-s6t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we will process the inputs to ensure they are properly formatted and ready for use with the VLM."],"metadata":{"id":"1bZfc8OG_HN5"}},{"cell_type":"code","source":["image_inputs, _ = process_vision_info(chat_template)\n","inputs = vl_model_processor(\n","    text=[text],\n","    images=image_inputs,\n","    padding=True,\n","    return_tensors='pt'\n",")\n","inputs = inputs.to('cuda')"],"metadata":{"id":"ZuKaa9Ub_L5-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we are ready to generate the answer."],"metadata":{"id":"KZEViIaU_WWa"}},{"cell_type":"code","source":["generated_ids = vl_model.generate(**inputs, max_new_tokens=500)"],"metadata":{"id":"RmIF3hto_Y-x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once the model generates the outputs, we will postprocess it to generate the final answer."],"metadata":{"id":"qr8ToAl8_dLO"}},{"cell_type":"code","source":["generated_ids_trimmed = [\n","    out_ids[len(in_ids) :]\n","    for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n","]\n","output_text = vl_model_processor.batch_decode(\n","    generated_ids_trimmed,\n","    skip_special_tokens=True,\n","    clean_up_tokenization_spaces=False\n",")"],"metadata":{"id":"wMFJwSKK_hF8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(output_text[0])"],"metadata":{"id":"dHg2iORn_vJF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Assemble everything"],"metadata":{"id":"PfLrXa6B_wve"}},{"cell_type":"markdown","source":["Now we can create a function tha encompasses the entire pipeline, allowing us to easily reuse it in future applications."],"metadata":{"id":"hgS0ZLF0_zDD"}},{"cell_type":"code","source":["def answer_with_multimodal_rag(\n","        vl_model,\n","        vl_model_processor,\n","        docs_retrieval_model,\n","        all_images,\n","        text_query,\n","        retrieval_top_k,\n","        reranker_top_k,\n","        max_new_tokens,\n","):\n","    # RAG retrieval\n","    results = docs_retrieval_model.search(text_query, k=retrieval_top_k)\n","    grouped_images = get_grouped_images(results, all_images)\n","\n","    # RAG Reranker\n","    base64_list = images_to_base64(grouped_images)\n","    results = ranker.rank(text_query, base64_list)\n","    grouped_images = process_ranker_reuslts(\n","        results,\n","        grouped_images,\n","        top_k=reranker_top_k\n","    )\n","\n","    # Chat template\n","    chat_template = [\n","        {\n","            'role': 'user',\n","            'content': [\n","                {\n","                    'type': 'image',\n","                    'image': image\n","                }\n","                for image in grouped_images\n","            ]\n","            + [\n","                {\n","                    'type': 'text',\n","                    'text': text_query\n","                }\n","            ]\n","        }\n","    ]\n","\n","    # Prepare the inputs\n","    text = vl_model_processor.apply_chat_template(\n","        chat_template,\n","        tokenize=False,\n","        add_generation_prompt=True\n","    )\n","    image_inputs, video_inputs = process_vision_info(chat_template)\n","    inputs = vl_model_processor(\n","        text=[text],\n","        images=image_inputs,\n","        padding=True,\n","        return_tensors='pt'\n","    )\n","    inputs = inputs.to('cuda')\n","\n","    # Generate text from vl_model\n","    generated_ids = vl_model.generate(**inputs, max_new_tokens=max_new_tokens)\n","    generated_ids_trimmed = [\n","        out_ids[len(in_ids) :]\n","        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n","    ]\n","\n","    # Decode the generated ids\n","    output_text = vl_model_processor.batch_decode(\n","        generated_ids_trimmed,\n","        skip_special_tokens=True,\n","        clean_up_tokenization_spaces=False\n","    )\n","\n","    return output_text"],"metadata":{"id":"kQ1cJTtV_yPI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_query = \"What is the overall trend in life expectancy across different countries and regions?\"\n","\n","output_text = answer_with_multimodal_rag(\n","    vl_model=vl_model,\n","    vl_model_processor=vl_model_processor,\n","    docs_retrieval_model=docs_retrieval_model,\n","    all_images=all_images,\n","    text_query=text_query,\n","    retrieval_top_k=3,\n","    reranker_top_k=1,\n","    max_new_tokens=500,\n",")\n","\n","print(output_text[0])"],"metadata":{"id":"J2axg1LeBaK2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","torch.cuda.empty_cache()\n","torch.cuda.synchronize()\n","print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n","print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"],"metadata":{"id":"DZ6iSUS1Brbh"},"execution_count":null,"outputs":[]}]}