{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16459,
     "status": "ok",
     "timestamp": 1733497696501,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "ZMV3Au5sq107",
    "outputId": "2622db00-6c41-4dbe-cf2a-c06eec150f16"
   },
   "outputs": [],
   "source": [
    "!pip install -qU langgraph langchain openai tavily-python langchain_openai langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1733497696502,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "tcIbr6qXrFqD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'YOUR_OPENAI_API_KEY'\n",
    "os.environ['TAVILY_API_KEY'] = 'YOUR_TAVILY_API_KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAlycYpyrJfR"
   },
   "source": [
    "# Language Agent Tree Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr2ZBohtrLlu"
   },
   "source": [
    "**Language Agent Tree Search (LATS)** is a general LLM agent search algorithm that combines reflection/evaluation and search (specifically monte-carlo trees search) to achieve better overall task performance compared to a similar techniques like ReACT, Reflexion, or Tree of Thoughts.\n",
    "\n",
    "The Language Agent Tree Search has four main steps:\n",
    "1. **Select**: pick the best next actions based on the aggregate rewards from Step (2). Either respond (if a solution is found or the max search depth is reached) or continue searching.\n",
    "2. **Expand and simulate**: select the \"best\" 5 potential actions to take and execute them in parallel.\n",
    "3. **Reflect + Evaluate**: observe the outcomes of these actions and score the decisions based on reflection (and possibly external feedback).\n",
    "4 **Backpropagate**: update the scores of the root trajectories based on the outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PclO3zymS6sL"
   },
   "source": [
    "# Graph State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMJqXB-IS8P7"
   },
   "source": [
    "LATS is based on a (greedy) Monte-Carlo tree search. For each search steps, it picks the node with the highest \"upper confidence bound\", which is a metric that balances exploitation (highest average reward) and exploration (lowest visits).\n",
    "\n",
    "Starting from that node, it generates N new candidate actions to take, and adds them to the tree. It stops searching either when it has generated a valid solution OR when it has reached the maximum number of rollouts (search tree depth).\n",
    "\n",
    "Our Graph state will be composed of two items:\n",
    "1. The root of the search tree, and\n",
    "2. The user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1040,
     "status": "ok",
     "timestamp": 1733497697534,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "T8LLdtr4rLPD"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import deque\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1733497697535,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "yW-v0ztWUZjt"
   },
   "outputs": [],
   "source": [
    "class Reflection(BaseModel):\n",
    "    reflections: str = Field(\n",
    "        description=\"The critique and reflections on the sufficiency, superfluency,\"\n",
    "        \" and general quality of the response\"\n",
    "    )\n",
    "    score: int = Field(\n",
    "        description=\"Score from 0-10 on the quality of the candidate response.\",\n",
    "        gte=0,\n",
    "        lte=10,\n",
    "    )\n",
    "    found_solution: bool = Field(\n",
    "        description='Whether the response has fully solved the question or task.'\n",
    "    )\n",
    "\n",
    "    def as_message(self):\n",
    "        return HumanMessage(\n",
    "            content=f\"Reasoning: {self.reflections}\\nScore: {self.score}\"\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def normalized_score(self) -> float:\n",
    "        return self.score / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 84,
     "status": "ok",
     "timestamp": 1733500178008,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "v5KRXm41UewK"
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(\n",
    "        self,\n",
    "        messages: list[BaseMessage],\n",
    "        reflection: Reflection,\n",
    "        parent: Optional[\"Node\"] = None,\n",
    "    ):\n",
    "        self.messages = messages\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.value = 0\n",
    "        self.visits = 0\n",
    "        self.reflection = reflection\n",
    "        self.depth = parent.depth + 1 if parent is not None else 1\n",
    "        self._is_solved = reflection.found_solution if reflection else False\n",
    "\n",
    "        if self._is_solved:\n",
    "            self._mark_tree_as_solved()\n",
    "\n",
    "        self.backpropagate(reflection.normalized_score)\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"<Node value={self.value}, visits={self.visits},\"\n",
    "            f\" solution={self.messages} reflection={self.reflection}/>\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def _mark_tree_as_solved(self):\n",
    "        parent = self.parent\n",
    "        while parent:\n",
    "            parent._is_solved = True\n",
    "            parent = parent.parent\n",
    "\n",
    "    def backpropagate(self, reward: float):\n",
    "        \"\"\"Update the score of this node and its parents\"\"\"\n",
    "        node = self\n",
    "        while node:\n",
    "            node.visits += 1\n",
    "            node.value = (node.value * (node.visits - 1) + reward) / node.visits\n",
    "            node = node.parent\n",
    "\n",
    "\n",
    "    @property\n",
    "    def is_solved(self):\n",
    "        \"\"\"If any solutions exist, we can end the search\"\"\"\n",
    "        return self._is_solved\n",
    "\n",
    "    @property\n",
    "    def is_terminal(self):\n",
    "        return not self.children\n",
    "\n",
    "    @property\n",
    "    def best_child_score(self):\n",
    "        \"\"\"Return the child with the highest score\"\"\"\n",
    "        if not self.children:\n",
    "            return None\n",
    "\n",
    "        return max(self.children, key=lambda child: int(child.is_solved) * child.value)\n",
    "\n",
    "    @property\n",
    "    def height(self) -> int:\n",
    "        \"\"\"Check for how far we have rolled out the tree\"\"\"\n",
    "        if self.children:\n",
    "            return 1 + max([child.height for child in self.children])\n",
    "        return 1\n",
    "\n",
    "\n",
    "    def upper_confidence_bound(self, exploration_weight=1.0):\n",
    "        \"\"\"Return the upper confidence bound score.\n",
    "        This helps balance exploration vs. exploitation of a branch\"\"\"\n",
    "        if self.parent is None:\n",
    "            raise ValueError(\"Cannot obtain upper confidence bound of root node\")\n",
    "        if self.visits == 0:\n",
    "            return self.value\n",
    "\n",
    "        # Encourage exploitation of high-value trajectories\n",
    "        average_reward = self.value / self.visits\n",
    "\n",
    "        # Encourage exploration of less-visits trajectories\n",
    "        exploration_term = math.sqrt(math.log(self.parent.visits) / self.visits)\n",
    "\n",
    "        return average_reward + exploration_weight * exploration_term\n",
    "\n",
    "\n",
    "    def get_messages(self, include_reflections: bool = True):\n",
    "        if include_reflections:\n",
    "            return self.messages + [self.reflection.as_message()]\n",
    "\n",
    "        return self.messages\n",
    "\n",
    "\n",
    "    def get_trajectory(self, include_reflections: bool = True) -> list[BaseMessage]:\n",
    "        \"\"\"Get messages representing this search branch\"\"\"\n",
    "        messages = []\n",
    "        node = self\n",
    "\n",
    "        while node:\n",
    "            messages.extend(\n",
    "                node.get_messages(include_reflections=include_reflections)[::-1]\n",
    "            )\n",
    "            node = node.parent\n",
    "\n",
    "        # Reverse the final back-tracked trajectory to return the correct order\n",
    "        return messages[::-1] # root solution, reflection, child 1, ...\n",
    "\n",
    "    def _get_all_children(self):\n",
    "        all_nodes = []\n",
    "        nodes = deque()\n",
    "        nodes.append(self)\n",
    "\n",
    "        while nodes:\n",
    "            node = nodes.popleft()\n",
    "            all_nodes.extend(node.children)\n",
    "            for n in node.children:\n",
    "                nodes.append(n)\n",
    "\n",
    "        return all_nodes\n",
    "\n",
    "\n",
    "    def get_best_solution(self):\n",
    "        \"\"\"Return the best solution from within the current sub-tree\"\"\"\n",
    "        all_nodes = [self] + self._get_all_children()\n",
    "        best_node = max(\n",
    "            all_nodes,\n",
    "            # Filter out all non-terminal, non-solution trajectories\n",
    "            key=lambda node: int(node.is_terminal and node.is_solved) * node.value,\n",
    "        )\n",
    "\n",
    "        return best_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOMS6kWfbj3Q"
   },
   "source": [
    "## The graph state itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rw-LyA8wbnBL"
   },
   "source": [
    "The main component is the tree, represented by the root node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 98,
     "status": "ok",
     "timestamp": 1733500181331,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "BnQ4EMnAbi5M"
   },
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "class TreeState(TypedDict):\n",
    "    # The full tree\n",
    "    root: Node\n",
    "    # The original input\n",
    "    input: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thUWaQELbyUE"
   },
   "source": [
    "# Define Language Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STJhFA2Rb0db"
   },
   "source": [
    "Our agent will have three primary LLM-powered processes:\n",
    "1. **Reflect**: score the action based on the tool response.\n",
    "2. **Initial response**: to create the root node and start the search.\n",
    "3. **Expand**: generate 5 candidate \"next steps\" from the best spot in the current tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 181,
     "status": "ok",
     "timestamp": 1733500181986,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "hy3KYxiqbyB3"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcEWbm6ghApV"
   },
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 70,
     "status": "ok",
     "timestamp": 1733500182225,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "aZECyUBue9I0"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "search = TavilySearchAPIWrapper()\n",
    "tavily_tool = TavilySearchResults(api_wrapper=search, max_results=5)\n",
    "\n",
    "tools = [tavily_tool]\n",
    "tool_node = ToolNode(tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v44tTaFnhUh6"
   },
   "source": [
    "## Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPADSEI6hV-P"
   },
   "source": [
    "The reflection chain will score agent outputs based on the decision and the tool responses. We will call this within the other two nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 80,
     "status": "ok",
     "timestamp": 1733500182642,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "RS8z3eN-hT87"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.openai_tools import JsonOutputToolsParser, PydanticToolsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import chain as as_runnable\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            'Reflect and grade the assistant response to the user question below.',\n",
    "        ),\n",
    "        (\n",
    "            'user',\n",
    "            '{input}',\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name='candidate'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "reflection_llm_chain = (\n",
    "    prompt\n",
    "    | llm.bind_tools(tools=[Reflection], tool_choice='Reflection').with_config(\n",
    "        run_name='Reflection'\n",
    "    )\n",
    "    | PydanticToolsParser(tools=[Reflection])\n",
    ")\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def reflection_chain(inputs) -> Reflection:\n",
    "    tool_choices = reflection_llm_chain.invoke(inputs)\n",
    "    reflection = tool_choices[0]\n",
    "\n",
    "    if not isinstance(inputs['candidate'][-1], AIMessage):\n",
    "        reflection.found_solution = False\n",
    "\n",
    "    return reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mnZlWMyiaAA"
   },
   "source": [
    "## Initial Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLD8UO9YicGH"
   },
   "source": [
    "We start with a single root node, generated by this first step. It responds to the user input either with a tool invocation or a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 95,
     "status": "ok",
     "timestamp": 1733500183325,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "B-HE6UDhiZiF"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompt_values import ChatPromptValue\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            'You are an AI assistant',\n",
    "        ),\n",
    "        (\n",
    "            'user',\n",
    "            '{input}',\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name='messages', optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "initial_answer_chain = (\n",
    "    prompt_template\n",
    "    | llm.bind_tools(tools=tools).with_config(\n",
    "        run_name='GenerateInitialCandidate'\n",
    "    )\n",
    ")\n",
    "\n",
    "parser = JsonOutputToolsParser(return_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1052,
     "status": "ok",
     "timestamp": 1733500184469,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "kYY0SSphjENa",
    "outputId": "be061302-a7b3-4822-9914-2b93f354fc7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_hXQlu5NTeSKSB1dDw6r2Vjlb', 'function': {'arguments': '{\"query\":\"lithium pollution research report\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 93, 'total_tokens': 116, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bba3c8e70b', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-ecdd92da-de03-45c1-a8a9-bd151f6348ef-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'lithium pollution research report'}, 'id': 'call_hXQlu5NTeSKSB1dDw6r2Vjlb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 93, 'output_tokens': 23, 'total_tokens': 116, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_response = initial_answer_chain.invoke(\n",
    "    {'input': \"Write a research report on lithium pollution.\"}\n",
    ")\n",
    "\n",
    "initial_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sUIz8L1jVP0"
   },
   "source": [
    "### Starting Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVvAEJG4jXhs"
   },
   "source": [
    "We will package up the candidate generation and reflection in a single node of our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1733500185238,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "etw7cFJSjR4f"
   },
   "outputs": [],
   "source": [
    "# Define the node\n",
    "def generate_initial_response(state: TreeState) -> dict:\n",
    "    \"\"\"Generate the initial candidate response\"\"\"\n",
    "    res = initial_answer_chain.invoke({'input': state['input']})\n",
    "    parsed = parser.invoke(res)\n",
    "\n",
    "    tool_responses = [\n",
    "        tool_node.invoke(\n",
    "            {\n",
    "                'messages': [\n",
    "                    AIMessage(\n",
    "                        content=\"\",\n",
    "                        tool_calls=[\n",
    "                            {\n",
    "                                'name': r['type'],\n",
    "                                'args': r['args'],\n",
    "                                'id': r['id'],\n",
    "                            }\n",
    "                        ]\n",
    "                    )\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        for r in parsed\n",
    "    ]\n",
    "\n",
    "    output_messages = [res] + [tr['messages'][0] for tr in tool_responses]\n",
    "\n",
    "    reflection = reflection_chain.invoke(\n",
    "        {\n",
    "            'input': state['input'],\n",
    "            'candidate': output_messages,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    root = Node(output_messages, reflection=reflection)\n",
    "\n",
    "    return {**state, 'root': root}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vm7-Fz_kOby"
   },
   "source": [
    "## Candidate Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbDfuaoTkQf6"
   },
   "source": [
    "We now use the same LLM to generate N additional candidates to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 83,
     "status": "ok",
     "timestamp": 1733500186658,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "cI1Qy5pOkNuD"
   },
   "outputs": [],
   "source": [
    "# This generates N candidate values\n",
    "# for a single input to sample actions from the environment\n",
    "def generate_candidates(messages: ChatPromptValue, config: RunnableConfig):\n",
    "    n = config['configurable'].get(\"N\", 5)\n",
    "    bound_kwargs = llm.bind_tools(tools=tools).kwargs\n",
    "\n",
    "    chat_result = llm.generate(\n",
    "        [messages.to_messages()],\n",
    "        n=n,\n",
    "        callbacks=config['callbacks'],\n",
    "        run_name='GenerateCandidates',\n",
    "        **bound_kwargs,\n",
    "    )\n",
    "\n",
    "    return [gen.message for gen in chat_result.generations[0]]\n",
    "\n",
    "\n",
    "expansion_chain = prompt_template | generate_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 647,
     "status": "ok",
     "timestamp": 1733500187406,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "qQ0Mc9Ekk3B8",
    "outputId": "09a16517-802a-4af6-aba1-adaeb3314b1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_b7AWGy5InOfSnoQepEhdXGCg', 'function': {'arguments': '{\"query\":\"lithium pollution research report\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'finish_reason': 'tool_calls', 'logprobs': None}, id='run-13b6b6d2-dbde-4f1b-ad41-dd636e76e18b-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'lithium pollution research report'}, 'id': 'call_b7AWGy5InOfSnoQepEhdXGCg', 'type': 'tool_call'}], usage_metadata={'input_tokens': 93, 'output_tokens': 115, 'total_tokens': 208, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_b7AWGy5InOfSnoQepEhdXGCg', 'function': {'arguments': '{\"query\":\"lithium pollution research report\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'logprobs': None}, id='run-13b6b6d2-dbde-4f1b-ad41-dd636e76e18b-1', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'lithium pollution research report'}, 'id': 'call_b7AWGy5InOfSnoQepEhdXGCg', 'type': 'tool_call'}], usage_metadata={'input_tokens': 93, 'output_tokens': 115, 'total_tokens': 208, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_b7AWGy5InOfSnoQepEhdXGCg', 'function': {'arguments': '{\"query\":\"lithium pollution research report\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'logprobs': None}, id='run-13b6b6d2-dbde-4f1b-ad41-dd636e76e18b-2', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'lithium pollution research report'}, 'id': 'call_b7AWGy5InOfSnoQepEhdXGCg', 'type': 'tool_call'}], usage_metadata={'input_tokens': 93, 'output_tokens': 115, 'total_tokens': 208, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_b7AWGy5InOfSnoQepEhdXGCg', 'function': {'arguments': '{\"query\":\"lithium pollution research report\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'logprobs': None}, id='run-13b6b6d2-dbde-4f1b-ad41-dd636e76e18b-3', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'lithium pollution research report'}, 'id': 'call_b7AWGy5InOfSnoQepEhdXGCg', 'type': 'tool_call'}], usage_metadata={'input_tokens': 93, 'output_tokens': 115, 'total_tokens': 208, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_b7AWGy5InOfSnoQepEhdXGCg', 'function': {'arguments': '{\"query\":\"lithium pollution research report\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'logprobs': None}, id='run-13b6b6d2-dbde-4f1b-ad41-dd636e76e18b-4', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'lithium pollution research report'}, 'id': 'call_b7AWGy5InOfSnoQepEhdXGCg', 'type': 'tool_call'}], usage_metadata={'input_tokens': 93, 'output_tokens': 115, 'total_tokens': 208, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = expansion_chain.invoke(\n",
    "    {'input': \"Write a research report on lithium pollution.\"}\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rXzeNeTmn3a"
   },
   "source": [
    "### Candidate generation node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gf9qS1VTmppJ"
   },
   "source": [
    "We will package the candidate generation and reflection steps in the following `expand` node. We will do all the operations as a batch process to speed up execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1733500188242,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "ht5hTWdIk90a"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def select(root: Node) -> dict:\n",
    "    \"\"\"Starting from the root node,\n",
    "    a child node is selected at each tree level until a leaf node is reached\n",
    "    \"\"\"\n",
    "    if not root.children:\n",
    "        return root\n",
    "\n",
    "    node = root\n",
    "    while node.children:\n",
    "        max_child = max(node.children, key=lambda child: child.upper_confidence_bound())\n",
    "        node = max_child\n",
    "\n",
    "    return node\n",
    "\n",
    "\n",
    "def expand(state: TreeState, config: RunnableConfig) -> dict:\n",
    "    \"\"\"Starting from the \"best\" node in the tree,\n",
    "    generate N candidates for the next step.\n",
    "    \"\"\"\n",
    "    root = state['root']\n",
    "    best_candidate: Node = select(root)\n",
    "    messages = best_candidate.get_trajectory()\n",
    "\n",
    "    # Generate N candidates from the single child candidate\n",
    "    new_candidates = expansion_chain.invoke(\n",
    "        {\n",
    "            'input': state['input'],\n",
    "            'messages': messages,\n",
    "        },\n",
    "        config\n",
    "    )\n",
    "    parsed = parser.batch(new_candidates)\n",
    "\n",
    "    flattened = [\n",
    "        (i, tool_call)\n",
    "        for i, tool_calls in enumerate(parsed)\n",
    "        for tool_call in tool_calls\n",
    "    ]\n",
    "\n",
    "    tool_responses = [\n",
    "        (\n",
    "            i,\n",
    "            tool_node.invoke(\n",
    "                {\n",
    "                    'messages': [\n",
    "                        AIMessage(\n",
    "                            content=\"\",\n",
    "                            tool_calls=[\n",
    "                                {\n",
    "                                    'name': tool_call['type'],\n",
    "                                    'args': tool_call['args'],\n",
    "                                    'id': tool_call['id'],\n",
    "                                }\n",
    "                            ],\n",
    "                        )\n",
    "                    ]\n",
    "                }\n",
    "            ),\n",
    "        )\n",
    "        for i, tool_call in flattened\n",
    "    ]\n",
    "\n",
    "    collected_responses = defaultdict(list)\n",
    "    for i, resp in tool_responses:\n",
    "        collected_responses[i].append(resp['messages'][0])\n",
    "\n",
    "    output_messages = []\n",
    "    for i, candidate in enumerate(new_candidates):\n",
    "        output_messages.append([candidate] + collected_responses[i])\n",
    "\n",
    "\n",
    "    # Reflect on each candidate\n",
    "    # For tasks with external validation, we'd add things here\n",
    "    reflections = reflection_chain.batch(\n",
    "        [{'input': state['input'], 'candidate': msgs} for msgs in output_messages],\n",
    "        config,\n",
    "    )\n",
    "\n",
    "    # Grow tree\n",
    "    child_nodes = [\n",
    "        Node(cand, parent=best_candidate, reflection=reflection)\n",
    "        for cand, reflection in zip(output_messages, reflections)\n",
    "    ]\n",
    "    best_candidate.children.extend(child_nodes)\n",
    "\n",
    "    # We have extended the tree above, so only return the state\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VblN9sQpKUJ"
   },
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 88,
     "status": "ok",
     "timestamp": 1733500189545,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "7cGfIPrWpJ-w"
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "\n",
    "\n",
    "def should_loop(state: TreeState):\n",
    "    \"\"\"Determine whether to continue the tree search\"\"\"\n",
    "    root = state['root']\n",
    "\n",
    "    if root.is_solved:\n",
    "        return END\n",
    "    if root.height > 5:\n",
    "        return END\n",
    "\n",
    "    return \"expand\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1733500189711,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "lEEcMwrnpeQ1"
   },
   "outputs": [],
   "source": [
    "builder = StateGraph(TreeState)\n",
    "# Add nodes\n",
    "builder.add_node('start', generate_initial_response)\n",
    "builder.add_node('expand', expand)\n",
    "# Add edges\n",
    "builder.add_edge(START, 'start')\n",
    "builder.add_conditional_edges(\n",
    "    'start',\n",
    "    # Either expand/rollout or finish\n",
    "    should_loop,\n",
    "    ['expand', END],\n",
    ")\n",
    "builder.add_conditional_edges(\n",
    "    'expand',\n",
    "    # Either continue to rollout or finish\n",
    "    should_loop,\n",
    "    ['expand', END],\n",
    ")\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "executionInfo": {
     "elapsed": 170,
     "status": "ok",
     "timestamp": 1733500189955,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "fxK30Kjfp76Q",
    "outputId": "db9ea7df-944c-40d4-dbe1-757cac8f5061"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcZ3JohxqA6N"
   },
   "source": [
    "# Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33407,
     "status": "ok",
     "timestamp": 1733500225415,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "uEo3PwKRp_Dp",
    "outputId": "06210ea2-ad57-4376-91ab-dd576912bff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "rolled out:  1\n",
      "---\n",
      "expand\n",
      "rolled out:  2\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Generate a table with the average size and weight,\n",
    "            as well as the oldest recorded instance for each of the top 5 most common birds.\n",
    "        \"\"\"\n",
    "\n",
    "last_step = None\n",
    "for step in graph.stream({\"input\": question}):\n",
    "    last_step = step\n",
    "    step_name, step_state = next(iter(step.items()))\n",
    "    print(step_name)\n",
    "    print(\"rolled out: \", step_state[\"root\"].height)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 85,
     "status": "ok",
     "timestamp": 1733500229207,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "R6uW_YAoqHvt",
    "outputId": "62b2cba7-725d-4712-81ee-c2871dd0e4ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information gathered, here is a table summarizing the average size and weight, as well as the oldest recorded instance for each of the top 5 most common birds:\n",
      "\n",
      "| Bird Species         | Average Size (Length) | Average Weight | Oldest Recorded Instance |\n",
      "|----------------------|----------------------|----------------|--------------------------|\n",
      "| Domestic Chicken     | 45-50 cm             | 1.5-4 kg       | 16 years                  |\n",
      "| Red-billed Quelea    | 12-13 cm             | 30-40 g        | 4 years                   |\n",
      "| Common Starling      | 20-23 cm             | 60-100 g       | 15 years                  |\n",
      "| House Sparrow        | 14-16 cm             | 24-39 g        | 13 years                  |\n",
      "| Barn Swallow         | 14-20 cm             | 16-20 g        | 8 years                   |\n",
      "\n",
      "### Notes:\n",
      "- The size and weight ranges are averages and can vary based on individual birds and their specific environments.\n",
      "- The oldest recorded instances are approximate and can vary based on different sources and studies.\n",
      "\n",
      "This table organizes the relevant data clearly to address your request. If you need more detailed information or additional birds, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "solution_node = last_step[\"expand\"][\"root\"].get_best_solution()\n",
    "best_trajectory = solution_node.get_trajectory(include_reflections=False)\n",
    "print(best_trajectory[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48043,
     "status": "ok",
     "timestamp": 1733500281219,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "-cI3JebeqL48",
    "outputId": "8ff8e9dd-7b04-4cd0-9f32-d6f504237c72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "rolled out:  1\n",
      "---\n",
      "expand\n",
      "rolled out:  2\n",
      "---\n",
      "expand\n",
      "rolled out:  3\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "question = \"Write out magnus carlson series of moves in his game against Alireza Firouzja and propose an alternate strategy\"\n",
    "last_step = None\n",
    "for step in graph.stream({\"input\": question}):\n",
    "    last_step = step\n",
    "    step_name, step_state = next(iter(step.items()))\n",
    "    print(step_name)\n",
    "    print(\"rolled out: \", step_state[\"root\"].height)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1733500281219,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "KaYFZgcmqMYz",
    "outputId": "cae48417-5f71-43c8-93c4-0f3865dec6c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Magnus Carlsen vs. Alireza Firouzja: Move List\n",
      "\n",
      "**Game:** SpeedChess Final 2024  \n",
      "**Opening:** French, Exchange Variation (C01)  \n",
      "**Result:** 1-0  \n",
      "\n",
      "1. e4 e6  \n",
      "2. d4 d5  \n",
      "3. exd5 exd5  \n",
      "4. Nf3 Bd6  \n",
      "5. c4 Nf6  \n",
      "6. c5 Be7  \n",
      "7. h3 O-O  \n",
      "8. Bd3 Nc6  \n",
      "9. O-O Ne4  \n",
      "10. Nc3 Bf5  \n",
      "11. Re1 Bf6  \n",
      "12. Nxe4 dxe4  \n",
      "13. Bxe4 Bxe4  \n",
      "14. Rxe4 Qd5  \n",
      "15. Rf4 Rad8  \n",
      "16. Be3 Rfe8  \n",
      "17. Qc2 Nb4  \n",
      "1-0\n",
      "\n",
      "In this game, Carlsen displayed his strategic prowess by maintaining a slight material advantage and managing the board effectively, eventually converting it into a win.\n",
      "\n",
      "### Proposed Alternate Strategy\n",
      "\n",
      "In analyzing the game, an alternate strategy for Firouzja could involve a more aggressive approach in the opening phase, particularly from move 6 onwards. Instead of playing 6...Be7, which is more passive, he could have considered:\n",
      "\n",
      "**6...f5:**  \n",
      "This move aims to take control of the center and create potential threats against White's kingside. It allows for more dynamic piece play and can lead to more tactical opportunities, putting pressure on Carlsen early in the game. \n",
      "\n",
      "Following this, Firouzja could aim to develop his pieces rapidly and look to create imbalances, potentially leading to more complex positions where Carlsen would need to navigate carefully.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Overall, while Carlsen's performance was impressive, an alternate strategy focusing on aggressive control of the center could have given Firouzja more chances to challenge the reigning champion.\n"
     ]
    }
   ],
   "source": [
    "solution_node = last_step[\"expand\"][\"root\"].get_best_solution()\n",
    "best_trajectory = solution_node.get_trajectory(include_reflections=False)\n",
    "print(best_trajectory[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHWbiK2oqmv4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOGbUuPdCyH9mQsFRkVR10f",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
