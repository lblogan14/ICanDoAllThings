{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPJoYyF3ZZlAc8uCoEGSC5s"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Fine-Tuning a Vision Language Model (Qwen2-VL-7B) with the HuggingFace Ecosystem (TRL)"],"metadata":{"id":"5LofuJl2b0Mb"}},{"cell_type":"markdown","source":["In this example, we will finetune a Vision Language Model (VLM) using the Transformer Reinforcement Leanring library (`trl`).\n","\n","We will finetune the [`Qwen2-VL-7B`](https://qwenlm.github.io/blog/qwen2-vl/) model on the [`ChatQA`](https://huggingface.co/datasets/HuggingFaceM4/ChartQA) dataset. This dataset includes images of various chart types paired with question-answer pairs - ideal for enchancing the model's visual question-answering capabilities."],"metadata":{"id":"kE_4de-pb6O1"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"Mdq8RDlqca6a"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KQYCaGf6bwS4"},"outputs":[],"source":["!pip install  -U -q git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git datasets bitsandbytes peft qwen-vl-utils wandb accelerate\n","# Tested with transformers==4.47.0.dev0, trl==0.12.0.dev0, datasets==3.0.2, bitsandbytes==0.44.1, peft==0.13.2, qwen-vl-utils==0.0.8, wandb==0.18.5, accelerate==1.0.1"]},{"cell_type":"code","source":["!pip install -q torch==2.4.1+cu121 torchvision==0.19.1+cu121 torchaudio==2.4.1+cu121 --extra-index-url https://download.pytorch.org/whl/cu121"],"metadata":{"id":"SHPQQNu_ceCV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load dataset"],"metadata":{"id":"i6AIlja8cfPe"}},{"cell_type":"markdown","source":["Before loading the dataset, we will generate a system message for the VLM. In this case, we want to create a system that acts as an expert in analyzing chart images and providing concise answers to questions based on them."],"metadata":{"id":"htO_mk2Ic9TJ"}},{"cell_type":"code","source":["system_message = \"\"\"You are a Vision Language Model specialized in interpreting visual data from chart images.\n","Your task is to analyze the provided chart image and respond to queries with concise answers, usually a single word, number, or short phrase.\n","The charts include a variety of types (e.g., line charts, bar charts) and contain colors, labels, and text.\n","Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.\"\"\""],"metadata":{"id":"hyiEgnNCcgYb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will format the dataset into a chatbot structure for interaction. Each interaction will consist of a system message, followed by the image and the user's query, and the answer to the query."],"metadata":{"id":"X2tn97lsdIW-"}},{"cell_type":"code","source":["def format_data(sample):\n","    return [\n","        {\n","            'role': 'system',\n","            'content': [{'type': 'text', 'text': system_message}]\n","        },\n","        {\n","            'role': 'user',\n","            'content': [\n","                {\n","                    'type': 'image',\n","                    'image': sample['image']\n","                },\n","                {\n","                    'type': 'text',\n","                    'text': sample['query']\n","                }\n","            ]\n","        },\n","        {\n","            'role': 'assistant',\n","            'content': [{'type': 'text', 'text': sample['label'][0]}]\n","        }\n","    ]"],"metadata":{"id":"OUqE7kzEdPcG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset_id = 'HuggingFaceM4/ChartQA'\n","train_dataset, eval_dataset, test_dataset = load_dataset(\n","    dataset_id,\n","    split=['train[:10%]', 'val[:10%]', 'test[:10%]']\n",")"],"metadata":{"id":"OXXdhWuNdrw7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset"],"metadata":{"id":"OI-Xhx4fd4O6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset[0]"],"metadata":{"id":"9SY7J3Kud5jh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we need to format the data using the our chatbot template, which will allow us to set up the interactions appropriately for our model."],"metadata":{"id":"7BO15GyWd6gf"}},{"cell_type":"code","source":["train_dataset = [format_data(sample) for sample in train_dataset]\n","eval_dataset = [format_data(sample) for sample in eval_dataset]\n","test_dataset = [format_data(sample) for sample in test_dataset]"],"metadata":{"id":"5NDBdDuNeCNl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset[0]"],"metadata":{"id":"uyU_oc4LeGpX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load model and check performance"],"metadata":{"id":"0lx5HN-6eI_e"}},{"cell_type":"markdown","source":["We can always check the [WildVision Arena](https://huggingface.co/WildVision) or the [OpenVLM Leaderboard](https://huggingface.co/spaces/opencompass/open_vlm_leaderboard) to find the best performing VLMs.\n","\n","In this example, we will use [`Qwen/Qwen2-VL-7B-Instruct`](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct)"],"metadata":{"id":"k9tTWqr_Aw0K"}},{"cell_type":"code","source":["import torch\n","from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n","\n","model_id = 'Qwen/Qwen2-VL-7B-Instruct'\n","\n","processor = Qwen2VLProcessor.from_pretrained(model_id)\n","model = Qwen2VLForConditionalGeneration.from_pretrained(\n","    model_id,\n","    device_map='auto',\n","    torch_dtype=torch.bfloat16\n",")"],"metadata":{"id":"pa8o4ZdPBCGy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To evaluate the model's performance, we will use a sample from the dataset."],"metadata":{"id":"C9SOt89IBffA"}},{"cell_type":"code","source":["sample = train_dataset[0]\n","sample"],"metadata":{"id":"9yH05QiSeK3c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We need to use the sample without the system message to assess the VLM's raw understanding. Hence, the input we will use:"],"metadata":{"id":"ew_kxclXBlkD"}},{"cell_type":"code","source":["sample[1:2]"],"metadata":{"id":"qzZQ2ckKBp8B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample[1]['content'][0]['image']"],"metadata":{"id":"m1b1n5e2BwVK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We need a function to take the model, processor, and sample as inputs to generate the model's answer."],"metadata":{"id":"2EG5W1ymB3M7"}},{"cell_type":"code","source":["from qwen_vl_utils import process_vision_info\n","\n","def generate_text_from_sample(model, processor, sample, max_new_tokens=1024, device='cuda'):\n","    # Prepare the text input by applying the chat template\n","    text_input = processor.apply_chat_template(\n","        sample[1:2], # use the sample without the system message\n","        tokenize=False,\n","        add_generation_prompt=True\n","    )\n","\n","    # Process the inputs from the sample\n","    image_inputs, _ = process_vision_info(sample)\n","\n","    # Prepare the inputs for the model\n","    model_inputs = processor(\n","        text=[text_input],\n","        images=image_inputs,\n","        return_tensors='pt'\n","    ).to(device)\n","\n","    # Generate text with the model\n","    generated_ids = model.generate(\n","        **model_inputs,\n","        max_new_tokens=max_new_tokens\n","    )\n","\n","    # Trim the generated ids to remove the input ids\n","    trimmed_generated_ids = [\n","        out_ids[len(in_ids) :]\n","        for in_ids, out_ids in zip (model_inputs['input_ids'], generated_ids)\n","    ]\n","\n","    # Decode the output text\n","    output_text = processor.batch_decode(\n","        trimmed_generated_ids,\n","        skip_special_tokens=True,\n","        clean_up_tokenization_spaces=False\n","    )\n","\n","    return output_text[0]"],"metadata":{"id":"sWXo90IVB7-7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = generate_text_from_sample(model, processor, sample)\n","output"],"metadata":{"id":"l2enGAFYCZPH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Remove model and clean GPU"],"metadata":{"id":"sAr8Zba2C08i"}},{"cell_type":"code","source":["import gc\n","import time\n","\n","\n","def clear_memory():\n","    # Delete variables if they exist in the current global scope\n","    if \"inputs\" in globals():\n","        del globals()[\"inputs\"]\n","    if \"model\" in globals():\n","        del globals()[\"model\"]\n","    if \"processor\" in globals():\n","        del globals()[\"processor\"]\n","    if \"trainer\" in globals():\n","        del globals()[\"trainer\"]\n","    if \"peft_model\" in globals():\n","        del globals()[\"peft_model\"]\n","    if \"bnb_config\" in globals():\n","        del globals()[\"bnb_config\"]\n","    time.sleep(2)\n","\n","    # Garbage collection and clearing CUDA memory\n","    gc.collect()\n","    time.sleep(2)\n","    torch.cuda.empty_cache()\n","    torch.cuda.synchronize()\n","    time.sleep(2)\n","    gc.collect()\n","    time.sleep(2)\n","\n","    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n","    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n","\n","\n","clear_memory()"],"metadata":{"id":"Z_8zWh3EC2YW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Finetune the model using TRL"],"metadata":{"id":"yhECoZZ0C5AJ"}},{"cell_type":"markdown","source":["### Load the quantized model for training"],"metadata":{"id":"ezpX0tpBnnq4"}},{"cell_type":"code","source":["from transformers import BitsAndBytesConfig\n","\n","# bitsandbytes int-4 config\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type='nf4',\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","processor = Qwen2VLProcessor.from_pretrained(model_id)\n","model = Qwen2VLForConditionalGeneration.from_pretrained(\n","    model_id,\n","    device_map='auto',\n","    torch_dtype=torch.bfloat16,\n","    quantization_config=bnb_config\n",")"],"metadata":{"id":"HPO4u7-3C7x7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Set up Q-LoRA and SFTConfig"],"metadata":{"id":"-SAo9u3yn9QD"}},{"cell_type":"markdown","source":["Q-LoRA enables efficient fine-tuning of large language models while significantly reducing the memory footprint compared to traditional methods. Unlike standard LoRA, which reduces memory usage by applying a low-rank approximation, Q-LoRA takes it a step further by quantizing the weights of the LoRA adapters. This leads to even lower memory requirements and improved training efficiency, making it an excellent choice for optimizing our model's performance without sacrificing quality."],"metadata":{"id":"2Xs3rc49oEky"}},{"cell_type":"code","source":["from peft import LoraConfig, get_peft_model\n","\n","peft_config = LoraConfig(\n","    lora_alpha=16,\n","    lora_dropout=0.05,\n","    r=8,\n","    bias='none',\n","    target_module=['q_proj', 'v_proj'],\n","    task_type='CAUSAL_LM'\n",")\n","\n","# Apply PEFT model adaptation\n","peft_model = get_peft_model(model, peft_config)\n","\n","peft_model.print_trainable_parameters()"],"metadata":{"id":"0zR4nPATn_cH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["SFT allows us to provide labeled data, helping the model learn to generate more accurate responses based on the input it receives. This approach ensures that the model is tailored to our specific use case, leading to better performance in understanding and responding to visual queries."],"metadata":{"id":"0u9LkWuoocQ-"}},{"cell_type":"code","source":["from trl import SFTConfig\n","\n","training_args = SFTConfig(\n","    output_dir='qwen2-7b-instruct-trl-sft-ChartQA',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=4, # batch size for training\n","    per_device_eval_batch_size=4, # batch size for evaluation\n","    gradient_accumulation_steps=8, # steps to accumulate gradients\n","    gradient_checkpointing=True,\n","\n","    # Optimizer\n","    optim='adamw_torch_fused',\n","    learning_rate=2e-4,\n","    lr_scheduler_type='constant',\n","\n","    # Logging\n","    logging_steps=10,\n","    eval_steps=10,\n","    eval_strategy='steps',\n","    save_strategy='steps',\n","    save_steps=20,\n","    metric_for_best_model='eval_loss',\n","    greater_is_better=False,\n","    load_best_model_at_end=True,\n","\n","    # Mixed precision\n","    bf16=True,\n","    tf32=True,\n","    max_grad_norm=0.3,\n","    warmup_ratio=0.03,\n","\n","    push_to_hub=False,\n","    report_to='wandb',\n","    gradient_checkpointing_kwargs={'use_reentrant': False},\n","    dataset_text_field='',\n","    dataset_kwargs={'skip_prepare_dataset': True}\n","    remove_unused_columns=False\n",")"],"metadata":{"id":"Us0nuQZ1ofxf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train the model"],"metadata":{"id":"8V-xDOoxpaAo"}},{"cell_type":"code","source":["import wandb\n","\n","wandb.init(\n","    project='qwen2-7b-instruct-trl-sft-ChartQA',\n","    name='qwen2-7b-instruct-trl-sft-ChartQA',\n","    config=training_args\n",")"],"metadata":{"id":"ZnYXvvSLpcQs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We also need a collator function to properly retrieve and batch the data during the training procedure. This function will handle the formatting of our dataset inputs, ensuring they are correctly structured for the model."],"metadata":{"id":"OwkQ2Ws4pjMT"}},{"cell_type":"code","source":["def collate_fn(examples):\n","    texts = [\n","        processor.apply_chat_template(example, tokenize=False)\n","        for example in examples\n","    ]\n","    image_inputs = [\n","        process_vision_info(example)[0] for example in examples\n","    ]\n","\n","    # Tokenize the texts and process the images\n","    batch = processor(\n","        text=texts,\n","        images=image_inputs,\n","        padding=True,\n","        return_tensors='pt'\n","    )\n","\n","    # The labels are the input_ids, and we mask the padding tokens in the loss computation\n","    labels = batch['input_ids'].clone()\n","    labels[labels == processor.tokenizer.pad_token_id] = -100\n","\n","    # Ignore the image token index in the loss computation (model specific)\n","    if isinstance(processor, Qwen2VLProcessor):\n","        image_tokens = [151652, 151653, 151655]  # Specific image token IDs for Qwen2VLProcessor\n","    else:\n","        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]  # Convert image token to ID\n","\n","    # Mask image token IDs in the labels\n","    for image_token_id in image_token:\n","        labels[labels == image_token_id] = -100\n","\n","    batch['labels'] = labels # Add labels to the batch\n","\n","    return batch"],"metadata":{"id":"5w9xECG1psKO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We now define the `SFTTrainer`."],"metadata":{"id":"Ki7fZ_YsqyWe"}},{"cell_type":"code","source":["from trl import SFTTrainer\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    data_collator=collate_fn,\n","    peft_config=peft_config,\n","    tokenizer=processor.tokenizer\n",")"],"metadata":{"id":"w3u0HwqRq07c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"89kWxrJcq_pg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.save_model(training_args.output_dir)"],"metadata":{"id":"PASwAIuErBJe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test the finetuned model"],"metadata":{"id":"UomsuHHnrCKe"}},{"cell_type":"code","source":["clear_memory()"],"metadata":{"id":"bYxXdgXyrD_k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Qwen2VLForConditionalGeneration.from_pretrained(\n","    model_id,\n","    device_map=\"auto\",\n","    torch_dtype=torch.bfloat16,\n",")\n","\n","processor = Qwen2VLProcessor.from_pretrained(model_id)"],"metadata":{"id":"IPe_qGYZrGoS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["adapter_path = \"sergiopaniego/qwen2-7b-instruct-trl-sft-ChartQA\"\n","model.load_adapter(adapter_path)"],"metadata":{"id":"SpKO9qZbrHED"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample = train_dataset[0]\n","sample[:2]"],"metadata":{"id":"7t1Un6OorJKC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample[1]['content'][0]['image']"],"metadata":{"id":"nuLvT-VRrLwz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = generate_text_from_sample(model, processor, sample)\n","output"],"metadata":{"id":"sRy6BA2wrPPn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["smaple = test_dataset[0]\n","sample[:2]"],"metadata":{"id":"THa54m84rSlQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample[1]['content'][0]['image']"],"metadata":{"id":"YAbslPozrVEd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = generate_text_from_sample(model, processor, sample)\n","output"],"metadata":{"id":"n1yRAAZDrX6K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Compare finetuned model with base model + prompting"],"metadata":{"id":"qSRCStGHraKG"}},{"cell_type":"code","source":["clear_memory()"],"metadata":{"id":"Sp912l3Arcwu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# base model\n","model = Qwen2VLForConditionalGeneration.from_pretrained(\n","    model_id,\n","    device_map=\"auto\",\n","    torch_dtype=torch.bfloat16,\n",")\n","\n","processor = Qwen2VLProcessor.from_pretrained(model_id)"],"metadata":{"id":"624Ey4wKre2o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample = train_dataset[0]\n","sample[:2]"],"metadata":{"id":"QeP6h7NTrgza"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = processor.apply_chat_template(\n","    sample[:2], # we pass the system message to the base model this time\n","    tokenize=False,\n","    add_generation_prompt=True\n",")\n","\n","image_inputs, _ = process_vision_info(train_dataset[0])\n","\n","inputs = processor(\n","    text=[text],\n","    images=image_inputs,\n","    return_tensors=\"pt\",\n",")\n","\n","inputs = inputs.to(\"cuda\")\n","\n","generated_ids = model.generate(**inputs, max_new_tokens=1024)\n","generated_ids_trimmed = [\n","    out_ids[len(in_ids) :]\n","    for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n","]\n","\n","output_text = processor.batch_decode(\n","    generated_ids_trimmed,\n","    skip_special_tokens=True,\n","    clean_up_tokenization_spaces=False\n",")\n","\n","output_text[0]"],"metadata":{"id":"80tfTbv-rkKP"},"execution_count":null,"outputs":[]}]}