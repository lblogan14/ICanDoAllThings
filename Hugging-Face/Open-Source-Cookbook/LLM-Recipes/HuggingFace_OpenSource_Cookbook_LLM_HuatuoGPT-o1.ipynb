{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNNKoNjTVNUTlhC/6kouJKs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# HuatuoGPT-o1 Medical RAG and Reasoning"],"metadata":{"id":"nunbFhx7K09Z"}},{"cell_type":"markdown","source":["In this example, we will explore an end-to-end system using HuatuoGPT-o1 for medical question answering with RAG and reasoning. We will leverage the HuatuoGPT-o1 model, a medical LLM designed for advanced medical reasoning, to provide detailed and well-structured answers to medical queries.\n","\n","[**HuatuoGPT-o1**](https://huggingface.co/FreedomIntelligence/HuatuoGPT-o1-7B) is a medical LLM that excels at identifying mistakes, exploring alternative strategies, and refining its answers. It utilizes verifiable medical problems and a specialized medical verifier to enhance its reasoning capabilities."],"metadata":{"id":"YcTY10IEK4Nx"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"tOohl4UiLk0C"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ShzoLQ9kKviF"},"outputs":[],"source":["!pip install -qU transformers datasets sentence-transformers scikit-learn"]},{"cell_type":"markdown","source":["## Load the dataset"],"metadata":{"id":"hccl0mxiLpJh"}},{"cell_type":"markdown","source":["We will use the [`Chat-Doctor-HealthCareMagic-100k`](https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k) dataset, which contains 100K real-world patient-doctor interactions, providing a rich knowledge base for our RAG system."],"metadata":{"id":"r44yg1sTLt7S"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"lavita/Chat-Doctor-HealthCareMagic-100k\")"],"metadata":{"id":"ecWvCcI2LqAr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Initialize the models"],"metadata":{"id":"eFOwLdnnL9Jw"}},{"cell_type":"markdown","source":["We need to initialize two models:\n","- `HuotuoGPT-o1`: the medical LLM for generating responses.\n","- `all-MiniLM-L6-v2`: an embedding model from sentence transformers for creating vector representations of text, which we will use for retrieval."],"metadata":{"id":"IE-CfVZHRnSQ"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from sentence_transformers import SentenceTransformer\n","\n","model_name = 'FreedomIntelligence/HuatuoGPT-o1-7B'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    torch_dtype='auto',\n","    device_map='auto'\n",")\n","\n","embed_model = SentenceTransformer('all-MiniLM-L6-v2')"],"metadata":{"id":"HNGNp6swL-nV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prepare the knowledge base"],"metadata":{"id":"GKSI1BhGSPIH"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","# convert dataset to dataframe\n","df = pd.DataFrame(dataset['train'])\n","\n","# combine question and answer for context\n","df['combined'] = df['input'] + ' ' + df['output']\n","\n","# generate embeddings\n","print('Generating embeddings for the knowledge base...')\n","embeddings = embed_model.encode(\n","    df['combined'].tolist(),\n","    show_progress_bar=True,\n","    batch_size=128\n",")\n","print('Embeddings generated.')"],"metadata":{"id":"pQUemnh_SSHV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Implement retrieval"],"metadata":{"id":"SSaaUoKVSqao"}},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","def retrieve_relevant_contexts(query: str, k: int = 3) -> list:\n","    \"\"\"Retrieve the k most relevant contexts to a given query.\n","\n","    Parameters\n","    ----------\n","    query : str\n","        The query to retrieve relevant contexts for.\n","    k : int\n","        The number of relevant contexts to retrieve.\n","\n","    Returns\n","    -------\n","    list\n","        A list of dictionaries, each containing a relevant context.\n","    \"\"\"\n","    # generate query embedding\n","    query_embedding = embed_model.encode([query])[0]\n","\n","    # calculate similarities\n","    similarities = cosine_similarity([query_embedding], embeddings)[0]\n","\n","    # get top-k similar contexts\n","    top_k_indices = np.argsort(similarities)[-k:][::-1]\n","\n","    contexts = []\n","    for idx in top_k_indices:\n","        contexts.append({\n","            'question': df.iloc[idx]['input'],\n","            'answer': df.iloc[idx]['output'],\n","            'similarity': similarities[idx]\n","        })\n","\n","    return contexts"],"metadata":{"id":"OMMU2X4GSsQG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Implement response generation"],"metadata":{"id":"uL1AhUS6TbnF"}},{"cell_type":"code","source":["def generate_structured_response(query: str, contexts: list) -> str:\n","    \"\"\"Generate a detailed response using the retrieved contexts.\n","\n","    Parameters\n","    ----------\n","    query : str\n","        The query to generate a response for.\n","    contexts : list\n","        A list of dictionaries, each containing a relevant context.\n","\n","    Returns\n","    -------\n","    str\n","        The generated structured response.\n","    \"\"\"\n","    # prepare prompt with retrieved contexts\n","    context_prompt = \"\\n\".join(\n","        [\n","            f\"Reference {i+1}:\\nQuestion: {ctx['question']}\\nAnswer: {ctx['answer']}\"\n","            for i, ctx in enumerate(contexts)\n","        ]\n","    )\n","\n","    prompt = f\"\"\"Based on the following references and your medical knowledge, provide a detailed response:\n","    References:\n","    {context_prompt}\n","\n","    Question: {query}\n","\n","    By considering:\n","    1. The key medical concepts in the question.\n","    2. How the reference cases relate to this question.\n","    3. What medical principles should be applied.\n","    4. Any potential complications or considerations.\n","\n","    Give the final response:\n","    \"\"\"\n","\n","    # generate response\n","    messages = [{'role': 'user', 'content': prompt}]\n","    inputs = tokenizer(\n","        tokenizer.apply_chat_template(\n","            messages,\n","            tokenize=False,\n","            add_generation_prompt=True\n","        ),\n","        return_tensors='pt'\n","    ).to(model.device)\n","\n","    outputs = model.generate(\n","        **inputs,\n","        max_new_tokens=1024,\n","        temperature=0.7,\n","        num_beams=1,\n","        do_sample=True,\n","    )\n","\n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    # extract the final response\n","    final_response = response.split(\"Give the final response:\\n\")[-1]\n","\n","    return final_response"],"metadata":{"id":"RDgVSVioTdIP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Put it all together"],"metadata":{"id":"vwXX-MjCUetK"}},{"cell_type":"markdown","source":["Define a function to process a query end-to-end and then use it with an example."],"metadata":{"id":"i5S3uMVPUhBM"}},{"cell_type":"code","source":["def process_query(query: str, k: int = 3) -> tuple:\n","    \"\"\"Process a medical query end-to-end\n","\n","    Parameters\n","    ----------\n","    query: str\n","        The user's medical query\n","    k: int\n","        The number of relevant contexts to retrieve\n","\n","    Returns\n","    -------\n","    tuple\n","        The generated response and the retrieved contexts\n","    \"\"\"\n","    contexts = retrieve_relevant_contexts(query, k)\n","    response = generate_structured_response(query, contexts)\n","    return response, contexts"],"metadata":{"id":"G2St1X9WUgAJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# example query\n","query = \"I've been experiencing persistent headaches and dizziness for the past week. What could be the cause?\"\n","\n","# process query\n","response, contexts = process_query(query)\n","\n","# print results\n","print(f'Query: {query}')\n","print('\\nRelevant Contexts:')\n","for i, ctx in enumerate(contexts, 1):\n","    print(f\"\\nReference {i} (Similarity: {ctx['similarity']:.3f}):\")\n","    print(f\"Q: {ctx['question']}\")\n","    print(f\"A: {ctx['answer']}\")\n","\n","print('\\nGenerated Response:')\n","print(response)"],"metadata":{"id":"DMl6fZkHVDxu"},"execution_count":null,"outputs":[]}]}