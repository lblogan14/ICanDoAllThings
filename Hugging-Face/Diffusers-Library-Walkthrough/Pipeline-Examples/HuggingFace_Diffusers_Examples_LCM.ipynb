{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPORXSJ1wireou11KfirACK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Latent Consistency Model"],"metadata":{"id":"1Eb8yjJs4ghZ"}},{"cell_type":"markdown","source":["**Latent Consistency Models (LCMs) enable fast high-quality image generation by directly predicting the reverse diffusion process in the latent rather than pixel space.\n","\n","LCMs try to predict the noiseless image from the noisy image in contrast to typical diffusion models that iteratively remove noise from the noisy image. By avoiding the iterative sampling process, LCMs are able to generate high-quality images in 2-4 steps instead of 20-30 steps.\n","\n","LCMs are distilled from pretrained models which requires ~32 hours of A100 compute. To speed this up, `LCM-LoRAs` train a `LoRA adapter` which have much fewer parameters to train compared to the full models. The LCM-LoRA can be plugged into a diffusion model once it has been trained."],"metadata":{"id":"A0ng3emrMCfJ"}},{"cell_type":"markdown","source":["## Text-to-image"],"metadata":{"id":"xmFmIw7hMvRr"}},{"cell_type":"markdown","source":["##### LCM"],"metadata":{"id":"TKkHJ3DSMyiE"}},{"cell_type":"markdown","source":["To use LCMs, we need to load the LCM checkpoint for our supported model into `UNet2DConditonModel` and replace the scheduler with the `LCMScheduler`.\n","\n","Note to LCM:\n","* Batch size is doubled inside the pipeline for classifier-free guidance, but LCM applies guidance with guidance embeddings and does not need to double the batch size, which leads to faster inference. The downside is that negative prompts do not work with LCM because they do not have any effect on the denoising process.\n","* The ideal range for `guidance_scale` is between 3 and 13 because that is what the UNet was trained with. However, disabling `guidance_scale` with a a value of 1.0 is also effective in most cases."],"metadata":{"id":"SSpdN5cbM1iU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZmv5moV4cBL"},"outputs":[],"source":["from diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, LCMScheduler\n","import torch\n","\n","unet = UNet2DConditionModel.from_pretrained(\n","    'latent-consistency/lcm-sdxl',\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",")\n","\n","pipe = StableDiffusionXLPipeline.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    unet=unet,\n","    torch_dtype=torch.float16,\n","    variant='fp16\n",").to('cuda')\n","pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)"]},{"cell_type":"code","source":["prompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n","generator = torch.manual_seed(111)\n","\n","image = pipe(\n","    prompt,\n","    num_inference_steps=4, # LCM needs fewer steps\n","    guidance_scale=8.0,\n","    generator=generator,\n",").images[0]\n","image"],"metadata":{"id":"bFECoDsWN0x_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = pipe(\n","    prompt,\n","    num_inference_steps=4, # LCM needs fewer steps\n","    guidance_scale=1.0, # disable guidance_scale\n","    generator=generator,\n",").images[0]\n","image"],"metadata":{"id":"tya_ZSPWOENI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### LCM-LoRA"],"metadata":{"id":"KqcHpMfDOJM3"}},{"cell_type":"markdown","source":["To use LCM-LoRAs, we need to replace the scheduler with the `LCMScheduler` and load the LCM-LoRA weights with the `load_lora_weights`.\n","\n","Note to LCM-LoRA:\n","* Batch size is doubled inside the pipeline for CFG.\n","* We should use guidance with LCM-LoRAs, but it is very sensitive to high `guidance_scale` values and can lead to artifacts in the generated image. The best values are between 1 and 2.\n","* Replacing `stabilityai/stable-diffusion-xl-base-1.0` with any finetuned model should be fine."],"metadata":{"id":"5ZCaDuQDOLlJ"}},{"cell_type":"code","source":["from diffusers import DiffusionPipeline, LCMScheduler\n","import torch\n","\n","pipe = DiffusionPipeline.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",").to('cuda')\n","pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n","# load LCM-LoRA\n","pipe.load_lora_weights('latent-consistency/lcm-lora-sdxl')"],"metadata":{"id":"a3GEnI65OKmW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n","generator = torch.manual_seed(111)\n","\n","image = pipe(\n","    prompt,\n","    num_inference_steps=4,\n","    guidance_scale=1.0,\n","    generator=generator,\n",").images[0]\n","image"],"metadata":{"id":"9aVvWDbeWU0w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = pipe(\n","    prompt,\n","    num_inference_steps=4,\n","    guidance_scale=2.0,\n","    generator=generator,\n",").images[0]\n","image"],"metadata":{"id":"AqBdTdxZWcA-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Image-to-image"],"metadata":{"id":"4rO5lVJ0WepV"}},{"cell_type":"markdown","source":["##### LCM"],"metadata":{"id":"9CwTKbviWgZK"}},{"cell_type":"markdown","source":["To use LCMs for image-to-image, we need to load the LCM checkpoint for our supported model into `UNet2DConditionModel` and replace the scheduler with the `LCMScheduler`."],"metadata":{"id":"ZJKadoQfWiC7"}},{"cell_type":"code","source":["from diffusers import AutoPipelineForImage2Image, UNet2DConditionModel, LCMScheduler\n","from diffusers.utils import load_image, make_image_grid\n","import torch\n","\n","unet = UNet2DConditionModel.from_pretrained(\n","    'SimianLuo/LCM_Dreamshaper_v7',\n","    subfolder='unet',\n","    torch_dtype=torch.float16\n",")\n","\n","pipe = AutoPipelineForImage2Image.from_pretrained(\n","    'Lykon/dreamshaper-7',\n","    unet=unet, # LCM\n","    torch_dtype=torch.float16,\n","    variant='fp16',\n",").to('cuda')\n","pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)"],"metadata":{"id":"l6_JKVngWf4N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["init_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\")\n","prompt = \"Astronauts in a jungle, cold color palette, muted colors, detailed, 8k\"\n","generator = torch.manual_seed(0)\n","\n","image = pipe(\n","    prompt,\n","    image=init_image,\n","    num_inference_steps=4,\n","    guidance_scale=7.5,\n","    strength=0.5,\n","    generator=generator\n",").images[0]\n","make_image_grid([init_image, image], rows=1, cols=2)"],"metadata":{"id":"wlj4pNlfXMfE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### LCM-LoRA"],"metadata":{"id":"cXYTZ2PvXWnn"}},{"cell_type":"markdown","source":["To use LCM-LoRA for image-to-image, we need to replace the scheduler with the `LCMScheduler` and load the LCM-LoRA weights with the `load_lora_weights`."],"metadata":{"id":"3TQf7IjRXYqL"}},{"cell_type":"code","source":["from diffusers import AutoPipelineForImage2Image, LCMScheduler\n","from diffusers.utils import load_image, make_image_grid\n","import torch\n","\n","pipe = AutoPipelineForImage2Image.from_pretrained(\n","    'Lykon/dreamshaper',\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",").to('cuda')\n","pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n","# load LCM-LoRA\n","pipe.load_lora_weights('latent-consistency/lcm-lora-sdv1-5')"],"metadata":{"id":"M89iRsS9XX8m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["init_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\")\n","prompt = \"Astronauts in a jungle, cold color palette, muted colors, detailed, 8k\"\n","generator = torch.manual_seed(0)\n","\n","image = pipe(\n","    prompt,\n","    image=init_image,\n","    num_inference_steps=4,\n","    guidance_scale=1,\n","    strength=0.6,\n","    generator=generator\n",").images[0]\n","make_image_grid([init_image, image], rows=1, cols=2)"],"metadata":{"id":"6GTjEyhlYAgD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Inpainting"],"metadata":{"id":"816R8WpJYMTO"}},{"cell_type":"markdown","source":["To use LCM-LoRAs for inpainting, we need to replace the scheduler with the `LCMScheduler` and load the LCM-LoRA weights with the `load_lora_weights`."],"metadata":{"id":"VMIhJRZ_rk6c"}},{"cell_type":"code","source":["from diffusers import AutoPipelineForInpainting, LCMScheduler\n","from diffusers.utils import load_image, make_image_grid\n","import torch\n","\n","pipe = AutoPipelineForInpainting.from_pretrained(\n","    'runwayml/stable-diffusion-inpainting',\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",").to('cuda')\n","\n","pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n","# load LCM-LoRA\n","pipe.load_lora_weights('latent-consistency/lcm-lora-sdv1-5')"],"metadata":{"id":"mamxckULYNZx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["init_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\n","mask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")\n","\n","prompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\n","generator = torch.manual_seed(111)\n","\n","image = pipe(\n","    prompt,\n","    image=init_image,\n","    mask_image=mask_image,\n","    generator=generator,\n","    num_infernece_steps=4,\n","    guidance_scale=4,\n",").images[0]\n","make_image_grid([init_image, mask_image, image], rows=1, cols=3)"],"metadata":{"id":"Wkf10luYr-OF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Adapters"],"metadata":{"id":"GXF04pjKsMAN"}},{"cell_type":"markdown","source":["LCMs are compatible with adapters like LoRA, ControlNet, T2I-Adapter, and AnimateDiff. We can bring the speed of LCM to these adapters to generate images in a certain style or condition the model on another input."],"metadata":{"id":"nSlMwtgMsTDZ"}},{"cell_type":"markdown","source":["### LoRA"],"metadata":{"id":"AAY6Yb78siJZ"}},{"cell_type":"markdown","source":["##### LCM"],"metadata":{"id":"taLWDML4stcB"}},{"cell_type":"markdown","source":["* Load the LCM checkpoint into `UNet2DConditionModel` and replace the scheduler with the `LCMScheduler`\n","* Use the `load_lora_weights` to load the LoRA weights"],"metadata":{"id":"Qjdh8CJQtFmm"}},{"cell_type":"code","source":["from diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, LCMScheduler\n","import torch\n","\n","unet = UNet2DConditionModel.from_pretrained(\n","    'latent-consistency/lcm-sdxl',\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",")\n","\n","pipe = StableDiffusionXLPipeline.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    unet=unet,\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",").to('cuda')\n","pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n","\n","pipe.load_lora_weights(\n","    'TheLastBen/Papercut_SDXL',\n","    weight_name='papercut.safetensors',\n","    adapter_name='papercut'\n",")"],"metadata":{"id":"abo91e4QsSH_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"papercut, a cute fox\"\n","generator = torch.manual_seed(0)\n","\n","image = pipe(\n","    prompt,\n","    num_inference_steps=4,\n","    generator=generator,\n","    guidance_scale=8.0\n",").images[0]\n","image"],"metadata":{"id":"a1c2kAPXtbXs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### LCM-LoRA"],"metadata":{"id":"oGp1COAhtj97"}},{"cell_type":"markdown","source":["* Replace the scheduler with the `LCMScheduler`\n","* Use the `load_lora_weights` to load LCM-LoRA weights and the style LoRA.\n","* Combine both LoRA adapters"],"metadata":{"id":"neUW5QYkt9Ms"}},{"cell_type":"code","source":["from diffusers import DiffusionPipeline, LCMScheduler\n","import torch\n","\n","pipe = DiffusionPipeline.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",").to('cuda')\n","pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n","\n","pipe.load_lora_weights(\n","    'latent-consistency/lcm-lora-sdxl',\n","    adapter_name='lcm'\n",")\n","pipe.load_lora_weights(\n","    'TheLastBen/Papercut_SDXL',\n","    weight_name='papercut.safetensors',\n","    adapter_name='papercut'\n",")\n","\n","pipe.set_adapters(\n","    ['lcm', 'papercut'],\n","    adapter_weights=[1.0, 0.8]\n",")"],"metadata":{"id":"jAsZH6SFtmbE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"papercut, a cute fox\"\n","generator = torch.manual_seed(0)\n","image = pipe(\n","    prompt,\n","    num_inference_steps=4,\n","    guidance_scale=1, # note the scale here\n","    generator=generator\n",").images[0]\n","image"],"metadata":{"id":"hviGqgOqumwd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ControlNet"],"metadata":{"id":"F85c-GFJuvI9"}},{"cell_type":"markdown","source":["##### LCM"],"metadata":{"id":"zWCXhjGSxOsz"}},{"cell_type":"code","source":["from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, LCMScheduler\n","from diffusers.utils import load_image, make_image_grid\n","import torch\n","import cv2\n","import numpy as np\n","from PIL import Image\n","\n","controlnet = ControlNetModel.from_pretrained(\n","    'lllyasviel/sd-controlnet-canny',\n","    torch_dtype=torch.float16,\n",")\n","\n","pipe = StableDiffusionControlNetPipeline.from_pretrained(\n","    \"SimianLuo/LCM_Dreamshaper_v7\",\n","    controlnet=controlnet,\n","    torch_dtype=torch.float16,\n",").to('cuda')\n","pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)"],"metadata":{"id":"RdwzJ7NvuxNS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["original_image = load_image(\n","    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",").resize((512, 512))\n","\n","image = np.array(original_image)\n","\n","low_threshold = 100\n","high_threshold = 200\n","\n","image = cv2.Canny(image, low_threshold, high_threshold)\n","image = image[:, :, None]\n","image = np.concatenate([image, image, image], axis=2)\n","canny_image = Image.fromarray(image)"],"metadata":{"id":"mFLTbI4zxzis"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = 'the mona lisa'\n","generator = torch.manual_seed(111)\n","\n","image = pipe(\n","    prompt,\n","    image=canny_image,\n","    num_inference_steps=4,\n","    generator=generator,\n","    guidance_scale=7.5\n",").images[0]\n","make_image_grid([original_image, canny_image, image], rows=1, cols=3)"],"metadata":{"id":"1yQOU23dx6FO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### LCM-LoRA"],"metadata":{"id":"fvv2m6eHyLgY"}},{"cell_type":"code","source":["from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, LCMScheduler\n","from diffusers.utils import load_image, make_image_grid\n","import torch\n","import cv2\n","import numpy as np\n","from PIL import Image\n","\n","controlnet = ControlNetModel.from_pretrained(\n","    \"lllyasviel/sd-controlnet-canny\",\n","    torch_dtype=torch.float16\n",")\n","\n","pipe = StableDiffusionControlNetPipeline.from_pretrained(\n","    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", # this is regular sd15\n","    controlnet=controlnet,\n","    torch_dtype=torch.float16,\n","    safety_checker=None,\n","    variant=\"fp16\"\n",").to(\"cuda\")\n","pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n","\n","pipe.load_lora_weights('latent-consistency/lcm-lora-sdv1-5')"],"metadata":{"id":"5HhnT7RdyMx4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["original_image = load_image(\n","    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",").resize((512, 512))\n","\n","image = np.array(original_image)\n","\n","low_threshold = 100\n","high_threshold = 200\n","\n","image = cv2.Canny(image, low_threshold, high_threshold)\n","image = image[:, :, None]\n","image = np.concatenate([image, image, image], axis=2)\n","canny_image = Image.fromarray(image)"],"metadata":{"id":"WAwZQMZ-1_Wc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = 'the mona lisa'\n","generator = torch.manual_seed(111)\n","\n","image = pipe(\n","    prompt,\n","    image=canny_image,\n","    num_inference_steps=4,\n","    guidance_scale=1.5, # note the scale here\n","    controlnet_conditioning_scale=0.8,\n","    cross_attention_kwargs={'scale': 1},\n","    generator=generator\n",").images[0]\n","make_image_grid([original_image, canny_image, image], rows=1, cols=3)"],"metadata":{"id":"iulFN9GD2Dtm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### T2I-Adapter"],"metadata":{"id":"qxEiXZm72Uzf"}},{"cell_type":"markdown","source":["##### LCM"],"metadata":{"id":"KbPx0MFd2gnJ"}},{"cell_type":"code","source":["from diffusers import StableDiffusionXLAdapterPipeline, UNet2DConditionModel, T2IAdapter, LCMScheduler\n","from diffusers.utils import load_image, make_image_grid\n","import torch\n","import cv2\n","import numpy as np\n","from PIL import Image"],"metadata":{"id":"uph9ZvvO2Xhr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["adapter = T2IAdapter.from_pretrained(\n","    'TencentARC/t2i-adapter-canny-sdxl-1.0',\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",").to('cuda')\n","\n","unet = UNet2DConditionModel.from_pretrained(\n","    'latent-consistency/lcm-sdxl',\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",")\n","\n","pipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    unet=unet,\n","    adapter=adapter,\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",").to('cuda')\n","pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)"],"metadata":{"id":"fSCAkwMu21U-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["original_image = load_image(\n","    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",").resize((384, 384))\n","\n","image = np.array(original_image)\n","\n","low_threshold = 100\n","high_threshold = 200\n","\n","image = cv2.Canny(image, low_threshold, high_threshold)\n","image = image[:, :, None]\n","image = np.concatenate([image, image, image], axis=2)\n","canny_image = Image.fromarray(image).resize((1024, 1024))"],"metadata":{"id":"uIR9m_7o22Zf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"the mona lisa, 4k picture, high quality\"\n","negative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n","\n","generator = torch.manual_seed(111)\n","\n","image = pipe(\n","    prompt,\n","    negative_prompt=negative_prompt,\n","    image=canny_image,\n","    num_inference_steps=4,\n","    guidance_scale=5,\n","    adapter_conditioning_scale=0.8,\n","    adapter_conditioning_factor=1,\n","    generator=generator\n",").images[0]\n","make_image_grid([original_image, canny_image, image], rows=1, cols=3)"],"metadata":{"id":"5Qz2co-a3XAb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### LCM-LoRA"],"metadata":{"id":"PxiQZLV13mcR"}},{"cell_type":"code","source":["from diffusers import StableDiffusionXLAdapterPipeline, UNet2DConditionModel, T2IAdapter, LCMScheduler\n","from diffusers.utils import load_image, make_image_grid\n","import torch\n","import cv2\n","import numpy as np\n","from PIL import Image"],"metadata":{"id":"YoHKvkIT3n0H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["adapter = T2IAdapter.from_pretrained(\n","    'TencentARC/t2i-adapter-canny-sdxl-1.0',\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",").to('cuda')\n","\n","pipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    adapter=adapter,\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",").to('cuda')\n","pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n","\n","pipe.load_lora_weights('latent-consistency/lcm-lora-sdxl')"],"metadata":{"id":"T020PuKI3tn4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["original_image = load_image(\n","    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",").resize((384, 384))\n","\n","image = np.array(original_image)\n","\n","low_threshold = 100\n","high_threshold = 200\n","\n","image = cv2.Canny(image, low_threshold, high_threshold)\n","image = image[:, :, None]\n","image = np.concatenate([image, image, image], axis=2)\n","canny_image = Image.fromarray(image).resize((1024, 1024))"],"metadata":{"id":"axgIWcpL3t0Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"the mona lisa, 4k picture, high quality\"\n","negative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n","\n","generator = torch.manual_seed(111)\n","\n","image = pipe(\n","    prompt,\n","    negative_prompt=negative_prompt,\n","    image=canny_image,\n","    num_inference_steps=4,\n","    guidance_scale=1.5,\n","    adapter_conditioning_scale=0.8,\n","    adapter_conditioning_factor=1,\n","    generator=generator\n",").images[0]\n","make_image_grid([original_image, canny_image, image], rows=1, cols=3)"],"metadata":{"id":"yMgZWWXa36Sc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### AnimateDiff"],"metadata":{"id":"NQww72UA4CYU"}},{"cell_type":"code","source":["from diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler, LCMScheduler\n","from diffusers.utils import export_to_gif\n","import torch\n","\n","adapter = MotionAdapter.from_pretrained(\n","    'guoyww/animatediff-motion-adapter-v1-5'\n",")\n","\n","pipe = AnimateDiffPipeline.from_pretrained(\n","    'frankjoshua/toonyou_beta6',\n","    motion_adapter=adapter,\n",").to('cuda')\n","pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n","\n","# load LCM-LoRA\n","pipe.load_lora_weights(\n","    'latent-consistency/lcm-lora-sdv1-5',\n","    adapter_name='lcm'\n",")\n","\n","# load animatediff lora\n","pipe.load_lora_weights(\n","    'guoyww/animatediff-motion-lora-zoom-in',\n","    weight_name='diffusion_pytorch_model.safetensors',\n","    adapter_name='motion-lora'\n",")\n","\n","pipe.set_adapters(\n","    ['lcm', 'motion-lora'],\n","    adapter_weights=[0.55, 1.2]\n",")"],"metadata":{"id":"fWAvvKJP4LUD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"best quality, masterpiece, 1girl, looking at viewer, blurry background, upper body, contemporary, dress\"\n","generator = torch.manual_seed(111)\n","\n","frames = pipe(\n","    prompt,\n","    num_inference_steps=5,\n","    guidance_scale=1.25,\n","    cross_attention_kwargs={'scale': 1},\n","    num_frames=24,\n","    generator=generator,\n",").frames[0]\n","export_to_gif(frames, 'animate.gif')"],"metadata":{"id":"3sWNRMVI41KQ"},"execution_count":null,"outputs":[]}]}