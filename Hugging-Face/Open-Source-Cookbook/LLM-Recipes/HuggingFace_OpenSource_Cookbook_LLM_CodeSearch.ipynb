{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP11xJBV6lFAUVrE/2VMsAl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Code Search with Vector Embeddings and Qdrant"],"metadata":{"id":"eWqsZqsb5kDm"}},{"cell_type":"markdown","source":["In this example, we will use vector embeddings to navigate a codebase, and find relevant code sniipets. We will search codebases using natural semantic queries, and search for code based on a smiliar logic. There is a [live deployment](https://code-search.qdrant.tech/) from the Qdrant codebase for search with a web interface.\n","\n","We need two models to accomplish our goal:\n","* NLP model - general usage neural encoder for NLP, in this example, we will use [`sentence-transformers/all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).\n","* Code model - specialized embeddings for code-to-code similarity search. We will use [`jinaai/jina-embeddings-v2-base-code`](https://huggingface.co/jinaai/jina-embeddings-v2-base-code), which supports English and 30 widely used programming languages with a 8192 sequence length."],"metadata":{"id":"a9GDir145nRz"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"deaDhOkw80DZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fMq1vOiC5hRT"},"outputs":[],"source":["!pip install -qU inflection qdrant-client fastembed"]},{"cell_type":"markdown","source":["* `inflection` - a string transformation library. It singularizes and pluralizes English words, and transforms CamelCase to underscored string\n","* `fastembed` - a CPU-first, lightweight library for generating vector embeddings. GPU support is also available\n","* `qdrant-client` - interface with the Qdrant server"],"metadata":{"id":"oUTrR5AB84wz"}},{"cell_type":"markdown","source":["## Data preparation"],"metadata":{"id":"YcIRuHo89OzN"}},{"cell_type":"markdown","source":["Chunking the application sources into smaller parts is non-trivial task. In general, functions, class methods, structs, enums, and all the other language-specific constructs are good candidates for chunks. They are big enough to contain some meaningful information, but small enough to be processed by embedding models with a limited context window. Not to mention that we can also use docstrings, comments, and other metadata to enrich the chunks with additional information.\n","\n","Text-based search is based on function signatures, but code search may return smaller pieces, such as loops. So, if we receive a particular function signature from the NLP model and part of its implementation from the code model, we will merge the results."],"metadata":{"id":"sdxw-bo1ALoL"}},{"cell_type":"markdown","source":["## Parsing the codebase"],"metadata":{"id":"SJSYZY9AAxWR"}},{"cell_type":"markdown","source":["In this example, we will use the [`Qdrant`](https://github.com/qdrant/qdrant) repository.\n","\n","While this codebase uses Rust, we can use this approach with any other language. We can use an **Language Server Protocal (LSP)** tool to build a graph of the codebase, and then extract chunks. We will use the [`rust-analyzer`](https://rust-analyzer.github.io/) and export the parsed codebase into the LSIF (Language Server Index Format) format, a standard for code intelligence data. Next we will use the LSIF data to navigate the codebase and extract the chunks.\n","\n","The same approach can be applied for other languages as well.\n","\n","We will then export the chunks into JSON documents with not only the code itself, but also context with the location of the code in the project. We can examine the Qdrant structures, parsed in JSON, in the [`structured.jsonl`](https://storage.googleapis.com/tutorial-attachments/code-search/structures.jsonl) file. We need to download it and use it as a source of data for our code search."],"metadata":{"id":"OrY5tH0wV_Mw"}},{"cell_type":"code","source":["!wget https://storage.googleapis.com/tutorial-attachments/code-search/structures.jsonl"],"metadata":{"id":"sXYHQOOd9NdM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We then load the file and parse the lines into a list of dictionaries."],"metadata":{"id":"vSI9jwRiatel"}},{"cell_type":"code","source":["import json\n","\n","structures = []\n","with open('structures.jsonl', 'r') as fp:\n","    for i, row in enumerate(fp):\n","        entry = json.loads(row)\n","        structures.append(entry)"],"metadata":{"id":"CFTB_Gtpawhu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["structures[0]"],"metadata":{"id":"K-LOFAKza6iC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Code to natural language conversion"],"metadata":{"id":"lIGnZDXVa83-"}},{"cell_type":"markdown","source":["Each programming language has its own syntax which is not a part of the natural language. Thus, a general-purpose model may not understand the code as is. We need to normalize the data by removing code specifics and including additional context, such as module, class, function, and file name:\n","1. Extract the signature of the function, method, or other code construct.\n","2. Divide camel case and snake case names into separate words.\n","3. Take the docstring, comments, and other important metadata.\n","4. Build a sentence from the extracted data using a predefined template.\n","5. Remove the speical characters and replace them with spaces.\n","\n","We can now define the `textify` function that uses the `inflection` library to carry out our conversions."],"metadata":{"id":"VyEnaZHfa_m2"}},{"cell_type":"code","source":["import inflection\n","import re\n","from typing import Dict, Any\n","\n","\n","def textify(chunk: Dict[str, Any]) -> str:\n","    # Get rid of all the camel case / snake case\n","    # - inflection.underscore changes the camel case to snake case\n","    # - inflection.humanize converts the snake case to human reable form\n","    name = inflection.humanize(inflection.underscore(chunk['name']))\n","    signature = inflection.humanize(inflection.underscore(chunk['signature']))\n","\n","    # Check if docstring is provided\n","    docstring = ''\n","    if chunk['docstring']:\n","        docstring = f\"that does {chunk['docstring']} \"\n","\n","    # Extract the location of that snippet of code\n","    context = f\"module {chunk['context']['module']} file {chunk['context']['file_name']}\"\n","    if chunk['context']['struct_name']:\n","        struct_name = inflection.humanize(inflection.underscore(chunk['context']['struct_name']))\n","        context = f\"defined in struct {struct_name} {context}\"\n","\n","    # Combine all the bits and pieces together\n","    text_representation = f\"{chunk['code_type']} {name} {docstring} defined as {signature} {context}\"\n","\n","    # Remove any special characters and concatenate the tokens\n","    tokens = re.split(r\"\\W\", text_representation)\n","    tokens = filter(lambda x: x, tokens)\n","\n","    return ' '.join(tokens)"],"metadata":{"id":"pcPHCAoHa-oF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can use `textify` to convert all chunks into text representations."],"metadata":{"id":"YsN94NR_c9uS"}},{"cell_type":"code","source":["text_representations = list(map(textify, structures))"],"metadata":{"id":"Ak-7U_zDdCNd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_representations[0]"],"metadata":{"id":"3Cg23m4udEuH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Natural language embeddings"],"metadata":{"id":"b0_qOH4adGFw"}},{"cell_type":"code","source":["from fastembed import TextEmbedding\n","\n","batch_size = 5\n","\n","nlp_model = TextEmbedding(\n","    'sentence-transformers/all-MiniLM-L6-v2',\n","    threads=0\n",")\n","\n","nlp_embeddings = nlp_model.embed(\n","    text_representations,\n","    batch_size=batch_size\n",")"],"metadata":{"id":"54YHSkW7dHzS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Code Embeddings"],"metadata":{"id":"juGcsXEddUAo"}},{"cell_type":"code","source":["code_snippets = [\n","    structure['context']['snippet'] for structure in structures\n","]\n","\n","code_model = TextEmbedding('jinaai/jina-embeddings-v2-base-code')\n","\n","code_embeddings = code_model.embed(\n","    code_snippets,\n","    batch_size=batch_size\n",")"],"metadata":{"id":"bJCsYU6vdVe8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Building Qdrant collection"],"metadata":{"id":"5bfz3VKejMmk"}},{"cell_type":"markdown","source":["Qdrant supports multiple modes of deployment, including in-memory for prototyping, Docker and Qdrant Cloud.\n","\n","We will use an in-memory instance in this example."],"metadata":{"id":"suwb869mjO1_"}},{"cell_type":"code","source":["from qdrant_client import QdrantClient, models\n","\n","COLLECTION_NAME = 'qdrant-sources'\n","\n","# use in-memory storage\n","client = QdrantClient(':memory:')"],"metadata":{"id":"x00xtJAMjOWz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will create a collection to store our vectors."],"metadata":{"id":"1_I-nyrkj1P4"}},{"cell_type":"code","source":["client.create_collection(\n","    COLLECTION_NAME,\n","    vectors_config={\n","        'text': models.VectorParams(\n","            size=384,\n","            distance=models.Distance.COSINE\n","        ),\n","        'code': models.VectorParams(\n","            size=768,\n","            distance=models.Distance.COSINE\n","        )\n","    }\n",")"],"metadata":{"id":"371idVdYjoYn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","points = []\n","total = len(structures)\n","print('Number of points to upload: ', total)\n","\n","for id, (text_embedding, code_embedding, structure) in tqdm(\n","    enumerate(zip(nlp_embeddings, code_embeddings, structures)),\n","    total=total\n","):\n","    # FastEmbed returns generator. Embeddings are computed as consumed\n","    points.append(\n","        models.PointStruct(\n","            id=id,\n","            vector={\n","                'text': text_embedding,\n","                'code': code_embedding\n","            },\n","            payload=structure\n","        )\n","    )\n","\n","    # Upload points in batches\n","    if len(points) >= batch_size:\n","        client.upload_points(COLLECTION_NAME, points=points, wait=True)\n","        points = []\n","\n","# Ensure any remaining points are uploaded\n","if points:\n","    client.upload_points(COLLECTION_NAME, points=points)\n","print(f\"Total points in collection: {client.count(COLLECTION_NAME).count}\")"],"metadata":{"id":"mTdXQyz0j5eg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The uploaded points are immediately available for search."],"metadata":{"id":"M704pS6hkmyi"}},{"cell_type":"markdown","source":["## Querying the codebase"],"metadata":{"id":"YILAYsUukpyT"}},{"cell_type":"markdown","source":["We will use one of the models to search the collection via Qdrant's Query API. Start with text embeddings."],"metadata":{"id":"UJnYHTjwkt8N"}},{"cell_type":"code","source":["query = \"How do I count points in a collection?\"\n","\n","hits = client.query_points(\n","    COLLECTION_NAME,\n","    query=next(nlp_model.query_embed(query)).tolist(),\n","    using='text',\n","    limit=3\n",").points"],"metadata":{"id":"uPYL-63mkpNa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hits"],"metadata":{"id":"CttlO8MRk_TC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next we try the code embeddings."],"metadata":{"id":"hovyLiZTlCdT"}},{"cell_type":"code","source":["hits = client.query_points(\n","    COLLECTION_NAME,\n","    query=next(code_model.query_embed(query)).tolist(),\n","    using='code',\n","    limit=3\n",").points\n","\n","hits"],"metadata":{"id":"HlCrVSCblFnu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Code and text embeddings can capture different aspects of the codebase. We can use both models to query the collection and then combine the results to get the most relevant code snippets."],"metadata":{"id":"fKnb-yYSlK_4"}},{"cell_type":"code","source":["from qdrant_client import models\n","\n","hits = client.query_points(\n","    collection_name=COLLECTION_NAME,\n","    prefetch=[\n","        models.Prefetch(\n","            query=next(nlp_model.query_embed(query)).tolist(),\n","            using='text',\n","            limit=5\n","        ),\n","        models.Prefetch(\n","            query=next(code_model.query_embed(query)).tolist(),\n","            using='code',\n","            limit=5\n","        )\n","    ],\n","    query=models.FusionQuery(fusion=models.Fusion.RRF)\n",").points"],"metadata":{"id":"IuaHBJG1lSgi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for hit in hits:\n","    print(\n","        \"| \",\n","        hit.payload[\"context\"][\"module\"],\n","        \" | \",\n","        hit.payload[\"context\"][\"file_path\"],\n","        \" | \",\n","        hit.score,\n","        \" | `\",\n","        hit.payload[\"signature\"],\n","        \"` |\",\n","    )"],"metadata":{"id":"6toZ0pYmllH8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is how we can fuse the results from different models. In real-world scenario, we may run some reranking and deduplication, as well as additional processing of the results."],"metadata":{"id":"yegyL_delsX_"}},{"cell_type":"markdown","source":["## Grouping the results"],"metadata":{"id":"V1QENYY5l1p5"}},{"cell_type":"markdown","source":["We can improve the search results by grouping them by payload properties. In this example, we can group the results by the module. If we use code embeddings, we can see multiple results from the `map_index` module."],"metadata":{"id":"o3pMmRgOl3m7"}},{"cell_type":"code","source":["reuslts = client.query_points_groups(\n","    COLLECTION_NAME,\n","    query=next(code_model.query_embed(query)).tolist(),\n","    using='code',\n","    group_by='context.module',\n","    limit=5,\n","    group_size=1\n",")"],"metadata":{"id":"r2RTjvVllzh0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for group in results.groups:\n","    for hit in group.hits:\n","        print(\n","            \"| \",\n","            hit.payload[\"context\"][\"module\"],\n","            \" | \",\n","            hit.payload[\"context\"][\"file_name\"],\n","            \" | \",\n","            hit.score,\n","            \" | `\",\n","            hit.payload[\"signature\"],\n","            \"` |\",\n","        )"],"metadata":{"id":"Z89OpRn9mL9t"},"execution_count":null,"outputs":[]}]}