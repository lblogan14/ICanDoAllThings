{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM5uTgHt3/ltKLOmb81IZzJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# CogVideoX"],"metadata":{"id":"if6M1Xy0uT4q"}},{"cell_type":"markdown","source":["**CogVideoX** is a text-to-video generation model focused on creating more coherent videos aligned with a prompt. It achieves this using\n","* a 3D variational autoencoder that compresses videos spatially and temporally, improving compression rate and video accuracy.\n","* an expert transformer block to help align text and video, and a 3D full attention module for capturing and creating spatially and temporally accurate videos."],"metadata":{"id":"b5m12mKFuWoM"}},{"cell_type":"markdown","source":["## Data preparation"],"metadata":{"id":"l9Drd0L8u8Uu"}},{"cell_type":"markdown","source":["The training scripts accepts data in two formats. The first format is suited for small-scale training, and the second format uses a CSV format, which is more appropriate for streaming data for large-scale training."],"metadata":{"id":"bhNZElCV5dBa"}},{"cell_type":"markdown","source":["### Small format"],"metadata":{"id":"h-e9hKBb5x1o"}},{"cell_type":"markdown","source":["Two files where one file contains line-separted prompts and another file contains line-separated paths to video data (the path to video files must be relative to the path we pass when specifying `--instance_data_root`).\n","\n","Assume we have specified `--instance_data_root` as `/dataset`, and that directory contains the files: `prompts.txt` and `videos.txt`\n","\n","The `prompts.txt` file should contain line-separated prompts:\n","```\n","A black and white animated sequence featuring a rabbit, named Rabbity Ribfried, and an anthropomorphic goat in a musical, playful environment, showcasing their evolving interaction.\n","A black and white animated sequence on a ship's deck features a bulldog character, named Bully Bulldoger, showcasing exaggerated facial expressions and body language. The character progresses from confident to focused, then to strained and distressed, displaying a range of emotions as it navigates challenges. The ship's interior remains static in the background, with minimalistic details such as a bell and open door. The character's dynamic movements and changing expressions drive the narrative, with no camera movement to distract from its evolving reactions and physical gestures.\n","...\n","```\n","\n","The `videos.txt` file should contain line-separated paths to video files. The path should be *relative* to the `--instance_data_root` directory:\n","```\n","videos/00000.mp4\n","videos/00001.mp4\n","...\n","```\n","Therefore, the dataset root directory looks like this:\n","```\n","/dataset\n","├── prompts.txt\n","├── videos.txt\n","├── videos\n","    ├── videos/00000.mp4\n","    ├── videos/00001.mp4\n","    ├── ...\n","```\n","\n","When using this format, the `--caption_column` must be `prompts.txt` and the `--video_column` must be `videos.txt`."],"metadata":{"id":"okizqcF_50fi"}},{"cell_type":"markdown","source":["### Stream format"],"metadata":{"id":"k0AFrZ4N63jA"}},{"cell_type":"markdown","source":["We could use a single CSV file. Assume we have a `metadata.csv` file whose format is\n","```\n","<CAPTION_COLUMN>,<PATH_TO_VIDEO_COLUMN>\n","\"\"\"A black and white animated sequence featuring a rabbit, named Rabbity Ribfried, and an anthropomorphic goat in a musical, playful environment, showcasing their evolving interaction.\"\"\",\"\"\"00000.mp4\"\"\"\n","\"\"\"A black and white animated sequence on a ship's deck features a bulldog character, named Bully Bulldoger, showcasing exaggerated facial expressions and body language. The character progresses from confident to focused, then to strained and distressed, displaying a range of emotions as it navigates challenges. The ship's interior remains static in the background, with minimalistic details such as a bell and open door. The character's dynamic movements and changing expressions drive the narrative, with no camera movement to distract from its evolving reactions and physical gestures.\"\"\",\"\"\"00001.mp4\"\"\"\n","...\n","```\n","\n","Here, the `--instance_data_root` should be the location where the videos are stored and `--dataset_name` should be either a path or local folder or a `load_dataset` compatible dataset hosted on the Hub.\n","\n","[Hub dataset] Assume we have videos of Minecraft gameplay at `https://huggingface.co/datasets/my-awesome-username/minecraft-videos`, then we would have to specify `--dataset_name my-awesome-username/minecraft-videos`.\n","\n","Using this format, the `--caption_column` must be `<CAPTION_COLUMN>` and `--video_column` must be `<PATH_TO_VIDEO_COLUMN>`."],"metadata":{"id":"MK-E52-e7QFJ"}},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"ff2vyyGL8SnG"}},{"cell_type":"markdown","source":["We must install necessary requirements:\n","* PyTorch 2.0 or above for quantized/deepspeed training\n","* `pip install diffusers transformers accelerate peft huggingface_hub` for all things modeling and training related\n","* `pip install datasets decord` for loading video training data\n","* `pip install bitsandbytes` for using 8-bit Adam and AdamW optimizers for memory-optimized training\n","* `pip install wandb` optionally for monitoring trianing logs\n","* `pip install deepspeed` optionally for **Deepspeed** training\n","* `pip install prodigyopt` optionally if we would like to use the Prodigy optimizer for training\n","\n"],"metadata":{"id":"WzzFVjQQ8UzN"}},{"cell_type":"markdown","source":["As always, make sure to install the `diffusers` library from source:\n","```bash\n","git clone https://github.com/huggingface/diffusers\n","cd diffusers\n","pip install .\n","```\n","\n","We need to navigate to the following folder and install the corresponding dependencies:\n","```bash\n","cd examples/cogvideo\n","pip install -r requirements.txt\n","```\n","\n","Now we can initialize a HuggingFace Accelerate environment\n","```bash\n","accelerate config\n","```\n","To set up a default Acclerate environment without choosing any configurations:\n","```bash\n","accelerate config default\n","```\n","Or if our environment does not support an interactive shell like a notebook, we can use:\n","```python\n","from accelerate.utils import write_basic_config\n","write_basic_config()\n","```"],"metadata":{"id":"mnen3aT79Dw9"}},{"cell_type":"markdown","source":["If our data is prepared as suggusted in the Data preparation section, we can start training. Assuming we will train on 50 videos of a similar concept, we have found 1500-2000 steps to work well. The official recommendation is 100 videos with a total of 4000 steps. Assuming we will train on a single GPU with a `--train_batch_size 1`:\n","* 1500 steps on 50 videos correspond to 30 training epochs\n","* 4000 steps on 100 videos correspond to 40 training epochs\n","\n","We can specify the following in the bash file to run:\n","```bash\n","GPU_IDS=\"0\"\n","\n","accelerate launch --gpu_ids $GPU_IDS examples/cogvideo/train_cogvideox_lora.py \\\n","  --pretrained_model_name_or_path THUDM/CogVideoX-2b \\\n","  --cache_dir <CACHE_DIR> \\\n","  --instance_data_root <PATH_TO_WHERE_VIDEO_FILES_ARE_STORED> \\\n","  --dataset_name my-awesome-name/my-awesome-dataset \\\n","  --caption_column <CAPTION_COLUMN> \\\n","  --video_column <PATH_TO_VIDEO_COLUMN> \\\n","  --id_token <ID_TOKEN> \\\n","  --validation_prompt \"<ID_TOKEN> Spiderman swinging over buildings:::A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance\" \\\n","  --validation_prompt_separator ::: \\\n","  --num_validation_videos 1 \\\n","  --validation_epochs 10 \\\n","  --seed 42 \\\n","  --rank 64 \\\n","  --lora_alpha 64 \\\n","  --mixed_precision fp16 \\\n","  --output_dir /raid/aryan/cogvideox-lora \\\n","  --height 480 --width 720 --fps 8 --max_num_frames 49 --skip_frames_start 0 --skip_frames_end 0 \\\n","  --train_batch_size 1 \\\n","  --num_train_epochs 30 \\\n","  --checkpointing_steps 1000 \\\n","  --gradient_accumulation_steps 1 \\\n","  --learning_rate 1e-3 \\\n","  --lr_scheduler cosine_with_restarts \\\n","  --lr_warmup_steps 200 \\\n","  --lr_num_cycles 1 \\\n","  --enable_slicing \\\n","  --enable_tiling \\\n","  --optimizer Adam \\\n","  --adam_beta1 0.9 \\\n","  --adam_beta2 0.95 \\\n","  --max_grad_norm 1.0 \\\n","  --report_to wandb\n","```\n","Note:\n","* `--report_to wandb` will ensure the training runs are tracked on Weights and Biases. To use it, be sure to install wandb with pip install wandb.\n","* `--validation_prompt` and `--validation_epochs` to allow the script to do a few validation inference runs. This allows us to qualitatively check if the training is progressing as expected."],"metadata":{"id":"i-UYddxg9OeN"}},{"cell_type":"markdown","source":["## Inference"],"metadata":{"id":"xAeFgpXs-Da-"}},{"cell_type":"markdown","source":["Once we have trained our LoRA model, the inference can be done simply loading the LoRA weights into the `CogVideoXPipeline`."],"metadata":{"id":"5esl5Hgq-JKJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BW24njK-uRgw"},"outputs":[],"source":["import torch\n","from diffusers import CogVideoXPipeline\n","from diffusers.utils import export_to_video\n","\n","pipe = CogVideoXPipeline.from_pretrained(\n","    \"THUDM/CogVideoX-2b\",\n","    torch_dtype=torch.float16\n",")\n","# pipe.load_lora_weights(\"/path/to/lora/weights\", adapter_name=\"cogvideox-lora\") # Or,\n","pipe.load_lora_weights(\n","    \"my-awesome-hf-username/my-awesome-lora-name\",\n","    adapter_name=\"cogvideox-lora\"\n",") # If loading from the HF Hub\n","pipe.to(\"cuda\")\n","\n","# Assuming lora_alpha=32 and rank=64 for training. If different, set accordingly\n","pipe.set_adapters([\"cogvideox-lora\"], [32 / 64])"]},{"cell_type":"code","source":["prompt = \"A vast, shimmering ocean flows gracefully under a twilight sky, its waves undulating in a mesmerizing dance of blues and greens. The surface glints with the last rays of the setting sun, casting golden highlights that ripple across the water. Seagulls soar above, their cries blending with the gentle roar of the waves. The horizon stretches infinitely, where the ocean meets the sky in a seamless blend of hues. Close-ups reveal the intricate patterns of the waves, capturing the fluidity and dynamic beauty of the sea in motion.\"\n","frames = pipe(prompt, guidance_scale=6, use_dynamic_cfg=True).frames[0]\n","export_to_video(frames, \"output.mp4\", fps=8)"],"metadata":{"id":"crM4TGvK-Vok"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Reduce memory usage"],"metadata":{"id":"SHI5rweA-Yzb"}},{"cell_type":"markdown","source":["While testing using the diffusers library, all optimizations included in the diffusers library were enabled. This scheme has not been tested for actual memory usage on devices outside of **NVIDIA A100 / H100** architectures. Generally, this scheme can be adapted to all **NVIDIA Ampere architecture** and above devices. If optimizations are disabled, memory consumption will multiply, with peak memory usage being about 3 times the value in the table, but the speed will increase by about 3-4 times.\n","\n","To selectively disable some optimizations, we can\n","```python\n","pipe.enable_sequential_cpu_offload()\n","pipe.vae.enable_slicing()\n","pipe.vae.enable_tiling()\n","```\n","\n"],"metadata":{"id":"q2AulFVS-gO5"}},{"cell_type":"code","source":[],"metadata":{"id":"Vhxb_sfP-aAR"},"execution_count":null,"outputs":[]}]}