{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN2KFu+mbmMF4Qdbx22DU9V"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# InstructPix2Pix"],"metadata":{"id":"Qg-SFDyEnGSN"}},{"cell_type":"markdown","source":["**InstructPix2Pix** is a Stable Diffusion model trained to edit images from human-provided instructions. This model is conditioned on the text prompt (or editing instruction) and the input image.\n","\n","We will explore the [`train_instruct_pix2pix.py`](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix.py)\n","\n","As always, make sure to install the `diffusers` library from source:\n","```bash\n","git clone https://github.com/huggingface/diffusers\n","cd diffusers\n","pip install .\n","```\n","\n","We need to navigate to the following folder and install the corresponding dependencies:\n","```bash\n","cd examples/instruct_pix2pix\n","pip install -r requirements.txt\n","```\n","HuggingFace Accelerate is also helpful to train on multiple GPUs with mixed-precision.\n","```bash\n","pip install accelerate\n","```\n","\n","Now we can initialize a HuggingFace Accelerate environment\n","```bash\n","accelerate config\n","```\n","To set up a default Acclerate environment without choosing any configurations:\n","```bash\n","accelerate config default\n","```\n","Or if our environment does not support an interactive shell like a notebook, we can use:\n","```python\n","from accelerate.utils import write_basic_config\n","write_basic_config()\n","```"],"metadata":{"id":"UeLZv0OgnIYi"}},{"cell_type":"markdown","source":["## Script parameters"],"metadata":{"id":"aEWKqoQanjqX"}},{"cell_type":"markdown","source":["The training script provides many parameters to customize the training run. We can find all of the parameters and their descriptions in the `parse_args()` function.\n","\n","To speed up training with mixed precision using the `fp16` format, add the `--mixed_precision` parameter to the training command:\n","```bash\n","accelerate launch train_text_to_image.py --mixed_precision=\"fp16\"\n","```\n","Most of the parameters are identical to the parameters in the **Text-to-image** training guide. The paramters more aligned with InstructPix2Pix:\n","* `--original_image_column`: the original image before the edits are made\n","* `--edited_image_column`: the image after the edits are made\n","* `--edit_prompt_column`: the instructions to edit the image\n","* `--conditioning_dropout_prob`: the dropout probability for the edited image and edit prompts during training which enables classifier-free guidance (CFG) for one or both conditioning inputs"],"metadata":{"id":"egypFxBYqPYw"}},{"cell_type":"markdown","source":["## Training script"],"metadata":{"id":"qdohwE10qdlG"}},{"cell_type":"markdown","source":["The general training script is provided in the **Text-to-image** training guide. This guide is specifically about the InstructPix2Pix.\n","\n","The dataset preprocessing code and training loop are found in the `main()` function.\n","\n","The script begins by modifying the number of input channels in the first convolutional layer of the UNet to account for InstructPix2Pix's additional conditioning image:\n","```python\n","    # InstructPix2Pix uses an additional image for conditioning. To accommodate that,\n","    # it uses 8 channels (instead of 4) in the first (conv) layer of the UNet. This UNet is\n","    # then fine-tuned on the custom InstructPix2Pix dataset. This modified UNet is initialized\n","    # from the pre-trained checkpoints. For the extra channels added to the first layer, they are\n","    # initialized to zero.\n","    logger.info(\"Initializing the InstructPix2Pix UNet from the pretrained UNet.\")\n","    in_channels = 8\n","    out_channels = unet.conv_in.out_channels\n","    unet.register_to_config(in_channels=in_channels)\n","\n","    with torch.no_grad():\n","        new_conv_in = nn.Conv2d(\n","            in_channels, out_channels, unet.conv_in.kernel_size, unet.conv_in.stride, unet.conv_in.padding\n","        )\n","        new_conv_in.weight.zero_()\n","        new_conv_in.weight[:, :4, :, :].copy_(unet.conv_in.weight)\n","        unet.conv_in = new_conv_in\n","```\n","\n","These UNet parameters are updated by the optimizer:\n","```python\n","    # Initialize the optimizer\n","    if args.use_8bit_adam:\n","        try:\n","            import bitsandbytes as bnb\n","        except ImportError:\n","            raise ImportError(\n","                \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\n","            )\n","\n","        optimizer_cls = bnb.optim.AdamW8bit\n","    else:\n","        optimizer_cls = torch.optim.AdamW\n","\n","    optimizer = optimizer_cls(\n","        unet.parameters(),\n","        lr=args.learning_rate,\n","        betas=(args.adam_beta1, args.adam_beta2),\n","        weight_decay=args.adam_weight_decay,\n","        eps=args.adam_epsilon,\n","    )\n","```\n","Next, the edited images and edit instructions are preprocessed and tokenized. The same image transformations are applied to the original and edited images.\n","```python\n","    # Preprocessing the datasets.\n","    # We need to tokenize input captions and transform the images.\n","    def tokenize_captions(captions):\n","        inputs = tokenizer(\n","            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n","        )\n","        return inputs.input_ids\n","\n","    # Preprocessing the datasets.\n","    train_transforms = transforms.Compose(\n","        [\n","            transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution),\n","            transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x),\n","        ]\n","    )\n","\n","    def preprocess_images(examples):\n","        original_images = np.concatenate(\n","            [convert_to_np(image, args.resolution) for image in examples[original_image_column]]\n","        )\n","        edited_images = np.concatenate(\n","            [convert_to_np(image, args.resolution) for image in examples[edited_image_column]]\n","        )\n","        # We need to ensure that the original and the edited images undergo the same\n","        # augmentation transforms.\n","        images = np.concatenate([original_images, edited_images])\n","        images = torch.tensor(images)\n","        images = 2 * (images / 255) - 1\n","        return train_transforms(images)\n","\n","    def preprocess_train(examples):\n","        # Preprocess images.\n","        preprocessed_images = preprocess_images(examples)\n","        # Since the original and edited images were concatenated before\n","        # applying the transformations, we need to separate them and reshape\n","        # them accordingly.\n","        original_images, edited_images = preprocessed_images.chunk(2)\n","        original_images = original_images.reshape(-1, 3, args.resolution, args.resolution)\n","        edited_images = edited_images.reshape(-1, 3, args.resolution, args.resolution)\n","\n","        # Collate the preprocessed images into the `examples`.\n","        examples[\"original_pixel_values\"] = original_images\n","        examples[\"edited_pixel_values\"] = edited_images\n","\n","        # Preprocess the captions.\n","        captions = list(examples[edit_prompt_column])\n","        examples[\"input_ids\"] = tokenize_captions(captions)\n","        return examples\n","```\n","Finally, in the training loop, it starts by encoding edited images into latent space, applies dropout to the original image and edit instruction embeddings to support CFG. This is what enables the model to modulate the influence of the edit instruction and original image on the edited image."],"metadata":{"id":"Maj_X36GqmG6"}},{"cell_type":"markdown","source":["## Launch the script"],"metadata":{"id":"O-HKZBI2tVmn"}},{"cell_type":"markdown","source":["We will use the `fusing/instructpix2pix-1000-samples` dataset to train our custom InstructPix2Pix model.\n","\n","Run the script to start training:\n","```bash\n","accelerate launch --mixed_precision=\"fp16\" train_instruct_pix2pix.py \\\n","    --pretrained_model_name_or_path=$MODEL_NAME \\\n","    --dataset_name=$DATASET_ID \\\n","    --enable_xformers_memory_efficient_attention \\\n","    --resolution=256 \\\n","    --random_flip \\\n","    --train_batch_size=4 \\\n","    --gradient_accumulation_steps=4 \\\n","    --gradient_checkpointing \\\n","    --max_train_steps=15000 \\\n","    --checkpointing_steps=5000 \\\n","    --checkpoints_total_limit=1 \\\n","    --learning_rate=5e-05 \\\n","    --max_grad_norm=1 \\\n","    --lr_warmup_steps=0 \\\n","    --conditioning_dropout_prob=0.05 \\\n","    --mixed_precision=fp16 \\\n","    --seed=42 \\\n","    --push_to_hub\n","```"],"metadata":{"id":"7XeKrVS7tZb9"}},{"cell_type":"markdown","source":["After training is completed, we can use our new InstructPix2Pix for inference:"],"metadata":{"id":"fPBrQsAntlft"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nNBC_Fv2nDMn"},"outputs":[],"source":["import PIL\n","import requests\n","import torch\n","from diffusers import StableDiffusionInstructPix2PixPipeline\n","from diffusers.utils import load_image\n","\n","pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n","    \"path/to/our/custom/model\",\n","    torch_dtype=torch.float16\n",").to(\"cuda\")"]},{"cell_type":"code","source":["image = load_image(\"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/test_pix2pix_4.png\")\n","prompt = \"add some ducks to the lake\"\n","num_inference_steps = 20\n","image_guidance_scale = 1.5\n","guidance_scale = 10\n","generator = torch.Generator(\"cuda\").manual_seed(111)\n","\n","edited_image = pipeline(\n","   prompt,\n","   image=image,\n","   num_inference_steps=num_inference_steps,\n","   image_guidance_scale=image_guidance_scale,\n","   guidance_scale=guidance_scale,\n","   generator=generator,\n",").images[0]\n","edited_image"],"metadata":{"id":"DxVluqoCtzhM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Stable Diffusion XL"],"metadata":{"id":"gBSGt30ct4YQ"}},{"cell_type":"markdown","source":["SDXL adds a second text-encoder to its architecture. We need to use the [`train_instruct_pix2pix_sdxl.py`](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix_sdxl.py) script to train a SDXL model to follow image editing instructions."],"metadata":{"id":"N1ZasheSt6mV"}},{"cell_type":"code","source":[],"metadata":{"id":"cWp4PLnFt5wc"},"execution_count":null,"outputs":[]}]}