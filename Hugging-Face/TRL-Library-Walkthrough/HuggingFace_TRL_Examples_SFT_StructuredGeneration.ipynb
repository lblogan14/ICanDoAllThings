{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMzjU/qB77ZyNeefgWr7Uio"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Fine-tuning LLM to Generate Persian Product Catalogs in JSON Format"],"metadata":{"id":"058hSkZF3mLC"}},{"cell_type":"markdown","source":["In this example, we will fine-tune an LLM to generae Persian product catalogs and produce structured output in JSON format.\n","\n","The fine-tuned LLM is list on [their HuggingFace account](https://huggingface.co/BaSalam/Llama2-7b-entity-attr-v1)."],"metadata":{"id":"N8T-6vPJBly4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UdOKwMGR3YCz"},"outputs":[],"source":["import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"]},{"cell_type":"code","source":["# base model to train\n","model_name = 'NousResearch/Llama-2-7b-chat-hf'\n","# instruction dataset to use\n","dataset_name = 'BaSalam/entity-attribute-dataset-GPT-3.5-generated-v1'\n","# fine-tuned lora adapter name\n","new_model = 'llama-persian-catalog-generator'\n","\n","# LoRA parameters\n","lora_r = 64\n","lora_alpha = lora_r * 2\n","lora_dropout = 0.l\n","target_modules = ['q_proj', 'v_proj', 'k_proj']\n","\n","# Additionally, Q-LoRA parameters\n","load_in_4bit= True\n","bnb_4bit_compute_dtype = 'float16'\n","bnb_4bit_quant_type = 'nf4'\n","bnb_4bit_use_double_quant = True"],"metadata":{"id":"d2n_LDqJCWT-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**LoRA** stores changes in weights by constructing and adding a low-rank matrix to each model layer. This method opens only these layers for fine-tuning, without changing the original model weights or requiring lengthy training. The resulting weights are lightweight and can be produced multiple times, allowing for the fine-tuning of multiple tasks with an LLM loaded into RAM.\n","\n","**Q-LoRA** enables LLMs to run on smaller GPUs by using 4-bit quantization. This method preserves the full performance of 16-bit fine-tuning while reducing memory usage, making it possible to fine-tune models with up to 65B parameters on a single 48GB GPU. Q-LoRA combines 4-bit NormalFloat data types, double quantization, and paged optimizers to manage memory efficiently. It allows fine-tuning of models with low-rank adapters, significantly enhancing accessibilitiy for AI model development."],"metadata":{"id":"p_aiPj59DCjZ"}},{"cell_type":"code","source":["# Training hyperparameters\n","num_train_epochs = 1\n","fp16 = False\n","bf16 = False\n","per_device_train_batch_size = 4\n","gradient_accumulation_steps = 1\n","gradient_checkpointing = True\n","learning_rate = 0.00015\n","weight_decay = 0.01\n","optim = 'paged_adamw_32bit'\n","lr_scheduler_type = 'cosine'\n","max_steps = -1\n","warmup_ratio = 0.03\n","group_by_length = True\n","save_steps = 0\n","logging_steps = 25\n","\n","# SFT parameters\n","max_seq_length = None\n","packing = False\n","device_map = {\"\": 0}\n","\n","# Dataset parameters\n","use_special_template = True\n","response_template = \" ### Answer:\"\n","instruction_prompt_template = '\"### Human:\"'\n","use_llama_like_model = True"],"metadata":{"id":"PPVcq_C5DwO3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model training"],"metadata":{"id":"syJJzFEVKgNc"}},{"cell_type":"code","source":["# load dataset\n","dataset = load_dataset(dataset_name, split=\"train\")\n","percent_of_train_dataset = 0.95\n","other_columns = [\n","    i for i in dataset.column_names\n","    if i not in ['instruction', 'output']\n","]\n","\n","dataset = dataset.remove_columns(other_columns)\n","split_dataset = dataset.train_test_split(\n","    train_size=int(dataset.num_rows * percent_of_train_dataset),\n","    seed=111,\n","    shuffle=False\n",")\n","train_dataset = split_dataset['train']\n","eval_dataset = split_dataset['test']\n","print(f\"Size of the train dataset: {len(train_dataset)}\")\n","print(f\"Size of the eval dataset: {len(eval_dataset)}\")"],"metadata":{"id":"eEoVDg3ZKhoF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load LoRA config\n","peft_config = LoraConfig(\n","    r=lora_r,\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    bias='none',\n","    task_type='CAUSAL_LM',\n","    target_modules=target_modules\n",")"],"metadata":{"id":"ClR5Fd7WK847"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The parameters in the `LoraConfig`:\n","* `r` - rank of the low-rank matrices used in LoRA, controling the dimensionality of the low-rank adaptation and directly impacting the model's capacity to adapt and the computational cost.\n","* `lora_alpha` - controls the scaling factor for the low-rank adaptation matrices. Higher alpha increases the model's capacity to learn new tasks.\n","* `lora_dropout` - dropout rate for LoRA, preventing overfitting during fine-tuning.\n","* `bias` - specifies whether to add a bias term to the low-rank matrices. `'none'` means that no bias term will be added.\n","* `task_type` - defines the type of task for which the model is being fine-tuned.\n","* `tatget_modules` - specifies the modules in the model to which LoRA will be applied."],"metadata":{"id":"6_x_fBOYLIcr"}},{"cell_type":"code","source":["# load Q-LoRA config\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=load_in_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=bnb_4bit_use_double_quant,\n",")"],"metadata":{"id":"W_kYrs6TL4dk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The **BitsAndBytes (bnb)** library provides efficient memory management and compression techniques for PyTorch models. It defines how the model weights will be loaded and quantized in 4-bit precision, which is useful for reducing memory usage and potentially speeding up inference.\n","\n","The parameters in the `BitsAndBytesConfig` for the Q-LoRA setup:\n","* `load_in_4bit` - determines whether to load the model in 4-bit precision.\n","* `bnb_4bit_quant_type` - specifies the type of 4-bit quantization to use. Here we set to NF4 (NormalFloat 4) quantization type, which is information-theoretically optimal for normally distributed weights, providing an efficient way to quatnize the model for fine-tuning.\n","* `bnb_4bit_compute_dtype` - sets the data type used for computations involving the quantized model. In Q-LoRA, it is set to `\"float16\"`, which is commonly used for mixed-precision training to balance performance and precision.\n","* `bnb_4bit_use_double_quant` - determines whether to use dobule quantization.\n","\n","Q-LoRA employs two distinct data types (`quant_type` and `compute_type`):\n","* one for storing base model weights (in here 4-bit NormalFloat), and\n","* another for computational operations (16-bit).\n","\n","During the forward and backward passes, Q-LoRA dequantizes the weights from the storage format to the computational format. However, it only calculates gradients for the LoRA parameters, which utilize 16-bit bfloat. This approach ensures that weights are decompressed only when necessary, maintaining low memory usage throughout both training and inference phases."],"metadata":{"id":"gz7g1I6vMEP2"}},{"cell_type":"code","source":["# load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False"],"metadata":{"id":"K3rL4icxNhgL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training arguments\n","training_args = TrainingArguments(\n","    output_dir=new_model,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    grane_checkpointing=gradient_checkpointing,\n","    group_by_length=group_by_length\n","    lr_scheduler_type=lr_scheduler_type\n",")"],"metadata":{"id":"VRWR0JlONsCo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = 'right' # fix weird overflow issue with fp16 training\n","if not tokenizer.chat_template:\n","    tokenizer.chat_template = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}\""],"metadata":{"id":"qANhKV0pN7ON"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The chat template is used to understand the structure of the conversation between the user and the model during model training. A series of reserved phrases are created to separate the user's message and the model's response. This ensures that the model precisely understands where each message comes from and maintains a sense of the conversational structure.\n","\n","Adhering to a chat template helps increase accuracy in the intended task; however, when there is a distribution shift between the fine-tuning dataset and the model, using a specific chat template may be even more helpful."],"metadata":{"id":"O4emz88guOE7"}},{"cell_type":"code","source":["def special_formatting_prompts(example):\n","    output_texts = []\n","    for i in range(len(example['instruction'])):\n","        text = f\"{instruction_prompt_template}{example['instruction'][i]\\n{response_template} {example['output'][i]}}\"\n","        output_texts.append(text)\n","    return output_texts\n","\n","\n","def normal_formatting_prompts(example):\n","    output_texts = []\n","    for i in range(len(example['instruction'])):\n","        chat_temp = [\n","            {'role': 'system', 'content': example['instruction'][i]},\n","            {'role': 'assistant', 'content': example['output'][i]}\n","        ]\n","        text = tokenizer.apply_chat_template(chat_temp, tokenize=False)\n","        output_texts.append(text)\n","    return output_texts"],"metadata":{"id":"472ebTL_uu_F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if use_special_template:\n","    formatting_func = special_formatting_prompts\n","    if use_llama_like_model:\n","        response_template_ids = tokenizer.encode(response_template, add_special_tokens=False)[2:]\n","        collator = DataCollatorForCompletionOnlyLM(response_template=response_template_ids, tokenizer=tokenizer)\n","    else:\n","        collator = DataCollatorForCompletionOnlyLM(response_template=response_template, tokenizer=tokenizer)\n","else:\n","    formatting_func = normal_formatting_prompts"],"metadata":{"id":"RS0WjT0I21D9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    peft_config=peft_config,\n","    formatting_func=formatting_func,\n","    data_collator=collator,\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_args,\n","    packing=packing\n",")"],"metadata":{"id":"kpYkwNyB3GdB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `SFTTrainer` is then instantiated to handle supervised fune-tuning (SFT) of the model. This trainer is specifically designed for SFT and includes additional parameters such as `formatting_func` and `packing` which are not found in standard trainers:\n","* `formatting_func` - a custom function to format training examples by combining instruction and response templates.\n","* `packing` - disables packing multiple samples into one sequence, which is not a standard parameter in the typical `Trainer` class."],"metadata":{"id":"NgSSqBgt3PNV"}},{"cell_type":"code","source":["trianer.train()\n","\n","# save fine-tuned lora\n","trainer.model.save_pretrained(new_model)"],"metadata":{"id":"mFwJdgg23clX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Inference"],"metadata":{"id":"9auf7Vdp3tsP"}},{"cell_type":"code","source":["import torch\n","import gc\n","\n","def clear_hardwares():\n","    torch.clear_autocast_cache()\n","    torch.cuda.ipc_collect()\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","clear_hardwares()"],"metadata":{"id":"yD1RIPd43udZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate(model, prompt: str, kwargs):\n","    tokenized_prompt = tokenizer(prompt, return_tensors='pt').to(model.device)\n","    prompt_length = len(tokenized_prompt.get('input_ids')[0])\n","\n","    with torch.cuda.amp.autocast():\n","        output_tokens = model.generate(**tokenized_prompt, **kwargs) if kwargs else model.generate(**tokenized_prompt)\n","        output = tokenizer.decode(output_tokens[0][prompt_legnth:], skip_special_tokens=True)\n","\n","    return output"],"metadata":{"id":"UCzHTnD91thd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_model = AutoModelForCausalLM.from_pretrained(\n","    new_model,\n","    return_dict=True,\n","    device_map='auto',\n","    token=''\n",")\n","tokenizer = AutoTokenizer.from_pretrained(new_model, max_length=max_seq_length)\n","model = PeftModel.from_pretrained(base_model, new_model)\n","del base_model"],"metadata":{"id":"WH1Tmkuz2sKY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample = eval_dataset[0]\n","if use_special_template:\n","    prompt = f\"{instruction_prompt_template}{sample['instruction']}\\n{response_template}\"\n","else:\n","    chat_temp = [{'role': 'system', 'content': sample['instruction']}]\n","    prompt = tokenizer.apply_chat_template(\n","        chat_temp,\n","        tokenize=False,\n","        add_generation_prompt=True\n","    )\n","\n","\n","gen_kwargs = {'max_new_tokens': 1024}\n","generated_texts = generate(model, prompt, gen_kwargs)\n","print(generated_texts)"],"metadata":{"id":"gzmYsFMR29dw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Merge to base model"],"metadata":{"id":"7-GzDE-M49yu"}},{"cell_type":"code","source":["clear_hardwares()"],"metadata":{"id":"tkfrgvTc4_Zu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["merged_model = model.merge_and_unload()\n","clear_hardwares()\n","del model\n","\n","adapter_model_name = '<hf_account_name>/merged-llama-persian-catalog-generator'\n","merged_model.push_to_hub(adapter_model_name)"],"metadata":{"id":"_syWFjt25Azi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fast inference with `vllm`"],"metadata":{"id":"yzkBBhCl5NZm"}},{"cell_type":"code","source":["from vllm import LLM, SamplingParams\n","\n","prompt = \"\"\"### Question: here is a product title from a Iranian marketplace.  \\n\n","    give me the Product Entity and Attributes of this product in Persian language.\\n\n","    give the output in this json format: {'attributes': {'attribute_name' : <attribute value>, ...}, 'product_entity': '<product entity>'}.\\n\n","    Don't make assumptions about what values to plug into json. Just give Json not a single word more.\\n         \\n\n","    product title:\n","\"\"\"\n","user_prompt_template = \"### Question: \"\n","response_template = \" ### Answer:\"\n","\n","llm = LLM(\n","    model='BaSalam/Llama2-7b-entity-attr-v1',\n","    gpu_memory_utilization=0.9,\n","    trust_remote_code=True\n",")\n","\n","product = 'مانتو اسپرت پانیذ قد جلوی کار حدودا 85 سانتی متر قد پشت کار حدودا 88 سانتی متر'\n","sampling_params = SamplingParams(temperature=0., max_tokens=75)\n","prompt = f\"{user_prompt_template} {prompt}{product}\\n {response_template}\"\n","outputs = llm.generate(prompt, sampling_params)\n","\n","print(outputs[0].outputs[0].text)"],"metadata":{"id":"onw0DvTD5PNd"},"execution_count":null,"outputs":[]}]}