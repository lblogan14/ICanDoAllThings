{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyM/c85cYFG1wLyjlcZP3RWW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"N6G6itqErcdR"},"outputs":[],"source":["!pip install -qU diffusers transformers accelerate"]},{"cell_type":"markdown","source":["# Kandinsky"],"metadata":{"id":"Lb2Cx_xvrg3j"}},{"cell_type":"markdown","source":["The Kandinsky models are a series of multilingual text-to-image generation models.\n","\n","* The Kandinsky 2.0 model uses two multilingual text encoders and concatenates those results for the UNet.\n","\n","* Kandinsky 2.1 changes the architecture to include an image prior model (`CLIP`) to generate a mapping between text and image embeddings. Kandinsky 2.1 also uses a `Modulating Quantized Vectors` (MoVQ) decoder, which adds a spatial conditional normalization layer to increase photorealism to decode the latents into images.\n","\n","* Kandinsky 2.2 improves on the previous model by replacing the image encoder of the image prior model with a larger CLIP-ViT-G model to improve quality.\n","\n","* Kandinsky 3 simplifies the architecture and shifts away from the two-stage generation process involving the prior model and diffusion model. Instead, Kandinsky 3 uses `Flan-UL2` to encode text, a UNet with `BigGan-Deep` blocks, and `Sber-MoVQGAN` to decode the latents into images."],"metadata":{"id":"qFhc80ZH0HtB"}},{"cell_type":"markdown","source":["## Text-to-image"],"metadata":{"id":"Sr93mW9B1i_K"}},{"cell_type":"markdown","source":["For Kandinsky models, we always start by setting up the prior pipeline to encode the prompt and generate the image embeddings."],"metadata":{"id":"6OhsMYrx3sIZ"}},{"cell_type":"markdown","source":["##### Kandinsky 2.1"],"metadata":{"id":"Iaa3d83X4BLG"}},{"cell_type":"code","source":["from diffusers import KandinskyPriorPipeline, KandinskyPipeline\n","import torch\n","\n","prior_pipeline = KandinskyPriorPipeline.from_pretrained(\n","    'kandinsky-community/kandinsky-2-1-prior',\n","    torch_dtype=torch.float16,\n",").to('cuda')\n","\n","pipeline = KandinskyPipeline.from_pretrained(\n","    'kandinsky-community/kandinsky-2-1',\n","    torch_dtype=torch.float16\n",").to('cuda')"],"metadata":{"id":"uOU0Djnsrhp_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting\"\n","negative_prompt = \"low quality, bad quality\"\n","\n","image_embeds, negative_image_embeds = prior_pipeline(\n","    prompt,\n","    negative_prompt=negative_prompt,\n","    guidance_scale=1.0\n",").to_tuple()\n","\n","image = pipeline(\n","    prompt,\n","    negative_prompt=negative_prompt,\n","    image_embeds=image_embeds,\n","    negative_image_embeds=negative_image_embeds,\n","    height=768,\n","    width=768\n",").images[0]\n","image"],"metadata":{"id":"PSugZZ2F4ZQo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Kandinsky 2.2"],"metadata":{"id":"CSe45MS549jy"}},{"cell_type":"code","source":["from diffusers import KandinskyV22PriorPipeline, KandinskyV22Pipeline\n","import torch\n","\n","prior_pipeline = KandinskyV22PriorPipeline.from_pretrained(\n","    'kandinsky-community/kandinsky-2-2-prior',\n","    torch_dtype=torch.float16\n",").to('cuda')\n","\n","pipeline = KandinskyV22Pipeline.from_pretrained(\n","    'kandinsky-community/kandinsky-2-2-decoder',\n","    torch_dtype=torch.float16\n",").to('cuda')"],"metadata":{"id":"rg_ybZnn4_BD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting\"\n","negative_prompt = \"low quality, bad quality\"\n","\n","image_embeds, negative_image_embeds = prior_pipeline(\n","    prompt,\n","    guidance_scale=1.0\n",").to_tuple()\n","\n","image = pipeline(\n","    image_embeds=image_embeds,\n","    negative_image_embeds=negative_image_embeds,\n","    height=768,\n","    width=768\n",").images[0]\n","image"],"metadata":{"id":"GLZ7XyT25VhM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Kandinsky 3"],"metadata":{"id":"hy6PqDJS5mpO"}},{"cell_type":"markdown","source":["Kandinsky 3 does not require a prior model so we can directly load the `Kandinsky3Pipeline` and pass a prompt to generate an image"],"metadata":{"id":"gSQCVQOa5p3S"}},{"cell_type":"code","source":["from diffusers import Kandinsky3Pipeline\n","import torch\n","\n","pipeline = Kandinsky3Pipeline.from_pretrained(\n","    \"kandinsky-community/kandinsky-3\",\n","    variant=\"fp16\",\n","    torch_dtype=torch.float1\n",")\n","pipeline.enable_model_cpu_offload()"],"metadata":{"id":"7EhO117p5o0A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting\"\n","image = pipeline(prompt).images[0]\n","image"],"metadata":{"id":"75DF9JnI56S4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### AutoPipeline end-to-end API\n"],"metadata":{"id":"U1zN1joE6FC2"}},{"cell_type":"markdown","source":["Diffusers provides an end-to-end API with the `KandinskyCombinedPipeline` and `KandinskyV22CombinedPipeline`. The combined pipeline automatically loads both the prior model and the decoder."],"metadata":{"id":"K5Z_98l76KXT"}},{"cell_type":"markdown","source":["##### Kandinsky 2.1"],"metadata":{"id":"Wjurq8C96YQ2"}},{"cell_type":"code","source":["from diffusers import AutoPipelineForText2Image\n","import torch\n","\n","pipeline = AutoPipelineForText2Image.from_pretrained(\n","    \"kandinsky-community/kandinsky-2-1\",\n","    torch_dtype=torch.float16\n",")\n","pipeline.enable_model_cpu_offload()"],"metadata":{"id":"0NUOsUt16ImF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting\"\n","negative_prompt = \"low quality, bad quality\"\n","\n","image = pipeline(\n","    prompt,\n","    negative_prompt=negative_prompt,\n","    prior_guidance_scale=1.0,\n","    guidance_scale=4.0,\n","    height=768,\n","    width=768\n",").images[0]\n","image"],"metadata":{"id":"s3Sm9YNl6iTm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Kandinsky 2.2"],"metadata":{"id":"LOPse97R6oZd"}},{"cell_type":"code","source":["from diffusers import AutoPipelineForText2Image\n","import torch\n","\n","pipeline = AutoPipelineForText2Image.from_pretrained(\n","    \"kandinsky-community/kandinsky-2-2-decoder\",\n","    torch_dtype=torch.float16\n",")\n","pipeline.enable_model_cpu_offload()"],"metadata":{"id":"_Jmwgoui6pVp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting\"\n","negative_prompt = \"low quality, bad quality\"\n","\n","image = pipeline(\n","    prompt,\n","    negative_prompt=negative_prompt,\n","    prior_guidance_scale=1.0,\n","    guidance_scale=4.0,\n","    height=768,\n","    width=768\n",").images[0]\n","image"],"metadata":{"id":"FyxWXaAK6vXl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Image-to-image"],"metadata":{"id":"2sOOsm-w6v3V"}},{"cell_type":"markdown","source":["##### Kandinsky 2.1"],"metadata":{"id":"e5dLVfqFLjLP"}},{"cell_type":"code","source":["from diffusers import KandinskyImg2ImgPipeline, KandinskyPriorPipeline\n","from diffusers.utils import load_image, make_image_grid\n","import torch\n","\n","prior_pipeline = KandinskyPriorPipeline.from_pretrained(\n","    \"kandinsky-community/kandinsky-2-1-prior\",\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",").to('cuda')\n","\n","pipeline = KandinskyImg2ImgPipeline.from_pretrained(\n","    \"kandinsky-community/kandinsky-2-1\",\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",").to(\"cuda\")"],"metadata":{"id":"Pj4enZKr6xaZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n","original_image = load_image(url)\n","original_image = original_image.resize((768, 512))\n","\n","prompt = \"A fantasy landscape, Cinematic lighting\"\n","negative_prompt = \"low quality, bad quality\"\n","\n","image_embeds, negative_image_embeds = prior_pipeline(\n","    prompt,\n","    negative_prompt\n",").to_tuple()\n","\n","image = pipeline(\n","    prompt,\n","    negative_prompt=negative_prompt,\n","    image=original_image,\n","    image_embeds=image_embeds,\n","    negative_image_embeds=negative_image_embeds,\n","    height=768,\n","    width=768,\n","    strength=0.3\n",").images[0]\n","make_image_grid([original_image.resize((512,512)), image.resize((512,512))], rows=1, cols=2)"],"metadata":{"id":"6B9cZim1MAxU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Kandinsky 2.2"],"metadata":{"id":"92tjkTRwMvdg"}},{"cell_type":"code","source":["from diffusers import KandinskyV22Img2ImgPipeline, KandinskyPriorPipeline\n","from diffusers.utils import load_image, make_image_grid\n","import torch\n","\n","prior_pipeline = KandinskyPriorPipeline.from_pretrained(\n","    \"kandinsky-community/kandinsky-2-2-prior\",\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",").to(\"cuda\")\n","\n","pipeline = KandinskyV22Img2ImgPipeline.from_pretrained(\n","    \"kandinsky-community/kandinsky-2-2-decoder\",\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",").to(\"cuda\")"],"metadata":{"id":"zI5XsxAZMWy5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n","original_image = load_image(url)\n","original_image = original_image.resize((768, 512))\n","\n","prompt = \"A fantasy landscape, Cinematic lighting\"\n","negative_prompt = \"low quality, bad quality\"\n","\n","image_embeds, negative_image_embeds = prior_pipeline(prompt, negative_prompt).to_tuple()\n","\n","image = pipeline(\n","    image=original_image,\n","    image_embeds=image_embeds,\n","    negative_image_embeds=negative_image_embeds,\n","    height=768,\n","    width=768,\n","    strength=0.3\n",").images[0]\n","\n","make_image_grid([original_image.resize((512, 512)), image.resize((512, 512))], rows=1, cols=2)"],"metadata":{"id":"NEFPGNwyM_7N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Kandinsky 3"],"metadata":{"id":"NqgYQKg_NRtY"}},{"cell_type":"code","source":["from diffusers import Kandinsky3Img2ImgPipeline\n","from diffusers.utils import load_image, make_image_grid\n","import torch\n","\n","pipeline = Kandinsky3Img2ImgPipeline.from_pretrained(\n","    \"kandinsky-community/kandinsky-3\",\n","    variant=\"fp16\",\n","    torch_dtype=torch.float16\n",")\n","pipeline.enable_model_cpu_offload()"],"metadata":{"id":"WKAnd6f6NTEd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n","original_image = load_image(url)\n","original_image = original_image.resize((768, 512))\n","\n","prompt = \"A fantasy landscape, Cinematic lighting\"\n","negative_prompt = \"low quality, bad quality\"\n","\n","image_embeds, negative_image_embeds = prior_pipeline(prompt, negative_prompt).to_tuple()\n","\n","image = pipeline(\n","    prompt,\n","    negative_prompt=negative_prompt,\n","    image=image,\n","    strength=0.75,\n","    num_inference_steps=25\n",").images[0]\n","make_image_grid([original_image.resize((512, 512)), image.resize((512, 512))], rows=1, cols=2)"],"metadata":{"id":"SfJDj9KONdVT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### AutoPipeline end-to-end API"],"metadata":{"id":"FL4FSA0CNmqk"}},{"cell_type":"markdown","source":["##### Kandinsky 2.1"],"metadata":{"id":"GBMUxwYjNs4N"}},{"cell_type":"code","source":["from diffusers import AutoPipelineForImage2Image\n","from diffusers.utils import make_image_grid, load_image\n","import torch\n","\n","pipeline = AutoPipelineForImage2Image.from_pretrained(\n","    'kandinsky-community/kandinsky-2-1',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",")\n","pipeline.enable_model_cpu_offload()"],"metadata":{"id":"nEO8v6O-Nukd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n","original_image = load_image(url)\n","\n","prompt = \"A fantasy landscape, Cinematic lighting\"\n","negative_prompt = \"low quality, bad quality\"\n","\n","original_image.thumbnail((768, 768))\n","\n","image = pipeline(\n","    prompt,\n","    negative_prompt=negative_prompt,\n","    image=original_image,\n","    strength=0.3\n",").images[0]\n","make_image_grid([original_image.resize((512, 512)), image.resize((512, 512))], rows=1, cols=2)"],"metadata":{"id":"_lWJZhpT1jWz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Kandinsky 2.2"],"metadata":{"id":"_FxRWB6C1zkN"}},{"cell_type":"code","source":["from diffusers import AutoPipelineForImage2Image\n","from diffusers.utils import make_image_grid, load_image\n","import torch\n","\n","pipeline = AutoPipelineForImage2Image.from_pretrained(\n","    'kandinsky-community/kandinsky-2-2-decoder',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",")\n","pipeline.enable_model_cpu_offload()"],"metadata":{"id":"v2LbrUAP11xf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n","original_image = load_image(url)\n","\n","prompt = \"A fantasy landscape, Cinematic lighting\"\n","negative_prompt = \"low quality, bad quality\"\n","\n","original_image.thumbnail((768, 768))\n","\n","image = pipeline(\n","    prompt,\n","    negative_prompt=negative_prompt,\n","    image=original_image,\n","    strength=0.3\n",").images[0]\n","make_image_grid([original_image.resize((512, 512)), image.resize((512, 512))], rows=1, cols=2)"],"metadata":{"id":"nJYNNm2A15q8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Inpainting"],"metadata":{"id":"3r8fMVgy2Cs-"}},{"cell_type":"markdown","source":["The Kandinsky models use **white pixels** to represent the masked area now instead of black pixels.\n","\n","SO if our mask does not have this format, we need to inverse that\n","```python\n","# For PIL input\n","import PIL.ImageOps\n","mask = PIL.ImageOps.invert(mask)\n","\n","# For PyTorch and NumPy input\n","mask = 1 - mask\n","```"],"metadata":{"id":"Dn7wbiba2HJG"}},{"cell_type":"markdown","source":["##### Kandinsky 2.1"],"metadata":{"id":"6CV3v2x62spH"}},{"cell_type":"code","source":["from diffusers import KandinskyInpaintPipeline, KandinskyPriorPipeline\n","from diffusers.utils import load_image, make_image_grid\n","import torch\n","import numpy as np\n","from PIL import Image\n","\n","prior_pipeline = KandinskyPriorPipeline.from_pretrained(\n","    'kandinsky-community/kandinsky-2-1-prior',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",").to('cuda')\n","\n","pipeline = KandinskyInpaintPipeline.from_pretrained(\n","    'kandinsky-community/kandinsky-2-1-inpaint',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",").to('cuda')"],"metadata":{"id":"nOcoJW5W2EMd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["init_image = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png\")\n","\n","mask = np.zeros(init_image.size, dtype=np.float32)\n","# mask area\n","mask[:250, 250:-250] = 1\n","\n","prompt = 'a hat'\n","\n","prior_output = prior_pipeline(prompt)\n","\n","output_image = pipeline(\n","    prompt,\n","    image=init_image,\n","    mask_image=mask,\n","    **prior_output,\n","    num_inference_steps=150,\n","    heihgt=768,\n","    width=768,\n",").images[0]\n","\n","mask = Image.fromarray((mask*255).astype('uint8'), 'L')\n","\n","make_image_grid([init_image, mask, output_image], rows=1, cols=3)"],"metadata":{"id":"uif4spuB5TEa","executionInfo":{"status":"ok","timestamp":1739023868242,"user_tz":360,"elapsed":240,"user":{"displayName":"Bin Liu","userId":"03585165976699804089"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["The end-to-end `KandinskyInpaintCombinedPipeline`:"],"metadata":{"id":"DGWxOVqK6IgS"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from PIL import Image\n","from diffusers import AutoPipelineForInpainting\n","from diffusers.utils import load_image, make_image_grid\n","\n","pipe = AutoPipelineForInpainting.from_pretrained(\n","    'kandinsky-community/kandinsky-2-1-inpaint',\n","    torch_dtype=torch.float16,\n",")\n","pipe.enable_model_cpu_offload()"],"metadata":{"id":"BiVMVI1l5pvI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["init_image = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png\")\n","mask = np.zeros((768, 768), dtype=np.float32)\n","\n","mask[:250, 250:-250] = 1\n","prompt = 'a hat'\n","\n","output_image = pipe(\n","    prompt,\n","    image=init_image,\n","    mask_image=mask\n",").images[0]\n","mask = Image.fromarray((mask*255).astype('uint8'), 'L')\n","make_image_grid([init_image, mask, output_image], rows=1, cols=3)"],"metadata":{"id":"Hpl0fFk35mlw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Kandinsky 2.2"],"metadata":{"id":"ONHfwyfD9zBd"}},{"cell_type":"code","source":["from diffusers import KandinskyV22InpaintPipeline, KandinskyV22PriorPipeline\n","from diffusers.utils import load_image, make_image_grid\n","import torch\n","import numpy as np\n","from PIL import Image\n","\n","prior_pipeline = KandinskyV22PriorPipeline.from_pretrained(\n","    \"kandinsky-community/kandinsky-2-2-prior\",\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",").to(\"cuda\")\n","\n","pipeline = KandinskyV22InpaintPipeline.from_pretrained(\n","    \"kandinsky-community/kandinsky-2-2-decoder-inpaint\",\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",").to(\"cuda\")"],"metadata":{"id":"4p3QDfNa90TD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["init_image = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png\")\n","\n","mask = np.zeros(init_image.size, dtype=np.float32)\n","# mask area\n","mask[:250, 250:-250] = 1\n","\n","prompt = 'a hat'\n","\n","prior_output = prior_pipeline(prompt)\n","\n","output_image = pipeline(\n","    image=init_image,\n","    mask_image=mask,\n","    **prior_output,\n","    height=768,\n","    width=768,\n","    num_inference_steps=150\n",").images[0]\n","mask = Image.fromarray((mask*255).astype('uint8'), 'L')\n","make_image_grid([init_image, mask, output_image], rows=1, cols=3)"],"metadata":{"id":"DYvJe6vP-J4M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The end-to-end `KandinskyV22InpaintCombinedPipeline`"],"metadata":{"id":"rkW5TIIE-cfQ"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from PIL import Image\n","from diffusers import AutoPipelineForInpainting\n","from diffusers.utils import load_image, make_image_grid\n","\n","pipe = AutoPipelineForInpainting.from_pretrained(\n","    \"kandinsky-community/kandinsky-2-2-decoder-inpaint\",\n","    torch_dtype=torch.float16\n",")\n","pipe.enable_model_cpu_offload()"],"metadata":{"id":"d08hRT-3-gTx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["init_image = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png\")\n","mask = np.zeros((768, 768), dtype=np.float32)\n","\n","mask[:250, 250:-250] = 1\n","prompt = 'a hat'\n","\n","output_image = pipe(\n","    prompt,\n","    image=original_image,\n","    mask_image=mask\n",").images[0]\n","mask = Image.fromarray((mask*255).astype('uint8'), 'L')\n","make_image_grid([init_image, mask, output_image], rows=1, cols=3)"],"metadata":{"id":"33sLdDBM-lNx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Interpolation"],"metadata":{"id":"woM7e4fH-v9Y"}},{"cell_type":"markdown","source":["Interpolation allows us to explore the latent space between the image and text embeddings."],"metadata":{"id":"97L7NjoD-yFb"}},{"cell_type":"markdown","source":["##### Kandinsky 2.1"],"metadata":{"id":"ridMq5Hd-70R"}},{"cell_type":"code","source":["from diffusers import KandinskyPriorPipeline, KandinskyPipeline\n","from diffusers.utils import load_image, make_image_grid\n","import torch\n","\n","prior_pipeline = KandinskyPriorPipeline.from_pretrained(\n","    \"kandinsky-community/kandinsky-2-1-prior\",\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",").to('cuda')\n","\n","pipeline = KandinskyPipeline.from_pretrained(\n","    \"kandinsky-community/kandinsky-2-1\",\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",").to(\"cuda\")"],"metadata":{"id":"KlzmEXrq-xaO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_1 = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png\")\n","img_2 = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/starry_night.jpeg\")\n","make_image_grid([img_1.resize((512,512)), img_2.resize((512,512))], rows=1, cols=2)"],"metadata":{"id":"agWo_YkuA-XN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can specify the text or images to interpolate, and set the weights for each text or image."],"metadata":{"id":"0VVB-Ok-A_sD"}},{"cell_type":"code","source":["images_texts = ['a cat', img_1, img_2]\n","weights = [0.3, 0.3, 0.4]\n","\n","# prompt can be left empty\n","prompt = \"\"\n","prior_out = prior_pipeline.interpolate(images_texts, weights)\n","\n","image = pipeline(\n","    prompt,\n","    **prior_out,\n","    height=768,\n","    width=768,\n",").images[0]\n","make_image_grid([img_1.resize((512,512)), img_2.resize((512,512)), image.resize((512,512))], rows=1, cols=3)"],"metadata":{"id":"ZkmJQrWpBEE5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Kandinsky 2.2"],"metadata":{"id":"hDPU3v8fBjK_"}},{"cell_type":"code","source":["from diffusers import KandinskyV22PriorPipeline, KandinskyV22Pipeline\n","from diffusers.utils import load_image, make_image_grid\n","import torch\n","\n","prior_pipeline = KandinskyV22PriorPipeline.from_pretrained(\n","    \"kandinsky-community/kandinsky-2-2-prior\",\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",").to(\"cuda\")\n","\n","pipeline = KandinskyV22Pipeline.from_pretrained(\n","    \"kandinsky-community/kandinsky-2-2-decoder\",\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",").to(\"cuda\")"],"metadata":{"id":"BUsxP5ncBkTU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_1 = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png\")\n","img_2 = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/starry_night.jpeg\")\n","make_image_grid([img_1.resize((512,512)), img_2.resize((512,512))], rows=1, cols=2)"],"metadata":{"id":"jdrV82X5BthY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images_texts = ['a cat', img_1, img_2]\n","weights = [0.3, 0.3, 0.4]\n","\n","# prompt can be left empty\n","prompt = \"\"\n","prior_out = prior_pipeline.interpolate(images_texts, weights)\n","\n","image = pipeline(\n","    prompt,\n","    **prior_out,\n","    height=768,\n","    width=768,\n",").images[0]\n","make_image_grid([img_1.resize((512,512)), img_2.resize((512,512)), image.resize((512,512))], rows=1, cols=3)"],"metadata":{"id":"gW4SODADB2Ty"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ControlNet"],"metadata":{"id":"QxzizSMmB6G4"}},{"cell_type":"markdown","source":["ControlNet is only supported for Kandinsky 2.2."],"metadata":{"id":"X0N10VwHB8bB"}},{"cell_type":"code","source":["from diffusers.utils import load_image, make_image_grid\n","import torch\n","import numpy as np\n","from transformers import pipeline\n","\n","img = load_image(\n","    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinskyv22/cat.png\"\n",").resize((768, 768))\n","img"],"metadata":{"id":"LNJPAQJGB7xP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def make_hint(image, depth_estimator):\n","    image = depth_estimator(image)['depth']\n","    image = np.array(image)\n","    image = image[:, :, None]\n","    image = np.concatenate([image, image, image], axis=2)\n","    detected_map = torch.from_numpy(image).float() / 255.\n","    hint = detected_map.permute(2, 0, 1)\n","    return hint\n","\n","\n","depth_estimator = pipeline('depth-estimation')"],"metadata":{"id":"DPVlNKapCJKj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hint = make_hint(img, depth_estimator).unsqueeze(0).half().to('cuda')"],"metadata":{"id":"WXYL7o3gCfO1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Text-to-image"],"metadata":{"id":"nrKkQJNmCmga"}},{"cell_type":"code","source":["from diffusers import KandinskyV22PriorPipeline, KandinskyV22ControlnetPipeline\n","\n","prior_pipeline = KandinskyV22PriorPipeline.from_pretrained(\n","    \"kandinsky-community/kandinsky-2-2-prior\",\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",").to(\"cuda\")\n","\n","pipeline = KandinskyV22ControlnetPipeline.from_pretrained(\n","    \"kandinsky-community/kandinsky-2-2-controlnet-depth\",\n","    torch_dtype=torch.float16\n",").to(\"cuda\")"],"metadata":{"id":"PeDVNxhnCoX-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"A robot, 4k photo\"\n","negative_prior_prompt = \"lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature\"\n","\n","generator = torch.Generator('cuda').manual_seed(111)\n","\n","image_emb, zero_image_emb = prior_pipeline(\n","    prompt,\n","    negative_prompt=negative_prior_prompt,\n","    generator=generator,\n",").to_tuple()\n","\n","image = pipeline(\n","    image_embeds=image_emb,\n","    negative_image_embeds=zero_image_emb,\n","    hint=hint,\n","    num_inference_steps=50,\n","    generator=generator,\n","    height=768,\n","    width=768,\n",").images[0]\n","make_image_grid([img, image], rows=1, cols=2)"],"metadata":{"id":"dUm4DRZFCy3X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Image-to-image"],"metadata":{"id":"XxSyh-erDft0"}},{"cell_type":"code","source":["from diffusers import KandinskyV22PriorEmb2EmbPipeline, KandinskyV22ControlnetImg2ImgPipeline\n","\n","prior_pipeline = KandinskyV22PriorEmb2EmbPipeline.from_pretrained(\n","    \"kandinsky-community/kandinsky-2-2-prior\",\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",").to(\"cuda\")\n","\n","pipeline = KandinskyV22ControlnetImg2ImgPipeline.from_pretrained(\n","    \"kandinsky-community/kandinsky-2-2-controlnet-depth\",\n","    torch_dtype=torch.float16\n",").to(\"cuda\")"],"metadata":{"id":"XpkSiuJdDg32"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"A robot, 4k photo\"\n","negative_prior_prompt = \"lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature\"\n","\n","generator = torch.Generator('cuda').manual_seed(111)\n","\n","img_emb = prior_pipeline(\n","    prompt,\n","    image=img,\n","    strength=0.85,\n","    generator=generator\n",")\n","negative_emb = prior_pipeline(\n","    negative_prior_prompt,\n","    image=img,\n","    strength=1,\n","    generator=generator\n",")"],"metadata":{"id":"kuSOr9SWD9yc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = pipeline(\n","    image=img,\n","    strength=0.5,\n","    image_embeds=img_emb.image_embeds,\n","    negative_image_embeds=negative_emb.image_embeds,\n","    hint=hint,\n","    num_inference_steps=50,\n","    generator=generator,\n","    height=768,\n","    width=768\n",").images[0]\n","make_image_grid([img.resize((512, 512)), image.resize((512, 512))], rows=1, cols=2)"],"metadata":{"id":"jdcQ2w2gELkk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Optimizations"],"metadata":{"id":"-cEISBUXESeS"}},{"cell_type":"markdown","source":["Since Kandinsky requires a prior pipeline to generator the mappings and a second pipeline to decode the latents into an image, our optimization should be focused on the second pipeline because that is where the bulk of the computation is done."],"metadata":{"id":"gaz3emi7EVzm"}},{"cell_type":"markdown","source":["1. Enable `xFormers` if we use PyTorch < 2.0"],"metadata":{"id":"bCigToWHEizq"}},{"cell_type":"code","source":["from diffusers import DiffusionPipeline\n","import torch\n","\n","pipe = DiffusionPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16)\n","pipe.enable_xformers_memory_efficient_attention()"],"metadata":{"id":"6Jb5DSQPETiX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Enable `torch.compile` if we use PyTorch >= 2.0 to automatically use scaled dot-product attention (SDPA):"],"metadata":{"id":"UOJZJtu_Esak"}},{"cell_type":"code","source":["pipe.unet.to(memory_format=torch.channels_last)\n","pipe.unet = torch.compile(pipe.unet, mode='reduce-overhead', fullgraph=True)"],"metadata":{"id":"lx3WsGi4Ey96"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is the same as explicitly setting the attention processor to use `AttnAddedKVProcessor2_0`:"],"metadata":{"id":"YMSK5pJbE8cw"}},{"cell_type":"code","source":["from diffusers.models.attention_processor import AttnAddedKVProcessor2_0\n","\n","pipe.unet.set_attn_processor(AttnAddedKVProcessor2_0())"],"metadata":{"id":"Q1mHK_lQFDzE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Offload the model to the CPU with `enable_model_cpu_offload()` to avoid out-of-memory errors:"],"metadata":{"id":"bHtx915sFLwh"}},{"cell_type":"code","source":["pipe.enable_model_cpu_offload()"],"metadata":{"id":"R3fDFDjlFQf2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. By default, the text-to-image pipeline uses the `DDIMScheduler` but we can replace it with another scheduler to see how that affects the tradeoff between inference speed and image quality:"],"metadata":{"id":"ddm0gj_uFUTP"}},{"cell_type":"code","source":["from diffusers import DDPMScheduler\n","from diffusers import DiffusionPipeline\n","\n","scheduler = DDPMScheduler.from_pretrained(\n","    \"kandinsky-community/kandinsky-2-1\",\n","    subfolder=\"ddpm_scheduler\"\n",")\n","pipe = DiffusionPipeline.from_pretrained(\n","    \"kandinsky-community/kandinsky-2-1\",\n","    scheduler=scheduler,\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",").to(\"cuda\")"],"metadata":{"id":"HTE4vBF8FdtL"},"execution_count":null,"outputs":[]}]}