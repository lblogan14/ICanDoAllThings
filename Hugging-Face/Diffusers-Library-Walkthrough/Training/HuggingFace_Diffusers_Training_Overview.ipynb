{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMbV/XL42IJTcwEBymX1rlN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -qU diffusers accelerate datasets huggingface_hub"],"metadata":{"id":"heT8gRcXPH5Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"L4okIwwnEXTZ"}},{"cell_type":"markdown","source":["Diffusers provides a collection of traiing scripts to train our own diffusion models.\n","\n","Each training script is\n","* **Self-contained**: the training script does not depend on any local files, and all packages required to run the script are installed from the `requirements.txt` file.\n","* **Easy-to-tweak**: the training scripts are an example of how to train a diffusion model for a specific task and will not work out-of-the-box for every training scenario.\n","* **Beginner-friendly**: the training scripts are designed to be beginner-friendly and easy to understand, rather than including the latest state-of-the-art methods to get the best and most competitive results.\n","* *Single-purpose**: each training script is expressly designed for only one task to keep it readable and understandable.\n","\n","To install the latest version of Diffusers library:\n","```bash\n","git clone https://github.com/huggingface/diffusers\n","cd diffusers\n","pip install .\n","```\n","\n","To speedup training and reduce memory-usage, we should\n","* use PyTorch 2.0 or higher to automatically use `scaled_dot_product_attention` during training\n","* install `xFormers` to enable memory-efficient attention"],"metadata":{"id":"DnJg0YxlEZn-"}},{"cell_type":"markdown","source":["# Create a dataset for training"],"metadata":{"id":"-IgK-Kd2GCNg"}},{"cell_type":"markdown","source":["There are many datasets on the [HuggingFace Hub](https://huggingface.co/datasets?task_categories=task_categories:text-to-image&sort=downloads) to train a model on, but if we cannot find one we are interested in or want to use our own, we can create a dataset with the HuggingFace Datasets library."],"metadata":{"id":"I7K2mXoyKLE1"}},{"cell_type":"markdown","source":["## Provide a dataset as a folder"],"metadata":{"id":"gMG1yNByOAF_"}},{"cell_type":"markdown","source":["For unconditional generation, we can provide our own dataset as a folder of images. The training script uses the `ImageFolder` builder from HuggingFace Datasets to automatically build a dataset from the folder.\n","\n","Our directory structure should look like:\n","```\n","data_dir/xxx.png\n","data_dir/xxy.png\n","...\n","data_dir/[...]/xxs.png\n","```\n","\n","We then pass the path to the dataset directory to the `--train_data_dir` argument, and start training:\n","```bash\n","accelerate launch train_unconditional.py --train_data_dir <path-to-train-directory> <other-arguments>\n","```"],"metadata":{"id":"m4cPiSPcOC0t"}},{"cell_type":"markdown","source":["## Upload our data to the hub"],"metadata":{"id":"4Klg0-DlOuii"}},{"cell_type":"markdown","source":["We start by creating a dataset with the `ImageFolder` feature, which creates an `image` column containing the PIL-encoded images.\n","\n","We can use the `data_dir` or `data_files` to specify the location of the dataset. The `data_files` supports mapping specific files to dataset splits like `train` or `test`:"],"metadata":{"id":"YrBBh6rFO3vq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ssl9qZOiERMD"},"outputs":[],"source":["from datasets import load_dataset\n","\n","# Example 1: local folder\n","dataset = load_dataset(\n","    'imagefolder',\n","    data_dir='path_to_our_folder'\n",")\n","\n","# Example 2: local files (supported formats are tar, gzip, zip, sz, rar, zstd)\n","dataset = load_dataset(\n","    'imagefolder',\n","    data_files='path_to_zip_file'\n",")\n","\n","# Example 3: remote files (supported formats are tar, gzip, zip, xz, rar, zstd)\n","dataset = load_dataset(\n","    'imagefolder',\n","    data_files='https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip'\n",")\n","\n","# Example 4: providing several splits\n","dataset = load_dataset(\n","    'imagefolder',\n","    data_files={\n","        'train': ['path/to/file1', 'path/to/file2'],\n","        'test': ['path/to/file3', 'path/to/file4']\n","    }\n",")"]},{"cell_type":"markdown","source":["Then we use the `push_to_hub` to upload the dataset to the Hub:"],"metadata":{"id":"P-F9ZIrSQGPh"}},{"cell_type":"code","source":["dataset.push_to_hub('name_of_our_dataset')\n","\n","# if we want to push to a private repo\n","dataset.push_to_hub('name_of_our_dataset', private=True)"],"metadata":{"id":"filcJ_nXQJLt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now the dataset is available for training by passing the dataset name to the `--dataset_name`:\n","```bash\n","accelerate launch --mixed_precision=\"fp16\" train_text_to_image.py \\\n","  --pretrained_model_name_or_path=\"stable-diffusion-v1-5/stable-diffusion-v1-5\" \\\n","  --dataset_name=\"name_of_our_dataset\" \\\n","  <other-arguments>\n","```"],"metadata":{"id":"0wzL4-dJQSbr"}},{"cell_type":"markdown","source":["# Adapt a model to a new task"],"metadata":{"id":"EZ8xD0M8QqyY"}},{"cell_type":"markdown","source":["Many diffusion systems share the same components, allowing us to adapt a pretrained model for one task to an entirely different task."],"metadata":{"id":"8USCh8gIsZzy"}},{"cell_type":"markdown","source":["## Configure `UNet2DConditionModel` parameters"],"metadata":{"id":"yw2W8IKksg3f"}},{"cell_type":"markdown","source":["The `UNet2DConditionModel` by default accepts 4 channels in the input sample."],"metadata":{"id":"hZF9u9xMxtgp"}},{"cell_type":"code","source":["from diffusers import StableDiffusionPipeline\n","\n","pipeline = StableDiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    use_safetensors=True\n",")"],"metadata":{"id":"MuRmy5khQW5O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipeline.unet.config['in_channels']"],"metadata":{"id":"YXAlhlUPx6vS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Inpainting requires 9 channels in the input sample."],"metadata":{"id":"0ltAGayNx9b1"}},{"cell_type":"code","source":["pipeline = StableDiffusionPipeline.from_pretrained(\n","    'runwayml/stable-diffusion-inpainting',\n","    use_safetensors=True\n",")"],"metadata":{"id":"AYB22lkNyBDr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipeline.unet.config['in_channels']"],"metadata":{"id":"J61AURiLyJWT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To adapt our text-to-image model for inpainting, we need to change the `in_channels` from 4 to 9.\n","\n","We can initialize a `UNet2DConditionModel` with the pretrained text-to-image model weights, and change `in_channels` to 9. This means that we need to set `ignore_mismatched_sizes=True` and `low_cpu_mem_usage=False` to avoid a size mismatch error because the shape is different."],"metadata":{"id":"wj4Iy_7GyL-s"}},{"cell_type":"code","source":["from diffusers import UNet2DConditionModel\n","\n","model_id = 'stable-diffusion-v1-5/stable-diffusion-v1-5'\n","unet = UNet2DConditionModel.from_pretrained(\n","    model_id,\n","    subfolder='unet',\n","    in_channels=9,\n","    low_cpu_mem_usage=False,\n","    ignore_mismatched_sizes=True,\n","    use_safetensors=True\n",")"],"metadata":{"id":"J6I3IkjgygkG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The pretrained weights of the other components from the text-to-image model are initialized from their checkpoints, but the input channel weights (`conv_in.weight`) of the `unet` are randomly initialized. We need to finetune the model for inpainting beucase otherwise this model will return nothing but noise."],"metadata":{"id":"aFu4QXb-ytjk"}}]}