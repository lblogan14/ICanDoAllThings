{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM29V+Lfs3o9B/Tx+KKmx+7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Text-to-image"],"metadata":{"id":"sAbB8plt-UYi"}},{"cell_type":"markdown","source":["Text-to-image models like SD are conditioned to generate images given a text prompt.\n","\n","We will use the [`train_text_to_image.py`](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py) script to train and adapt a model.\n","\n","As always, make sure to install the `diffusers` library from source:\n","```bash\n","git clone https://github.com/huggingface/diffusers\n","cd diffusers\n","pip install .\n","```\n","\n","We need to navigate to the following folder and install the corresponding dependencies:\n","```bash\n","cd examples/text_to_image\n","pip install -r requirements.txt\n","```\n","HuggingFace Accelerate is also helpful to train on multiple GPUs with mixed-precision.\n","```bash\n","pip install accelerate\n","```\n","\n","Now we can initialize a HuggingFace Accelerate environment\n","```bash\n","accelerate config\n","```\n","To set up a default Acclerate environment without choosing any configurations:\n","```bash\n","accelerate config default\n","```\n","Or if our environment does not support an interactive shell like a notebook, we can use:\n","```python\n","from accelerate.utils import write_basic_config\n","write_basic_config()\n","```"],"metadata":{"id":"4k8mQzGInkUr"}},{"cell_type":"markdown","source":["## Script parameters"],"metadata":{"id":"2ozDeLltpjoc"}},{"cell_type":"markdown","source":["The training script provides many parameters to customize the training run. We can find all of the parameters and their descriptions in the `parse_args()` function.\n","\n","To speed up training with mixed precision using the `fp16` format, add the `--mixed_precision` parameter to the training command:\n","```bash\n","accelerate launch train_text_to_image.py --mixed_precision=\"fp16\"\n","```\n","\n","Some important parameters:\n","* `--pretrained_model_name_or_path`: the name of the model on the Hub or a local path to the pretrained model\n","* `--dataset_name`: the name of the dataset on the Hub or a local path to the dataset to train on\n","* `--image_column`: the name of the image column in the dataset to train on\n","* `--caption_column`: the name of the text column in the dataset to train on\n","* `--output_dir`: where to save the trained model\n","* `--push_to_hub`: whether to push the trained model to the Hub\n","* `--checkpointing_steps`: frequency of saving a checkpoint as the model train"],"metadata":{"id":"C8xx85PypnPy"}},{"cell_type":"markdown","source":["### Min-SNR weighting"],"metadata":{"id":"thAtilMYqSHM"}},{"cell_type":"markdown","source":["The `Min-SNR` weighting strategy can help with training by rebalancing the loss to achieve faster convergence. The training script supports predicting `epsilon` (noise) or `v_prediction`, but Min-SNR is compatible with both prediction types.\n","\n","We can add the `--snr_gamma` and set it to the recommended value of 5.0:\n","```bash\n","accelerate launch train_text_to_image.py --snr_gamma=5.0\n","```"],"metadata":{"id":"YJsBhV1wqVF1"}},{"cell_type":"markdown","source":["## Training script"],"metadata":{"id":"DnsCWSwVrKo7"}},{"cell_type":"markdown","source":["We can modify the dataset preprocessing and training loop in the `main()` function if necessary.\n","\n","The `train_text_to_image` script starts by loading a scheduler and tokenzier.\n","```python\n","    # Load scheduler, tokenizer and models.\n","    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n","    tokenizer = CLIPTokenizer.from_pretrained(\n","        args.pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=args.revision\n","    )\n","```\n","Then it loads the UNet model:\n","```python\n","    unet = UNet2DConditionModel.from_pretrained(\n","        args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.non_ema_revision\n","    )\n","```\n","Then, the text and image columns of the dataset need to be preprocessed. The `tokenize_captions` function handles tokenizing the inputs:\n","```python\n","    # Preprocessing the datasets.\n","    # We need to tokenize input captions and transform the images.\n","    def tokenize_captions(examples, is_train=True):\n","        captions = []\n","        for caption in examples[caption_column]:\n","            if isinstance(caption, str):\n","                captions.append(caption)\n","            elif isinstance(caption, (list, np.ndarray)):\n","                # take a random caption if there are multiple\n","                captions.append(random.choice(caption) if is_train else caption[0])\n","            else:\n","                raise ValueError(\n","                    f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n","                )\n","        inputs = tokenizer(\n","            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n","        )\n","        return inputs.input_ids\n","```\n","The `train_transforms` function specifies the type of transforms to apply to the image:\n","```python\n","    # Preprocessing the datasets.\n","    train_transforms = transforms.Compose(\n","        [\n","            transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n","            transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution),\n","            transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.5], [0.5]),\n","        ]\n","    )\n","```\n","Then both of these functions are bundled into `preprocess_train`:\n","```python\n","    def preprocess_train(examples):\n","        images = [image.convert(\"RGB\") for image in examples[image_column]]\n","        examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n","        examples[\"input_ids\"] = tokenize_captions(examples)\n","        return examples\n","```\n","\n","Finally, the training loop handles everything else:\n","* It encodes images into latent space,\n","* adds noise to the latents,\n","* computes the text embeddings to condition on,\n","* updates the model parameters, and\n","* saves and pushes the model to the Hub."],"metadata":{"id":"TlBWxYmarMvf"}},{"cell_type":"markdown","source":["## Launch the script"],"metadata":{"id":"hZh2Pfj3s8eo"}},{"cell_type":"markdown","source":["In this example, we can train our model on the Naruto BLIP captions dataset to generate our own Naruto characters.\n","```bash\n","export MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n","export dataset_name=\"lambdalabs/naruto-blip-captions\"\n","\n","accelerate launch train_text_to_image.py \\\n","  --mixed_precision=\"fp16\" \\\n","  --pretrained_model_name_or_path=$MODEL_NAME \\\n","  --dataset_name=$dataset_name \\\n","  --use_ema \\\n","  --resolution=512 \\\n","  --center_crop \\\n","  --random_flip \\\n","  --train_batch_size=1 \\\n","  --gradient_accumulation_steps=4 \\\n","  --gradient_checkpointing \\\n","  --max_train_steps=15000 \\\n","  --learning_rate=1e-05 \\\n","  --max_grad_norm=1 \\\n","  --enable_xformers_memory_efficient_attention \\\n","  --lr_scheduler=\"constant\" \\\n","  --lr_warmup_steps=0 \\\n","  --output_dir=\"sd-naruto-model\" \\\n","  --push_to_hub\n","```"],"metadata":{"id":"Sliv-F7RtALr"}},{"cell_type":"markdown","source":["Once training completed, we can try our trained model for inference:"],"metadata":{"id":"r7tNg5OKt3Vh"}},{"cell_type":"code","source":["from diffusers import StableDiffusionPipeline\n","import torch\n","\n","pipeline = StableDiffusionPipeline.from_pretrained(\n","    'path/to/saved_model',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",").to('cuda')"],"metadata":{"id":"K6Rl6k_ps-A8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = pipeline(prompt=\"yoda\").images[0]\n","image"],"metadata":{"id":"qp0DH61yuG2K"},"execution_count":null,"outputs":[]}]}