{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMFXZnDTS2IB2BfW6iacM5s"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# T2I-Adapter"],"metadata":{"id":"0QE3LtzQCpEi"}},{"cell_type":"markdown","source":["**T2I-Adapter** is a lightweight adapter model that provides an additional conditioning input image to better control image generation. It is similar to a ControlNet, but it is a lot smaller (~77M parameters and ~300MB file size) because it only inserts weights into the UNet instead of copying and training it...\n","\n","The T2I-Adapter is only available for training with the SDXL model.\n","\n","We will explore the [`train_t2i_adapter_sdxl.py`](https://github.com/huggingface/diffusers/blob/main/examples/t2i_adapter/train_t2i_adapter_sdxl.py) script.\n","\n","As always, make sure to install the `diffusers` library from source:\n","```bash\n","git clone https://github.com/huggingface/diffusers\n","cd diffusers\n","pip install .\n","```\n","\n","We need to navigate to the following folder and install the corresponding dependencies:\n","```bash\n","cd examples/t2i_adapter\n","pip install -r requirements.txt\n","```\n","HuggingFace Accelerate is also helpful to train on multiple GPUs with mixed-precision.\n","```bash\n","pip install accelerate\n","```\n","\n","Now we can initialize a HuggingFace Accelerate environment\n","```bash\n","accelerate config\n","```\n","To set up a default Acclerate environment without choosing any configurations:\n","```bash\n","accelerate config default\n","```\n","Or if our environment does not support an interactive shell like a notebook, we can use:\n","```python\n","from accelerate.utils import write_basic_config\n","write_basic_config()\n","```"],"metadata":{"id":"VIpnxY7diYNY"}},{"cell_type":"markdown","source":["## Script parameters"],"metadata":{"id":"w-EbjnhPjNrP"}},{"cell_type":"markdown","source":["The training script provides many parameters to customize the training run. We can find all of the parameters and their descriptions in the `parse_args()` function.\n","\n","To speed up training with mixed precision using the `fp16` format, add the `--mixed_precision` parameter to the training command:\n","```bash\n","accelerate launch train_text_to_image.py --mixed_precision=\"fp16\"\n","```\n","Most of the parameters are identical to the parameters in the **Text-to-image** training guide. The paramters more aligned with T2I-Adapter:\n","* `--pretrained_vae_model_name_or_path`: path to a pretrained VAE; the SDXL VAE is known to suffer from numerical instability, so this parameter allows us to specify a better VAE\n","* `--crops_coords_top_left_h` and `--crops_coords_top_left_w`: height and width coordinates to include in SDXLâ€™s crop coordinate embeddings\n","* `--conditioning_image_column`: the column of the conditioning images in the dataset\n","* `--proportion_empty_prompts`: the proportion of image prompts to replace with empty strings"],"metadata":{"id":"XdtUPvsIjUzh"}},{"cell_type":"markdown","source":["## Training script"],"metadata":{"id":"wxE6oWaYkngs"}},{"cell_type":"markdown","source":["The general training script is provided in the **Text-to-image** training guide. This guide is specifically about the T2I-Adapter.\n","\n","The training script starts by preparing the dataset. This includes tokenizing the prompt and applying transforms to the image and conditioning images.\n","```python\n","# Adapted from pipelines.StableDiffusionXLPipeline.encode_prompt\n","def encode_prompt(prompt_batch, text_encoders, tokenizers, proportion_empty_prompts, is_train=True):\n","    prompt_embeds_list = []\n","\n","    captions = []\n","    for caption in prompt_batch:\n","        if random.random() < proportion_empty_prompts:\n","            captions.append(\"\")\n","        elif isinstance(caption, str):\n","            captions.append(caption)\n","        elif isinstance(caption, (list, np.ndarray)):\n","            # take a random caption if there are multiple\n","            captions.append(random.choice(caption) if is_train else caption[0])\n","\n","    with torch.no_grad():\n","        for tokenizer, text_encoder in zip(tokenizers, text_encoders):\n","            text_inputs = tokenizer(\n","                captions,\n","                padding=\"max_length\",\n","                max_length=tokenizer.model_max_length,\n","                truncation=True,\n","                return_tensors=\"pt\",\n","            )\n","            text_input_ids = text_inputs.input_ids\n","            prompt_embeds = text_encoder(\n","                text_input_ids.to(text_encoder.device),\n","                output_hidden_states=True,\n","            )\n","\n","            # We are only ALWAYS interested in the pooled output of the final text encoder\n","            pooled_prompt_embeds = prompt_embeds[0]\n","            prompt_embeds = prompt_embeds.hidden_states[-2]\n","            bs_embed, seq_len, _ = prompt_embeds.shape\n","            prompt_embeds = prompt_embeds.view(bs_embed, seq_len, -1)\n","            prompt_embeds_list.append(prompt_embeds)\n","\n","    prompt_embeds = torch.concat(prompt_embeds_list, dim=-1)\n","    pooled_prompt_embeds = pooled_prompt_embeds.view(bs_embed, -1)\n","    return prompt_embeds, pooled_prompt_embeds\n","\n","\n","def prepare_train_dataset(dataset, accelerator):\n","    image_transforms = transforms.Compose(\n","        [\n","            transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n","            transforms.CenterCrop(args.resolution),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.5], [0.5]),\n","        ]\n","    )\n","\n","    conditioning_image_transforms = transforms.Compose(\n","        [\n","            transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n","            transforms.CenterCrop(args.resolution),\n","            transforms.ToTensor(),\n","        ]\n","    )\n","\n","    def preprocess_train(examples):\n","        images = [image.convert(\"RGB\") for image in examples[args.image_column]]\n","        images = [image_transforms(image) for image in images]\n","\n","        conditioning_images = [image.convert(\"RGB\") for image in examples[args.conditioning_image_column]]\n","        conditioning_images = [conditioning_image_transforms(image) for image in conditioning_images]\n","\n","        examples[\"pixel_values\"] = images\n","        examples[\"conditioning_pixel_values\"] = conditioning_images\n","\n","        return examples\n","\n","    with accelerator.main_process_first():\n","        dataset = dataset.with_transform(preprocess_train)\n","\n","    return dataset\n","```\n","\n","Within the `main()` funciton, the T2I-Adapter is either loaded from a pretrained adapter or it is randomly initialized:\n","```python\n","    if args.adapter_model_name_or_path:\n","        logger.info(\"Loading existing adapter weights.\")\n","        t2iadapter = T2IAdapter.from_pretrained(args.adapter_model_name_or_path)\n","    else:\n","        logger.info(\"Initializing t2iadapter weights.\")\n","        t2iadapter = T2IAdapter(\n","            in_channels=3,\n","            channels=(320, 640, 1280, 1280),\n","            num_res_blocks=2,\n","            downscale_factor=16,\n","            adapter_type=\"full_adapter_xl\",\n","        )\n","```\n","\n","The optimizer is initialized for the T2I-Adapter parameters:\n","```python\n","    # Optimizer creation\n","    params_to_optimize = t2iadapter.parameters()\n","    optimizer = optimizer_class(\n","        params_to_optimize,\n","        lr=args.learning_rate,\n","        betas=(args.adam_beta1, args.adam_beta2),\n","        weight_decay=args.adam_weight_decay,\n","        eps=args.adam_epsilon,\n","    )\n","```\n","\n","Finally, in the training loop, the adapter conditioning image and the text embeddings are passed to the UNet to predict the noise residual."],"metadata":{"id":"wILDqtBHlV5X"}},{"cell_type":"markdown","source":["## Launch the script"],"metadata":{"id":"B6DSBU1tmYEd"}},{"cell_type":"markdown","source":["We will use the `fusing/fill50k` dataset to train our custom T2I-Adatper.\n","\n","Before training, we also need to download the following iamges to condition our training:\n","```bash\n","wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png\n","wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png\n","```\n","\n","Now we can run the script:\n","```bash\n","export MODEL_DIR=\"stabilityai/stable-diffusion-xl-base-1.0\"\n","export OUTPUT_DIR=\"path to save model\"\n","\n","accelerate launch train_t2i_adapter_sdxl.py \\\n"," --pretrained_model_name_or_path=$MODEL_DIR \\\n"," --output_dir=$OUTPUT_DIR \\\n"," --dataset_name=fusing/fill50k \\\n"," --mixed_precision=\"fp16\" \\\n"," --resolution=1024 \\\n"," --learning_rate=1e-5 \\\n"," --max_train_steps=15000 \\\n"," --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\" \\\n"," --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\n"," --validation_steps=100 \\\n"," --train_batch_size=1 \\\n"," --gradient_accumulation_steps=4 \\\n"," --report_to=\"wandb\" \\\n"," --seed=42 \\\n"," --push_to_hub\n","```"],"metadata":{"id":"q9-8WZzvmahs"}},{"cell_type":"markdown","source":["Once training is complete, we can use our T2I-Adapter for inference:"],"metadata":{"id":"UH6PwHF0mxjQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QYbJSdvJCl5c"},"outputs":[],"source":["from diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, EulerAncestralDiscreteSchedulerTest\n","from diffusers.utils import load_image\n","import torch\n","\n","adapter = T2IAdapter.from_pretrained(\n","    \"path/to/adapter\",\n","    torch_dtype=torch.float16\n",")\n","pipeline = StableDiffusionXLAdapterPipeline.from_pretrained(\n","    \"stabilityai/stable-diffusion-xl-base-1.0\",\n","    adapter=adapter,\n","    torch_dtype=torch.float16\n",")\n","\n","pipeline.scheduler = EulerAncestralDiscreteSchedulerTest.from_config(pipe.scheduler.config)\n","pipeline.enable_xformers_memory_efficient_attention()\n","pipeline.enable_model_cpu_offload()"]},{"cell_type":"code","source":["control_image = load_image(\"./conditioning_image_1.png\")\n","prompt = \"pale golden rod circle with old lace background\"\n","\n","generator = torch.manual_seed(1111)\n","image = pipeline(\n","    prompt,\n","    image=control_image,\n","    generator=generator\n",").images[0]\n","image"],"metadata":{"id":"5Tq69zcqm9D4"},"execution_count":null,"outputs":[]}]}