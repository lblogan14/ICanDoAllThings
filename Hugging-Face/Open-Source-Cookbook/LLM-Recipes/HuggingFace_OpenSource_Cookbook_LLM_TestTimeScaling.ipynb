{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOEgPrCHHVJiuvQf+HzJ5Ls"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Scaling Test-Time Compute for Longer Thinking in LLMs"],"metadata":{"id":"xwuxY3KmWgyB"}},{"cell_type":"markdown","source":["In this example, we will extend the inference time for an **Instruct LLM system** using **test-time compute** to solve more challenging problems, such as **complex math problems**. This approach, inspired by OpenAI reasoning models, demonstrates that **longer reasoning time** during inference can enhance model peformance.\n","\n","The [blog post](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute) shows that smaller models, like the 1B and 3B Llama Instruct models, can outperform much larger ones on the MATH-500 benchmark when given enough \"time to think\".\n","\n","Research from Deepmind suggests that **test-time compte** can be scaled optimally through strategies like iterative self-refinement or using a reward model.\n","\n","The blog also introduces the [`search-and-learn`](https://github.com/huggingface/search-and-learn) repository. In this example, we will build a small chatbot that engages in longer reasoning to tackle harder problems using small open models."],"metadata":{"id":"VZD9DZC1a2kj"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"1PG-V4hbeAPx"}},{"cell_type":"markdown","source":["We need to install the `search-and-learn` repository."],"metadata":{"id":"i8AfOOlkO4l_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CzI_3mWMWbC4"},"outputs":[],"source":["!git clone https://github.com/huggingface/search-and-learn"]},{"cell_type":"code","source":["%cd search-and-learn\n","!pip install -e '.[dev]'"],"metadata":{"id":"-LN8dT6hPIBm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Set up the LLM and the Process Reward Model (PRM)"],"metadata":{"id":"EBbNiqQYPNuK"}},{"cell_type":"markdown","source":["The entire system consists of\n","- an LLM that generates intermediate answers based on user input,\n","- a PRM model, based on the paper [*Solving math word problems with process- and outcome-based feedback*](https://huggingface.co/papers/2211.14275), that evaluates and scores these answers,\n","- a search strategy that uses the PRM feedback to guide the subsequent steps in the search process until reaching the final answer.\n","\n","![system_LLM_PRM](https://huggingface.co/datasets/HuggingFaceH4/blogpost-images/resolve/main/system.png)\n","\n","\n","In this example, we will use the [`meta-llama/Llama-3.2-1B-Instruct`](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) model as the LLM. Next, we will use the [`RLHFlow/Llama3.1-8B-PRM-Deepseek-Data`](https://huggingface.co/RLHFlow/Llama3.1-8B-PRM-Deepseek-Data) model as our PRM."],"metadata":{"id":"8M1_mhlNPSzl"}},{"cell_type":"code","source":["import torch\n","from vllm import LLM\n","from sal.models.reward_models import RLHFFlow # sal --> search-and-learn\n","\n","llm_path = 'meta-llama/Llama-3.2-1B-Instruct'\n","prm_path = 'RLHFlow/Llama3.1-8B-PRM-Deepseek-Data'\n","\n","llm = LLM(\n","    model=llm_path,\n","    gpu_memory_utilization=0.5, # utilize 50% of GPU memory\n","    enable_prefix_caching=True, # optimize repeated prefix computations\n","    seed=111\n",")\n","\n","prm = RLHFFlow(prm_path)"],"metadata":{"id":"KoscGrruPSQZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Instantiate the question, search strategy, and call the pipeline"],"metadata":{"id":"e6fj1xWv6Uem"}},{"cell_type":"markdown","source":["1. **Instantiate the Question**: We will define the input question that the system will answer, considering the given context.\n","2. **Search Strategy**: The system supports: `best_of_n`, `beam_search`, and `dvts` (diverse verifier tree search). We will use `best_of_n` in this example.\n","3. **Call the pipeline**: With the question and search strategy in place, we will call the inference pipeline, processing the inputs through both the LLM and PRM to generate the final answer."],"metadata":{"id":"y56UnPo_6Zf5"}},{"cell_type":"markdown","source":["The first step is to clearly define the question that the system will answer."],"metadata":{"id":"qgShItYK7IPg"}},{"cell_type":"code","source":["question_text = \"Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$\"\n","input_batch = {'problem': [question_text]}"],"metadata":{"id":"GmBVW6ey6X9j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we define the configuration, including parameters like the number of candidate answers, and choose the search strategy that will be used. With the question and configuration in place, we use the selected search strategy to generate multiple candidate answers. These candidates are evalauted based on their relevance and quality and the final answer is returned."],"metadata":{"id":"tbI2mXQT7MMi"}},{"cell_type":"code","source":["from sal.config import Config\n","from sal.search import beam_search, best_of_n, dvts\n","\n","config = Config()\n","config.n = 32 # number of answers to generate during the search\n","\n","search_result = best_of_n(\n","    x=input_batch,\n","    config=config,\n","    llm=llm,\n","    prm=prm\n",")"],"metadata":{"id":"IReMrgDz7g0s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Display the final result"],"metadata":{"id":"iXtjIRoY7_ah"}},{"cell_type":"markdown","source":["Once the pipeline has processed the question through the LLM and PRM, we can display the final result. This result will be the model's output after considering the intermediate answers and scoring them using the PRM."],"metadata":{"id":"TMbifWMX8BdU"}},{"cell_type":"code","source":["search_reuslt['pred'][0]"],"metadata":{"id":"P4zjD8e18A4A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We may need to remove speical tokens from specific models."],"metadata":{"id":"jnC48vBm8Rzd"}},{"cell_type":"code","source":["formatted_output = search_result['pred'][0].replace(\n","    \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n","    \"\"\n",").strip()\n","formatted_output"],"metadata":{"id":"vGLbDbvX8Zjh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After removing any speical tokens, we can display the final answer to the user."],"metadata":{"id":"IjqCI4Uy8jos"}},{"cell_type":"code","source":["from IPython.display import display, Markdown\n","display(Markdown(formatted_output))"],"metadata":{"id":"iMq9sSJO8nNb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Assemble it all"],"metadata":{"id":"67QISmal8pQQ"}},{"cell_type":"markdown","source":["Now we will create a function that encapsulates the entire pipeline so that we can easily reuse the process in the future."],"metadata":{"id":"N16J2itx85TC"}},{"cell_type":"code","source":["import time\n","\n","def generate_with_search_and_learn(question, config, llm, prm, method='best_of_n'):\n","    \"\"\"Generate an answer for a given question using the search-and-learn pipeline.\n","\n","    Parameters\n","    ----------\n","    question: str\n","        The input question to generate an answer for\n","    config: Config\n","        Configuration object containing parameters for search strategy\n","    llm: LLM\n","        Pretrained large language model used for generating answers\n","    prm: RLHFFlow\n","        Process reward model used for evaluating answers\n","    method: str\n","        Search strategy to use for generating answers\n","\n","    Returns\n","    -------\n","    str\n","        The formatted output after processing the question\n","    \"\"\"\n","    batch = {'problem': [question]}\n","\n","    start_time = time.time()\n","    if method == 'best_of_n':\n","        result = best_of_n(x=batch, config=config, llm=llm, prm=prm)\n","    elif method == 'beam_search':\n","        result = beam_search(examples=batch, config=config, llm=llm, prm=prm)\n","    elif method == 'dvts':\n","        result = dvts(examples=batch, config=config, llm=llm, prm=prm)\n","\n","    elapsed_time = time.time() - start_time\n","    print(f\"\\nFinished in {elapsed_time:.2f} seconds\\n\")\n","\n","    tokenizer = llm.get_tokenizer()\n","    total_tokens = 0\n","    for completion in reuslt['completions']:\n","        for comp in completion:\n","            output_tokens = tokenizer.encode(comp)\n","            total_tokens += len(output_tokens)\n","\n","    print(f\"Total tokens in all completions: {total_tokens}\")\n","\n","    formatted_output = result[\"pred\"][0].replace(\n","        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n","        \"\"\n","    ).strip()\n","    return formatted_output"],"metadata":{"id":"mE9WCb568rRl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### `best_of_n`"],"metadata":{"id":"IDGRo2o8-JIC"}},{"cell_type":"code","source":["question = \"Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$\"\n","\n","config.n = 8\n","\n","formatted_output = generate_with_search_and_learn(\n","    question=question,\n","    config=config,\n","    llm=llm,\n","    prm=prm,\n","    method=\"best_of_n\"\n",")"],"metadata":{"id":"CR8I-hzp-Lzg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display(Markdown(formatted_output))"],"metadata":{"id":"K3jRRnwE-OSt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### `beam_search`"],"metadata":{"id":"i3T2Bud9-Ozi"}},{"cell_type":"code","source":["question = \"Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$\"\n","\n","config.n = 8\n","# beam search specific\n","config.sort_completed = True\n","config.filter_duplicates = True\n","\n","formatted_output = generate_with_search_and_learn(\n","    question=question,\n","    config=config,\n","    llm=llm,\n","    prm=prm,\n","    method=\"beam_search\"\n",")"],"metadata":{"id":"Hs5pn794-QDg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display(Markdown(formatted_output))"],"metadata":{"id":"iwfULTvi-R9N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### `dvts` (Diverse Verifier Tree Search)"],"metadata":{"id":"w9ZX1t-R-Spq"}},{"cell_type":"code","source":["question = \"Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$\"\n","\n","config.n = 8\n","# dvts specific\n","config.n_beams = config.n // config.beam_width\n","\n","formatted_output = generate_with_search_and_learn(\n","    question=question,\n","    config=config,\n","    llm=llm,\n","    prm=prm,\n","    method=\"dvts\"\n",")"],"metadata":{"id":"tWHfV_dr-V9b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display(Markdown(formatted_output))"],"metadata":{"id":"WUaSsDRy-YXS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Test the system with a simple question"],"metadata":{"id":"tvac3_NO-Z-_"}},{"cell_type":"markdown","source":["We also want to test system using a straightforward question to observe how it performs in simpler cases. This allows us to verify that the system works as expected even for basic queries."],"metadata":{"id":"Han06Lvg-cuf"}},{"cell_type":"code","source":["question = \"What's the capital of Spain?\"\n","\n","config.n = 32\n","\n","formatted_output = generate_with_search_and_learn(\n","    question=question,\n","    config=config,\n","    llm=llm,\n","    prm=prm,\n","    method=\"best_of_n\"\n",")"],"metadata":{"id":"bs_wPLY1-bw3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display(Markdown(formatted_output))"],"metadata":{"id":"CDlOvYS5-onr"},"execution_count":null,"outputs":[]}]}