{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxSXF9aT1y8Y"
   },
   "source": [
    "# RAG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GncLqT8J10uB"
   },
   "source": [
    "In this example, we will evaluate our RAG by building a synthetic evaluation dataset and using LLM-as-a-judge to compute the accuracy of our system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNCjwZt52uYM"
   },
   "source": [
    "## Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 133378,
     "status": "ok",
     "timestamp": 1744142584701,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "ZujXmKTc2vr-",
    "outputId": "03c16cbf-2dd7-47b8-8bad-01d979c2c73b"
   },
   "outputs": [],
   "source": [
    "!pip install -qU torch transformers langchain sentence-transformers tqdm openpyxl openai pandas datasets langchain-community ragatouille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55_3LWEn2x6V"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UyMQuyG26Pg"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JlT1KLF2Cf4"
   },
   "source": [
    "## Evaluate RAG performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0t7f1nT2MyF"
   },
   "source": [
    "For our evaluation pipeline, we will need\n",
    "1. an evaluation dataset with question - answer couples (QA couples)\n",
    "2. an evaluator to compute the accuracy of our system on the above evaluation dataset.\n",
    "\n",
    "We can apply LLMs to help us solve the above two goals:\n",
    "1. The evaluation dataset will be synthetically generated by an LLM, and questions will be filtered out by other LLMs.\n",
    "2. An [LLM-as-a-judge](https://huggingface.co/papers/2306.05685) will then perform the evaluation on this synthetic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpXpvYBn3AsX"
   },
   "source": [
    "### Load our knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "e7d9b381943b4227b7c7641adfff1a83",
      "f055b4104d0a49e7a1e5bd7da5a5b207",
      "68c32a21af96465d9fc6857029fccba2",
      "cafe8d6c6dd14caa9e514f1fa9e76084",
      "db67adef870f4632b81723414873cf24",
      "8a280285e8564bc7a9191729d9e2ebc0",
      "165980de44fe4525a828d5465b1d4385",
      "f9bbe99211874da5afb89fcd4fae00ea",
      "f544273a86e2412ba28fcad531acc0b5",
      "726a26c6510149b6bcef64c3fffe95fb",
      "ff230fdc29744aefb68d84edc97c34b5",
      "deb6c2803e954e15923a089e6182f9f8",
      "c74bc18e7bb84ae2b244ee545ce7f7e8",
      "67ebacc03373438eb80a7386b83c036e",
      "ce7aa2debdab4326891ff311952d0306",
      "e8e1437fe70943bca01c5268884d3d3e",
      "ddc2f4b063444aedae943c76ac862fa8",
      "0d553945833048d49fda8178f7324dcf",
      "a24aeb8ddd0641c3b993ba1dadc9a11d",
      "f8aeb8f1a9864733a0f4ed1feb1caa3c",
      "c1c2363c822f44899cfee49df4c92470",
      "7ff1a5358a25448599401ae2d19d725c",
      "c7c0b503629f41419aa3c3ca942ffed1",
      "463b249bdb574dc497fe47db67130646",
      "b5d4d92cda5949229389527d527f36ef",
      "8a9e02f56d6344488a370f16247f135d",
      "d264ad64ec384df3a7985744053cb0fb",
      "21b1a6017f6e45e9afde017b4d70eb04",
      "57a23eb9dbf74ed99a906c7c0efebf7f",
      "ff8a4aba8a8f40f5be6d1b6853fd880c",
      "f235e1bcce8c4377869f1aa815e11704",
      "2e69c2c07cc745479977a087cbccd9b8",
      "ca6b9793e86a491896bdaad3043241ec"
     ]
    },
    "executionInfo": {
     "elapsed": 4554,
     "status": "ok",
     "timestamp": 1744143612028,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "4hUt63Ee1vV5",
    "outputId": "1f3662b4-db31-4050-da34-90df30d7de01"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d9b381943b4227b7c7641adfff1a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb6c2803e954e15923a089e6182f9f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "huggingface_doc.csv:   0%|          | 0.00/22.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c0b503629f41419aa3c3ca942ffed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2647 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = datasets.load_dataset('m-ric/huggingface_doc', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94XI5iAv3HI4"
   },
   "source": [
    "## Build a synthetic dataset for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrYBoHys3KuO"
   },
   "source": [
    "We first build a synthetic dataset of questions and associated contexts. This method is to get elements from our knowledge base, and ask an LLM to generate questions based on these documents. Then we set up other LLM agents to act as quality filters for the generated QA couples: each of them will act as the filter for a specific flaw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyN7NxEt3feh"
   },
   "source": [
    "### Prepare source documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "cce96efd839f4161951a15222d1c367e",
      "e658fbd19d2a42fea1ab51336cb1e523",
      "f8ac2da15fe3425cb4a7fc436dd612ae",
      "0e5754124c2644fe8de852094d9e1b00",
      "5ee28a22e54a4ad3b170ca79a3555a3a",
      "2dce3c527137466eb58033de9fed4541",
      "e21bcd389054465aabd1199e0de4bf81",
      "9c5e681a851b430588b4c22315bcae28",
      "02ae72e679734d98aa7c61a82d0806c5",
      "a6a4d9368d584850906dfa15724737ef",
      "6604566c902c4715ba68957ab6d5f5a4"
     ]
    },
    "executionInfo": {
     "elapsed": 2930,
     "status": "ok",
     "timestamp": 1744143626513,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "fnSOPPqc3JSX",
    "outputId": "5e89832a-5d9a-49a6-c9e6-58932c34b272"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce96efd839f4161951a15222d1c367e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "langchain_docs = [\n",
    "    LangchainDocument(\n",
    "        page_content=doc['text'],\n",
    "        metadata={'source': doc['source']}\n",
    "    )\n",
    "    for doc in tqdm(ds)\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    separators=['\\n\\n', '\\n', '.', ' ', '']\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cj2ZQuu4FEw"
   },
   "source": [
    "### Setup agents for question generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGAul-Mu4Ihf"
   },
   "source": [
    "We use [`Mixtral-8x7B-Instruct-v0.1`](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) for QA couple generation in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1744143945608,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "u2uoGPDb4G0i",
    "outputId": "e8a76457-827e-4c9f-b035-4bf5079553ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'This is a test context for the `@mui/material` library.\\n\\n## Installation\\n\\n```sh\\nnpm install @mui/material\\n```\\n\\n## Usage\\n\\n```jsx\\nimport React from \\'react\\';\\nimport { Button } from \\'@mui/material\\';\\n\\nfunction App() {\\n  return (\\n    <div className=\"App\">\\n      <Button variant=\"contained\" color=\"primary\">\\n        Hello World\\n      </Button>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\n## Documentation\\n\\n- [Material-UI](https://material-ui.com/)\\n- [Material Design](https://material.io/)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "repo_id = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120\n",
    ")\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.text_generation(\n",
    "        prompt,\n",
    "        max_new_tokens=1000\n",
    "    )\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "call_llm(llm_client, \"This is a test context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "suh9CvJZ4y9W"
   },
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWPESjeO5SMj"
   },
   "source": [
    "Now we can generate our QA couples. In this example, we will generate 10 QA couples and load the rest from the Hub.\n",
    "\n",
    "For our specific knowledge base, given that we want to get at least ~100 test samples, and accounting for the fact that we will filter out around half of these with our critique agents later on, we should generate much more, in aout >200 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "aa62285e008b420c841a1930a009b47b",
      "e55eadac10e242cd860d3fd81083b5dd",
      "64a3bde810b24fa5bf7825ac93e4a543",
      "6c749aa475d643c898899e3f767f445a",
      "05dea80217674d418af69654a98bb813",
      "9ff2e7fbba8946008765ad18941238e7",
      "7306e93fee704cd2b2930b1b062896fe",
      "e351451215d9497693d752611de882c3",
      "e9350be5dc3c4b5cb32a0e0756cd641c",
      "ab66c645787041b49137ad8a8b75e369",
      "c9694ef2bb7347d18d70aa719f054f80"
     ]
    },
    "executionInfo": {
     "elapsed": 19085,
     "status": "ok",
     "timestamp": 1744143760883,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "I29ydUaW5n54",
    "outputId": "c02daaab-1002-48f6-813b-e20e03c9812f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10 QA couples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa62285e008b420c841a1930a009b47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# limit this for cost and time consideraton\n",
    "N_GENERATIONS = 10\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    # Generate QA couple\n",
    "    output_QA_couple = call_llm(\n",
    "        llm_client,\n",
    "        QA_generation_prompt.format(context=sampled_context.page_content)\n",
    "    )\n",
    "    try:\n",
    "        question = output_QA_couple.split(\"Factoid question: \")[-1].split('Answer: ')[0]\n",
    "        answer = output_QA_couple.split(\"Answer: \")[-1]\n",
    "        assert len(answer) < 300, \"Answer is too long\"\n",
    "\n",
    "        outputs.append({\n",
    "            'context': sampled_context.page_content,\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'source_doc': sampled_context.metadata['source']\n",
    "        })\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 941
    },
    "executionInfo": {
     "elapsed": 118,
     "status": "ok",
     "timestamp": 1744143763000,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "OJXj1eeT6mfu",
    "outputId": "8c02d882-9cc0-42f8-d125-e73c2da80c34"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"<figure class=\\\"image text-center\\\">\\n  <img src=\\\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/apiwizard.png\\\">\\n</figure> \\n\\nNow that the API is set up, you can make calls from your scripts to the API. Let's look at an example of performing a Sentence Similarity task:\\n\\n```\\nusing HuggingFace.API;\\n\\n/* other code */\\n\\n// Make a call to the API\\nvoid Query() {\\n    string inputText = \\\"I'm on my way to the forest.\\\";\\n    string[] candidates = {\\n        \\\"The player is going to the city\\\",\\n        \\\"The player is going to the wilderness\\\",\\n        \\\"The player is wandering aimlessly\\\"\\n    };\\n    HuggingFaceAPI.SentenceSimilarity(inputText, OnSuccess, OnError, candidates);\\n}\\n\\n// If successful, handle the result\\nvoid OnSuccess(float[] result) {\\n    foreach(float value in result) {\\n        Debug.Log(value);\\n    }\\n}\\n\\n// Otherwise, handle the error\\nvoid OnError(string error) {\\n    Debug.LogError(error);\\n}\\n\\n/* other code */\\n```\\n\\n## Supported Tasks and Custom Models\\n\\nThe Hugging Face Unity API also currently supports the following tasks:\\n\\n- [Conversation](https://huggingface.co/tasks/conversational)\\n- [Text Generation](https://huggingface.co/tasks/text-generation)\\n- [Text to Image](https://huggingface.co/tasks/text-to-image)\\n- [Text Classification](https://huggingface.co/tasks/text-classification)\\n- [Question Answering](https://huggingface.co/tasks/question-answering)\\n- [Translation](https://huggingface.co/tasks/translation)\\n- [Summarization](https://huggingface.co/tasks/summarization)\\n- [Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition)\\n\\nUse the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.\\n\\nTo use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.\\n\\n## Usage Tips\",\n          \"```py\\noutputs = model(**batch)\\nprint(outputs.loss, outputs.logits.shape)\\n```\\n\\n```python out\\ntensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])\\n```\\n\\nAll \\ud83e\\udd17 Transformers models will return the loss when `labels` are provided, and we also get the logits (two for each input in our batch, so a tensor of size 8 x 2).\\n\\nWe're almost ready to write our training loop! We're just missing two things: an optimizer and a learning rate scheduler. Since we are trying to replicate what the `Trainer` was doing by hand, we will use the same defaults. The optimizer used by the `Trainer` is `AdamW`, which is the same as Adam, but with a twist for weight decay regularization (see [\\\"Decoupled Weight Decay Regularization\\\"](https://arxiv.org/abs/1711.05101) by Ilya Loshchilov and Frank Hutter):\\n\\n```py\\nfrom transformers import AdamW\\n\\noptimizer = AdamW(model.parameters(), lr=5e-5)\\n```\\n\\nFinally, the learning rate scheduler used by default is just a linear decay from the maximum value (5e-5) to 0. To properly define it, we need to know the number of training steps we will take, which is the number of epochs we want to run multiplied by the number of training batches (which is the length of our training dataloader). The `Trainer` uses three epochs by default, so we will follow that:\\n\\n```py\\nfrom transformers import get_scheduler\\n\\nnum_epochs = 3\\nnum_training_steps = num_epochs * len(train_dataloader)\\nlr_scheduler = get_scheduler(\\n    \\\"linear\\\",\\n    optimizer=optimizer,\\n    num_warmup_steps=0,\\n    num_training_steps=num_training_steps,\\n)\\nprint(num_training_steps)\\n```\\n\\n```python out\\n1377\\n```\\n\\n### The training loop[[the-training-loop]]\\n\\nOne last thing: we will want to use the GPU if we have access to one (on a CPU, training might take several hours instead of a couple of minutes). To do this, we define a `device` we will put our model and our batches on:\\n\\n```py\\nimport torch\\n\\ndevice = torch.device(\\\"cuda\\\") if torch.cuda.is_available() else torch.device(\\\"cpu\\\")\\nmodel.to(device)\\ndevice\\n```\",\n          \"Scores are calculated for individual translated segments\\u2014generally sentences\\u2014by comparing them with a set of good quality reference translations. Those scores are then averaged over the whole corpus to reach an estimate of the translation's overall quality. Neither intelligibility nor grammatical correctness are not taken into account.\\n\\n## Intended Uses\\nBLEU and BLEU-derived metrics are most often used for machine translation.\\n\\n## How to Use\\n\\nThis metric takes as input a list of predicted sentences and a list of lists of reference sentences (since each predicted sentence can have multiple references):\\n\\n```python\\n>>> predictions = [\\\"hello there general kenobi\\\", \\\"foo bar foobar\\\"]\\n>>> references = [\\n...     [\\\"hello there general kenobi\\\", \\\"hello there !\\\"],\\n...     [\\\"foo bar foobar\\\"]\\n... ]\\n>>> bleu = evaluate.load(\\\"bleu\\\")\\n>>> results = bleu.compute(predictions=predictions, references=references)\\n>>> print(results)\\n{'bleu': 1.0, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.1666666666666667, 'translation_length': 7, 'reference_length': 6}\\n```\\n\\n### Inputs\\n- **predictions** (`list` of `str`s): Translations to score.\\n- **references** (`list` of `list`s of `str`s): references for each translation.\\n- ** tokenizer** : approach used for standardizing `predictions` and `references`.\\n    The default tokenizer is `tokenizer_13a`, a relatively minimal tokenization approach that is however equivalent to `mteval-v13a`, used by WMT.\\n    This can be replaced by another tokenizer from a source such as [SacreBLEU](https://github.com/mjpost/sacrebleu/tree/master/sacrebleu/tokenizers).\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"What tasks does the Hugging Face Unity API support?\\n\",\n          \"What is the number of training steps?\\n\",\n          \"What is the input format for BLEU metric?\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"The Hugging Face Unity API supports the following tasks: Conversation, Text Generation, Text to Image, Text Classification, Question Answering, Translation, Summarization, and Speech Recognition.\",\n          \"The number of training steps is 1377.\",\n          \"The BLEU metric takes as input a list of predicted sentences and a list of lists of reference sentences.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source_doc\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"huggingface/blog/blob/main/unity-api.md\",\n          \"huggingface/course/blob/main/chapters/en/chapter3/4.mdx\",\n          \"huggingface/evaluate/blob/main/metrics/bleu/README.md\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-6045362e-b9f7-4153-84a6-b3b4343eb21a\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;figure class=\"image text-center\"&gt;\\n  &lt;img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/apiwizard.png\"&gt;\\n&lt;/figure&gt; \\n\\nNow that the API is set up, you can make calls from your scripts to the API. Let's look at an example of performing a Sentence Similarity task:\\n\\n```\\nusing HuggingFace.API;\\n\\n/* other code */\\n\\n// Make a call to the API\\nvoid Query() {\\n    string inputText = \"I'm on my way to the forest.\";\\n    string[] candidates = {\\n        \"The player is going to the city\",\\n        \"The player is going to the wilderness\",\\n        \"The player is wandering aimlessly\"\\n    };\\n    HuggingFaceAPI.SentenceSimilarity(inputText, OnSuccess, OnError, candidates);\\n}\\n\\n// If successful, handle the result\\nvoid OnSuccess(float[] result) {\\n    foreach(float value in result) {\\n        Debug.Log(value);\\n    }\\n}\\n\\n// Otherwise, handle the error\\nvoid OnError(string error) {\\n    Debug.LogError(error);\\n}\\n\\n/* other code */\\n```\\n\\n## Supported Tasks and Custom Models\\n\\nThe Hugging Face Unity API also currently supports the following tasks:\\n\\n- [Conversation](https://huggingface.co/tasks/conversational)\\n- [Text Generation](https://huggingface.co/tasks/text-generation)\\n- [Text to Image](https://huggingface.co/tasks/text-to-image)\\n- [Text Classification](https://huggingface.co/tasks/text-classification)\\n- [Question Answering](https://huggingface.co/tasks/question-answering)\\n- [Translation](https://huggingface.co/tasks/translation)\\n- [Summarization](https://huggingface.co/tasks/summarization)\\n- [Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition)\\n\\nUse the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.\\n\\nTo use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.\\n\\n## Usage Tips</td>\n",
       "      <td>What tasks does the Hugging Face Unity API support?\\n</td>\n",
       "      <td>The Hugging Face Unity API supports the following tasks: Conversation, Text Generation, Text to Image, Text Classification, Question Answering, Translation, Summarization, and Speech Recognition.</td>\n",
       "      <td>huggingface/blog/blob/main/unity-api.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>```py\\noutputs = model(**batch)\\nprint(outputs.loss, outputs.logits.shape)\\n```\\n\\n```python out\\ntensor(0.5441, grad_fn=&lt;NllLossBackward&gt;) torch.Size([8, 2])\\n```\\n\\nAll ðŸ¤— Transformers models will return the loss when `labels` are provided, and we also get the logits (two for each input in our batch, so a tensor of size 8 x 2).\\n\\nWe're almost ready to write our training loop! We're just missing two things: an optimizer and a learning rate scheduler. Since we are trying to replicate what the `Trainer` was doing by hand, we will use the same defaults. The optimizer used by the `Trainer` is `AdamW`, which is the same as Adam, but with a twist for weight decay regularization (see [\"Decoupled Weight Decay Regularization\"](https://arxiv.org/abs/1711.05101) by Ilya Loshchilov and Frank Hutter):\\n\\n```py\\nfrom transformers import AdamW\\n\\noptimizer = AdamW(model.parameters(), lr=5e-5)\\n```\\n\\nFinally, the learning rate scheduler used by default is just a linear decay from the maximum value (5e-5) to 0. To properly define it, we need to know the number of training steps we will take, which is the number of epochs we want to run multiplied by the number of training batches (which is the length of our training dataloader). The `Trainer` uses three epochs by default, so we will follow that:\\n\\n```py\\nfrom transformers import get_scheduler\\n\\nnum_epochs = 3\\nnum_training_steps = num_epochs * len(train_dataloader)\\nlr_scheduler = get_scheduler(\\n    \"linear\",\\n    optimizer=optimizer,\\n    num_warmup_steps=0,\\n    num_training_steps=num_training_steps,\\n)\\nprint(num_training_steps)\\n```\\n\\n```python out\\n1377\\n```\\n\\n### The training loop[[the-training-loop]]\\n\\nOne last thing: we will want to use the GPU if we have access to one (on a CPU, training might take several hours instead of a couple of minutes). To do this, we define a `device` we will put our model and our batches on:\\n\\n```py\\nimport torch\\n\\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\\nmodel.to(device)\\ndevice\\n```</td>\n",
       "      <td>What is the number of training steps?\\n</td>\n",
       "      <td>The number of training steps is 1377.</td>\n",
       "      <td>huggingface/course/blob/main/chapters/en/chapter3/4.mdx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scores are calculated for individual translated segmentsâ€”generally sentencesâ€”by comparing them with a set of good quality reference translations. Those scores are then averaged over the whole corpus to reach an estimate of the translation's overall quality. Neither intelligibility nor grammatical correctness are not taken into account.\\n\\n## Intended Uses\\nBLEU and BLEU-derived metrics are most often used for machine translation.\\n\\n## How to Use\\n\\nThis metric takes as input a list of predicted sentences and a list of lists of reference sentences (since each predicted sentence can have multiple references):\\n\\n```python\\n&gt;&gt;&gt; predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\\n&gt;&gt;&gt; references = [\\n...     [\"hello there general kenobi\", \"hello there !\"],\\n...     [\"foo bar foobar\"]\\n... ]\\n&gt;&gt;&gt; bleu = evaluate.load(\"bleu\")\\n&gt;&gt;&gt; results = bleu.compute(predictions=predictions, references=references)\\n&gt;&gt;&gt; print(results)\\n{'bleu': 1.0, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.1666666666666667, 'translation_length': 7, 'reference_length': 6}\\n```\\n\\n### Inputs\\n- **predictions** (`list` of `str`s): Translations to score.\\n- **references** (`list` of `list`s of `str`s): references for each translation.\\n- ** tokenizer** : approach used for standardizing `predictions` and `references`.\\n    The default tokenizer is `tokenizer_13a`, a relatively minimal tokenization approach that is however equivalent to `mteval-v13a`, used by WMT.\\n    This can be replaced by another tokenizer from a source such as [SacreBLEU](https://github.com/mjpost/sacrebleu/tree/master/sacrebleu/tokenizers).</td>\n",
       "      <td>What is the input format for BLEU metric?\\n</td>\n",
       "      <td>The BLEU metric takes as input a list of predicted sentences and a list of lists of reference sentences.</td>\n",
       "      <td>huggingface/evaluate/blob/main/metrics/bleu/README.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6045362e-b9f7-4153-84a6-b3b4343eb21a')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-6045362e-b9f7-4153-84a6-b3b4343eb21a button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-6045362e-b9f7-4153-84a6-b3b4343eb21a');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-7700e3ff-c34b-4e7a-ba5a-aa2633eecf52\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7700e3ff-c34b-4e7a-ba5a-aa2633eecf52')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-7700e3ff-c34b-4e7a-ba5a-aa2633eecf52 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     context  \\\n",
       "0                                                                                                                                                                  <figure class=\"image text-center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/apiwizard.png\">\\n</figure> \\n\\nNow that the API is set up, you can make calls from your scripts to the API. Let's look at an example of performing a Sentence Similarity task:\\n\\n```\\nusing HuggingFace.API;\\n\\n/* other code */\\n\\n// Make a call to the API\\nvoid Query() {\\n    string inputText = \"I'm on my way to the forest.\";\\n    string[] candidates = {\\n        \"The player is going to the city\",\\n        \"The player is going to the wilderness\",\\n        \"The player is wandering aimlessly\"\\n    };\\n    HuggingFaceAPI.SentenceSimilarity(inputText, OnSuccess, OnError, candidates);\\n}\\n\\n// If successful, handle the result\\nvoid OnSuccess(float[] result) {\\n    foreach(float value in result) {\\n        Debug.Log(value);\\n    }\\n}\\n\\n// Otherwise, handle the error\\nvoid OnError(string error) {\\n    Debug.LogError(error);\\n}\\n\\n/* other code */\\n```\\n\\n## Supported Tasks and Custom Models\\n\\nThe Hugging Face Unity API also currently supports the following tasks:\\n\\n- [Conversation](https://huggingface.co/tasks/conversational)\\n- [Text Generation](https://huggingface.co/tasks/text-generation)\\n- [Text to Image](https://huggingface.co/tasks/text-to-image)\\n- [Text Classification](https://huggingface.co/tasks/text-classification)\\n- [Question Answering](https://huggingface.co/tasks/question-answering)\\n- [Translation](https://huggingface.co/tasks/translation)\\n- [Summarization](https://huggingface.co/tasks/summarization)\\n- [Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition)\\n\\nUse the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.\\n\\nTo use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.\\n\\n## Usage Tips   \n",
       "1  ```py\\noutputs = model(**batch)\\nprint(outputs.loss, outputs.logits.shape)\\n```\\n\\n```python out\\ntensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])\\n```\\n\\nAll ðŸ¤— Transformers models will return the loss when `labels` are provided, and we also get the logits (two for each input in our batch, so a tensor of size 8 x 2).\\n\\nWe're almost ready to write our training loop! We're just missing two things: an optimizer and a learning rate scheduler. Since we are trying to replicate what the `Trainer` was doing by hand, we will use the same defaults. The optimizer used by the `Trainer` is `AdamW`, which is the same as Adam, but with a twist for weight decay regularization (see [\"Decoupled Weight Decay Regularization\"](https://arxiv.org/abs/1711.05101) by Ilya Loshchilov and Frank Hutter):\\n\\n```py\\nfrom transformers import AdamW\\n\\noptimizer = AdamW(model.parameters(), lr=5e-5)\\n```\\n\\nFinally, the learning rate scheduler used by default is just a linear decay from the maximum value (5e-5) to 0. To properly define it, we need to know the number of training steps we will take, which is the number of epochs we want to run multiplied by the number of training batches (which is the length of our training dataloader). The `Trainer` uses three epochs by default, so we will follow that:\\n\\n```py\\nfrom transformers import get_scheduler\\n\\nnum_epochs = 3\\nnum_training_steps = num_epochs * len(train_dataloader)\\nlr_scheduler = get_scheduler(\\n    \"linear\",\\n    optimizer=optimizer,\\n    num_warmup_steps=0,\\n    num_training_steps=num_training_steps,\\n)\\nprint(num_training_steps)\\n```\\n\\n```python out\\n1377\\n```\\n\\n### The training loop[[the-training-loop]]\\n\\nOne last thing: we will want to use the GPU if we have access to one (on a CPU, training might take several hours instead of a couple of minutes). To do this, we define a `device` we will put our model and our batches on:\\n\\n```py\\nimport torch\\n\\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\\nmodel.to(device)\\ndevice\\n```   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                              Scores are calculated for individual translated segmentsâ€”generally sentencesâ€”by comparing them with a set of good quality reference translations. Those scores are then averaged over the whole corpus to reach an estimate of the translation's overall quality. Neither intelligibility nor grammatical correctness are not taken into account.\\n\\n## Intended Uses\\nBLEU and BLEU-derived metrics are most often used for machine translation.\\n\\n## How to Use\\n\\nThis metric takes as input a list of predicted sentences and a list of lists of reference sentences (since each predicted sentence can have multiple references):\\n\\n```python\\n>>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\\n>>> references = [\\n...     [\"hello there general kenobi\", \"hello there !\"],\\n...     [\"foo bar foobar\"]\\n... ]\\n>>> bleu = evaluate.load(\"bleu\")\\n>>> results = bleu.compute(predictions=predictions, references=references)\\n>>> print(results)\\n{'bleu': 1.0, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.1666666666666667, 'translation_length': 7, 'reference_length': 6}\\n```\\n\\n### Inputs\\n- **predictions** (`list` of `str`s): Translations to score.\\n- **references** (`list` of `list`s of `str`s): references for each translation.\\n- ** tokenizer** : approach used for standardizing `predictions` and `references`.\\n    The default tokenizer is `tokenizer_13a`, a relatively minimal tokenization approach that is however equivalent to `mteval-v13a`, used by WMT.\\n    This can be replaced by another tokenizer from a source such as [SacreBLEU](https://github.com/mjpost/sacrebleu/tree/master/sacrebleu/tokenizers).   \n",
       "\n",
       "                                                question  \\\n",
       "0  What tasks does the Hugging Face Unity API support?\\n   \n",
       "1                What is the number of training steps?\\n   \n",
       "2            What is the input format for BLEU metric?\\n   \n",
       "\n",
       "                                                                                                                                                                                                answer  \\\n",
       "0  The Hugging Face Unity API supports the following tasks: Conversation, Text Generation, Text to Image, Text Classification, Question Answering, Translation, Summarization, and Speech Recognition.   \n",
       "1                                                                                                                                                                The number of training steps is 1377.   \n",
       "2                                                                                             The BLEU metric takes as input a list of predicted sentences and a list of lists of reference sentences.   \n",
       "\n",
       "                                                source_doc  \n",
       "0                  huggingface/blog/blob/main/unity-api.md  \n",
       "1  huggingface/course/blob/main/chapters/en/chapter3/4.mdx  \n",
       "2    huggingface/evaluate/blob/main/metrics/bleu/README.md  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(outputs).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R16rZfJn6poX"
   },
   "source": [
    "### Setup critique agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uw3uOvfE7wau"
   },
   "source": [
    "After the previous agent generated questions, we should do a quality check before validating these questions, so we build a critique agent that will rate each question on several crteria according to the paper [*ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent*](https://huggingface.co/papers/2312.10003):\n",
    "* **Groundedness** - Can the question be answered from the given context?\n",
    "* **Relevance** - Is this question relevant to users? For example, `\"What is the date when transformers 4.29.1 was released?\"` is not relevant for ML practitioners.\n",
    "\n",
    "Another failure case is when a function is tailored for the particular setting where the question was generated, but undecipherable by itself, for example, `\"What is the name of the function used in this guide?\"`. Hence, we also build a critique agent for this criteria:\n",
    "* **Standalone** - Is the question understandable free of any context, for someone with domain knowledge/Internet access? The opposite of this would be `\"What is the function used in this article?\"` for a question generated from a specific article.\n",
    "\n",
    "\n",
    "We systematically score functions with all these agents, and whenever the score is too low for any one of the agents, we will eliminate the question from our eval dataset.\n",
    "\n",
    "When asking the agents to output a score, we first ask them to produce its rationale. This will help us verify scores, but most importantly, asking it to first output raionale give the model more tokens to think and elaborate an answer before summarizing it into a single score token.\n",
    "\n",
    "Now that we have defined everything, we can build and run these critique agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpPrvJKY6q-0"
   },
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer:::\n",
    "\"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer:::\n",
    "\"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independent this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer:::\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121,
     "referenced_widgets": [
      "d5774f1c21ed442ca2f7b60efa0f3beb",
      "59acd8d10634491597c72fa70eab3bdb",
      "b79a8aea8bee4542aa5284f58ec1ba36",
      "9eac826b95334b57b6d09024153e8a2c",
      "215897fd1dbe44eaafd9360c36db3e57",
      "1d171882cc134d5e853fe2cc41291607",
      "3809f6f35d99425db67364d3efe12b77",
      "027439343e7e47918ed9820b38a5ee6f",
      "896e92e2f0014757994225bd216cf2ef",
      "26f7b403bd954ddd8297116c053b349d",
      "5985819b965c466483c2ff2f388529db"
     ]
    },
    "executionInfo": {
     "elapsed": 45490,
     "status": "ok",
     "timestamp": 1744143831683,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "eAniZqp89dad",
    "outputId": "ae1e8ae6-2c78-47c5-e689-9a9b82774710"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating critique for each QA couple...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5774f1c21ed442ca2f7b60efa0f3beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "for output in tqdm(outputs):\n",
    "    evaluations = {\n",
    "        'groundedness': call_llm(\n",
    "            llm_client,\n",
    "            question_groundedness_critique_prompt.format(\n",
    "                context=output['context'],\n",
    "                question=output['question']\n",
    "            )\n",
    "        ),\n",
    "        'relevance': call_llm(\n",
    "            llm_client,\n",
    "            question_relevance_critique_prompt.format(\n",
    "                question=output['question']\n",
    "            )\n",
    "        ),\n",
    "        'standalone': call_llm(\n",
    "            llm_client,\n",
    "            question_standalone_critique_prompt.format(\n",
    "                question=output['question']\n",
    "            )\n",
    "        )\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            # key - criterion, value - evaluation\n",
    "            score, eval_res = (\n",
    "                int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
    "                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1]\n",
    "            )\n",
    "            output.update(\n",
    "                {\n",
    "                    f\"{criterion}_score\": score,\n",
    "                    f\"{criterion}_eval\": eval_res\n",
    "                }\n",
    "            )\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6ef2BXn-q8T"
   },
   "source": [
    "Now we need to filter out bad questions based on our critique agent scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jqc7MQN3-uKn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "\n",
    "print(\"Evaluation dataset before filtering:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            'question',\n",
    "            'answer',\n",
    "            'groundedness_score',\n",
    "            'relevance_score',\n",
    "            'standalone_score'\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "generated_questions = generated_questions.loc[\n",
    "    (generated_questions['groundedness_score'] >= 4)\n",
    "    & (generated_questions['relevance_score'] >= 4)\n",
    "    & (generated_questions['standalone_score'] >= 4)\n",
    "]\n",
    "print('='*50)\n",
    "print('Final evaluation dataset:')\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            'question',\n",
    "            'answer',\n",
    "            'groundedness_score',\n",
    "            'relevance_score',\n",
    "            'standalone_score'\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_dataset = datasets.Dataset.from_pandas(\n",
    "    generated_questions,\n",
    "    split='train',\n",
    "    preserve_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_lm8ngX_fHV"
   },
   "source": [
    "Now our synthetic evaluation dataset is complete. We can evaluate different RAG systems on this evaluation dataset.\n",
    "\n",
    "We have generated only a few QA couples in this example to reduce time and cost. To move forward, we will load a pre-generated dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYMDe9JM_qg6"
   },
   "outputs": [],
   "source": [
    "eval_dataset = datasets.load_dataset('m-ric/huggingface_doc_eval', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NB8RKORF_tgH"
   },
   "outputs": [],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62KJ4jSc_vVu"
   },
   "source": [
    "## Build our RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBybCuIT_xj_"
   },
   "source": [
    "### Preprocess documents to build our vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zlDhWIuCKpz"
   },
   "source": [
    "We will split the documents from our knowledge base into smaller chunks. These will be the snippets that we picked by the Retriever, to then be ingested by the Reader LLM as supporting elements for its answer.\n",
    "\n",
    "The goal is to build semantically relevant snippets: not too small to be sufficient for supporting an answer, and not too large too avoid diluting individual ideas.\n",
    "\n",
    "There are many options for text splitting:\n",
    "* split every `n` words/characters, but this has the risk of cutting in half paragraphs or even sentences\n",
    "* split after `n` words/characters, but only on sentence boundaries\n",
    "* **recursive split** tries to preserve even more of the document structure, by processing it tree-like way, splitting first on the largest units (chapters) then recursively splitting on smaller units (paragraphs, sentences).\n",
    "\n",
    "We can visualize how different splitting options affect the chunks we get in the [chunk visualizer](https://huggingface.co/spaces/m-ric/chunk_visualizer) space.\n",
    "\n",
    "To measure chunk length in out text splitter, our length function will not be the count of characters, but the **count of tokens in the tokenized text**: for subsequent embedder that processes token, measuring length in tokens is more relevant and empirically performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tK0mYdN__w6B"
   },
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(\n",
    "        page_content=doc['text'],\n",
    "        metadata={'source': doc['source']}\n",
    "    )\n",
    "    for doc in tqdm(ds)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRBWe-0NDtPt"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def split_documents(\n",
    "        chunk_size: int,\n",
    "        knowledge_base: List[LangchainDocument],\n",
    "        tokenizer_name: str\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"Split documents into chunks of size `chunk_size` characters\n",
    "    and return a list of documents\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=['\\n\\n', '\\n', '.', ' ', '']\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DRrtUdhGO7p"
   },
   "source": [
    "### Retriever - embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwANTf9VGRAV"
   },
   "source": [
    "The **retriever acts like an internal search engine**: given the user query, it returns the most relevant documents from our knowledge base.\n",
    "\n",
    "In this example, we will use Langchain vector databases since it offers a convenient FAISS index and allows us to keep document metadata throughout the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKCk0PiVGQg2"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "def load_embeddings(\n",
    "        langchain_docs: List[LangchainDocument],\n",
    "        chunk_size: int,\n",
    "        embedding_model_name: Optional[str] = 'thenlper/gte-small'\n",
    ") -> FAISS:\n",
    "    \"\"\"Create a FAISS index from the given embedding model and documents.\n",
    "    Loads the index directly if it already exists.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    langchain_docs: List[LangchainDocument]\n",
    "        Documents to be indexed.\n",
    "    chunk_size: int\n",
    "        Size of the chunks to split the documents into\n",
    "    embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    FAISS index\n",
    "    \"\"\"\n",
    "    # load embedding model\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        multi_process=True,\n",
    "        model_kwargs={'device': 'cuda'},\n",
    "        encode_kwargs={'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "    )\n",
    "\n",
    "    # check if embeddings already exist on disk\n",
    "    index_name = f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
    "    index_folder_path = f\"./data/indexes/{index_name}\"\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "    else:\n",
    "        print(\"Index not found, generating it...\")\n",
    "        docs_process = split_documents(\n",
    "            chunk_size,\n",
    "            langchain_docs,\n",
    "            embedding_model_name\n",
    "        )\n",
    "\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_process,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "\n",
    "        return knowledge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oRuwmpJIT6_"
   },
   "source": [
    "### Reader - LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxpEtLXCIVpe"
   },
   "source": [
    "The Reader LLM reads the retrieved documents to formulate its answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyMs6T0eIafc"
   },
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4QQ6CI8IdR1"
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "repo_id = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "READER_MODEL_NAME = 'zephyr-7b-beta'\n",
    "#HP_API_TOEKN = \"\"\n",
    "\n",
    "READER_LLM = HuggingFaceHub(\n",
    "    repo_id=repo_id,\n",
    "    task='text-generation',\n",
    "    #huggingfacehub_api_token=HP_API_TOEKN,\n",
    "    model_kwargs={\n",
    "        'max_new_tokens': 512,\n",
    "        'top_k': 30,\n",
    "        'temperature': 0.1,\n",
    "        'repetition_penalty': 1.03\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjT98yryJDcL"
   },
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "def answer_with_rag(\n",
    "        question: str,\n",
    "        llm: LLM,\n",
    "        knowledge_index: VectorStore,\n",
    "        reranker: Optional[RAGPretrainedModel] = None,\n",
    "        num_retrieved_docs: int = 30,\n",
    "        num_docs_final: int = 7\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(\n",
    "        query=question,\n",
    "        k=num_retrieved_docs\n",
    "    )\n",
    "    # only keep the text\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(\n",
    "            question,\n",
    "            relevant_docs,\n",
    "            k=num_docs_final\n",
    "        )\n",
    "        relevant_docs = [doc['content'] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([\n",
    "        f\"Document {str(i)}:::\\n\" + doc\n",
    "        for i, doc in enumerate(relevant_docs)\n",
    "    ])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Generate an answer\n",
    "    answer = llm(final_prompt)\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hzbrDKZKFH1"
   },
   "source": [
    "## Benchmarking the RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-IvXiVtKHhi"
   },
   "source": [
    "The RAG system and the evaluation datasets are ready. The last step is to judge the output of the RAG system on this evaluation dataset. We need to set up a judge agent.\n",
    "\n",
    "Among different RAG evaluation metrics, we choose to focus only on faithfulness since it the best end-to-end metric of our system's performance.\n",
    "\n",
    "Here we use GPT4 as a judge for its empirically good performance, but we could try other models such as `kaist-ai/prometheus-13b-v1.0` or `BAAI/JudgeLM-33B-v1.0`.\n",
    "\n",
    "In the evaluation prompt, we give a detailed description each metric on the scale 1-5, as is done in [**Prometheus' prompt template**](https://huggingface.co/prometheus-eval/prometheus-13b-v1.0): this helps the model ground its metric precisely. If instead we give the judge LLM a vague scale to work with, the outputs will not be consistent enough between different examples.\n",
    "\n",
    "Don't forget that *Prompting the LLM to output rationale before giving its final score gives it more tokens to help it formalize and elaborate a judgement*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HC9vXestKGtr"
   },
   "outputs": [],
   "source": [
    "from langchain_core.language_models import BaseChatModel\n",
    "\n",
    "def run_rag_tests(\n",
    "        eval_dataset: datasets.Dataset,\n",
    "        llm: BaseChatModel,\n",
    "        knowledge_index: VectorStore,\n",
    "        output_file: str,\n",
    "        reranker: Optional[RAGPretrainedModel] = None,\n",
    "        verbose: Optional[bool] = True,\n",
    "        test_settings: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Run RAG tests on the given dataset and saves the results to the given output file\"\"\"\n",
    "    try:\n",
    "        # load previous generation if they exist\n",
    "        with open(output_file, 'r') as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example['question']\n",
    "        if question in [output['question'] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(\n",
    "            question,\n",
    "            llm,\n",
    "            knowledge_index,\n",
    "            reranker=reranker\n",
    "        )\n",
    "\n",
    "\n",
    "        result = {\n",
    "            'question': question,\n",
    "            'true_answer': example['answer'],\n",
    "            'source_doc': example['source_doc'],\n",
    "            'generated_answer':answer,\n",
    "            'retrieved_docs': [doc for doc in relevant_docs]\n",
    "        }\n",
    "        if test_settings:\n",
    "            result['test_settings'] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1744204222817,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "BY4Tvzr9qG2D",
    "outputId": "51c75c56-1251-47c0-83cf-3140059dc884"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['instruction', 'reference_answer', 'response'], input_types={}, partial_variables={}, messages=[SystemMessage(content='You are a fair evaluator language model.', additional_kwargs={}, response_metadata={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['instruction', 'reference_answer', 'response'], input_types={}, partial_variables={}, template='###Task Description:\\nAn instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\\n1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\\n2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\\n3. The output format should look as follows: \"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\"\\n4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\\n\\n###The instruction to evaluate:\\n{instruction}\\n\\n###Response to evaluate:\\n{response}\\n\\n###Reference Answer (Score 5):\\n{reference_answer}\\n\\n###Score Rubrics:\\n[Is the response correct, accurate, and factual based on the reference answer?]\\nScore 1: The response is completely incorrect, inaccurate, and/or not factual.\\nScore 2: The response is mostly incorrect, inaccurate, and/or not factual.\\nScore 3: The response is somewhat correct, accurate, and/or factual.\\nScore 4: The response is mostly correct, accurate, and factual.\\nScore 5: The response is completely correct, accurate, and factual.\\n\\n###Feedback:'), additional_kwargs={})])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content='You are a fair evaluator language model.'),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT)\n",
    "    ]\n",
    ")\n",
    "evaluation_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNvgHUj9q_L_"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "OPENAI_API_KEY = ''\n",
    "\n",
    "eval_chat_model = ChatOpenAI(\n",
    "    model='gpt-4o',\n",
    "    temperature=0,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "evaluator_name = 'GPT4'\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "        answer_path: str,\n",
    "        eval_chat_model: ChatOpenAI,\n",
    "        evaluator_name: str,\n",
    "        evaluation_prompt_template: ChatPromptTemplate,\n",
    ") -> None:\n",
    "    \"\"\"Evaluate generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):\n",
    "        # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, 'r'))\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment['question'],\n",
    "            response=experiment['generated_answer'],\n",
    "            reference_answer=experiment['true_answer']\n",
    "        )\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "        feedback, score = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, 'w') as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAaOU0RRsa_3"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./output'):\n",
    "    os.mkdir('./output')\n",
    "\n",
    "for chunk_size in [128, 256]: # add other chunk size (in tokens) as needed\n",
    "    for embedding_model_name in ['thenlper/gte-small']: # add other embedding models as needed\n",
    "        for rerank in [True, False]:\n",
    "            settings_name = f\"chunk:{chunk_size}_embedding:{embedding_model_name.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n",
    "            output_file_name = f'./output/rag_{settings_name}.json'\n",
    "\n",
    "            print(f\"Running evaluation for {settings_name}\")\n",
    "\n",
    "            print('Loading knowledge base embeddings...')\n",
    "            knowledge_index = load_embeddings(\n",
    "                RAW_KNOWLEDGE_BASE,\n",
    "                chunk_size=chunk_size,\n",
    "                embedding_model_name=embedding_model_name\n",
    "            )\n",
    "\n",
    "            print('Running RAG...')\n",
    "            reranker = RAGPretrainedModel.from_pretrained('colbert-ir/colbertv2.0') if rerank else None\n",
    "            run_rag_tests(\n",
    "                eval_dataset=eval_dataset,\n",
    "                llm=READER_LLM,\n",
    "                knowledge_index=knowledge_index,\n",
    "                output_file=output_file_name,\n",
    "                reranker=reranker,\n",
    "                verbose=False,\n",
    "                test_settings=settings_name\n",
    "            )\n",
    "\n",
    "            print('Running evaluation...')\n",
    "            evaluate_answers(\n",
    "                answer_path=output_file_name,\n",
    "                eval_chat_model=eval_chat_model,\n",
    "                evaluator_name=evaluator_name,\n",
    "                evaluation_prompt_template=evaluation_prompt_template\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a5YXa9Et6Ol"
   },
   "source": [
    "### Inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqYgOAN2sdXs"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "outputs = []\n",
    "for file in glob.glob('./output/*.json'):\n",
    "    output = pd.DataFrame(json.load(open(file, 'r')))\n",
    "    output['settings'] = file\n",
    "    outputs.append(output)\n",
    "\n",
    "result = pd.concat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qL0-NCCHuKXs"
   },
   "outputs": [],
   "source": [
    "result[\"eval_score_GPT4\"] = result[\"eval_score_GPT4\"].apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
    "result[\"eval_score_GPT4\"] = (result[\"eval_score_GPT4\"] - 1) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffDsrTFquRwb"
   },
   "outputs": [],
   "source": [
    "average_scores = result.groupby(\"settings\")[\"eval_score_GPT4\"].mean()\n",
    "average_scores.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laMsPy6cuTLE"
   },
   "source": [
    "### Example results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWETAr_iuVZc"
   },
   "source": [
    "This is just the result for this example. We should try several diferent method combinations when tuning our RAG systems for other cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9yDhiWvuUMI"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "scores = datasets.load_dataset(\"m-ric/rag_scores_cookbook\", split=\"train\")\n",
    "scores = pd.Series(scores[\"score\"], index=scores[\"settings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgIKB9COunsh"
   },
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    scores,\n",
    "    color=scores,\n",
    "    labels={\n",
    "        \"value\": \"Accuracy\",\n",
    "        \"settings\": \"Configuration\",\n",
    "    },\n",
    "    color_continuous_scale=\"bluered\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    barmode=\"group\",\n",
    "    yaxis_range=[0, 100],\n",
    "    title=\"<b>Accuracy of different RAG configurations</b>\",\n",
    "    xaxis_title=\"RAG settings\",\n",
    "    font=dict(size=15),\n",
    ")\n",
    "fig.layout.yaxis.ticksuffix = \"%\"\n",
    "fig.update_coloraxes(showscale=False)\n",
    "fig.update_traces(texttemplate=\"%{y:.1f}\", textposition=\"outside\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPwtXJ8a6am6llL+tLSJ+E9",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "027439343e7e47918ed9820b38a5ee6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "02ae72e679734d98aa7c61a82d0806c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "05dea80217674d418af69654a98bb813": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d553945833048d49fda8178f7324dcf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0e5754124c2644fe8de852094d9e1b00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6a4d9368d584850906dfa15724737ef",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6604566c902c4715ba68957ab6d5f5a4",
      "value": "â€‡2647/2647â€‡[00:00&lt;00:00,â€‡2558.04it/s]"
     }
    },
    "165980de44fe4525a828d5465b1d4385": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1d171882cc134d5e853fe2cc41291607": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "215897fd1dbe44eaafd9360c36db3e57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21b1a6017f6e45e9afde017b4d70eb04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26f7b403bd954ddd8297116c053b349d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2dce3c527137466eb58033de9fed4541": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e69c2c07cc745479977a087cbccd9b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3809f6f35d99425db67364d3efe12b77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "463b249bdb574dc497fe47db67130646": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21b1a6017f6e45e9afde017b4d70eb04",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_57a23eb9dbf74ed99a906c7c0efebf7f",
      "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
     }
    },
    "57a23eb9dbf74ed99a906c7c0efebf7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5985819b965c466483c2ff2f388529db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "59acd8d10634491597c72fa70eab3bdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d171882cc134d5e853fe2cc41291607",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3809f6f35d99425db67364d3efe12b77",
      "value": "100%"
     }
    },
    "5ee28a22e54a4ad3b170ca79a3555a3a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64a3bde810b24fa5bf7825ac93e4a543": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e351451215d9497693d752611de882c3",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e9350be5dc3c4b5cb32a0e0756cd641c",
      "value": 10
     }
    },
    "6604566c902c4715ba68957ab6d5f5a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "67ebacc03373438eb80a7386b83c036e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a24aeb8ddd0641c3b993ba1dadc9a11d",
      "max": 21954601,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f8aeb8f1a9864733a0f4ed1feb1caa3c",
      "value": 21954601
     }
    },
    "68c32a21af96465d9fc6857029fccba2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9bbe99211874da5afb89fcd4fae00ea",
      "max": 21,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f544273a86e2412ba28fcad531acc0b5",
      "value": 21
     }
    },
    "6c749aa475d643c898899e3f767f445a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab66c645787041b49137ad8a8b75e369",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c9694ef2bb7347d18d70aa719f054f80",
      "value": "â€‡10/10â€‡[00:19&lt;00:00,â€‡â€‡1.35s/it]"
     }
    },
    "726a26c6510149b6bcef64c3fffe95fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7306e93fee704cd2b2930b1b062896fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ff1a5358a25448599401ae2d19d725c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "896e92e2f0014757994225bd216cf2ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8a280285e8564bc7a9191729d9e2ebc0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a9e02f56d6344488a370f16247f135d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e69c2c07cc745479977a087cbccd9b8",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ca6b9793e86a491896bdaad3043241ec",
      "value": "â€‡2647/2647â€‡[00:00&lt;00:00,â€‡3458.23â€‡examples/s]"
     }
    },
    "9c5e681a851b430588b4c22315bcae28": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9eac826b95334b57b6d09024153e8a2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_26f7b403bd954ddd8297116c053b349d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5985819b965c466483c2ff2f388529db",
      "value": "â€‡8/8â€‡[00:45&lt;00:00,â€‡â€‡6.27s/it]"
     }
    },
    "9ff2e7fbba8946008765ad18941238e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a24aeb8ddd0641c3b993ba1dadc9a11d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6a4d9368d584850906dfa15724737ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa62285e008b420c841a1930a009b47b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e55eadac10e242cd860d3fd81083b5dd",
       "IPY_MODEL_64a3bde810b24fa5bf7825ac93e4a543",
       "IPY_MODEL_6c749aa475d643c898899e3f767f445a"
      ],
      "layout": "IPY_MODEL_05dea80217674d418af69654a98bb813"
     }
    },
    "ab66c645787041b49137ad8a8b75e369": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5d4d92cda5949229389527d527f36ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff8a4aba8a8f40f5be6d1b6853fd880c",
      "max": 2647,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f235e1bcce8c4377869f1aa815e11704",
      "value": 2647
     }
    },
    "b79a8aea8bee4542aa5284f58ec1ba36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_027439343e7e47918ed9820b38a5ee6f",
      "max": 8,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_896e92e2f0014757994225bd216cf2ef",
      "value": 8
     }
    },
    "c1c2363c822f44899cfee49df4c92470": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c74bc18e7bb84ae2b244ee545ce7f7e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ddc2f4b063444aedae943c76ac862fa8",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0d553945833048d49fda8178f7324dcf",
      "value": "huggingface_doc.csv:â€‡100%"
     }
    },
    "c7c0b503629f41419aa3c3ca942ffed1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_463b249bdb574dc497fe47db67130646",
       "IPY_MODEL_b5d4d92cda5949229389527d527f36ef",
       "IPY_MODEL_8a9e02f56d6344488a370f16247f135d"
      ],
      "layout": "IPY_MODEL_d264ad64ec384df3a7985744053cb0fb"
     }
    },
    "c9694ef2bb7347d18d70aa719f054f80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca6b9793e86a491896bdaad3043241ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cafe8d6c6dd14caa9e514f1fa9e76084": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_726a26c6510149b6bcef64c3fffe95fb",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ff230fdc29744aefb68d84edc97c34b5",
      "value": "â€‡21.0/21.0â€‡[00:00&lt;00:00,â€‡590B/s]"
     }
    },
    "cce96efd839f4161951a15222d1c367e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e658fbd19d2a42fea1ab51336cb1e523",
       "IPY_MODEL_f8ac2da15fe3425cb4a7fc436dd612ae",
       "IPY_MODEL_0e5754124c2644fe8de852094d9e1b00"
      ],
      "layout": "IPY_MODEL_5ee28a22e54a4ad3b170ca79a3555a3a"
     }
    },
    "ce7aa2debdab4326891ff311952d0306": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1c2363c822f44899cfee49df4c92470",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7ff1a5358a25448599401ae2d19d725c",
      "value": "â€‡22.0M/22.0Mâ€‡[00:00&lt;00:00,â€‡87.2MB/s]"
     }
    },
    "d264ad64ec384df3a7985744053cb0fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5774f1c21ed442ca2f7b60efa0f3beb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_59acd8d10634491597c72fa70eab3bdb",
       "IPY_MODEL_b79a8aea8bee4542aa5284f58ec1ba36",
       "IPY_MODEL_9eac826b95334b57b6d09024153e8a2c"
      ],
      "layout": "IPY_MODEL_215897fd1dbe44eaafd9360c36db3e57"
     }
    },
    "db67adef870f4632b81723414873cf24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ddc2f4b063444aedae943c76ac862fa8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "deb6c2803e954e15923a089e6182f9f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c74bc18e7bb84ae2b244ee545ce7f7e8",
       "IPY_MODEL_67ebacc03373438eb80a7386b83c036e",
       "IPY_MODEL_ce7aa2debdab4326891ff311952d0306"
      ],
      "layout": "IPY_MODEL_e8e1437fe70943bca01c5268884d3d3e"
     }
    },
    "e21bcd389054465aabd1199e0de4bf81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e351451215d9497693d752611de882c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e55eadac10e242cd860d3fd81083b5dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ff2e7fbba8946008765ad18941238e7",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7306e93fee704cd2b2930b1b062896fe",
      "value": "100%"
     }
    },
    "e658fbd19d2a42fea1ab51336cb1e523": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2dce3c527137466eb58033de9fed4541",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e21bcd389054465aabd1199e0de4bf81",
      "value": "100%"
     }
    },
    "e7d9b381943b4227b7c7641adfff1a83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f055b4104d0a49e7a1e5bd7da5a5b207",
       "IPY_MODEL_68c32a21af96465d9fc6857029fccba2",
       "IPY_MODEL_cafe8d6c6dd14caa9e514f1fa9e76084"
      ],
      "layout": "IPY_MODEL_db67adef870f4632b81723414873cf24"
     }
    },
    "e8e1437fe70943bca01c5268884d3d3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9350be5dc3c4b5cb32a0e0756cd641c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f055b4104d0a49e7a1e5bd7da5a5b207": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8a280285e8564bc7a9191729d9e2ebc0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_165980de44fe4525a828d5465b1d4385",
      "value": "README.md:â€‡100%"
     }
    },
    "f235e1bcce8c4377869f1aa815e11704": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f544273a86e2412ba28fcad531acc0b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f8ac2da15fe3425cb4a7fc436dd612ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c5e681a851b430588b4c22315bcae28",
      "max": 2647,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_02ae72e679734d98aa7c61a82d0806c5",
      "value": 2647
     }
    },
    "f8aeb8f1a9864733a0f4ed1feb1caa3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f9bbe99211874da5afb89fcd4fae00ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff230fdc29744aefb68d84edc97c34b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ff8a4aba8a8f40f5be6d1b6853fd880c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
