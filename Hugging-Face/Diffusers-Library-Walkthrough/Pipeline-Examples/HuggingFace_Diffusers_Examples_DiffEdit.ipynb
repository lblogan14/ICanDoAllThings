{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPlsTXOcctnFY1hKAki+WFM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"nIp7STCvpm7g"},"outputs":[],"source":["!pip install -qU diffusers transformers accelerate"]},{"cell_type":"markdown","source":["# DiffEdit"],"metadata":{"id":"Hg2vX7uTrlHu"}},{"cell_type":"markdown","source":["Traditional image editing requires providing a mask of the area to be edited.\n","\n","**DiffEdit** automatically generates the mask based on a text query, making it easier overall to create a mask without image editing software.\n","\n","The DiffEdit algorithm works in three steps:\n","1. the diffusion model denoises an image conditioned on some query text and reference text which produces different noise estimates for different areas of the image; the difference is used to infer a mask to identify which area of the image needs to be changed to match the query text\n","2. the input image is encoded into latent space with DDIM\n","3. the latents are decoded with the diffusion model conditioned on the text query, using the mask as a guide such that pixels outside the mask remain the same as in the input image"],"metadata":{"id":"YSun811srncx"}},{"cell_type":"markdown","source":["The `StableDiffusionDiffEditPipeline` requires an image mask and a set of partially inverted latents.\n","\n","The image mask is generated from the `generate_mask()`, and includes two parameters, `source_prompt` and `target_prompt`. These parameters determine what to edit in the image."],"metadata":{"id":"IW00tkzhs0c9"}},{"cell_type":"code","source":["source_prompt = 'a bowl of fruits'\n","target_prompt = 'a bowl of pears'"],"metadata":{"id":"Nv4QVgewrmQn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The partially inverted latents are generated from the `invert()` function, and it is generally a good idea to include a `prompt` or caption describing the image to help guide the inverse latent sampling process. The caption can be our `source_prompt`."],"metadata":{"id":"_NBnoBYItLlv"}},{"cell_type":"code","source":["from diffusers import DDIMScheduler, DDIMInverseScheduler, StableDiffusionDiffEditPipeline\n","import torch\n","\n","pipeline = StableDiffusionDiffEditPipeline.from_pretrained(\n","    'stabilityai/stable-diffusion-2-1',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True,\n",")\n","pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\n","pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)\n","pipeline.enable_model_cpu_offload()\n","pipeline.enable_vae_slicing()"],"metadata":{"id":"f4AGRL0ztY_s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from diffusers.utils import load_image, make_image_grid\n","\n","img_url = \"https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png\"\n","raw_image = load_image(img_url).resize((768, 768))\n","raw_image"],"metadata":{"id":"oUiqmbLCt0aq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Use the `generate_mask` function to generate the image mask."],"metadata":{"id":"a_b3NmzQt6YD"}},{"cell_type":"code","source":["from PIL import Image\n","\n","source_prompt = 'a bowl of fruits'\n","target_prompt = 'a basket of pears'\n","\n","mask_image = pipeline.generate_mask(\n","    image=raw_image,\n","    source_prompt=source_prompt,\n","    target_prompt=target_prompt\n",")\n","\n","mask_image = Image.fromarray(\n","    (mask_image.squeeze()*255).astype('unit8'),\n","    'L'\n",").resize((768, 768))"],"metadata":{"id":"XbWkuy2It9bu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, create the inverted latents and pass it to a caption describing the image:"],"metadata":{"id":"RBg13D6cugQf"}},{"cell_type":"code","source":["inv_latents = pipeline.invert(\n","    prompt=source_prompt,\n","    image=raw_image,\n",").latents"],"metadata":{"id":"s_VKC--yukO9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, pass the image mask and inverted latents to the pipeline. The `target_prompt` becomes the `prompt` now, and the `source_prompt` is used as the `negative_prompt`:"],"metadata":{"id":"tteTs-3CupHc"}},{"cell_type":"code","source":["output_image = pipeline(\n","    prompt=target_prompt,\n","    negative_prompt=source_prompt,\n","    mask_image=mask_image,\n","    image_latents=inv_latents\n",").images[0]\n","make_image_grid([raw_image, mask_image, output_image], rows=1, cols=3)"],"metadata":{"id":"T2jpKc3luzt0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generate source and target embeddings"],"metadata":{"id":"FGDGxYmSvSc0"}},{"cell_type":"markdown","source":["The source and target embeddings can be automatically generated with the `Flan-T5` model."],"metadata":{"id":"a84Ta3NC_y3I"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, T5ForConditionalGeneration\n","import torch\n","\n","tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-large')\n","model = T5ForConditionalGeneration.from_pretrained(\n","    'google/flan-t5-large',\n","    torch_dtype=torch.float16,\n","    device_map='auto'\n",")"],"metadata":{"id":"O4WeUurxvUUU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["source_concept = 'bowl'\n","target_concept = 'basket'\n","\n","source_text = f\"Provide a caption for images containing a {source_concept}\"\n","target_text = f\"Provide a caption for images containing a {target_concept}\"\n","print(f\"Source text: {source_text}\")\n","print(f\"Target text: {target_text}\")"],"metadata":{"id":"7GxI1ieDAFgH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@torch.no_grad()\n","def generate_prompts(input_prompt):\n","    input_ids = tokenizer(\n","        input_prompt,\n","        return_tensors='pt'\n","    ).input_ids.to('cuda')\n","\n","    outputs = model.generate(\n","        input_ids,\n","        temperature=0.8,\n","        num_return_sequences=16,\n","        do_sample=True,\n","        max_new_tokens=128,\n","        top_k=10\n","    )\n","    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","\n","source_prompts = generate_prompts(source_text)\n","target_prompts = generate_prompts(target_text)\n","print(f\"Source prompts: {source_prompts}\")\n","print(f\"Target prompts: {target_prompts}\")"],"metadata":{"id":"jTuS_sLuAQM5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from diffusers import StableDiffusionDiffEditPipeline\n","import torch\n","\n","pipeline = StableDiffusionDiffEditPipeline.from_pretrained(\n","    'stabilityai/stable-diffusion-2-1',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True,\n",")\n","pipeline.enable_model_cpu_offload()\n","pipeline.enable_vae_slicing()"],"metadata":{"id":"pnN4iJPEAk3k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@torch.no_grad()\n","def embed_prompts(sentences, tokenizer, text_encoder, device='cuda'):\n","    embeddings = []\n","    for sent in sentences:\n","        text_inputs = tokenizer(\n","            sent,\n","            padding='max_length',\n","            max_length=tokenizer.model_max_length,\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","        text_input_ids = text_inputs.input_ids\n","        prompt_embed = text_encoder(\n","            text_input_ids.to(device),\n","            attention_mask=None\n","        )[0]\n","        embeddings.append(prompt_embed)\n","\n","    return torch.concatenate(embeddings, dim=0).mean(dim=0).unsqueeze(0)\n","\n","\n","source_embeds = embed_prompts(source_prompts, tokenizer, pipeline.text_encoder)\n","target_embeds = embed_prompts(target_prompts, tokenizer, pipeline.text_encoder)"],"metadata":{"id":"EhjYGautA1zG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from diffusers import DDIMScheduler, DDIMInverseScheduler\n","from diffusers.utils import load_image, make_image_grid\n","from PIL import Image\n","\n","pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\n","pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)"],"metadata":{"id":"tvrLiO30BaTq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_url = \"https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png\"\n","raw_image = load_image(img_url).resize((768, 768))\n","\n","mask_image = pipeline.generate_mask(\n","    image=raw_image,\n","    source_prompt=source_embeds,\n","    target_prompt=target_embeds,\n",")\n","\n","inv_latents = pipeline.invert(\n","    prompt_embeds=source_embeds,\n","    image=raw_image,\n",").latents\n","\n","output_image = pipeline(\n","    mask_image=mask_image,\n","    image_latents=inv_latents,\n","    prompt_embeds=target_embeds,\n","    negative_prompt_embeds=source_embeds,\n",").images[0]\n","\n","make_image_grid([raw_image, mask_image, output_image], rows=1, cols=3)"],"metadata":{"id":"Q-sykxqmBob_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generate a caption for inversion"],"metadata":{"id":"DmOYcM3EB_8F"}},{"cell_type":"markdown","source":["While we can use the `source_prompt` as a caption to help generate the partially inverted latents, we can also use the `BLIP` model to automatically generate a caption."],"metadata":{"id":"InidqzY7CDNf"}},{"cell_type":"code","source":["from transformers import BlipForConditionalGeneration, BlipProcessor\n","import torch\n","\n","processor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')\n","model = BlipForConditionalGeneration.from_pretrained(\n","    'Salesforce/blip-image-captioning-base',\n","    torch_dtype=torch.float16,\n","    low_cpu_mem_usage=True\n",")"],"metadata":{"id":"-NcP5rjICCq_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@torch.no_grad()\n","def generate_caption(images, caption_generator, caption_processor):\n","    text = 'a photograph of'\n","\n","    inputs = caption_processor(\n","        images,\n","        text,\n","        return_tensors='pt'\n","    ).to(device='cuda', dtype=caption_generator.dtype)\n","\n","    caption_generator.to('cuda')\n","\n","    outputs = caption_generator.generate(\n","        **inputs,\n","        max_new_tokens=128\n","    )\n","\n","    # offload caption generator\n","    caption_generator.to('cpu')\n","\n","    caption = caption_processor.batch_decode(\n","        outputs,\n","        skip_special_tokens=True\n","    )[0]\n","    return caption"],"metadata":{"id":"IHLBaARJCb_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from diffusers.utils import load_image\n","\n","img_url = \"https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png\"\n","raw_image = load_image(img_url).resize((768, 768))\n","\n","caption = generate_caption(raw_image, model, processor)\n","caption"],"metadata":{"id":"Ny3TUHovC66X"},"execution_count":null,"outputs":[]}]}