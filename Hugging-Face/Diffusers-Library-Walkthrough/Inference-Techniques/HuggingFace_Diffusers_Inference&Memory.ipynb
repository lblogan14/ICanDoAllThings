{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMKKa3YgACHi1qJCs8e9NIO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dvEU4oeO9i-J"},"outputs":[],"source":["!pip installl -qU diffusers transformers torch xformers tomesd gate DeepCache accelerate xfuser"]},{"cell_type":"markdown","source":["# Speed up inference"],"metadata":{"id":"aYRsKBUC9nqB"}},{"cell_type":"markdown","source":["To optimize Diffusers for inference speed, we can\n","* reduce the computational burden by\n","  * lowering the data precision, or\n","  * using a lightweight distilled model.\n","* apply memory-efficient attention implementations, such as xFormers and scaled dot product attention in PyTorch 2.0.\n","\n","For example, if the prompt of a single 512x512 image is \"a photo of an astronaut riding a horse on mars\" with 50 DDIM steps on a NVIDIA A100, the inference time:\n","\n","| setup | latency | speed-up |\n","| ----- | ------- | -------- |\n","| baseline | 5.27s | x1 |\n","| tf32 | 4.14s | x1.27 |\n","| fp16 | 3.51s | x1.50 |\n","| combined | 3.41s | x1.54 |"],"metadata":{"id":"6D04hA-4viNv"}},{"cell_type":"markdown","source":["## TensorFloat-32"],"metadata":{"id":"h19CbjU3woGp"}},{"cell_type":"markdown","source":["By default, PyTorch enables tf32 mode for convolutions but not matrix multiplications. It is recommended to enable tf32 for matrix multiplications to significantly speed up computations with typically negligible loss in numerical accuracy."],"metadata":{"id":"YP98JJYFwrpV"}},{"cell_type":"code","source":["import torch\n","\n","torch.backends.cuda.matmul.allow_tf32 = True"],"metadata":{"id":"lcQH9PCi9pTY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Half-precision weights"],"metadata":{"id":"2JmRnblVxA-n"}},{"cell_type":"markdown","source":["To save GPU memory and get more speed, we can set `torch_dtype=torch.float16` to load and run the model weights directly with half-precision weights."],"metadata":{"id":"-2IgUJsfxDs4"}},{"cell_type":"code","source":["import torch\n","from diffusers import DiffusionPipeline\n","\n","pipe = DiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True,\n",").to('cuda')"],"metadata":{"id":"Z1aGArvcxDFC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note**: Do NOT use `torch.autocast` in any of the pipelines as it can lead to black images and is always slower than pure float16 precision."],"metadata":{"id":"FVMID3lYxVTk"}},{"cell_type":"markdown","source":["## Distilled model"],"metadata":{"id":"XtvVejCTxdqd"}},{"cell_type":"markdown","source":["We could also use a distilled Stable Diffusion model and autoencoder to speed up inference.\n","\n","For example, if the prompt of four 512x512 image is \"a photo of an astronaut riding a horse on mars\" with 25 PNDM steps on a NVIDIA A100, the inference time to generate 4 images:\n","\n","| setup | latency | speed-up |\n","| ----- | ------- | -------- |\n","| baseline | 6.37s | x1 |\n","| distilled | 4.18s | x1.52 |\n","| distilled + tiny autoencoder | 3.83s | x1.66 |"],"metadata":{"id":"5O8H3WOQxoLC"}},{"cell_type":"code","source":["from diffusers import StableDiffusionPipeline\n","import torch\n","\n","distilled = StableDiffusionPipeline.from_pretrained(\n","    'nota-ai/bk-sdm-small',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True,\n",").to('cuda')"],"metadata":{"id":"pAIVIwfzxdCK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"a golden vase with different flowers\"\n","generator = torch.manual_seed(111)\n","\n","image = distilled(\n","    prompt,\n","    num_inference_steps=25,\n","    generator=generator,\n",").images[0]\n","image"],"metadata":{"id":"lcOPB8F0yTLc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Tiny AutoEncoder"],"metadata":{"id":"Opws8SGSyfQo"}},{"cell_type":"code","source":["from diffusers import AutoencoderTiny, StableDiffusionPipeline\n","import torch\n","\n","distilled = StableDiffusionPipeline.from_pretrained(\n","    'nota-ai/bk-sdm-small',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True,\n",").to('cuda')\n","\n","distilled.vae = AutoencoderTiny.from_pretrained(\n","    'sayakpaul/taesd-diffusers',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True,\n",").to('cuda')"],"metadata":{"id":"bosHwDQByg4o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"a golden vase with different flowers\"\n","generator = torch.manual_seed(111)\n","\n","image = distilled(\n","    prompt,\n","    num_inference_steps=25,\n","    generator=generator,\n",").images[0]\n","image"],"metadata":{"id":"5aYcftERysXy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Reduce memory usage"],"metadata":{"id":"zwW99Z0PywHp"}},{"cell_type":"markdown","source":["Optimizing for memory or speed lead to improved performance in the other, so we should try to optimize for both whenver we can.\n","\n","For example, if the prompt of a single 512x512 image is \"a photo of an astronaut riding a horse on mars\" with 50 DDIM steps on a NVIDIA Titan RTX, the inference time:\n","\n","| setup | latency | speed-up |\n","| ----- | ------- | -------- |\n","| original | 9.50s | x1 |\n","| fp16 | 3.61s | x2.63 |\n","| channels last | 3.30s | x2.88 |\n","| traced UNet | 3.21s | x2.96 |\n","| memory-efficient attention | 2.63s | x3.61 |"],"metadata":{"id":"gNbUscrx32TW"}},{"cell_type":"markdown","source":["## Sliced VAE"],"metadata":{"id":"I6IqjsbK4bdN"}},{"cell_type":"markdown","source":["**Sliced VAE** enables decoding large batches of images with limited VRAM or batches with 32 images or more by decoding the batches of latents one image at a time. We will likely want to couple this with `enable_xformers_memory_efficient_attention()` to reduce memory use further if we have xFormers installed.\n","\n","To use sliced VAE, we need to call `enable_vae_slicing()` on our pipeline before inference:"],"metadata":{"id":"gC0oRh5g4dB3"}},{"cell_type":"code","source":["from diffusers import StableDiffusionPipeline\n","import torch\n","\n","pipe = StableDiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True,\n",").to('cuda')\n","\n","# add here\n","pipe.enable_vae_slicing()\n","# if xFormers installed,\n","pipe.enable_xformers_memory_efficient_attention()"],"metadata":{"id":"yUv-97G4yxT9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"a photo of an astronaut riding a horse on mars\"\n","\n","images = pipe([prompt] * 32).images[0]\n","images"],"metadata":{"id":"GFyMT9Xy4_gA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We may a small performance boost in VAE decoding on multi-image batches, and there should be no performance impact on single-image batches."],"metadata":{"id":"yXV3MPxE5KCY"}},{"cell_type":"markdown","source":["## Tiled VAE"],"metadata":{"id":"EnC458X95R94"}},{"cell_type":"markdown","source":["**Tiled VAE** also enables working with large images on limited VRAM (for example, generating 4k images on 8GB of VRAM) by splitting the image into overlapping tiles, decoding the tiles, and then blending the outputs together to compose the final image. We should also use tiled VAE with `enable_xformers_memory_efficient_attention()` to reduce memory use further if we have xFormers installed.\n","\n","To use tiled VAE, we need to call `enable_vae_tiling()` on our pipeline before inference:"],"metadata":{"id":"zjvGE4eL5T23"}},{"cell_type":"code","source":["from diffusers import StableDiffusionPipeline, UniPCMultistepScheduler\n","import torch\n","\n","pipe = StableDiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True,\n",")\n","pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n","pipe = pipe.to('cuda')\n","\n","# add here\n","pipe.enable_vae_tiling()\n","# if xFormers installed\n","pipe.enable_xformers_memory_efficient_attention()"],"metadata":{"id":"nNK1L_u-5Rbr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"a beautiful landscape photograph\"\n","\n","image = pipe(\n","    prompt,\n","    width=3840,\n","    height=2224,\n","    num_inference_steps=20\n",").images[0]\n","image"],"metadata":{"id":"rXcjY3mO6C4J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The output image has some tile-to-tile tone variation because the tiles are decoded separately, but we should not see any sharp and obvious seams between the tiles.\n","\n","Tiling is turned off for images that are 512x512 or smaller."],"metadata":{"id":"U3tgWRJx6Luz"}},{"cell_type":"markdown","source":["## CPU offloading"],"metadata":{"id":"gooHqTxF6X6f"}},{"cell_type":"markdown","source":["Offloading the weights to the CPU and only loading them on the GPU when performing the forward pass can also save memory.\n","\n","To perform CPU offloading, we can call `enable_sequential_cpu_offload()`:"],"metadata":{"id":"v-MDoe8J8sr7"}},{"cell_type":"code","source":["from diffusers import StableDiffusionPipeline\n","import torch\n","\n","pipe = StableDiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",")\n","# do NOT move pipeline to CUDA\n","\n","# add here\n","pipe.enable_sequential_cpu_offload()"],"metadata":{"id":"QZry6WBS6Wy0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"a photo of an astronaut riding a horse on mars\"\n","image = pipe(prompt).images[0]\n","image"],"metadata":{"id":"BZMYjE8r9FTk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When using `enable_sequential_cpu_offload()`, do NOT move the pipeline to CUDA beforehand or else the gain in memory consumption will only be minimal."],"metadata":{"id":"LGsSzYvy9xCD"}},{"cell_type":"markdown","source":["CPU offloading works on submodules rather than whole models. This is the best way to minimize memory consumption, but inference is much slower due to the iterative nature of the diffusion process. The UNet component of the pipeline runs several times (as many as `num_inference_steps`); each time, the different UNet submodules are sequentially onloaded and offloaded as needed, resulting in a large number of memory transfers.\n","\n","Consider using model offloading if we want to optimize for speed because it is much faster. The tradeoff is our memory savings will not be as large."],"metadata":{"id":"08jzdbMn9INP"}},{"cell_type":"markdown","source":["## Model offloading"],"metadata":{"id":"ONa7kOlm95Zo"}},{"cell_type":"markdown","source":["As we saw in the previous section, Sequential CPU offloading preserves a lot of memory but it makes inference slower because submodules are moved to GPU as needed, and they are immediately returned to the CPU when a new module runs.\n","\n","Full-model offloading moves the whole models to the CPU, rather than handling each model's constituent submodules. There is a negligible impact on inference time (compared with moving the pipeline to CUDA), and it still provides some meomry savings.\n","\n","During model offloading, only one of the main components of the pipeline (typically the text encoder, UNet, and VAE) is placed on the GPU while the others wait on the CPU. Components like the UNet that run for multiple iterations stay on the GPU until they are no longer needed."],"metadata":{"id":"1w_uAJz1975R"}},{"cell_type":"code","source":["from diffusers import StableDiffusionPipeline\n","import torch\n","\n","pipe = StableDiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",")\n","\n","# add here\n","pipe.enable_model_cpu_offload()"],"metadata":{"id":"_RCvGxIm96cq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"a photo of an astronaut riding a horse on mars\"\n","image = pipe(prompt).images[0]\n","image"],"metadata":{"id":"layfj7WD-39k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Channels-last memory format"],"metadata":{"id":"NtWASAkV_CsZ"}},{"cell_type":"markdown","source":["The **channels-last memory format** is used to order NCHW tensors in memory to preserve dimension ordering.\n","\n","Channels-last tensors are ordered in such a way that the channels become the densest dimension (storing images pixel-per-pixel). Since not all operators currently support the channels-last formst, it may result in worst performance but we should still try and see if it works for the model we choose."],"metadata":{"id":"YSRUYEUR_Fks"}},{"cell_type":"code","source":["from diffusers import StableDiffusionPipeline\n","import torch\n","\n","pipe = StableDiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",")"],"metadata":{"id":"XA8leS2u_E3q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"a photo of an astronaut riding a horse on mars\"\n","generator = torch.manual_seed(111)\n","image = pipe(\n","    prompt,\n","    generator=generator\n",").images[0]\n","image"],"metadata":{"id":"GFU1ztoqBO2i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For example, to set the pipeline's UNet to use the channels-last format:"],"metadata":{"id":"lyH4GUTlBU49"}},{"cell_type":"code","source":["print(pipe.unet.conv_out.state_dict()['weight'].stride())\n","pipe.unet.to(memory_format=torch.channels_last) # in-place operation\n","print(pipe.unet.conv_out.state_dict()['weight'].stride())"],"metadata":{"id":"eZJx2e1KBXuW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = pipe(\n","    prompt,\n","    generator=generator\n",").images[0]\n","image"],"metadata":{"id":"73KLmhdzBnrL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Tracing"],"metadata":{"id":"bBnR3egTBpvO"}},{"cell_type":"markdown","source":["**Tracing** runs an example input tensor through the model and captures the operations that are performed on it as that input makes its way through the model's layers.\n","\n","To trace a UNet:"],"metadata":{"id":"VFaZ7L-_Brd-"}},{"cell_type":"code","source":["import time\n","import torch\n","from diffusers import StableDiffusionPipeline\n","import functools\n","\n","# torch disable grad\n","torch.set_grad_enabled(False)\n","\n","# set variables\n","n_experiments = 2\n","unet_runs_per_experiment = 50\n","\n","# load inputs\n","def generate_inputs():\n","    sample = torch.randn((2, 4, 64, 64), device='cuda', dtype=torch.float16)\n","    timestep = torch.rand(1, device='cuda', dtype=torch.float16) * 999\n","    encoder_hidden_states = torch.randn((2, 77, 768), device='cuda', dtype=torch.float16)\n","\n","    return sample, timestep, encoder_hidden_states\n","\n","\n","pipe = StableDiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",").to('cuda')\n","\n","unet = pipe.unet\n","unet.eval()\n","unet.to(memory_format=torch.channels_last) # use channels_last memory format\n","unet.forward = functools.partial(unet.forward, return_dict=False) # set return_dict=False as default"],"metadata":{"id":"CYvYIQqCBqxK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# warmup\n","for _ in range(3):\n","    with torch.inference_mode():\n","        inputs = generate_inputs()\n","        orig_output = unet(*inputs)"],"metadata":{"id":"027WRuTOCoWD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# trace\n","print('tracing...')\n","unet_traced = torch.jit.trace(unet, inputs)\n","unet_traced.eval()\n","print('dont tracing')"],"metadata":{"id":"GVXFSLaJCvoD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# warmup and optimize graph\n","for _ in range(5):\n","    with torch.inference_mode():\n","        inputs = generate_inputs()\n","        orig_output = unet_traced(*inputs)"],"metadata":{"id":"0s0OZEs7C2pr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# benchmarking\n","with torch.inference_mode():\n","    for _ in range(n_experiments):\n","        torch.cuda.synchronize()\n","        start_time = time.time()\n","        for _ in range(unet_runs_per_experiment):\n","            orig_output = unet_traced(*inputs)\n","        torch.cuda.synchronize()\n","        print(f\"unet traced inference took {time.time() - start_time:.2f} seconds\")\n","\n","    for _ in range(n_experiments):\n","        torch.cuda.synchronize()\n","        start_time = time.time()\n","        for _ in range(unet_runs_per_experiment):\n","            orig_output = unet(*inputs)\n","        torch.cuda.synchronize()\n","        print(f\"unet inference took {time.time() - start_time:.2f} seconds\")\n","\n","# save the model\n","unet_traced.save('unet_traced.pt')"],"metadata":{"id":"X6J3mzCvC-Nl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Replace the `unet` attribute of the pipeline with the traced model:"],"metadata":{"id":"Dhs4BcyGDkAK"}},{"cell_type":"code","source":["from diffusers import StableDiffusionPipeline\n","import torch\n","from dataclasses import dataclass\n","\n","@dataclass\n","class UNet2DConditionOutput:\n","    sample: torch.Tensor\n","\n","\n","pipe = StableDiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",").to('cuda')\n","\n","# use jitted unet\n","unet_traced = torch.jit.load('unet_traced.pt')\n","\n","# del pipe.unet\n","class TracedUNet(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.in_channels = pipe.unet.config.in_channels\n","        self.device = pipe.unet.device\n","\n","    def forward(self, latent_model_input, t, encoder_hidden_states):\n","        # apply unet_traced here\n","        sample = unet_traced(latent_model_input, t, encoder_hidden_states)[0]\n","        return UNet2DConditionOutput(sample=sample)\n","\n","\n","pipe.unet = TracedUNet()"],"metadata":{"id":"cZmQLxUtEAqE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"a photo of an astronaut riding a horse on mars\"\n","\n","with torch.inference_mode():\n","    image = pipe(\n","        prompt,\n","        num_inference_steps=50\n","    ).images[0]\n","image"],"metadata":{"id":"YX6xys-7E5au"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Memory-efficient attention"],"metadata":{"id":"JLaExiNoE_hp"}},{"cell_type":"markdown","source":["To use **Flash Attention**, we need to install the following:\n","* PyTorch > 1.12\n","* CUDA available\n","* xFormers\n","\n","and then we can call `enable_xformers_memory_efficient_attention()`:"],"metadata":{"id":"zEBoY-mCFEvh"}},{"cell_type":"code","source":["from diffusers import DiffusionPipeline\n","import torch\n","\n","pipe = DiffusionPipeline.from_pretrained(\n","    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n","    torch_dtype=torch.float16,\n","    use_safetensors=True,\n",").to(\"cuda\")\n","\n","pipe.enable_xformers_memory_efficient_attention()"],"metadata":{"id":"MvC1brTsFBP7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"a photo of an astronaut riding a horse on mars\"\n","generator = torch.manual_seed(111)\n","\n","with torch.inference_mode():\n","    image = pipe(\n","        prompt,\n","        generator=generator\n","    ).images[0]\n","image"],"metadata":{"id":"BsFt5cqvFR-W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PyTorch 2.0"],"metadata":{"id":"d8rM1tsZLIp3"}},{"cell_type":"markdown","source":["HuggingFace Diffusers supports the latest optimizations from PyTorch 2.0:\n","1. A memory-efficient attention implementation, scaled dot product attention, without requiring any extra dependencies such as xFormers.\n","2. `torch.compile`, a just-in-time (JIT) compiler to provide an extra performance boost when individual models are compiled."],"metadata":{"id":"57RfFhxSLN5x"}},{"cell_type":"markdown","source":["## Scaled dot product attention"],"metadata":{"id":"-zmIzDsbLht5"}},{"cell_type":"markdown","source":["`torch.nn.functional.scaled_dot_product_attention` (SDPA) is an optimized and memory-efficient attention (similar to xFormers) that automatically enables several other optimizations depending on the model inputs and GPU type.\n","\n","SDPA is enabled by default if we use PyTorch 2.0 and the latest version of Diffusers.\n","\n","If we want to explicitly enable it, we can set a `DiffusionPipeline` to use `AttnProcessor2_0`:"],"metadata":{"id":"T08y3YTjLj3Z"}},{"cell_type":"code","source":["import torch\n","from diffusers import DiffusionPipeline\n","# import AttnProcessor2_0\n","from diffusers.models.attention_processor import AttnProcessor2_0\n","\n","pipe = DiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True,\n",").to('cuda')\n","# enable AttnProcessor2_0 in unet\n","pipe.unet.set_attn_processor(AttnProcessor2_0())"],"metadata":{"id":"2oqrURvHLJvx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"a photo of an astronaut riding a horse on mars\"\n","image = pipe(prompt).images[0]\n","image"],"metadata":{"id":"zEBIG_nBNIFN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In some cases - such as making the pipeline more deterministic or converting it to other formats - it may be helpful to use the vanilla attention processor, `AttnProcessor`. To revert to `AttnProcessor`, we need to call the `set_default_attn_processor()` function:"],"metadata":{"id":"W-Cul5OMNK7g"}},{"cell_type":"code","source":["pipe.unet.set_default_attn_processor()\n","\n","image = pipe(prompt).images[0]\n","image"],"metadata":{"id":"n9Me3hoqSimn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## `torch.compile`"],"metadata":{"id":"PLbF60OjSo_k"}},{"cell_type":"markdown","source":["It is usually best to wrap the UNet with `torch.compile` becuase it does most of the heavy lifting in the pipeline."],"metadata":{"id":"H-8GdviESrWs"}},{"cell_type":"code","source":["from diffusers import DiffusionPipeline\n","import torch\n","\n","pipe = DiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True,\n",").to('cuda')\n","\n","pipe.unet = torch.compile(\n","    pipe.unet,\n","    mode='reduce-overhead',\n","    fullgraph=True\n",")"],"metadata":{"id":"YZ_8uMWuSqPd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"a photo of an astronaut riding a horse on mars\"\n","images = pipe(\n","    prompt,\n","    num_inference_steps=50\n","    num_images_per_prompt=4\n",").images[0]\n","images"],"metadata":{"id":"WzeKW_T6TAxx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compilation requires some time to complete, so it is best suited for situations where we prepare our pipeline once and then perform the same type of inference operations multiple times. For example, calling the compiled pipeline on a different image size triggers compilation again which can be expensive."],"metadata":{"id":"qT0YLiJxTJ-K"}},{"cell_type":"markdown","source":["# xFormers"],"metadata":{"id":"-1E-VT3NThbZ"}},{"cell_type":"markdown","source":["It is recommended to use xFormers for both inference and training.\n","\n","After xFormers is installed (`pip install xformers`), we can use `enable_xformers_memory_efficient_attention()` for faster inference and reduced memory comsumption."],"metadata":{"id":"l3mDoPzOTjE7"}},{"cell_type":"markdown","source":["# Token Merging"],"metadata":{"id":"Fa8_YjmDTy8-"}},{"cell_type":"markdown","source":["**Token Merging (ToMe)** merges redundant tokens/patches progressively in the forward pass of a Transformer-based network which can speed-up the inference latency of `StableDiffusionPipeline`.\n","\n","To install ToMe, `pip install tomesd`.\n","\n","We can use ToMe from the `tomesd` library with the `apply_patch` function:"],"metadata":{"id":"AWNNdt8RT1sn"}},{"cell_type":"code","source":["from diffusers import StableDiffusionPipeline\n","import torch\n","import tomesd\n","\n","pipe = StableDiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True,\n",").to('cuda')\n","\n","# apply patch\n","tomesd.apply_patch(pipe, ratio=0.5)"],"metadata":{"id":"C-naXytBTihw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["  image = pipeline(\"a photo of an astronaut riding a horse on mars\").images[0]\n","  image"],"metadata":{"id":"nBEcRnULUziS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `apply_patch` function exposes a number of arguments to help strike a balance between pipeline inference speed and the quality of the generated tokens. The most important argument is `ratio` which controls the number of tokens that are merged during the forward pass.\n","\n","ToMe can greatly preserve the quality of the generated images while boosting inference speed. By increasing the `ratio`, we can speed-up inference even further, but at the cost of some degraded image quality."],"metadata":{"id":"38-ouuRVU3Ak"}},{"cell_type":"markdown","source":["# DeepCache"],"metadata":{"id":"wcJVKUNgVVFe"}},{"cell_type":"markdown","source":["**DeepCache** accelerates `StableDiffusionPipeline` and StableDiffusionXLPipeline` by strategically caching and reusing high-level features while efficiently updating low-level features by taking advantage of the UNet architecture.\n","\n","To install DeepCache, `pip install DeepCache`.\n","\n","Then we can load and enable the `DeepCacheSDHelper`:"],"metadata":{"id":"oHbYNjtmVYmJ"}},{"cell_type":"code","source":["import torch\n","from diffusers import StableDiffusionPipeline\n","\n","pipe = StableDiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True,\n",").to('cuda')"],"metadata":{"id":"62ct75DnVWLg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from DeepCache import DeepCacheSDHelper\n","\n","helper = DeepCacheSDHelper(pipe=pipe)\n","helper.set_params(\n","    cache_interval=3,\n","    cache_branch_id=0,\n",")\n","helper.enable()"],"metadata":{"id":"ySJQSI2gV5JE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = pipe(\"a photo of an astronaut on a moon\").images[0]\n","image"],"metadata":{"id":"uopoFIQqWERo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `set_params` method accepts\n","* `cache_interval`, the frequency of feature caching, specified as the number of steps between each cache operation.\n","* `cache_branch_id`, identifying which branch of the network (ordered from the shallowest to the deepest layer) is responsible for executing the caching process.\n","\n","a lower `cache_branch_id` or a larger `cache_interval` can lead to faster inference speed at the expense of reduced image quality."],"metadata":{"id":"hh4vLfzLWGpn"}},{"cell_type":"markdown","source":["# T-GATE"],"metadata":{"id":"l9QtFjpDZtIn"}},{"cell_type":"markdown","source":["**T-GATE** accelerates inference for Stable Diffusion, PixArt, and Latency Consistentcy Model pipelines by *skipping the cross-attention calculation* once it converges. This method does not require any additional training and it can speed up inference from 10-50%. T-GATE is also compatible with other optimization methods mentioned above.\n","\n","To install T-GATE, `pip install tgate`.\n","\n","To use T-GATE with a pipeline, we need to use its corresponding loader:\n","\n","| Pipeline | T-GATE Loader |\n","| -------- | ------------- |\n","| PixArt | TgatePixArtLoader |\n","| Stable Diffusion XL | TgateSDXLLoader |\n","| Stable Diffusion XL + DeepCache | TgateSDXLLDeepCacheLLoader |\n","| Stable Diffusion | TgateSDLoader |\n","| Stable Diffusion + DeepCache | TgateSDDeepCacheLoader |\n","\n","Next, we can create a `TgateLoader` with a pipeline, the gate step (the time step to stop calculating the cross attention), and the number of inference steps. Then we call the `tgate` method on the pipeline with a prompt, gate step, and the number of inference steps."],"metadata":{"id":"4bet_4OHZunm"}},{"cell_type":"markdown","source":["##### PixArt"],"metadata":{"id":"h8wZfLZVNNRX"}},{"cell_type":"code","source":["import torch\n","from diffusers import PixArtAlphaPipeline\n","from tgate import TgatePixArtLoader\n","\n","pipe = PixArtAlphaPipeline.from_pretrained(\n","    'PixArt-alpha/PixArt-XL-2-1024-MS',\n","    torch_dtype=torch.float16,\n",")\n","\n","gate_step = 8\n","inference_step = 25\n","pipe = TgatePixArtLoader(\n","    pipe,\n","    gate_step=gate_step,\n","    num_inference_steps=inference_step\n",").to('cuda')"],"metadata":{"id":"pHy-UQ_gZuGv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = pipe.tgate(\n","       \"An alpaca made of colorful building blocks, cyberpunk.\",\n","       gate_step=gate_step,\n","       num_inference_steps=inference_step,\n",").images[0]\n","image"],"metadata":{"id":"O-sJ_SQkNj1r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### SDXL"],"metadata":{"id":"RtFCkgDBOYEg"}},{"cell_type":"code","source":["import torch\n","from diffusers import StableDiffusionXLPipeline, DPMSolverMultistepScheduler\n","from tgate import TgateSDXLLoader\n","\n","pipe = StableDiffusionXLPipeline.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    torch_dtype=torch.float16,\n","    variant='fp16',\n","    use_safetensors=True,\n",")\n","pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n","\n","gate_step = 10\n","inference_step = 25\n","pipe = TgateSDXLLoader(\n","    pipe,\n","    gate_step=gate_step,\n","    num_inference_steps=inference_step\n",").to('cuda')"],"metadata":{"id":"_EoePDClOZ8m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = pipe.tgate(\n","       \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k.\",\n","       gate_step=gate_step,\n","       num_inference_steps=inference_step\n",").images[0]\n","image"],"metadata":{"id":"YUI1RLQeO2AA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### SDXL + DeepCache"],"metadata":{"id":"stLUTMLtO3kL"}},{"cell_type":"code","source":["import torch\n","from diffusers import StableDiffusionXLPipeline, DPMSolverMultistepScheduler\n","from tgate import TgateSDXLDeepCacheLoader\n","\n","pipe = StableDiffusionXLPipeline.from_pretrained(\n","            \"stabilityai/stable-diffusion-xl-base-1.0\",\n","            torch_dtype=torch.float16,\n","            variant=\"fp16\",\n","            use_safetensors=True,\n",")\n","pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n","\n","gate_step = 10\n","inference_step = 25\n","pipe = TgateSDXLDeepCacheLoader(\n","       pipe,\n","       cache_interval=3,\n","       cache_branch_id=0,\n",").to(\"cuda\")"],"metadata":{"id":"8KvX70FVO5bQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = pipe.tgate(\n","       \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k.\",\n","       gate_step=gate_step,\n","       num_inference_steps=inference_step\n",").images[0]\n","image"],"metadata":{"id":"RtBcmCrBO-rE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Latent Consistency Model"],"metadata":{"id":"ogOWH6eoPAtE"}},{"cell_type":"code","source":["import torch\n","from diffusers import (\n","    StableDiffusionXLPipeline, UNet2DConditionModel, LCMScheduler\n",")\n","from tgate import TgateSDXLLoader\n","\n","unet = UNet2DConditionModel.from_pretrained(\n","    'latent-consistency/lcm-sdxl',\n","    torch_dtype=torch.float16,\n","    variant='fp16',\n",")\n","\n","pipe = StableDiffusionXLPipeline.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    unet=unet,\n","    torch_dtype=torch.float16,\n","    variant='fp16',\n",")\n","pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n","\n","gate_step = 1\n","inference_step = 4\n","pipe = TgateSDXLLoader(\n","    pipe,\n","    gate_step=gate_step,\n","    num_inference_steps=inference_step,\n","    lcm=True\n",").to('cuda')"],"metadata":{"id":"Dxb6FcgZPC7N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = pipe.tgate(\n","       \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k.\",\n","       gate_step=gate_step,\n","       num_inference_steps=inference_step\n",").images[0]\n","image"],"metadata":{"id":"Nmds-KQoPl9O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# xDiT"],"metadata":{"id":"vc-NGQrYPsVH"}},{"cell_type":"markdown","source":["**xDiT** is an inference engine designed for the large scale parallel deployment of Diffusion Transformers (DiTs). xDiT provides a suite of efficient parallel approaches for Diffusion models, as welll as GPU kernel accelerations.\n","\n","Parallel methods in xDiT:\n","* Unified Sequence Parallelism\n","* PipeFusion\n","* CGF Parallelism\n","* Data Parallelism\n","\n","To install xDiT, `pip install xfuser`.\n","\n","Example of using xDiT to accelerate inference of a Diffusers model:"],"metadata":{"id":"kfvJa3KEPuBb"}},{"cell_type":"code","source":["import torch\n","from diffusers import StableDiffusion3Pipeline\n","\n","from xfuser import xFuserArgs, xDiTParallel\n","from xfuser.config import FlexibleArgumentParser\n","from xfuser.core.distributred import get_world_group\n","\n","\n","def main():\n","    parser = FlexibleArgumentParser(description='xFuser Arguments')\n","    args = xFuserArgs.add_cli_args(parser).parse_args()\n","    engine_args = xFuserArgs.from_cli_args(args)\n","    engine_config, input_config = engine_args.create_config()\n","\n","    local_rank = get_world_group().local_rank\n","\n","    pipe = StableDiffusion3Pipeline.from_pretrained(\n","        pretrained_model_name_or_path=engine_config.model_config.model,\n","        torch_dtype=torch.float16,\n","    ).to(f\"cuda:{local_rank}\")\n","\n","    pipe = xDiTParallel(\n","        pipe,\n","        engine_config,\n","        input_config\n","    )\n","\n","    image = pipe(\n","        height=input_config.height,\n","        width=input_config.height,\n","        prompt=input_config.prompt,\n","        num_inference_steps=input_config.num_inference_steps,\n","        output_type=input_config.output_type,\n","        generator=torch.Generator(device=\"cuda\").manual_seed(input_config.seed),\n","    )\n","\n","    if input_config.output_type == 'pil':\n","        pipe.save('results', 'stable_diffusion_3')\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"pTEV6IlvPter"},"execution_count":null,"outputs":[]}]}