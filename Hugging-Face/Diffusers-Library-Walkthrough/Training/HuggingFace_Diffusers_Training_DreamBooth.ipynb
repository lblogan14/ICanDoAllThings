{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPCj3NS7G3/QGTdaM1agFhL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# DreamBooth"],"metadata":{"id":"zLgNaQZjigfu"}},{"cell_type":"markdown","source":["**DreamBooth** is a training technique that updates the entire diffusion model by training no just a few images of a subject or style. It works by associating a special word in the prompt with the example images.\n","\n","We will explore the [`train_dreambooth.py`](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth.py).\n","\n","As always, make sure to install the `diffusers` library from source:\n","```bash\n","git clone https://github.com/huggingface/diffusers\n","cd diffusers\n","pip install .\n","```\n","\n","We need to navigate to the following folder and install the corresponding dependencies:\n","```bash\n","cd examples/dreambooth\n","pip install -r requirements.txt\n","```\n","HuggingFace Accelerate is also helpful to train on multiple GPUs with mixed-precision.\n","```bash\n","pip install accelerate\n","```\n","\n","Now we can initialize a HuggingFace Accelerate environment\n","```bash\n","accelerate config\n","```\n","To set up a default Acclerate environment without choosing any configurations:\n","```bash\n","accelerate config default\n","```\n","Or if our environment does not support an interactive shell like a notebook, we can use:\n","```python\n","from accelerate.utils import write_basic_config\n","write_basic_config()\n","```"],"metadata":{"id":"hZU7drkhiiDJ"}},{"cell_type":"markdown","source":["## Script parameters"],"metadata":{"id":"xoPMPmvii8_l"}},{"cell_type":"markdown","source":["**DreamBooth is very sensitive to training hyperparameters, and it is easy to overfit.**\n","\n","The training script provides many parameters to customize the training run. We can find all of the parameters and their descriptions in the `parse_args()` function.\n","\n","To speed up training with mixed precision using the `bp16` format, add the `--mixed_precision` parameter to the training command:\n","```bash\n","accelerate launch train_text_to_image.py --mixed_precision=\"bp16\"\n","```\n","\n","Some basic and important parameters to specify:\n","* `--pretrained_model_name_or_path`: the name of the model on the Hub or a local path to the pretrained model\n","* `--instance_data_dir`: path to a folder containing the training dataset (example images)\n","* `--instance_prompt`: the text prompt that contains the special word for the example images\n","* `--train_text_encoder`: whether to also train the text encoder\n","* `--output_dir`: where to save the trained model\n","* `--push_to_hub`: whether to push the trained model to the Hub\n","* `--checkpointing_steps`: frequency of saving a checkpoint as the model trains; this is useful if for some reason training is interrupted, you can continue training from that checkpoint by adding `--resume_from_checkpoint` to your training command"],"metadata":{"id":"dW7bd2OSi_q6"}},{"cell_type":"markdown","source":["### Min-SNR weighting"],"metadata":{"id":"gE3-2_EXnrms"}},{"cell_type":"markdown","source":["The **Min-SNR** weighting strategy can help with training by rebalancing the loss to achieve faster convergence. The training script supports predicting `epsilon` (noise) or `v_prediction`, but Min-SNR is compatible with both prediction types.\n","\n","Add the `--snr_gamma` parameter and set it to the recommended value of 5.0:\n","```bash\n","accelerate launch train_dreambooth.py --snr_gamma=5.0\n","```"],"metadata":{"id":"Am88JF_wnwwn"}},{"cell_type":"markdown","source":["### Prior preservation loss"],"metadata":{"id":"wa9Dd2hJoJVQ"}},{"cell_type":"markdown","source":["**Prior preservation loss** is a method that uses a model's own generated samples to help it learn how to generate more diverse images. Because these generated sample images belong to the same class as the images we provided, they help the model retain what it has learned about the class and how it can use what it already knows about the class to make new compositions.\n","\n","Some parameters:\n","* `--with_prior_preservation`: whether to use prior preservation loss\n","* `--prior_loss_weight`: controls the influence of the prior preservation loss on the model\n","* `--class_data_dir`: path to a folder containing the generated class sample images\n","* `--class_prompt`: the text prompt describing the class of the generated sample images\n","\n","Example `bash` script:\n","```bash\n","accelerate launch train_dreambooth.py \\\n","  --with_prior_preservation \\\n","  --prior_loss_weight=1.0 \\\n","  --class_data_dir=\"path/to/class/images\" \\\n","  --class_prompt=\"text prompt describing class\"\n","```"],"metadata":{"id":"Jrv1Si2doMCD"}},{"cell_type":"markdown","source":["### Train text encoder"],"metadata":{"id":"AJ6v-neppBaV"}},{"cell_type":"markdown","source":["To improve the quality of the generated outputs, we can also train the text encoder in addition to the UNet. This requires additional memory and we will need a GPU with at least 24GB of vRAM.\n","\n","Example `bash` script:\n","```bash\n","accelerate launch train_dreambooth.py --train_text_encoder\n","```"],"metadata":{"id":"uy63okpbpDIx"}},{"cell_type":"markdown","source":["## Training script"],"metadata":{"id":"i0GljWA7qoCF"}},{"cell_type":"markdown","source":["DreamBooth comes with its own dataset classes:\n","* `DreamBoothDataset`: preprocess the images and class images, and tokenize the prompts for training\n","* `PromptDataset`: generate the prompt embeddings to generate the class images\n","\n","If we enable the prior preservation loss, the class images are generated here:\n","```python\n","    # Generate class images if prior preservation is enabled.\n","    if args.with_prior_preservation:\n","        class_images_dir = Path(args.class_data_dir)\n","        if not class_images_dir.exists():\n","            class_images_dir.mkdir(parents=True)\n","        cur_class_images = len(list(class_images_dir.iterdir()))\n","\n","        if cur_class_images < args.num_class_images:\n","            torch_dtype = torch.float16 if accelerator.device.type == \"cuda\" else torch.float32\n","            if args.prior_generation_precision == \"fp32\":\n","                torch_dtype = torch.float32\n","            elif args.prior_generation_precision == \"fp16\":\n","                torch_dtype = torch.float16\n","            elif args.prior_generation_precision == \"bf16\":\n","                torch_dtype = torch.bfloat16\n","            pipeline = DiffusionPipeline.from_pretrained(\n","                args.pretrained_model_name_or_path,\n","                torch_dtype=torch_dtype,\n","                safety_checker=None,\n","                revision=args.revision,\n","            )\n","            pipeline.set_progress_bar_config(disable=True)\n","\n","            num_new_images = args.num_class_images - cur_class_images\n","            logger.info(f\"Number of class images to sample: {num_new_images}.\")\n","\n","            sample_dataset = PromptDataset(args.class_prompt, num_new_images)\n","            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\n","\n","            sample_dataloader = accelerator.prepare(sample_dataloader)\n","            pipeline.to(accelerator.device)\n","\n","            for example in tqdm(\n","                sample_dataloader, desc=\"Generating class images\", disable=not accelerator.is_local_main_process\n","            ):\n","                images = pipeline(example[\"prompt\"]).images\n","\n","                for i, image in enumerate(images):\n","                    hash_image = hashlib.sha1(image.tobytes()).hexdigest()\n","                    image_filename = class_images_dir / f\"{example['index'][i] + cur_class_images}-{hash_image}.jpg\"\n","                    image.save(image_filename)\n","```\n","\n","The `main()` function handles setting up the dataset for training and the training loop itself. The script loads the `tokenizer`, `scheduler`, and `models`:\n","```python\n","    # Load the tokenizer\n","    if args.tokenizer_name:\n","        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False)\n","    elif args.pretrained_model_name_or_path:\n","        tokenizer = AutoTokenizer.from_pretrained(\n","            args.pretrained_model_name_or_path,\n","            subfolder=\"tokenizer\",\n","            revision=args.revision,\n","            use_fast=False,\n","        )\n","\n","    # import correct text encoder class\n","    text_encoder_cls = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path, args.revision)\n","\n","    # Load scheduler and models\n","    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n","    text_encoder = text_encoder_cls.from_pretrained(\n","        args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\n","    )\n","\n","    if model_has_vae(args):\n","        vae = AutoencoderKL.from_pretrained(\n","            args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision\n","        )\n","    else:\n","        vae = None\n","\n","    unet = UNet2DConditionModel.from_pretrained(\n","        args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision\n","    )\n","```\n","\n","Then, we create the training dataset and dataloader from `DreamBoothDataset`:\n","```python\n","    # Dataset and DataLoaders creation:\n","    train_dataset = DreamBoothDataset(\n","        instance_data_root=args.instance_data_dir,\n","        instance_prompt=args.instance_prompt,\n","        class_data_root=args.class_data_dir if args.with_prior_preservation else None,\n","        class_prompt=args.class_prompt,\n","        class_num=args.num_class_images,\n","        tokenizer=tokenizer,\n","        size=args.resolution,\n","        center_crop=args.center_crop,\n","        encoder_hidden_states=pre_computed_encoder_hidden_states,\n","        class_prompt_encoder_hidden_states=pre_computed_class_prompt_encoder_hidden_states,\n","        tokenizer_max_length=args.tokenizer_max_length,\n","    )\n","\n","    train_dataloader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=args.train_batch_size,\n","        shuffle=True,\n","        collate_fn=lambda examples: collate_fn(examples, args.with_prior_preservation),\n","        num_workers=args.dataloader_num_workers,\n","    )\n","```\n","\n","Finally, the training loop takes care of the remaining steps such as converting images to latent space, adding noise to the input, predicting the noise residual, and calculating the loss."],"metadata":{"id":"VCtqRnrTqqQf"}},{"cell_type":"markdown","source":["## Launch the script"],"metadata":{"id":"TaPh1Oour-1o"}},{"cell_type":"markdown","source":["In this guide, we will download some images from the `dog` dataset and store them in a directory.\n","```python\n","from huggingface_hub import snapshot_download\n","\n","local_dir = './dog'\n","snapshot_download(\n","    'diffusers/dog-example',\n","    local_dir=local_dir,\n","    repo_type='dataset',\n","    ignore_patterns='.gitattributes'\n",")\n","\n","```\n","\n","If we want to follow along with the training process, we can periodically save generated images as training progresses. Add the following parameters to the training command:\n","```bash\n","  --validation_prompt=\"a photo of a sks dog\"\n","  --num_validation_images=4\n","  --validaiton_steps=100\n","```"],"metadata":{"id":"KGeELnb3sBer"}},{"cell_type":"markdown","source":["On a 12GB GPU, we need `bitsandbytes` 8-bit optimizer, gradient checkpointing, xFormers, and set the gradients to `None` instead of zero to reduce our memory-usage.\n","```bash\n","export MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n","export INSTANCE_DIR=\"./dog\"\n","export OUTPUT_DIR=\"path_to_saved_model\"\n","\n","accelerate launch train_dreambooth.py \\\n","  --pretrained_model_name_or_path=$MODEL_NAME  \\\n","  --instance_data_dir=$INSTANCE_DIR \\\n","  --output_dir=$OUTPUT_DIR \\\n","  --instance_prompt=\"a photo of sks dog\" \\\n","  --resolution=512 \\\n","  --train_batch_size=1 \\\n","  --gradient_accumulation_steps=1 \\\n","  --learning_rate=5e-6 \\\n","  --lr_scheduler=\"constant\" \\\n","  --lr_warmup_steps=0 \\\n","  --max_train_steps=400 \\\n","  --use_8bit_adam \\\n","  --gradient_checkpointing \\\n","  --enable_xformers_memory_efficient_attention \\\n","  --set_grads_to_none \\\n","  --push_to_hub\n","```"],"metadata":{"id":"WZwXkFSYskVZ"}},{"cell_type":"markdown","source":["On a 16GB GPU,\n","```bash\n","export MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n","export INSTANCE_DIR=\"./dog\"\n","export OUTPUT_DIR=\"path_to_saved_model\"\n","\n","accelerate launch train_dreambooth.py \\\n","  --pretrained_model_name_or_path=$MODEL_NAME  \\\n","  --instance_data_dir=$INSTANCE_DIR \\\n","  --output_dir=$OUTPUT_DIR \\\n","  --instance_prompt=\"a photo of sks dog\" \\\n","  --resolution=512 \\\n","  --train_batch_size=1 \\\n","  --gradient_accumulation_steps=1 \\\n","  --learning_rate=5e-6 \\\n","  --lr_scheduler=\"constant\" \\\n","  --lr_warmup_steps=0 \\\n","  --max_train_steps=400 \\\n","  --use_8bit_adam \\\n","  --gradient_checkpointing \\\n","  --push_to_hub\n","```"],"metadata":{"id":"bSRa_I6Ws8g4"}},{"cell_type":"markdown","source":["Once training is complete, we can use our newly trained model for inference:"],"metadata":{"id":"R7CBAgTItGxM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kiv9wKfQicbC"},"outputs":[],"source":["from diffusers import DiffusionPipeline\n","import torch\n","\n","pipeline = DiffusionPipeline.from_pretrained(\n","    \"path_to_saved_model\",\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",").to(\"cuda\")"]},{"cell_type":"code","source":["image = pipeline(\n","    \"A photo of sks dog in a bucket\",\n","    num_inference_steps=50,\n","    guidance_scale=7.5\n",").images[0]\n","image"],"metadata":{"id":"YiVVa9ObtRMw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LoRA"],"metadata":{"id":"F96iLJOptVZ1"}},{"cell_type":"markdown","source":["LoRA is a training technique for significantly reducing the number of trainable parameters. As a result, training is faster and it is easier to store the resulting weights because they are a lot smaller (~100MBs).\n","\n","We can use the [`train_dreambooth_lora.py`](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora.py) script to train with LoRA."],"metadata":{"id":"rNaizGRovwDn"}},{"cell_type":"markdown","source":["## SDXL"],"metadata":{"id":"wo9asuTrwJqc"}},{"cell_type":"markdown","source":["SDXL adds a second text-encoder to its architecture to generate high-resolution images.\n","\n","We can use the [`train_dreambooth_lora_sdxl.py`](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora_sdxl.py) script to train a SDXL model with LoRA."],"metadata":{"id":"NPbIihzMwLMZ"}},{"cell_type":"markdown","source":["## DeepFloyd IF"],"metadata":{"id":"divUiRCzwXwb"}},{"cell_type":"markdown","source":["**DeeppFloyd IF** is a cascading pixel diffusion model with three stages. The first stage generates a base image and the second and third stages progressively upscales the base image into a high-resolution 1024x1024 image.\n","\n","We can use [`train_dreambooth_lora.py`](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora.py) to train a DeepFloyd IF model with LoRA, or use the [`train_dreambooth.py`](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth.py) to train a DeepFloyd IF model with the full model.\n","\n","DeepFloyd IF uses predicted variance, but the Diffusers training scripts use predicted error so the trained DeepFloyd IF models are switched to a fixed variance schedule. The training scripts will update the scheduler config of the fully trained model for us. However, when we load the saved LoRA weights, we must also update the pipeline's scheduler config:"],"metadata":{"id":"sf3nwU9WwcKu"}},{"cell_type":"code","source":["from diffusers import DiffusionPipeline\n","\n","pipe = DiffusionPipeline.from_pretrained(\n","    'DeepFloyd/IF-I-XL-v1.0',\n","    use_safetensors=True\n",")\n","pipe.load_lora_weights('<lora weights path>')\n","\n","# Update scheduler config to fixed variance schedule\n","pipe.scheduler = pipe.scheduler.__class__.from_config(\n","    pipe.scheduler.config,\n","    variance_type='fixed_small'\n",")"],"metadata":{"id":"SESmQb0HtWoe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The stage 2 model requires additional validation images to upscale. We can download and use a downsized version of the training images for this:"],"metadata":{"id":"dqw5uGCPxntd"}},{"cell_type":"code","source":["from huggingface_hub import snapshot_download\n","\n","local_dir = './dog_downsized'\n","snapshot_download(\n","    'diffusers/dog-example-downsized',\n","    local_dir=local_dir,\n","    repo_type='dataset',\n","    ignore_patterns='.gitattributes'\n",")"],"metadata":{"id":"dxr9_ClJxs7u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The parameters below useful to train a DeepFloyd IF model with a combination of DreamBooth and LoRA:\n","* `--resolution=64`, a much smaller resolution is required because DeepFloyd IF is a pixel diffusion model and to work on uncompressed pixels, the input images must be smaller\n","* `--pre_compute_text_embeddings`, compute the text embeddings ahead of time to save memory because the `T5Model` can take up a lot of memory\n","* `--tokenizer_max_length=77`, we can use a longer default text length with T5 as the text encoder but the default model encoding procedure uses a shorter text length\n","* `--text_encoder_use_attention_mask`, pass the attention mask to the text encoder"],"metadata":{"id":"GdwIeiU-x-Lp"}},{"cell_type":"markdown","source":["##### Stage 1 LoRA DreamBooth"],"metadata":{"id":"ZBN05oYP0HQH"}},{"cell_type":"markdown","source":["Training stage 1 of DeepFloyd IF with LoRA and DreamBooth requires ~28GB of memory.\n","\n","```bash\n","export MODEL_NAME=\"DeepFloyd/IF-I-XL-v1.0\"\n","export INSTANCE_DIR=\"dog\"\n","export OUTPUT_DIR=\"dreambooth_dog_lora\"\n","\n","accelerate launch train_dreambooth_lora.py \\\n","  --report_to wandb \\\n","  --pretrained_model_name_or_path=$MODEL_NAME  \\\n","  --instance_data_dir=$INSTANCE_DIR \\\n","  --output_dir=$OUTPUT_DIR \\\n","  --instance_prompt=\"a sks dog\" \\\n","  --resolution=64 \\\n","  --train_batch_size=4 \\\n","  --gradient_accumulation_steps=1 \\\n","  --learning_rate=5e-6 \\\n","  --scale_lr \\\n","  --max_train_steps=1200 \\\n","  --validation_prompt=\"a sks dog\" \\\n","  --validation_epochs=25 \\\n","  --checkpointing_steps=100 \\\n","  --pre_compute_text_embeddings \\\n","  --tokenizer_max_length=77 \\\n","  --text_encoder_use_attention_mask\n","```"],"metadata":{"id":"mXgHxnJQ0J85"}},{"cell_type":"markdown","source":["##### Stage 2 LoRA DreamBooth"],"metadata":{"id":"YNMZyY3W0U_o"}},{"cell_type":"markdown","source":["For stage 2 of DeepFloyd IF with LoRA and DreamBooth, pay attention to these parameters:\n","* `--validation_images`, the images to upscale during validaiton\n","* `--class_labels_conditioning=\"timesteps\"`, additionally conditional the UNet as needed in stage 2\n","* `--learning_rate=1e-6`, a lower learning rate is used compared to stage 1\n","* `--resolution=256`, the expected resolution for the upscaler\n","\n","```bash\n","export MODEL_NAME=\"DeepFloyd/IF-II-L-v1.0\"\n","export INSTANCE_DIR=\"dog\"\n","export OUTPUT_DIR=\"dreambooth_dog_upscale\"\n","export VALIDATION_IMAGES=\"dog_downsized/image_1.png dog_downsized/image_2.png dog_downsized/image_3.png dog_downsized/image_4.png\"\n","\n","python train_dreambooth_lora.py \\\n","    --report_to wandb \\\n","    --pretrained_model_name_or_path=$MODEL_NAME \\\n","    --instance_data_dir=$INSTANCE_DIR \\\n","    --output_dir=$OUTPUT_DIR \\\n","    --instance_prompt=\"a sks dog\" \\\n","    --resolution=256 \\\n","    --train_batch_size=4 \\\n","    --gradient_accumulation_steps=1 \\\n","    --learning_rate=1e-6 \\\n","    --max_train_steps=2000 \\\n","    --validation_prompt=\"a sks dog\" \\\n","    --validation_epochs=100 \\\n","    --checkpointing_steps=500 \\\n","    --pre_compute_text_embeddings \\\n","    --tokenizer_max_length=77 \\\n","    --text_encoder_use_attention_mask \\\n","    --validation_images $VALIDATION_IMAGES \\\n","    --class_labels_conditioning=timesteps\n","```"],"metadata":{"id":"boDm_zw30X6i"}},{"cell_type":"markdown","source":["##### Stage 1 DreamBooth"],"metadata":{"id":"LyaEwRFI1yl9"}},{"cell_type":"markdown","source":["For stage 1 of DeepFloyd IF with DreamBooth,\n","* `--skip_save_text_encoder`, skip saving the full T5 text encoder with the finetuned model\n","* `--use_8bit_adam`, use 8bit Adam optimizer to save memory due to the size of the optimizer state when training the full model\n","* `--learning_rate=1e-7`, a really low learning rate should be used for full model training otherwise the model quality is degraded\n","\n","\n","Training with 8-bit Adam and a batch size of 4, the full model can be trained with ~48GB of memory.\n","```bash\n","export MODEL_NAME=\"DeepFloyd/IF-I-XL-v1.0\"\n","export INSTANCE_DIR=\"dog\"\n","export OUTPUT_DIR=\"dreambooth_if\"\n","\n","accelerate launch train_dreambooth.py \\\n","  --pretrained_model_name_or_path=$MODEL_NAME  \\\n","  --instance_data_dir=$INSTANCE_DIR \\\n","  --output_dir=$OUTPUT_DIR \\\n","  --instance_prompt=\"a photo of sks dog\" \\\n","  --resolution=64 \\\n","  --train_batch_size=4 \\\n","  --gradient_accumulation_steps=1 \\\n","  --learning_rate=1e-7 \\\n","  --max_train_steps=150 \\\n","  --validation_prompt \"a photo of sks dog\" \\\n","  --validation_steps 25 \\\n","  --text_encoder_use_attention_mask \\\n","  --tokenizer_max_length 77 \\\n","  --pre_compute_text_embeddings \\\n","  --use_8bit_adam \\\n","  --set_grads_to_none \\\n","  --skip_save_text_encoder \\\n","  --push_to_hub\n","```"],"metadata":{"id":"b7ZNOQ4T10pb"}},{"cell_type":"markdown","source":["##### Stage 2 DreamBooth"],"metadata":{"id":"WB013gdy2Yw3"}},{"cell_type":"markdown","source":["For stage 2 of DeepFloyd IF with DreamBooth, pay attention to these parameters:\n","\n","* `--learning_rate=5e-6`, use a lower learning rate with a smaller effective batch size\n","* `--resolution=256`, the expected resolution for the upscaler\n","* `--train_batch_size=2` and `--gradient_accumulation_steps=6`, to effectively train on images wiht faces requires larger batch sizes\n","\n","```bash\n","export MODEL_NAME=\"DeepFloyd/IF-II-L-v1.0\"\n","export INSTANCE_DIR=\"dog\"\n","export OUTPUT_DIR=\"dreambooth_dog_upscale\"\n","export VALIDATION_IMAGES=\"dog_downsized/image_1.png dog_downsized/image_2.png dog_downsized/image_3.png dog_downsized/image_4.png\"\n","\n","accelerate launch train_dreambooth.py \\\n","  --report_to wandb \\\n","  --pretrained_model_name_or_path=$MODEL_NAME \\\n","  --instance_data_dir=$INSTANCE_DIR \\\n","  --output_dir=$OUTPUT_DIR \\\n","  --instance_prompt=\"a sks dog\" \\\n","  --resolution=256 \\\n","  --train_batch_size=2 \\\n","  --gradient_accumulation_steps=6 \\\n","  --learning_rate=5e-6 \\\n","  --max_train_steps=2000 \\\n","  --validation_prompt=\"a sks dog\" \\\n","  --validation_steps=150 \\\n","  --checkpointing_steps=500 \\\n","  --pre_compute_text_embeddings \\\n","  --tokenizer_max_length=77 \\\n","  --text_encoder_use_attention_mask \\\n","  --validation_images $VALIDATION_IMAGES \\\n","  --class_labels_conditioning timesteps \\\n","  --push_to_hub\n","```"],"metadata":{"id":"x0NgzYnK2blM"}},{"cell_type":"markdown","source":["### Training tips"],"metadata":{"id":"vgU2fpkK2kyt"}},{"cell_type":"markdown","source":["* LoRA is sufficient for training the stage 1 model because the model's low resolution makes representing finer details difficult regardless.\n","* For common or simple objects, we do not necessarily need to finetune the upscaler. Make sure the prompt passed to the upscaler is adjusted to remove the new token from the instance prompt. For example, if our stage 1 prompt is \"a sks dog\" then our stage 2 prompt should be \"a dog\"\n","* For finer details like faces, fully training the stage 2 upscaler is better than training the stage 2 model with LoRA. It also helps to use lower learning rates with larger batch sizes.\n","* Lower learning rates should be used to train the stage 2 model.\n","* The `DDPMScheduler` works better than the DPMSolver used in the training scripts."],"metadata":{"id":"zraOu6iz2nWm"}},{"cell_type":"code","source":[],"metadata":{"id":"6otsJjK80Gmo"},"execution_count":null,"outputs":[]}]}