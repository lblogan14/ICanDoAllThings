{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPuPsqc8gNJvR11OJkPPTda"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Building a RAG E-book Librarian Using LlamaIndex"],"metadata":{"id":"leoKefJBh-sx"}},{"cell_type":"markdown","source":["In this example, we will build a RAG-based \"librarian\" for a local library.\n","\n","We would like our librarian to be lightweight and run locally as much as possible with minimal dependencies, which means that we will leverage open-source to the fullest extent possible, as well as bias towards models that can be executed locally on typical hardware.\n","\n","We will use the following\n","* LlamaIndex - a data framework for LLM-based applciation designed specifically for RAG( unlike LangChain)\n","* Ollama - a user-friendly solution for running LLMs\n","* `BAAI/bge-base-en-v1.5` embedding model - lightweight in size and good performance\n","* Llama 2, running via Ollama."],"metadata":{"id":"hddFDz9giKmX"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"D1tnD_Gpi7sq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6yru4WUVh6Ee"},"outputs":[],"source":["!pip install -qU llama-index EbookLib html2text llama-index-embeddings-huggingface llama-index-llms-ollama"]},{"cell_type":"markdown","source":["### Ollama installation"],"metadata":{"id":"ch8KSrKQjBxe"}},{"cell_type":"code","source":["!apt install pciutils lshw"],"metadata":{"id":"ptie8-pDjD2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!curl -fsSL https://ollama.com/install.sh | sh"],"metadata":{"id":"nNNVBi8njFRB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run Ollama service in the background"],"metadata":{"id":"tX1Hk0E1jF6h"}},{"cell_type":"code","source":["get_ipython().system_raw('ollama serve $')"],"metadata":{"id":"oYBxJloFjIVB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pull llama 2 from the Ollama library\n","!ollama pull llama2"],"metadata":{"id":"XYrFsCdVjMn5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test library setup"],"metadata":{"id":"S3V3ljXOjS-6"}},{"cell_type":"markdown","source":["We need to create a test \"library\". Assuming that our library is a **nested directory of `.epub` files**."],"metadata":{"id":"Y8eMbgwbjVJa"}},{"cell_type":"code","source":["!mkdir -p \"./test/library/jane-austen\"\n","!mkdir -p \"./test/library/victor-hugo\"\n","!wget https://www.gutenberg.org/ebooks/1342.epub.noimages -O \"./test/library/jane-austen/pride-and-prejudice.epub\"\n","!wget https://www.gutenberg.org/ebooks/135.epub.noimages -O \"./test/library/victor-hugo/les-miserables.epub\""],"metadata":{"id":"N3MCGA7WjUto"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## RAG with LlamaIndex"],"metadata":{"id":"g3lfZvt-jiYt"}},{"cell_type":"markdown","source":["RAG with LlamaIndex consists of the following broad phases:\n","1. **Loading** - we tell LlamaIndex where our data lives and how to load it\n","2. **Indexing** - we augment our loaded data to facilitate querying, e.g., with vector embeddings\n","3. **Querying** - we configure an LLM to act as the query interface for our indexed data"],"metadata":{"id":"g5K119WBjkfH"}},{"cell_type":"markdown","source":["### Loading"],"metadata":{"id":"ep-oO55rj5Jw"}},{"cell_type":"code","source":["from llama_index.core import SimpleDirectoryReader\n","\n","loader = SimpleDirectoryReader(\n","    input_dir='./test',\n","    recursive=True,\n","    required_exts=['.epub']\n",")\n","\n","documents = loader.load_data()"],"metadata":{"id":"QmGbzZY8jjzG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This converts our ebooks into a set of `Documents` for LlamaIndex to work with.\n","\n","Note that the documents here **have NOT been chunked at this stage**."],"metadata":{"id":"Cw4r5xx9kQyb"}},{"cell_type":"markdown","source":["### Indexing"],"metadata":{"id":"ABUCn7pNkbrF"}},{"cell_type":"markdown","source":["The indexing will allow our RAG pipeline to look up the relevant context for our query to pass to our LLM to **argument** their generated response. This is also where document chunking will take place.\n","\n","`VectorStoreIndex` is a default entrypoint for indexing in LlamaIndex. It uses a simple, in-memory dictionary to store the indices, but LlamaIndex also supports a wide variety of vector storage solution for us to scale.\n","\n","By default, LlamaIndex uses a chunk size of 1024 and a chunk overlap of 20.\n","\n","We will use the [`BAAI/bge-small-en-v1.5`](https://huggingface.co/BAAI/bge-base-en-v1.5) to generate our embeddings. By default, LlamaIndex uses OpenAI. However, LlamaIndex supports retrieving embedding models from HuggingFace through the `HuggingFaceEmbedding` class."],"metadata":{"id":"vjvyq0kFkepR"}},{"cell_type":"code","source":["from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n","\n","embedding_model = HuggingFaceEmbedding(model_name='BAAI/bge-base-en-v1.5')"],"metadata":{"id":"V15ohgNxkbIU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then we will pass that into `VectorStoreIndex` as our embeeding model to circumvent the OpenAI default behavior."],"metadata":{"id":"Gx89oylCmdyQ"}},{"cell_type":"code","source":["from llama_index.core import VectorStoreIndex\n","\n","index = VectorStoreIndex.from_documents(\n","    documents,\n","    embed_model=embedding_model\n",")"],"metadata":{"id":"Z5e8kFE_mkjl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Querying"],"metadata":{"id":"0tkcovCrmraH"}},{"cell_type":"markdown","source":["We will use Llama 2 for the purposes of this receipe.\n","\n","We need to start up the Ollama server. In a separate terminal, we run `ollama serve`.\n","\n","Then, we hook Llama 2 up to LlamaIndex and use it as the basis of our query engine."],"metadata":{"id":"KRS31bGRmtlR"}},{"cell_type":"code","source":["from llama_index.llms.ollama import Ollama\n","\n","llama = Ollama(\n","    model='llama2',\n","    request_timeout=40\n",")\n","\n","query_engine = index.as_query_engine(llm=llama)"],"metadata":{"id":"vkOVg6NhmsaN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Final Result"],"metadata":{"id":"dijcWUnVnNdQ"}},{"cell_type":"markdown","source":["With all of those setup, our basic RAG librarian is set up and we can start asking questions about our library."],"metadata":{"id":"f1DDpA8HnQMG"}},{"cell_type":"code","source":["print(\n","    query_engine.query(\n","        \"What are the titles of all the books available? Show me the context used to derive your answer.\"\n","    )\n",")"],"metadata":{"id":"FI5pwqvXnOmY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(query_engine.query(\"Who is the main character of 'Pride and Prejudice'?\"))"],"metadata":{"id":"1eKY26gnnYf4"},"execution_count":null,"outputs":[]}]}