{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMt2E4UlI2GEBs+GTB355+f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Migrating from OpenAI to Open LLMs Using TGI's Messages API"],"metadata":{"id":"E-EJsZmoLgLa"}},{"cell_type":"markdown","source":["[Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference) offers a Messages API, making it directly compatible with the OpenAI Chat Completion API, which means that any existing scripts that use OpenAI models (via the OpenAI client library or third-party tools like LangChain or LlamaIndex) can be directly swapped out to use any Open LLM running on a TGI endpoint.\n","\n","This allows us to quickly test and benefit from the numerous advantages offered by open models, including:\n","* complete control and transparency over models and data\n","* no more worrying about rate limits\n","* the ability to fully customize systems according to our specific needs"],"metadata":{"id":"Vh7YyZOQLmOV"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"A2i8Otx-Mglh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Yzu2TGBLZBf"},"outputs":[],"source":["!pip install --upgrade -q huggingface_hub langchain langchain-community langchainhub langchain-openai llama-index chromadb bs4 sentence_transformers torch torchvision torchaudio llama-index-llms-openai-like llama-index-embeddings-huggingface"]},{"cell_type":"code","source":["import os\n","import getpass\n","\n","# enter API key\n","os.environ[\"HF_TOKEN\"] = HF_API_KEY = getpass.getpass()"],"metadata":{"id":"iGCGhzwGMnaq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create an Inference Endpoint"],"metadata":{"id":"PIW8prXkMqHr"}},{"cell_type":"markdown","source":["To get started, we need to deploy a [`Nous-Hermes-2-Mixtral-8x7B-DPO`](https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO) model, a fine-tuned Mixtral model, to Inference Endpoint using TGI.\n","\n","Instead of directly using the [existing UI](https://endpoints.huggingface.co/new?vendor=aws&repository=NousResearch%2FNous-Hermes-2-Mixtral-8x7B-DPO&tgi_max_total_tokens=32000&tgi=true&tgi_max_input_length=1024&task=text-generation&instance_size=2xlarge&tgi_max_batch_prefill_tokens=2048&tgi_max_batch_total_tokens=1024000&no_suggested_compute=true&accelerator=gpu&region=us-east-1) to deploy a model, we will use the Hub library by specifying an endpoint name and model repository, along with the task of `text-generation`.\n","\n","We will use a `protected` type so access to the deployed model will require a valid HuggingFace token. We also need to configure the hardware requirements like vendor, region, accelerator, instance type, and size."],"metadata":{"id":"ml9JDUv-MtfR"}},{"cell_type":"code","source":["from huggingface_hub import create_inference_endpoint\n","\n","endpoint = create_inference_endpoint(\n","    'nous-hermes-2-mixtral-8x7b-demo',\n","    repository='NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO',\n","    framework='pytorch',\n","    task='text-generation',\n","    accelerator='gpu',\n","    vendor='aws',\n","    region='us-east-1',\n","    type='protected',\n","    instance_type='p4de',\n","    instance_size='2xlarge',\n","    custom_image={\n","        'health_route': '/health',\n","        'env': {\n","            'MAX_INPUT_LENGTH': '4096',\n","            'MAX_BATCH_PREFILL_TOKENS': '4096',\n","            'MAX_TOTAL_TOKENS': '32000',\n","            'MAX_BATCH_TOTAL_TOKENS': '1024000',\n","            'MODEL_ID': '/repository'\n","        },\n","        'url': \"ghcr.io/huggingface/text-generation-inference:sha-1734540\",  # must be >= 1.4.0\n","    }\n",")"],"metadata":{"id":"H3mpAKE4MslQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["endpoint.wait()\n","endpoint.status"],"metadata":{"id":"igclWzjxONp4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It will take a few minutes for the deployment to spin up. We can use the `.wait()` method to block the running thread until the endpoint reaches a final `\"running\"` state. Once running, we can confirm its status and test it via the UI Playground.\n","\n","When deploying with `huggingface_hub`, our endpoint will scale-to-zero after 15 minutes of idle time by default to optimize cost during periods of inactivity."],"metadata":{"id":"vtSjVh-bOXzz"}},{"cell_type":"markdown","source":["## Query the Inference Endpoint with OpenAI Client Libraries"],"metadata":{"id":"Acb2u4ySO04V"}},{"cell_type":"markdown","source":["Since our model is hosted with TGI, it now supports a Messages API meaning we can query it directly using the faimilar OpenAI client libraries."],"metadata":{"id":"QhZhSVD4O5Hh"}},{"cell_type":"code","source":["from openai import OpenAI\n","\n","BASE_URL = endpoint.url\n","\n","# initialize the client but point it to TGI\n","client = OpenAI(\n","    base_url=os.path.join(BASE_URL, 'v1/'),\n","    api_key=HF_API_KEY\n",")"],"metadata":{"id":"iDlCl-tDO0er"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chat_completion = client.chat.completions.create(\n","    model='tgi',\n","    messages=[\n","        {'role': 'system', 'content': \"You are a helpful assistant.\"},\n","        {'role': 'user', 'content': \"Why is open-source software important?\"}\n","    ],\n","    stream=True,\n","    max_tokens=512\n",")\n","\n","# iterate and print stream\n","for message in chat_completion:\n","    print(message.choices[0].delta.content, end=\"\")"],"metadata":{"id":"jyzvMo33Pc34"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["TGI's Messages API automatically converts the list of messages into the model's required instruction format using its chat template."],"metadata":{"id":"dNtx4IqWP0zW"}},{"cell_type":"markdown","source":["## Integrate with LangChain and LlamaIndex"],"metadata":{"id":"Q8atdxxuP9C6"}},{"cell_type":"markdown","source":["We can use the newly created endpoint with popular RAG frameworks like LangChain and LlamaIndex."],"metadata":{"id":"rYf1ggHmQBQm"}},{"cell_type":"markdown","source":["### Use with LangChain"],"metadata":{"id":"kL4Fs22-QHGG"}},{"cell_type":"markdown","source":["To use it in LangChain, we simply create an instance of `ChatOpenAI` and pass our `<ENDPOINT_URL>` and `<HF_API_KEY>` as follows:"],"metadata":{"id":"ScX-EWAfQJYV"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(\n","    model_name='tgi',\n","    openai_api_key=HF_API_KEY,\n","    openai_api_base=os.path.join(BASE_URL, 'v1/')\n",")"],"metadata":{"id":"r_oYhJAeP_uG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm.invoke('Why is open-source software important?')"],"metadata":{"id":"mrjsOeEwQkK9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We now can use our Mixtral model in a simple RAG pipeline to answer a question over the contents of a HF blog post."],"metadata":{"id":"0q8u-_zZQp_K"}},{"cell_type":"code","source":["from langchain import hub\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","from langchain_community.document_loaders import WebBaseLoader\n","from langchain_community.vectorstores import Chroma\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n","\n","# Load the blog contents\n","loader = WebBaseLoader(\n","    web_paths=('https://huggingface.co/blog/open-source-llms-as-agents',)\n",")\n","docs = loader.load()\n","\n","# Declare an HF embedding model\n","hf_embeddings = HuggingFaceEmbeddings(model_name='BAAI/bge-large-en-v1.5')\n","\n","# Chunk and index the docuemnt content\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=512,\n","    chunk_overlap=200\n",")\n","splits = text_splitter.split_documents(docs)\n","vectorstore = Chroma.from_documents(\n","    documents=splits,\n","    embedding=hf_embeddings\n",")\n","\n","# Retrieve the relevant snippets of the blog\n","retriever = vectorstore.as_retriever()\n","prompt = hub.pull('rlm/rag-prompt')\n","\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","rag_chain_from_docs = (\n","    RunnablePassthrough.assign(context=(lambda x: format_docs(x['context'])))\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","rag_chain_with_source = RunnableParallel(\n","    {\n","        'context': retriever,\n","        'question': RunnablePassthrough()\n","    }\n",").assign(answer=rag_chain_from_docs)\n","\n","# Generate the answer\n","rag_chain_with_source.invoke(\n","    \"According to this article which open-source model is the best for an agent behaviour?\"\n",")"],"metadata":{"id":"-abIvUNEQu-P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Use with LlamaIndex"],"metadata":{"id":"W6K21hMkSuYR"}},{"cell_type":"markdown","source":["Similarly, we can also use a TGI endpoint in LlamaIndex. We will use the `OpenAILike` class, and instantiate it by comfiguring some additional arguments. Note that the context window argument should match the value previously set for `MAX_TOTAL_TOKENS` of our endpoint."],"metadata":{"id":"xFglHj37SwQU"}},{"cell_type":"code","source":["from llama_index.llms.openai_like import OpenAILike\n","\n","llm = OpenAILike(\n","    model='tgi',\n","    api_key=HF_API_KEY,\n","    api_base=BASE_URL + '/v1/',\n","    is_chat_model=True,\n","    is_local=False,\n","    is_function_calling_model=False,\n","    context_window= 4096, # same as `MAX_TOTAL_TOKENS`\n",")"],"metadata":{"id":"5MLQK5m-Sv5W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm.complete('Why is open-source software important?')"],"metadata":{"id":"vSTB2hKiiGgO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can now use it in a similar RAG pipeline. Note that the previous choice of `MAX_INPUT_LENGTH` in our Inference Endpoint will directly influence the number of retrieved chunks (`similarity_top_k`) the model can process."],"metadata":{"id":"ORrGbnSLiJxU"}},{"cell_type":"code","source":["from llama_index.core import VectorStoreIndex, download_loader\n","from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n","from llama_index.core.query_engine import CitationQueryEngine\n","\n","SimpleWebPageReader = download_loader('SimpleWebPageReader')\n","\n","documents = SimpleWebPageReader(html_to_text=True).load_data(\n","    ['http://huggingface.co/blog/open-source-llms-as-agents']\n",")\n","\n","# Load embedding model\n","embed_model = HuggingFaceEmbedding(model_name='BAAI/bge-large-en-v1.5')\n","\n","# Pass LLM to pipeline\n","index = VectorStoreIndex.from_documents(\n","    documents,\n","    embed_model=embed_model,\n","    show_pregress=True\n",")\n","\n","# Query the index\n","query_engine = CitationQueryEngine.from_args(\n","    index,\n","    similarity_top_k=2,\n",")\n","\n","response = query_engine.query(\"According to this article which open-source model is the best for an agent behaviour?\")\n","response.response"],"metadata":{"id":"ocx6QwyeiWm0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ending"],"metadata":{"id":"MV4WKA6FjEkW"}},{"cell_type":"markdown","source":["After we are done with our endpoint, we can either pause or delete it."],"metadata":{"id":"3E45nUNKjIwc"}},{"cell_type":"code","source":["endpoint.pause()"],"metadata":{"id":"9iyIyF4yjIUn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# or\n","endpoint.delete()"],"metadata":{"id":"7MemUTGPjN_L"},"execution_count":null,"outputs":[]}]}