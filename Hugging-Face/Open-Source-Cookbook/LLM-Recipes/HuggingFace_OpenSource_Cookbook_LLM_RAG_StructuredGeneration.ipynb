{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaaGfuprtlvG"
   },
   "source": [
    "# RAG with Source Highlighting Using Structured Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXSQl4I2tu5n"
   },
   "source": [
    "**Structured generation** is a method that forces the LLM output to follow certain constraints, for example, to follow a speific pattern.\n",
    "\n",
    "Use cases of structured generation:\n",
    "* output a dictionary with specific keys\n",
    "* make sure the output will be longer than N characters\n",
    "* force the output to follow a certain regex pattern for downstream processing\n",
    "* highlight sources supporting the answer in RAG\n",
    "\n",
    "In this example, we will build a RAG system that not only provides an answer, but also highlights the supporting snippets that this answer is based on.\n",
    "\n",
    "We will apply a naive approach to structured generation via prompting and highlights its limits, then demonstrates constrained decoding for more efficient structured generation.\n",
    "\n",
    "We will use HuugingFace Inference Endpoints, and then applies a local pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zewmwdHZuv7y"
   },
   "source": [
    "## Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7511,
     "status": "ok",
     "timestamp": 1744509134637,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "BQ74kJs_tgiG",
    "outputId": "fa75b409-2063-4a35-e871-cd9b08b30229"
   },
   "outputs": [],
   "source": [
    "!pip install -qU pandas json huggingface_hub pydantic outlines accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2362,
     "status": "ok",
     "timestamp": 1744509137001,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "JeBV_vDBu4vB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1744509137019,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "hLLFlHYau_ht"
   },
   "outputs": [],
   "source": [
    "repo_id = 'meta-llama/Meta-LLama-3-8B-Instruct'\n",
    "\n",
    "llm_client = InferenceClient(model=repo_id, timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybYHEEdxvHkm"
   },
   "outputs": [],
   "source": [
    "# test\n",
    "llm_client.text_generation(prompt=\"How are you today?\", max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDZ6_1ukvNs8"
   },
   "source": [
    "## Prompting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2u6k-dVvPsq"
   },
   "source": [
    "To get structured outputs from our model, we can simply prompt a powerful enough models with appropriate guidelines. We also want the RAG model to generate not only an answer, but also a confidence score and some source snippets. We want to generate these a JSON dictionary to then easily parse it for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1WOfzWzvPEc"
   },
   "outputs": [],
   "source": [
    "RELEVANT_CONTEXT = \"\"\"\n",
    "Document:\n",
    "The weather is really nice in Paris today.\n",
    "To define a stop sequence in Transformers, you should pass the stop_sequence argument in your pipeline or model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07jwanWXvr8W"
   },
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE_JSON = \"\"\"\n",
    "Answer the user query based on the source documents.\n",
    "\n",
    "Here are the source documents: {context}\n",
    "\n",
    "\n",
    "You should provide your answer as a JSON blob, and also provide all relevant short source snippets from the documents on which you directly based your answer, and a confidence score as a float between 0 and 1.\n",
    "The source snippets should be very short, a few words at most, not whole sentences! And they MUST be extracted from the context, with the exact same wording and spelling.\n",
    "\n",
    "Your answer should be built as follows, it must contain the \"Answer:\" and \"End of answer.\" sequences.\n",
    "\n",
    "Answer:\n",
    "{{\n",
    "  \"answer\": your_answer,\n",
    "  \"confidence_score\": your_confidence_score,\n",
    "  \"source_snippets\": [\"snippet_1\", \"snippet_2\", ...]\n",
    "}}\n",
    "End of answer.\n",
    "\n",
    "Now begin!\n",
    "Here is the user question: {user_query}.\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBPacmfivu9J"
   },
   "outputs": [],
   "source": [
    "USER_QUERY = \"How can I define a stop sequence in Transformers?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4V_iy1YKvyju"
   },
   "outputs": [],
   "source": [
    "prompt = RAG_PROMPT_TEMPLATE_JSON.format(context=RELEVANT_CONTEXT, user_query=USER_QUERY)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEIcG00jv-JK"
   },
   "outputs": [],
   "source": [
    "answer = llm_client.text_generation(\n",
    "    prompt,\n",
    "    max_new_tokens=1000\n",
    ")\n",
    "\n",
    "answer = answer.split('End of answer.')[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dljl2Q_wHh4"
   },
   "source": [
    "The output of the LLM is a string representation of a dictionary, so we need to load it as a dictionary using `literal_eval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivSUiPQDwPmP"
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "parsed_answer = literal_eval(answer)\n",
    "parsed_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBYp4ssRwSkS"
   },
   "outputs": [],
   "source": [
    "def highlight(s):\n",
    "    return \"\\x1b[1;32m\" + s + \"\\x1b[0m\"\n",
    "\n",
    "\n",
    "def print_results(answer, source_text, highlight_snippets):\n",
    "    print(\"Answer:\", highlight(answer))\n",
    "    print('\\n\\n', '='*10 + ' Source documents ' + '='*10)\n",
    "\n",
    "    for snippet in highlight_snippets:\n",
    "        source_text = source_text.replace(snippet.strip(), highlight(snippet.strip()))\n",
    "    print(source_text)\n",
    "\n",
    "print_results(\n",
    "    answer=parsed_answer['answer'],\n",
    "    source_text=RELEVANT_CONTEXT,\n",
    "    highlight_snippets=parsed_answer['source_snippets']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pddxrDf0w_sg"
   },
   "source": [
    "We can also try a less powerful model and increase the temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4K7ZwKAxEak"
   },
   "outputs": [],
   "source": [
    "answer = llm_client.text_generation(\n",
    "    prompt,\n",
    "    max_new_tokens=250,\n",
    "    temperature=1.6,\n",
    "    return_full_text=False\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKHChnfVxNds"
   },
   "source": [
    "The output now is not valid JSON-format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ka6fRA0Cxc-U"
   },
   "source": [
    "## Constrained decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RS89sVOBxgVK"
   },
   "source": [
    "To force a JSON output, we will have to use **constrained decoding** where we force the LLM to only output tokens that conform to a set of rules called a **grammar**.\n",
    "\n",
    "The **grammar** can be defined using Pydantic models, JSON schema, or regular expressions. The model will then generate a response that conforms to the specified grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVPXk6ULxVhr"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, confloat, StringConstraints\n",
    "from typing import List, Annotated\n",
    "\n",
    "\n",
    "class AnswerWithSnippets(BaseModel):\n",
    "    answer: Annotated[\n",
    "        str,\n",
    "        StringConstraints(min_length=10, max_length=100)\n",
    "    ]\n",
    "\n",
    "    confidence: Annotated[\n",
    "        float,\n",
    "        confloat(ge=0.0, le=1.0)\n",
    "    ]\n",
    "\n",
    "    source_snippets: List[Annotated[str, StringConstraints(max_length=30)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbjhf3X_y1x-"
   },
   "source": [
    "Check if this schema correctly represents our requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ctpvCVNy7ax"
   },
   "outputs": [],
   "source": [
    "AnswerWithSnippets.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxJm3eafy9M2"
   },
   "source": [
    "Now we can use either the client's `text_generation` method or use its `post` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tGAU43hzC9P"
   },
   "outputs": [],
   "source": [
    "# Using `text_generation`\n",
    "answer = llm_client.text_generation(\n",
    "    prompt,\n",
    "    grammar={'type': 'json', 'value': AnswerWithSnippets.schema()},\n",
    "    max_new_tokens=250,\n",
    "    temperature=1.6,\n",
    "    return_full_text=False\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2pl6UnazV9i"
   },
   "outputs": [],
   "source": [
    "# Using post\n",
    "data = {\n",
    "    'inputs': prompt,\n",
    "    'parameters': {\n",
    "        'temperature': 1.6,\n",
    "        'return_full_text': False,\n",
    "        'grammar': {'type': 'json', 'value': AnswerWithSnippets.schema()},\n",
    "        'max_new_tokens': 250\n",
    "    }\n",
    "}\n",
    "answer = json.loads(llm_client.post(json=data))[0]['generated_text']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_I2w_nW1Gds"
   },
   "source": [
    "The generated output now has the correct JSON format with the exact keys and types we defined in our grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SWf230l1YBp"
   },
   "source": [
    "## Grammar on a local pipeline with Outlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkPIkDrd1gUe"
   },
   "source": [
    "[`outlines`](https://github.com/dottxt-ai/outlines) is the library that runs under the hood on our Inference API to constrain output generation.\n",
    "\n",
    "We can use it locally and it works by applying a bias on the logits to force selection of only the ones that conform to our constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZodToFb1blz"
   },
   "outputs": [],
   "source": [
    "import outlines\n",
    "\n",
    "repo_id = 'mustafaaljadery/gemma-2B-10M'\n",
    "\n",
    "model = outlines.models.transformers(repo_id)\n",
    "\n",
    "schema_as_str = json.dumps(AnswerWithSnippets.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wUOd1-M2DUM"
   },
   "outputs": [],
   "source": [
    "generator = outlines.generate.json(model, schema_as_str)\n",
    "result = generator(prompt)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNj49qvw764Ap1M3bEVE1uv",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
