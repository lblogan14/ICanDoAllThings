{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMbZ//vGm+Mph+TD8wXENtW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Latent Consistency Distillation"],"metadata":{"id":"NTeuOW_lD2JI"}},{"cell_type":"markdown","source":["**Latent Consistency Models (LCMs)** are able to generate high-quality images in just a few steps, representing a big leap forward because many pipelines require at least 25+ steps.\n","\n","LCMs are produced by applying the latent consistency disillation method to any Stable Diffusion model. This method works by applying *one-stage guided distillation* to the latent space, and incorporating a *skipping-step* method to consistently skip timesteps to accelerate the disillation process.\n","\n","We will explore the [`train_lcm_disill_sd_wds.py`](https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/train_lcm_distill_sd_wds.py)\n","\n","\n","As always, make sure to install the `diffusers` library from source:\n","```bash\n","git clone https://github.com/huggingface/diffusers\n","cd diffusers\n","pip install .\n","```\n","\n","We need to navigate to the following folder and install the corresponding dependencies:\n","```bash\n","cd examples/consistency_distillation\n","pip install -r requirements.txt\n","```\n","HuggingFace Accelerate is also helpful to train on multiple GPUs with mixed-precision.\n","```bash\n","pip install accelerate\n","```\n","\n","Now we can initialize a HuggingFace Accelerate environment\n","```bash\n","accelerate config\n","```\n","To set up a default Acclerate environment without choosing any configurations:\n","```bash\n","accelerate config default\n","```\n","Or if our environment does not support an interactive shell like a notebook, we can use:\n","```python\n","from accelerate.utils import write_basic_config\n","write_basic_config()\n","```"],"metadata":{"id":"veNdH-sgD5Ys"}},{"cell_type":"markdown","source":["## Script parameters"],"metadata":{"id":"catmmiXOjH-N"}},{"cell_type":"markdown","source":["The training script provides many parameters to customize the training run. We can find all of the parameters and their descriptions in the `parse_args()` function.\n","\n","Many parameters are described in the **Text-to-image** training guide, so we only focus on parameters relevant to LCD:\n","* `--pretrained_teacher_model`: the path to a pretrained latent diffusion model to use as the teacher model\n","* `--pretrained_vae_model_name_or_path`: path to a pretrained VAE; the SDXL VAE is known to suffer from numerical instability, so this parameter allows use to specify an alternative VAE\n","* `--w_min` and `--w_max`: the minimum and maximum guidance scale values for guidance scale sampling\n","* `--num_ddim_timesteps`: the number of teimsteps for DDIM sampling\n","* `--loss_type`: the type of loss (L2 or Huber) to calculate for latent consistency distillation; Huber loss is generally preferred because it's more robust to outliers\n","* `--huber_c`: the Huber loss parameter"],"metadata":{"id":"E8vlIbZhjNhb"}},{"cell_type":"markdown","source":["## Training script"],"metadata":{"id":"LNwRwPPCj5WK"}},{"cell_type":"markdown","source":["The training script starts by creating a dataset class `Text2ImageDataset` for preprocessing the images and creating a training dataset:\n","```python\n","        def transform(example):\n","            # resize image\n","            image = example[\"image\"]\n","            image = TF.resize(image, resolution, interpolation=transforms.InterpolationMode.BILINEAR)\n","\n","            # get crop coordinates and crop image\n","            c_top, c_left, _, _ = transforms.RandomCrop.get_params(image, output_size=(resolution, resolution))\n","            image = TF.crop(image, c_top, c_left, resolution, resolution)\n","            image = TF.to_tensor(image)\n","            image = TF.normalize(image, [0.5], [0.5])\n","\n","            example[\"image\"] = image\n","            return example\n","```\n","For improved performance on reading and writing large datasets stored in the cloud, the script uses the `WebDataset` format to create a preprocessing pipeline to apply transforms and create a dataset and dataloader for training. Images are processed and fed to the training loop without having to download the full dataset first:\n","```python\n","        processing_pipeline = [\n","            wds.decode(\"pil\", handler=wds.ignore_and_continue),\n","            wds.rename(image=\"jpg;png;jpeg;webp\", text=\"text;txt;caption\", handler=wds.warn_and_continue),\n","            wds.map(filter_keys({\"image\", \"text\"})),\n","            wds.map(transform),\n","            wds.to_tuple(\"image\", \"text\"),\n","        ]\n","\n","        # Create train dataset and loader\n","        pipeline = [\n","            wds.ResampledShards(train_shards_path_or_url),\n","            tarfile_to_samples_nothrow,\n","            wds.shuffle(shuffle_buffer_size),\n","            *processing_pipeline,\n","            wds.batched(per_gpu_batch_size, partial=False, collation_fn=default_collate),\n","        ]\n","```\n","\n","In the `main()` function, all the necessary components like the noise scheduler, tokenizers, text encoders, and VAE are loaded. The teacher UNet is also loaded here and then we can create a student UNet from the teacher UNet. The student UNet is updated by the optimizer during training.\n","```python\n","    # 5. Load teacher U-Net from SD-XL checkpoint\n","    teacher_unet = UNet2DConditionModel.from_pretrained(\n","        args.pretrained_teacher_model, subfolder=\"unet\", revision=args.teacher_revision\n","    )\n","\n","    # 6. Freeze teacher vae, text_encoder, and teacher_unet\n","    vae.requires_grad_(False)\n","    text_encoder.requires_grad_(False)\n","    teacher_unet.requires_grad_(False)\n","\n","    # 8. Create online (`unet`) student U-Nets. This will be updated by the optimizer (e.g. via backpropagation.)\n","    # Add `time_cond_proj_dim` to the student U-Net if `teacher_unet.config.time_cond_proj_dim` is None\n","    if teacher_unet.config.time_cond_proj_dim is None:\n","        teacher_unet.config[\"time_cond_proj_dim\"] = args.unet_time_cond_proj_dim\n","    unet = UNet2DConditionModel(**teacher_unet.config)\n","    # load teacher_unet weights into unet\n","    unet.load_state_dict(teacher_unet.state_dict(), strict=False)\n","    unet.train()\n","```\n","Then we can create the optimizer to update the UNet parameters:\n","```python\n","    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n","    if args.use_8bit_adam:\n","        try:\n","            import bitsandbytes as bnb\n","        except ImportError:\n","            raise ImportError(\n","                \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\n","            )\n","\n","        optimizer_class = bnb.optim.AdamW8bit\n","    else:\n","        optimizer_class = torch.optim.AdamW\n","\n","    # 12. Optimizer creation\n","    optimizer = optimizer_class(\n","        unet.parameters(),\n","        lr=args.learning_rate,\n","        betas=(args.adam_beta1, args.adam_beta2),\n","        weight_decay=args.adam_weight_decay,\n","        eps=args.adam_epsilon,\n","    )\n","```\n","Next, we create the dataset:\n","```python\n","    dataset = Text2ImageDataset(\n","        train_shards_path_or_url=args.train_shards_path_or_url,\n","        num_train_examples=args.max_train_samples,\n","        per_gpu_batch_size=args.train_batch_size,\n","        global_batch_size=args.train_batch_size * accelerator.num_processes,\n","        num_workers=args.dataloader_num_workers,\n","        resolution=args.resolution,\n","        shuffle_buffer_size=1000,\n","        pin_memory=True,\n","        persistent_workers=True,\n","    )\n","    train_dataloader = dataset.train_dataloader\n","```\n","Next, we are ready to set up the training loop and implement the latent consistency distillation method. This section takes care of adding noise to the latents, sampling and creating a guidance scale embedding, and predicting the original image from the noise.\n","\n","The training loop gets the teacher model predictions and the LCM predictions, calculates the loss, and then backpropagates it to the LCM."],"metadata":{"id":"RUXlRPFcj_vD"}},{"cell_type":"markdown","source":["## Launch the script"],"metadata":{"id":"OKd01YJtpMe5"}},{"cell_type":"markdown","source":["We will use the `--train_shards_path_or_url` to specify the path to the **Conceptual Captions 12M** dataset stored on the Hub.\n","```bash\n","export MODEL_DIR=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n","export OUTPUT_DIR=\"path/to/saved/model\"\n","\n","accelerate launch train_lcm_distill_sd_wds.py \\\n","    --pretrained_teacher_model=$MODEL_DIR \\\n","    --output_dir=$OUTPUT_DIR \\\n","    --mixed_precision=fp16 \\\n","    --resolution=512 \\\n","    --learning_rate=1e-6 --loss_type=\"huber\" --ema_decay=0.95 --adam_weight_decay=0.0 \\\n","    --max_train_steps=1000 \\\n","    --max_train_samples=4000000 \\\n","    --dataloader_num_workers=8 \\\n","    --train_shards_path_or_url=\"pipe:curl -L -s https://huggingface.co/datasets/laion/conceptual-captions-12m-webdataset/resolve/main/data/{00000..01099}.tar?download=true\" \\\n","    --validation_steps=200 \\\n","    --checkpointing_steps=200 --checkpoints_total_limit=10 \\\n","    --train_batch_size=12 \\\n","    --gradient_checkpointing --enable_xformers_memory_efficient_attention \\\n","    --gradient_accumulation_steps=1 \\\n","    --use_8bit_adam \\\n","    --resume_from_checkpoint=latest \\\n","    --report_to=wandb \\\n","    --seed=453645634 \\\n","    --push_to_hub\n","```"],"metadata":{"id":"sqGC6uHApPZW"}},{"cell_type":"markdown","source":["Once training is complete,"],"metadata":{"id":"I8cAg4ripbrq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jiLpmS9tDyBA"},"outputs":[],"source":["from diffusers import UNet2DConditionModel, DiffusionPipeline, LCMScheduler\n","import torch\n","\n","unet = UNet2DConditionModel.from_pretrained(\n","    'our-username/our-model',\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",")\n","\n","pipeline = DiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    unet=unet,\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",")\n","pipeline.scheduler = LCMScheduler.from_config(pipeline.scheduler.config)\n","pipeline.to('cuda')"]},{"cell_type":"code","source":["prompt = \"sushi rolls in the form of panda heads, sushi platter\"\n","\n","image = pipeline(\n","    prompt,\n","    num_inference_steps=4,\n","    guidance_scale=1.0\n",").images[0]\n","image"],"metadata":{"id":"EbnQOxmRp34q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LoRA"],"metadata":{"id":"FfKk5azWp8WA"}},{"cell_type":"markdown","source":["We can use [`train_lcm_distill_lora_sd_wds.py`](https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/train_lcm_distill_lora_sd_wds.py) or [`train_lcm_distill_lora_sdxl_wds.py`](https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/train_lcm_distill_lora_sdxl_wds.py) scripts to train SD or SDXL with LoRA, respectively."],"metadata":{"id":"aRnyVZXnp_l0"}},{"cell_type":"markdown","source":["## SDXL"],"metadata":{"id":"aEB6NSvsqUHw"}},{"cell_type":"markdown","source":["We can use [`train_lcm_distill_sdxl_wds.py`](https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/train_lcm_distill_sdxl_wds.py) script to train a SDXL model with LCD."],"metadata":{"id":"T1TfIzbsqXAF"}},{"cell_type":"code","source":[],"metadata":{"id":"Ydj9mts-p-Zp"},"execution_count":null,"outputs":[]}]}