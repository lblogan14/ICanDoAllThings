{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO1rITkR/TRyDKJ0v8a7cku"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Use tokenizers from HuggingFace Tokenizers"],"metadata":{"id":"PuRSkczf9xPj"}},{"cell_type":"markdown","source":["The `PretrainedTokenizerFast` depends on the HuggingFace Tokenizers library.\n","\n","Dummy example:\n","```python\n","from tokenizers import Tokenizer\n","from tokenizers.models import BPE\n","from tokenizers.trainers import BpeTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","\n","tokenizer = Tokenizer(\n","    BPE(unk_token='[UNK]')\n",")\n","\n","trainer = BpeTrainer(\n","    special_tokens=[\n","        '[UNK]', '[CLS]', '[SEP]', '[PAD]', '[MASK]'\n","    ]\n",")\n","\n","tokenizer.pre_tokenizer = Whitespace()\n","\n","files = [...]\n","tokenizer.train(files, trainer)\n","```"],"metadata":{"id":"fWVKdgTZ_CB9"}},{"cell_type":"markdown","source":["## Loading directly from the tokenizer object"],"metadata":{"id":"YLmfvoJV_szs"}},{"cell_type":"code","source":["from tokenizers import Tokenizer\n","from tokenizers.models import BPE\n","from tokenizers.trainers import BpeTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","\n","tokenizer = Tokenizer(\n","    BPE(unk_token='[UNK]')\n",")\n","\n","trainer = BpeTrainer(\n","    special_tokens=[\n","        '[UNK]', '[CLS]', '[SEP]', '[PAD]', '[MASK]'\n","    ]\n",")\n","\n","tokenizer.pre_tokenizer = Whitespace()"],"metadata":{"id":"_1BY-yt0_6aJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-KEYI169oZP"},"outputs":[],"source":["from transformers import PretrainedTokenizerFast\n","\n","fast_tokenizer = PretrainedTokenizerFast(tokenizer_object=tokenizer)"]},{"cell_type":"markdown","source":["This object can now be used with all the methods shared by the HuggingFace Transformers tokenizers."],"metadata":{"id":"m9g6vJb__-01"}},{"cell_type":"markdown","source":["## Loading from a JSON file"],"metadata":{"id":"tRXf6x8kAFwz"}},{"cell_type":"code","source":["tokenizer.save('tokenizer.json')"],"metadata":{"id":"m6IBbVS2ADio"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The path to which we saed this file can be passed to the `PreTrainedTokenizerFast` initialization method using the `tokenizer_file` parameter:"],"metadata":{"id":"beJPWdowAJ9s"}},{"cell_type":"code","source":["from transformers import PretrainedTokenizerFast\n","\n","fast_tokenizer = PretrainedTokenizerFast(tokenizer_file='tokenizer.json')"],"metadata":{"id":"rLg9gHE1AQH-"},"execution_count":null,"outputs":[]}]}