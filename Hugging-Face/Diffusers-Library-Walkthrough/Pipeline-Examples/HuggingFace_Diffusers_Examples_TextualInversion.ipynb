{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMpi6qHA2ECCxT327pEYtRG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -qU diffusers transformers accelerate huggingface_hub safetensors"],"metadata":{"id":"ZoKlxQFU84Qk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Textual Inversion"],"metadata":{"id":"fvWuB4cK5RRJ"}},{"cell_type":"markdown","source":["Textual inversion enables a model like Stable Diffusion to learn a new concept from just a few sample images. This gives us more control over the generated images and allow us to tailor the model towards specific concepts."],"metadata":{"id":"MpYFpBOO7Rg4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"577qwLph5DrB"},"outputs":[],"source":["from diffusers import StableDiffusionPipeline\n","from diffusers.utils import make_image_grid"]},{"cell_type":"markdown","source":["## Stable Diffusion 1 and 2"],"metadata":{"id":"bRr-YpAa7qTl"}},{"cell_type":"markdown","source":["We need a pre-learned concept from **Stable Diffusion Conceptualizer**"],"metadata":{"id":"fgtuSsN771I-"}},{"cell_type":"code","source":["pretrained_model_name_or_path = 'stable-diffusion-v1-5/stable-diffusion-v1-5'\n","repo_id_embeds = 'sd-concepts-library/cat-toy'\n","\n","pipeline = StableDiffusionPipeline.from_pretrained(\n","    pretrained_model_name_or_path,\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",").to('cuda')\n","\n","pipeline.load_textual_inversion(repo_id_embeds)"],"metadata":{"id":"IlWj15fM7sU0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create a prompt with the pre-learned concept by using the special placeholder token `<cat-toy>`, and choose the number of samples and rows of images we would like to generate:"],"metadata":{"id":"rDV9cH-R8Igf"}},{"cell_type":"code","source":["prompt = ' a grafitti in a favela wall with a <cat-toy> on it'\n","num_samples_per_row = 2\n","num_rows = 2\n","\n","all_images = []\n","for _ in range(num_rows):\n","    images = pipeline(\n","        prompt,\n","        num_images_per_prompt=num_samples_per_row,\n","        num_inference_steps=50,\n","        guidance_scale=7.5\n","    ).images\n","    all_images.extend(images)\n","\n","make_image_grid(all_images, rows=num_rows, cols=num_samples_per_row)"],"metadata":{"id":"e7KiFKrm8Q1q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Stable Diffusion XL"],"metadata":{"id":"qkA0ZKqi8sK0"}},{"cell_type":"markdown","source":["SDXL has two text encoders so we will need two textual inversion embeddings."],"metadata":{"id":"uuTet_vD8vG-"}},{"cell_type":"code","source":["from huggingface_hub import hf_hub_download\n","from safetensors.torch import load_file\n","\n","f = hf_hub_download('dn118/unaestheticXL', filename='unaestheticXLv31.safetensors')\n","state_dict = load_file(f)\n","state_dict"],"metadata":{"id":"ZoRKK2EE8tqt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There are two tensors, `'clip_g'` corresponds to the bigger text encoder in SDXL and refers to `pipe.text_encoder_2`, and `'clip_l'` refers to `pipe.text_encoder`."],"metadata":{"id":"ESZTNqVQ9KNo"}},{"cell_type":"code","source":["from diffusers import AutoPipelineForText2Image\n","import torch\n","\n","pipe = AutoPipelineForText2Image.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    torch_dtype=torch.float16,\n","    variant='fp16',\n",").to('cuda')\n","\n","pipe.load_textual_inversion(\n","    state_dict['clip_g'],\n","    token='unaestheticXLv31',\n","    text_encoder=pipe.text_encoder_2,\n","    tokenizer=pipe.tokenizer_2\n",")\n","pipe.load_textual_inversion(\n","    state_dict['clip_l'],\n","    token='unaestheticXLv31',\n","    text_encoder=pipe.text_encoder,\n","    tokenizer=pipe.tokenizer\n",")"],"metadata":{"id":"zW6pzIyJ9YXv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"a woman standing in front of a mountain\"\n","generator = torch.Generator('cuda').manual_seed(111)\n","\n","# the embedding should be used a a negative embedding\n","image = pipe(\n","    prompt,\n","    negative_prompt='unaestheticXLv31',\n","    generator=generator\n",").images[0]\n","image"],"metadata":{"id":"IhSe_gXA91-2"},"execution_count":null,"outputs":[]}]}