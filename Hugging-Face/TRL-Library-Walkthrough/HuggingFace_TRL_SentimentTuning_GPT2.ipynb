{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPPWY09VIJzvEtoa/1ZGQ4r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Tune GPT2 to Generate Positive Reviews"],"metadata":{"id":"eGppckGFMnQ8"}},{"cell_type":"markdown","source":["In this example, we will fine-tune GPT2 (small) to generate positive movie reviews based on the IMDB dataset. The model gets the start of the real review and is tasked to produce positive continuations.\n","\n","To reward positive continuations we use a BERT classifier to analyze the sentiment of the produced sentences and use the classifier's outputs as reward signals for PPO training."],"metadata":{"id":"izVSA4g8RwMX"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"N8o97cmbSE8z"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UVo39X1cL6OV"},"outputs":[],"source":["!pip install -qU transformers trl wandb"]},{"cell_type":"markdown","source":["## Configuration"],"metadata":{"id":"WRx5W62VSLQ9"}},{"cell_type":"code","source":["import torch\n","from tqdm import tqdm\n","import pandas as pd\n","import wandb\n","\n","from transformers import pipeline, AutoTokenizer\n","from datasets import load_dataset\n","\n","from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n","from trl.core import LengthSampler\n","\n","tqdm.pandas()\n","wandb.init()"],"metadata":{"id":"aL454_S3SMFa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config = PPOConfig(\n","    model_name='lvwerra/gpt2-imdb',\n","    learning_rate=1.41e-5,\n","    log_with='wandb'\n",")\n","\n","sent_kwargs = {\n","    'top_k': None,\n","    'function_to_apply': 'none',\n","    'batch_size': 16\n","}"],"metadata":{"id":"MXcEgTM3Sc_C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `gpt2-imdb` model will be additionally fine-tuned on the IMDB dataset for 1 epoch with the HuggingFace script. Parameters are mostly taken from the original paper [*Fine-Tuning Language Models from Human Preferences*](https://huggingface.co/papers/1909.08593)."],"metadata":{"id":"Lu7XnWb3SvrX"}},{"cell_type":"markdown","source":["## Load data and models"],"metadata":{"id":"CaFCQrVsTHL-"}},{"cell_type":"markdown","source":["### Load IMDB dataset"],"metadata":{"id":"doUysdI6TKrG"}},{"cell_type":"markdown","source":["We will load the IMDB dataset into a DataFrame and filter for comments that are at least 200 characters. Then we tokenize each text and cut it to random size with `LengthSampler`."],"metadata":{"id":"C33-0aIlTNxg"}},{"cell_type":"code","source":["def build_dataset(config, dataset_name='stanfordnlp/imdb', input_min_text_length=2, input_max_text_length=8):\n","    \"\"\"Build dataset for training.\n","\n","    This builds the dataset from `load_dataset`, one should\n","    customize this function to train the model on its own dataset.\n","\n","    Parameters\n","    ----------\n","    dataset_name: str\n","        Name of the dataset to be loaded\n","    input_min_text_length: int, optional, defaults to 2\n","        Determines the minimum length of the text in tokens\n","    input_max_text_length: int, optional, defaults to 8\n","        Determines the maximum length of the text in tokens\n","\n","    Returns\n","    -------\n","    torch.utils.data.DataLoader\n","        The dataloader for the dataset.\n","    \"\"\"\n","    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","    # load imdb\n","    ds = load_dataset(dataset_name, split='train')\n","    sd = ds.rename_columns({'text': 'review'})\n","    ds = ds.filter(lambda x: len(x['review']) > 200, batched=False)\n","\n","    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n","\n","    def tokenize(sample):\n","        sample['input_ids'] = tokenizer.encode(sample['review'])[:input_size()]\n","        sample['query'] = tokenizer.decode(sample['input_ids'])\n","        return sample\n","\n","    ds = ds.map(tokenize, batched=False)\n","    ds.set_format(type='torch')\n","\n","    return ds\n","\n","\n","def collator(data):\n","    return dict(\n","        (key, [d[key] for d in data]) for key in data[0]\n","    )"],"metadata":{"id":"S78mfR3rTGaf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = build_dataset(config)\n","dataset"],"metadata":{"id":"YnYhUQ6DDteY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load pretrained GPT2 language models"],"metadata":{"id":"0kYG877rD5fG"}},{"cell_type":"markdown","source":["We will load the GPT2 model with a value head and the tokenizer.\n","\n","We will load the model twice; the first model will be optimized while the second model serves as a reference to calculate the KL-divergence from the starting point. This serves as an additional reward signal in the PPO training to make sure the optimized model does not deviate too much from the original language model."],"metadata":{"id":"qWD7aYqWD8ww"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# model to fine-tune\n","model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n","# reference model\n","ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)"],"metadata":{"id":"VNFOKiB5D8Hq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Initialize `PPOTrainer`"],"metadata":{"id":"X2n6neW7EZ51"}},{"cell_type":"code","source":["trainer = PPOTrainer(\n","    config,\n","    model,\n","    ref_model=ref_model,\n","    tokenizer=tokenizer,\n","    train_dataset=dataset,\n","    data_collator=collator\n",")"],"metadata":{"id":"HgnyGm94EcGs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load BERT classifier"],"metadata":{"id":"aplhxTfiEzVa"}},{"cell_type":"markdown","source":["We load a BERT classifier fine-tuned on the IMDB dataset."],"metadata":{"id":"y-LHYixtE1rW"}},{"cell_type":"code","source":["device = trainer.accelerator.device\n","if trainer.accelerator.num_processes == 1:\n","    device = 0 if torch.cuda.is_available() else 'cpu' # to avoid a `pipeline` ug\n","\n","setiment_pipe = pipeline(\n","    'sentiment-analysis',\n","    model='lvwerra/distilbert-imdb',\n","    device=device\n",")"],"metadata":{"id":"lSDdFcuOE1Jv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model outputs are the logits for the negative and positive class. We will use the logits for positive class as a reward signal for the language model."],"metadata":{"id":"MJulWBJWFJEo"}},{"cell_type":"code","source":["text = 'this movie was really bad!!'\n","sentiment_pipe(text, **sent_kwargs) # `sent_kwargs` defined at the beginning"],"metadata":{"id":"g5eHCpLMFRE8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = 'this movie was really good!!'\n","sentiment_pipe(text, **sent_kwargs)"],"metadata":{"id":"YMvV4JnrFjRq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generation settings"],"metadata":{"id":"DYKC2echFm2i"}},{"cell_type":"markdown","source":["We use sampling and make sure top-k and nucleus sampling are turned off as well as a minimal length for the response generation."],"metadata":{"id":"Y9lf0FG8Fpdr"}},{"cell_type":"code","source":["# generation settings\n","gen_kwargs = {\n","    'min_length': -1,\n","    'top_k': 0.,\n","    'top_p': 1.,\n","    'do_sample': True,\n","    'pad_token_id': tokenizer.eos_token_id\n","}"],"metadata":{"id":"liegyVjPFoBw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Optimize model"],"metadata":{"id":"m_TXgsi8GCXM"}},{"cell_type":"markdown","source":["### Training loop"],"metadata":{"id":"-jVit5ikGENw"}},{"cell_type":"markdown","source":["The training loop consists of\n","1. Get the query responses from the policy network (GPT-2)\n","2. Get sentiments from query/response from BERT\n","3. Optimize policy with PPO using the (query, response, reward) triplet"],"metadata":{"id":"pWiIE7UXGFdh"}},{"cell_type":"code","source":["output_min_length = 4\n","output_max_length = 16\n","output_length_sampler = LengthSampler(output_min_length, output_max_length)\n","\n","\n","for epoch, batch in enumerate(tqdm(trainer.dataloader)):\n","    query_tensors = batch['input_ids']\n","\n","    # Get response from GPT2\n","    response_tensors = []\n","    for query in query_tensors:\n","        gen_len = output_length_sampler()\n","        gen_kwargs['max_new_tokens'] = gen_len\n","        query_response = trainer.generate(query, **gen_kwargs).squeeze()\n","        response_len = len(query_response) - len(query)\n","        response_tensors.append(query_response[-response_len:])\n","\n","    batch['response'] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n","\n","    # Compute sentiment score\n","    texts = [q + r for q, r in zip(batch['query'], batch['response'])]\n","    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n","    positive_scores = [\n","        item['score'] for output in pipe_outputs\n","        for item in output\n","        if item['label'] == 'POSITIVE'\n","    ]\n","    rewards = [torch.tensor(score) for score in positive_scores]\n","\n","    # Run PPO step\n","    stats = trainer.step(query_tensors, response_tensors, rewards)\n","    trainer.log_stats(stats, batch, rewards)"],"metadata":{"id":"Fi_0nJPXGDZH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model inspection"],"metadata":{"id":"eQRsdJW3Hv6K"}},{"cell_type":"markdown","source":["Now we can use `ref_model` to compare the tuned model `model`."],"metadata":{"id":"mf_OtePAHyfP"}},{"cell_type":"code","source":["# get a batch from the dataset\n","batch_size = 16\n","game_data = dict()\n","dataset.set_format('pandas')\n","df_batch = dataset[:].sample(batch_size)\n","game_data['query'] = df_batch['query'].tolist()\n","query_tensors = df_batch['input_ids'].tolist()\n","\n","response_tensors_ref, response_tensors = [], []"],"metadata":{"id":"AesGn9iiHxTe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get response from model and ref_model\n","for i in range(batch_size):\n","    query = torch.tensor(query_tensors[i]).to(device)\n","\n","    gen_len = output_length_sampler()\n","    # ref_model response\n","    query_response = ref_model.generate(\n","        query.unsqueeze(dim=0),\n","        max_new_tokens=gen_len,\n","        **gen_kwargs\n","    ).squeeze()\n","    response_len = len(query_response) - len(query)\n","    response_tensors_ref.append(query_response[-response_len:])\n","    # model response\n","    query_response = model.generate(\n","        query.unsqueeze(dim=0),\n","        max_new_tokens=gen_len,\n","        **gen_kwargs\n","    ).squeeze()\n","    response_len = len(query_response) - len(query)\n","    response_tensors.append(query_response[-response_len:])\n","\n","\n","# decode responses\n","game_data['response (before)'] = [\n","    tokenizer.decode(response_tensors_ref[i]) for i in range(batch_size)\n","]\n","game_data['response (after)'] = [\n","    tokenizer.decode(response_tensors[i]) for i in range(batch_size)\n","]\n","\n","\n","# sentiment analysis of query/response pairs before/after\n","texts = [\n","    q + r for q, r in zip(game_data['query'], game_data['response (before)'])\n","]\n","pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n","positive_scores = [\n","    item['score'] for output in pipe_outputs\n","    for item in output\n","    if item['label'] == 'POSITIVE'\n","]\n","game_data['rewards (before)'] = positive_scores\n","\n","texts = [\n","    q + r for q, r in zip(game_data['query'], game_data['response (after)'])\n","]\n","pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n","positive_scores = [\n","    item['score'] for output in pipe_outputs\n","    for item in output\n","    if item['label'] == 'POSITIVE'\n","]\n","game_data['rewards (after)'] = positive_scores\n","\n","# store results in a dataframe\n","df_results = pd.DataFrame(game_data)\n","df_results"],"metadata":{"id":"3iJaK6OoJhIV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The rewards after the training definitely increased."],"metadata":{"id":"5z292LA1MN2Y"}},{"cell_type":"code","source":["print('Mean:')\n","display(df_results[['rewards (before)', 'rewards (after)']].mean())\n","print()\n","print('Median:')\n","display(df_results[['rewards (before)', 'rewards (after)']].median())"],"metadata":{"id":"cLTrPXwwMTR4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Save models"],"metadata":{"id":"D4ddd9PlMeVW"}},{"cell_type":"code","source":["model.save_pretrained('gpt2-imdb-pos-v2', push_to_hub=False)\n","tokenizer.save_pretrained('gpt2-imdb-pos-v2', push_to_hub=False)"],"metadata":{"id":"xEMSOTEXMfgo"},"execution_count":null,"outputs":[]}]}