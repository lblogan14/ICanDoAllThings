{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPeYFEXKMUrsufQRG/Lfi5c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Multimodal Retrieval-Augmented Generation (RAG) with Document Retrieval (ColPali) and Vision Language Models (VLMs)"],"metadata":{"id":"VHEaoaAEqNsP"}},{"cell_type":"markdown","source":["In this example, we will build a **Multimodal Retrieval-Augmented Generation (RAG)** system by combining the [**ColPali**](https://huggingface.co/blog/manu/colpali) retriever for document retrieval with the [**Qwen2-VL**](https://qwenlm.github.io/blog/qwen2-vl/) Vision Language Model (VLM). This RAG system is capable of enhancing query responses with both text-based documents and visual data.\n","\n","Instead of relying on a complex document processor pipeline that extracts data through OCR, we will leverage a Document Rerieval Model to efficiently retrieve the relevant documents based on a specific user query."],"metadata":{"id":"O2cYrqUwqUz_"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"mFMZgkjhtZYW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBLM8oI8qEeD"},"outputs":[],"source":["!pip install -U -q byaldi pdf2image qwen-vl-utils transformers\n","# Tested with byaldi==0.0.4, pdf2image==1.17.0, qwen-vl-utils==0.0.8, transformers==4.45.0"]},{"cell_type":"markdown","source":["We also need to install `poppler-utils` to facilitate PDF manipulation. This utility provides essential tools for working with PDF files, ensuring we can efficiently hadnle any document-related tasks in our project."],"metadata":{"id":"jN5bbknGtdFA"}},{"cell_type":"code","source":["!sudo apt-get install -y poppler-utils"],"metadata":{"id":"ZtoXLEnntowz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load dataset"],"metadata":{"id":"8TlFeMnBtrSN"}},{"cell_type":"markdown","source":["In this section, we will utilize IKEA assembly instructions as our dataset. These PDFs contain step-by-step guidance for assembling various furniture pieces."],"metadata":{"id":"kVGF-ZZRtx_J"}},{"cell_type":"code","source":["import requests\n","import os\n","\n","pdfs = {\n","    \"MALM\": \"https://www.ikea.com/us/en/assembly_instructions/malm-4-drawer-chest-white__AA-2398381-2-100.pdf\",\n","    \"BILLY\": \"https://www.ikea.com/us/en/assembly_instructions/billy-bookcase-white__AA-1844854-6-2.pdf\",\n","    \"BOAXEL\": \"https://www.ikea.com/us/en/assembly_instructions/boaxel-wall-upright-white__AA-2341341-2-100.pdf\",\n","    \"ADILS\": \"https://www.ikea.com/us/en/assembly_instructions/adils-leg-white__AA-844478-6-2.pdf\",\n","    \"MICKE\": \"https://www.ikea.com/us/en/assembly_instructions/micke-desk-white__AA-476626-10-100.pdf\",\n","}\n","\n","output_dir = 'data'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","for name, url in pdfs.items():\n","    response = requests.get(url)\n","    pdf_path = os.path.join(output_dir, f'{name}.pdf')\n","\n","    with open(pdf_path, 'wb') as f:\n","        f.write(response.content)\n","\n","    print(f\"Downloaded {name} to {pdf_path}\")\n","\n","print('Downloaded files:', os.listdir(output_dir))"],"metadata":{"id":"rg1vgOFzts73"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After downloading the assembly instructions, we will convert the PDFs into images. This is required as it allows the document retrieval model (ColPali) to process and manipulate the visual content effectively."],"metadata":{"id":"xpHkHyPqvfW3"}},{"cell_type":"code","source":["import os\n","from pdf2image import convert_from_path\n","\n","\n","def convert_pdfs_to_images(pdf_folder):\n","    pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n","    all_images = {}\n","\n","    for doc_id, pdf_file in enumerate(pdf_files):\n","        pdf_path = os.path.join(pdf_folder, pdf_file)\n","        images = convert_from_path(pdf_path)\n","        all_images[doc_id] = images\n","\n","    return all_images\n","\n","\n","all_images = convert_pdfs_to_images('/content/data/')"],"metadata":{"id":"XlDwU6qLvqmL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can visualize a sample assembly guide to see how these instructions are presented. This will help us understand the format and layout of the content."],"metadata":{"id":"pIInFmb31Qov"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","fig, axes = plt.subplots(1, 8, figsize=(15, 10))\n","\n","for i, ax in enumerate(axes.flat):\n","    img = all_images[0][i]\n","    ax.imshow(img)\n","    ax.axis('off')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"Yc5WYmD41YX0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Initialize the ColPali multimodal document retrieval model"],"metadata":{"id":"dq2rjyYZ1l3_"}},{"cell_type":"markdown","source":["Now that our dataset is ready, we will initialize the Document Retrieval Model, which will be responsible for extracting relevant information from the raw images and providing us with the appropriate documents based on our queries.\n","\n","For this task, we will use the [`Byaldi`](https://github.com/AnswerDotAI/byaldi) model, a simple wrapper around the ColPali repository to make it easy to use late-interaction multi-modal models such as ColPali with a familiar API.\n","\n","Top-performing retreivers can be found in [ViDore (Visual Document Retrieval Benchmark)](https://huggingface.co/spaces/vidore/vidore-leaderboard)."],"metadata":{"id":"Sqjl8icw_AzH"}},{"cell_type":"code","source":["from byaldi import RAGMultiModalModel\n","\n","docs_retrieval_model = RAGMultiModalModel.from_pretrained('vidore/colpali-v1.2')"],"metadata":{"id":"UogEJBru1pf_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we can directly index our documents using the document retrieval model by specifying the folder where the PDFs are stored. This will allow the model to process and organize the documents for efficient retrieval based on our queries."],"metadata":{"id":"PV4q8WhmAfAE"}},{"cell_type":"code","source":["docs_retrieval_model.index(\n","    input_path='data/',\n","    index_name='image_index',\n","    store_collection_with_index=False,\n","    overwrite=True\n",")"],"metadata":{"id":"w0s7DNrLBKIq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Retrieving documents with the document retrieval model"],"metadata":{"id":"5y8rGOuhEYqT"}},{"cell_type":"code","source":["text_query = \"How many people are needed to assemble the Malm?\"\n","\n","results = docs_retrieval_model.search(text_query, k=3)\n","results"],"metadata":{"id":"wWa6_q_XEflO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_grouped_images(results, all_images):\n","    grouped_images = []\n","\n","    for result in results:\n","        doc_id = result['doc_id']\n","        page_num = result['page_num']\n","        grouped_images.append(\n","            all_images[doc_id][page_num - 1]\n","        ) # page_num is 1-indexed, while doc_id is 0-indexed\n","\n","    return grouped_images"],"metadata":{"id":"cQT4Gop_Ejdw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["grouped_images = get_grouped_images(results, all_images)"],"metadata":{"id":"4T2MGrYbE3d1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, axes = plt.subplots(1, 3, figsize=(15, 10))\n","\n","for i, ax in enumerate(axes.flat):\n","    img = grouped_images[i]\n","    ax.imshow(img)\n","    ax.axis('off')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"VE_LDDbDE6-0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Initialize the vision language model for question answering"],"metadata":{"id":"_G2xtCXUE_hJ"}},{"cell_type":"code","source":["from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n","from qwen_vl_utils import process_vision_info\n","import torch\n","\n","vl_model = Qwen2VLForConditionalGeneration.from_pretrained(\n","    'Qwen/Qwen2-VL-7B-Instruct',\n","    torch_dtype=torch.bfloat16,\n",")\n","vl_model.cuda().eval()"],"metadata":{"id":"Ma_qdZzeFCWz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["min_pixels = 224 * 224\n","max_pixels = 1024 * 1024\n","\n","vl_model_processor = Qwen2VLProcessor.from_pretrained(\n","    'Qwen/Qwen2-VL-7B-Instruct',\n","    min_pixels=min_pixels,\n","    max_pixels=max_pixels,\n",")"],"metadata":{"id":"57xM_UeoYm16"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Assembling the VLM model and testing the system"],"metadata":{"id":"aQvSdsZIYttT"}},{"cell_type":"markdown","source":["We will create the chat structure by providing the system with the three retrieved images along with the user query."],"metadata":{"id":"CkSRVAgHYx31"}},{"cell_type":"code","source":["chat_template = [\n","    {\n","        'role': 'user',\n","        'content': [\n","            {\n","                'type': 'image',\n","                'image': grouped_images[0],\n","            },\n","            {\n","                'type': 'image',\n","                'image': grouped_images[1],\n","            },\n","            {\n","                'type': 'image',\n","                'image': grouped_images[2],\n","            },\n","            {\n","                'type': 'text',\n","                'text': text_query,\n","            }\n","        ]\n","    }\n","]"],"metadata":{"id":"YAlQImr7YxFw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can apply to the processor:"],"metadata":{"id":"8halG_YDZH-0"}},{"cell_type":"code","source":["text = vl_model_processor.apply_chat_template(\n","    chat_template,\n","    tokenize=False,\n","    add_generation_prompt=True\n",")"],"metadata":{"id":"R3LNIjw7ZJ_H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text"],"metadata":{"id":"t_vqrHXyZPR2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we will process the inputs to ensure that they are properly formatted and ready to be used as input for the VLM."],"metadata":{"id":"NCk0PNfrZQ2l"}},{"cell_type":"code","source":["image_inputs, _ = process_vision_info(chat_template)\n","inputs = vl_model_processor(\n","    text=[text],\n","    images=image_inputs,\n","    padding=True,\n","    return_tensors='pt'\n",")\n","inputs = inputs.to('cuda')"],"metadata":{"id":"qFmIoIFZZVrL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we are ready to generate the answer."],"metadata":{"id":"th-RUoYrZi-3"}},{"cell_type":"code","source":["generated_ids = vl_model.generate(**inputs, max_new_tokens=500)\n","\n","# post-process\n","generated_ids_trimmed = [\n","    out_ids[len(in_ids) :]\n","    for in_ids, out_ids in zip(inputs.inputs_ids, generated_ids)\n","]\n","# decoding\n","output_text = vl_model_processor.batch_decode(\n","    generated_ids_trimed,\n","    skip_special_tokens=True,\n","    clean_up_tokenization_spaces=False\n",")"],"metadata":{"id":"2AtU1YZXZl0H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(output_text[0])"],"metadata":{"id":"AvYuBtI4Z7LW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Assembling everything"],"metadata":{"id":"GZrpqxM-Z9og"}},{"cell_type":"markdown","source":["We will create a method that encompasses the entire pipeline, allowing us to easily reuse it in future applications."],"metadata":{"id":"0nSBsD6qZ_rS"}},{"cell_type":"code","source":["def answer_with_multimodal_rag(\n","        vl_model,\n","        vl_model_processor,\n","        docs_retrieval_model,\n","        text_query,\n","        all_images,\n","        top_k,\n","        max_new_tokens\n","):\n","    # Retrieve documents\n","    results = docs_retrieval_model.search(text_query, k=top_k)\n","    # Get the retrieved images\n","    grouped_images = get_grouped_images(results, all_images)\n","\n","    # Construct chat template\n","    chat_template = [\n","        {\n","            'role': 'user',\n","            'content': [\n","                {'type': 'image', 'image': image} for image in grouped_images\n","            ] + [\n","                {'type': 'text', 'text': text_query}\n","            ]\n","        }\n","    ]\n","\n","    # Prepare the inputs\n","    text = vl_model_processor.apply_chat_template(\n","        chat_template,\n","        tokenize=False,\n","        add_generation_prompt=True\n","    )\n","    image_inputs, video_inputs = process_vision_info(chat_template)\n","    inputs = vl_model_processor(\n","        text=[text],\n","        images=images_inputs,\n","        padding=True,\n","        return_tensors='pt'\n","    )\n","    inputs = inputs.to('cuda')\n","\n","    # Generate text from the VLM\n","    generated_ids = vl_model.generate(**inputs, max_new_tokens=max_new_tokens)\n","    generated_ids_trimmed = [\n","        out_ids[len(in_ids) :]\n","        for in_ids, out_ids in zip(inputs.inputs_ids, generated_ids)\n","    ]\n","\n","    # Decode the generated text\n","    output_text = vl_model_processor.batch_decode(\n","        generated_ids_trimmed,\n","        skip_special_tokens=True,\n","        clean_up_tokenization_spaces=False\n","    )\n","\n","    return output_text"],"metadata":{"id":"c75Z1_qdZ_Lu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can apply the complete multimodal RAG system:"],"metadata":{"id":"hT8aXhIsbZnk"}},{"cell_type":"code","source":["text_query = 'How do I assemble the Miche desk?'\n","\n","output_text = answer_with_multimodal_rag(\n","    vl_model=vl_model,\n","    vl_model_processor=vl_model_processor,\n","    docs_retrieval_model=docs_retrieval_model,\n","    text_query=text_query,\n","    all_images=all_images,\n","    top_k=3,\n","    max_new_tokens=500\n",")\n","print(output_text[0])"],"metadata":{"id":"emUY4KCbbd7F"},"execution_count":null,"outputs":[]}]}