{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5nxjB0yAm2O"
   },
   "source": [
    "# Smol Multimodal RAG: Building with ColSmolVLM and SmolVLM on a Consumer GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No5YDiy0A4RH"
   },
   "source": [
    "In this example, we will build a **Multimodal Retrieval-Augmented Generation (RAG)** system by integrating [`ColSmolVLM`](https://huggingface.co/vidore/colsmolvlm-v0.1) for document retrieval and [`SmolVLM`](https://huggingface.co/blog/smolvlm) as the vision language model (VLM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwAgKjxeBMHm"
   },
   "source": [
    "## Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 141419,
     "status": "ok",
     "timestamp": 1745353618713,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "N4W489NRAhND",
    "outputId": "d19abdd1-e51f-49b1-aa6a-b0fe4e4cc829"
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/sergiopaniego/byaldi.git@colsmolvlm-support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FW9GoSltBaex"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tATnktQwB3p_"
   },
   "source": [
    "In this example, we will use charts and maps from the website [Our World in Data](https://ourworldindata.org/), an open-access platform offering a wealth of data and visualizations. We focus on the life expectancy data and load it from a [curated subset](https://huggingface.co/datasets/sergiopaniego/ourworldindata_example) hosted on HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZ6SShHlBbeJ"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"sergiopaniego/ourworldindata_example\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bgV_7mbFVSb"
   },
   "source": [
    "After downloading the visual data, we will save it locally to prepare it for the RAG (Retrieval-Augmented Generation) system. It enables the document retrieval model (ColSmolVLM) to efficiently index, process, and manipulate the visual content. Proper indexing ensures seamless integration and retrieval during system execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DLwecK0FaBf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def save_images_to_local(dataset, output_folder='data/'):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for image_id, image_data in enumerate(dataset):\n",
    "        image = image_data['image']\n",
    "\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image)\n",
    "\n",
    "        output_path = os.path.join(output_folder, f\"image_{image_id}.png\")\n",
    "        image.save(output_path, format='PNG')\n",
    "        print(f\"Image saved at: {output_path}\")\n",
    "\n",
    "save_images_to_local(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmJGS3LMFue_"
   },
   "source": [
    "Next, we will load the images to explore the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xOGRsHIFxU3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def load_png_images(image_folder):\n",
    "    png_files = [f for f in os.listdir(image_folder) if f.endswith('.png')]\n",
    "    all_images = []\n",
    "\n",
    "    for image_id, png_file in enumerate(png_files):\n",
    "        image_path = os.path.join(image_folder, png_file)\n",
    "        image = Image.open(image_path)\n",
    "        all_images[image_id] = image\n",
    "\n",
    "    return all_images\n",
    "\n",
    "all_images = load_png_images('/content/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TLM2P89G5uW"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 15))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = all_images[i]\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3AQvWFSHE9x"
   },
   "source": [
    "## Initialize the ColSmolVLM multimodal document retrieval model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVRpXtXvJy0M"
   },
   "source": [
    "The **Document Retrieval Model** will extract relevant information from the raw images and return the appropriate documents based on our queries.\n",
    "\n",
    "For this task, we will use the `Byaldi` library, designed to streamline multimodal RAG pipelines. Byaldi provides APIs that integrate multimodal retrievers and vision language models for efficient retrieval-augmented generation workflow. We will focus specifically on **ColSmolVLM**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1NBVi8vHIrr"
   },
   "outputs": [],
   "source": [
    "from byaldi import RAGMultiModalModel\n",
    "\n",
    "docs_retrieval_model = RAGMultiModalModel.from_pretrained('vidore/colsmolvlm-alpha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CI37tjuZLJiW"
   },
   "source": [
    "Next we will index our documents using the document retrieval model by specifying the oflder where the images are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycX7I0PGLQ-Y"
   },
   "outputs": [],
   "source": [
    "docs_retrieval_model.index(\n",
    "    input_path='data/',\n",
    "    index_name='image_index',\n",
    "    store_collection_with_index=False,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atl24a5sLYCs"
   },
   "source": [
    "## Retrieve documents with the document retrieval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOHRRkkBLatF"
   },
   "outputs": [],
   "source": [
    "text_query = \"What is the overall trend in life expectancy across different countries and regions?\"\n",
    "\n",
    "results = docs_retrieval_model.search(text_query, k=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzY1bZ4KLrO9"
   },
   "source": [
    "We can take a look at the retrieved document and check whether the model has correctly matched our query with the best possible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojGZGpuLL2Ux"
   },
   "outputs": [],
   "source": [
    "result_image = all_images[results[0]['doc_id']]\n",
    "result_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpOvrYrTL_KG"
   },
   "source": [
    "## Initialize the vision language model for question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JQ8rDtwMB7k"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Idefics3ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "model_id = 'HuggingFaceTB/SmolVLM-Instruct'\n",
    "vl_model_processor = AutoProcessor.from_pretrained(model_id)\n",
    "vl_model = Idefics3ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    _attn_implementation='eager'\n",
    ")\n",
    "vl_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9r-YP8vMaD9"
   },
   "source": [
    "## Assemble the VLM model and test the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioKqxGujMedm"
   },
   "source": [
    "With all components loaded, we are ready to assemble the system for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5EoDXNuMcl6"
   },
   "outputs": [],
   "source": [
    "chat_template = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': [\n",
    "            {\n",
    "                'type': 'image'\n",
    "            },\n",
    "            {\n",
    "                'type': 'text',\n",
    "                'text': text_query\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vYrz3ohMw8z"
   },
   "source": [
    "We will apply this chat template to set up the system for interacting with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BDoimkuNM0BS"
   },
   "outputs": [],
   "source": [
    "text = vl_model_processor.apply_chat_template(chat_template, add_generation_prompt=True)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHyGkba2M4W_"
   },
   "source": [
    "Next, we will process the inputs to ensure they are properly formatted and ready for use with the VLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJTDr2qPM9Ag"
   },
   "outputs": [],
   "source": [
    "inputs = vl_model_processor(\n",
    "    text=text,\n",
    "    images=[result_image],\n",
    "    return_tensors='pt'\n",
    ")\n",
    "inputs = inputs.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ED8UyeQPNCLg"
   },
   "outputs": [],
   "source": [
    "generated_ids = vl_model.generate(**inputs, max_new_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUHGY0jvNGAC"
   },
   "outputs": [],
   "source": [
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :]\n",
    "    for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "output_text = vl_model_processor.batch_decode(\n",
    "    generated_ids_trimmed,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1XuRVafNUxC"
   },
   "outputs": [],
   "source": [
    "print(output_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiFhUXFbNXAm"
   },
   "outputs": [],
   "source": [
    "print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwFW-lfvNXrr"
   },
   "source": [
    "## Assemble everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3isuCT1NauX"
   },
   "source": [
    "We will create a function to encompass the entire pipeline, allowing us to easily reuse it in future applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQTrpC3ENaDm"
   },
   "outputs": [],
   "source": [
    "def answer_with_multimodal_rag(\n",
    "        vl_model,\n",
    "        vl_model_processor,\n",
    "        docs_retrieval_model,\n",
    "        all_images,\n",
    "        text_query,\n",
    "        retrival_top_k,\n",
    "        max_new_tokens,\n",
    "):\n",
    "    results = docs_retrieval_model.search(text_query, k=retrival_top_k)\n",
    "    result_image = all_images[results[0]['doc_id']]\n",
    "\n",
    "    chat_template = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {\n",
    "                    'type': 'image'\n",
    "                },\n",
    "                {\n",
    "                    'type': 'text',\n",
    "                    'text': text_query\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Prepare the inputs\n",
    "    text = vl_model_processor.apply_chat_template(chat_template, add_generation_prompt=True)\n",
    "    inputs = vl_model_processor(\n",
    "        text=text,\n",
    "        images=[result_image],\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    inputs = inputs.to('cuda')\n",
    "\n",
    "    # Generate text from the vl_model\n",
    "    generated_ids = vl_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :]\n",
    "        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    # Decode the generated text\n",
    "    output_text = vl_model_processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cODzhs7FOOrF"
   },
   "source": [
    "This is the complete RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkU_F3WROQGj"
   },
   "outputs": [],
   "source": [
    "text_query = \"What is the overall trend in life expectancy across different countries and regions?\"\n",
    "\n",
    "output_text = answer_with_multimodal_rag(\n",
    "    vl_model=vl_model,\n",
    "    vl_model_processor=vl_model_processor,\n",
    "    docs_retrieval_model=docs_retrieval_model,\n",
    "    all_images=all_images,\n",
    "    text_query=text_query,\n",
    "    retrival_top_k=1,\n",
    "    max_new_tokens=500\n",
    ")\n",
    "\n",
    "print(output_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f17RekwfPEFa"
   },
   "outputs": [],
   "source": [
    "print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-n2qOyXcOam4"
   },
   "source": [
    "## Go even Smoler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDvNbCh9OebP"
   },
   "source": [
    "We could use a quantized version of the **SmolVLM** model to further reduce the system's resource requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJcxKZI1Odp5"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-c2gyZQDOmbN"
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p39ambaTOs1M"
   },
   "source": [
    "Next, we will load the model using the quantization configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4SJKdT1OvlN"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Idefics3ForConditionalGeneration\n",
    "\n",
    "model_id = 'HuggingFaceTB/SmolVLM-Instruct'\n",
    "vl_model_processor = AutoProcessor.from_pretrained(model_id)\n",
    "vl_model = Idefics3ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map='auto',\n",
    "    quantization_config=bnb_config,\n",
    "    _attn_implementation='eager'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlWeHov0O4ko"
   },
   "source": [
    "Now we can test the capabilities of our quantized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pcj9JLd6O67j"
   },
   "outputs": [],
   "source": [
    "text_query = \"What is the overall trend in life expectancy across different countries and regions?\"\n",
    "\n",
    "output_text = answer_with_multimodal_rag(\n",
    "    vl_model=vl_model,\n",
    "    vl_model_processor=vl_model_processor,\n",
    "    docs_retrieval_model=docs_retrieval_model,\n",
    "    all_images=all_images,\n",
    "    text_query=text_query,\n",
    "    retrival_top_k=1,\n",
    "    max_new_tokens=500\n",
    ")\n",
    "\n",
    "print(output_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-nIpVDB9PCnm"
   },
   "outputs": [],
   "source": [
    "print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNone/GpWWTrS0JMr0LwgqQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
