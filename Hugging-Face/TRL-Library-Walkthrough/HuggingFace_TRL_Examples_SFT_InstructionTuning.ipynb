{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPLPt2wPQes3JTlJNoj7PA9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Fine-tune Open LLMs with HuggingFace"],"metadata":{"id":"_V9weHC-HErL"}},{"cell_type":"markdown","source":["LLMs can handle many tasks out-of-the-box through prompting, including chatbots, question answering, and summarization.\n","\n","\n","For specialized applications requiring high accuracy or domain expertise, fine-tuning remains a powerful approach to achieve higher quality results than prompting alone, reduce costs by training smaller, more efficient models, and ensure reliability and consistency for specific use cases.\n","\n","**Q-LoRA (Quantized Low-Rank Adaptation)** enables efficient fine-tuning of LLMs using 4-bit quantization and minimal parameter updates, reducing resource needs but potentially impacting performance due to quantization trade-offs.\n","\n","**Spectrum** is a fine-tuning method that identifies the most informative layers of a LLM using Signal-to-Noise Ratio (SNR) analysis and selectively fine-tunes them, offering performance comparable to full fine-tuning with reduced resource usage, especially in distributed training setups.\n"],"metadata":{"id":"UI05hGlSJTkv"}},{"cell_type":"markdown","source":["## When to fine-tune?"],"metadata":{"id":"7-TCkggusttH"}},{"cell_type":"markdown","source":["Fine-tuning is particulary valuable and useful when we need to\n","* consistently improve performance on a specific set of tasks\n","* control the style and format of model outputs (e.g., enforcing a compnay's tone of voice)\n","* teach the model domain-specific knowledge or terminology\n","* reduce hallucinations for critical applications\n","* optimize for latency by creating smaller, specialized models\n","* ensure consistent adherence to specific guidelines or constraints"],"metadata":{"id":"dqDwN_1Ksy9u"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"WipgmcHdthwm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQ0_gLldGfQt"},"outputs":[],"source":["# Install Pytorch & other libraries\n","%pip install \"torch==2.4.1\" tensorboard flash-attn \"liger-kernel==0.4.2\" \"setuptools<71.0.0\" \"deepspeed==0.15.4\" openai \"lm-eval[api]==0.4.5\"\n","\n","# Install Hugging Face libraries\n","%pip install  --upgrade \\\n","  \"transformers==4.46.3\" \\\n","  \"datasets==3.1.0\" \\\n","  \"accelerate==1.1.1\" \\\n","  \"bitsandbytes==0.44.1\" \\\n","  \"trl==0.12.1\" \\\n","  \"peft==0.13.2\" \\\n","  \"lighteval==0.6.2\" \\\n","  \"hf-transfer==0.1.8\""]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","notebook_login"],"metadata":{"id":"WZQEZ1pFtmfv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create and prepare the dataset"],"metadata":{"id":"rY_nE-dptr2E"}},{"cell_type":"markdown","source":["Most datasets are created using automated synthetic workflows with LLMs, though several approaches exist:\n","* **Synthetic generation with LLMs**: Most common approach using frameworks like Distilabel to generate high-quality synthetic data at scale\n","* **Existing datasets**: using public datasets from HuggingFace Hub\n","* **Human annotation**: For highest quality but most expensive option\n","\n","\n","In this example, we will use [`orca-math-word-problems-200k`](https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k) dataset including 200,000 math world problems.\n","\n","Modern fine-tuning frameworks like `trl` support standard formats:\n","```yaml\n","// Conversation format\n","{\n","    'messages': [\n","        {'role': 'system', 'content': 'You are ...'},\n","        {'role': 'user', 'content': '...'},\n","        {'role': 'assistant', 'content': '...'}\n","    ]\n","}\n","\n","// Instruction format\n","{'prompt': '<prompt text>', 'completion': '<ideal generated text>'}\n","```\n","\n","\n","To prepare our datasets we will use the `datasets` library and convert it into the conversation fomrat, where we include the schema definition in the system message for our assistant. Then we save the dataset as `jsonl` file for fine-tuning."],"metadata":{"id":"0YTUmWT3tzvn"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","system_message = \"\"\"Solve the given high school math problem by providing a clear explanation of each step leading to the final solution.\n","\n","Provide a detailed breakdown of your calculations, beginning with an explanation of the problem and describing how you derive each formula, value, or conclusion. Use logical steps that build upon one another, to arrive at the final answer in a systematic manner.\n","\n","# Steps\n","\n","1. **Understand the Problem**: Restate the given math problem and clearly identify the main question and any important given values.\n","2. **Set Up**: Identify the key formulas or concepts that could help solve the problem (e.g., algebraic manipulation, geometry formulas, trigonometric identities).\n","3. **Solve Step-by-Step**: Iteratively progress through each step of the math problem, justifying why each consecutive operation brings you closer to the solution.\n","4. **Double Check**: If applicable, double check the work for accuracy and sense, and mention potential alternative approaches if any.\n","5. **Final Answer**: Provide the numerical or algebraic solution clearly, accompanied by appropriate units if relevant.\n","\n","# Notes\n","\n","- Always clearly define any variable or term used.\n","- Wherever applicable, include unit conversions or context to explain why each formula or step has been chosen.\n","- Assume the level of mathematics is suitable for high school, and avoid overly advanced math techniques unless they are common at that level.\n","\"\"\"\n","\n","def create_conversation(sample):\n","    return {\n","        'messages': [\n","            {'role': 'system', 'content': system_message},\n","            {'role': 'user', 'content': sample['question']},\n","            {'role': 'assistant', 'content': sample['answer']}\n","        ]\n","    }\n","\n","\n","# load dataset\n","dataset = load_dataset(\n","    'microsoft/orca-math-word-problems-200k',\n","    split='train'\n",")"],"metadata":{"id":"Ae2XC6B6ttwf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset[0]"],"metadata":{"id":"RdmBPhUz0QEb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# convert dataset to the OpenAI message template\n","dataset = dataset.map(\n","    create_conversation,\n","    remove_columns=dataset.features,\n","    batched=False\n",")"],"metadata":{"id":"VRNaK8-m0RcC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset[0]['messages']"],"metadata":{"id":"LXsGYzXU0X8g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save dataset to disk\n","dataset.to_json('train_dataset.json', orient='records')"],"metadata":{"id":"Yvh_07wL0aJY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fine-tune the model using `trl` and the `SFTTrainer` with QLoRA"],"metadata":{"id":"Vw-XoQvN0fFY"}},{"cell_type":"markdown","source":["We will use the `SFTTrainer` from `trl` to fine-tune our model. The `SFTTrainer` makes it straightforward to supervise fine-tune open LLMs.\n","\n","The `SFTTrainer` is a subclass of the `Trainer` from the `transformers` library and supports all the same features, including logging, evaluation, and checkpointing, and adds additional features:\n","* dataset formatting, including conversational and instructional format\n","* training on completions only, ignoring prompts\n","* packing datasets for more efficient training\n","* PEFT support including Q-LoRA, or Spectrum\n","* distributed trianing with `accelerate` and FSDP/DeepSpeed"],"metadata":{"id":"dIWe8zvr02zu"}},{"cell_type":"markdown","source":["The following `run_sft.py` script is used to run the fine-tuning with a yaml configuration. It uses the `TrlParser` to parses the yaml file and converts it into the `TrainingArguments` arguments."],"metadata":{"id":"yMl6nWEB19aF"}},{"cell_type":"code","source":["# -------- `run_sft.py` ---------\n","from dataclasses import dataclass\n","from datetime import datetime\n","from distutils.util import strtobool\n","import logging\n","import re\n","import os\n","from typing import Optional\n","os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, BitsAndBytesConfig\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import is_liger_kernel_available\n","from trl import SFTTrainer, TrlParser, ModelConfig, SFTConfig, get_peft_config\n","from datasets import load_dataset\n","from peft import AutoPeftModelForCausalLM\n","\n","if is_liger_kernel_available():\n","    from liger_kernel.transformers import AutoLigerKernelForCausalLM\n","\n","\n","\n","#######################\n","# Custom dataclasses\n","#######################\n","@dataclass\n","class ScriptArguments:\n","    dataset_id_or_path: str\n","    dataset_splits: str = 'train'\n","    tokenizer_name_or_path: str = None\n","    spectrum_config_path: Optional[str] = None\n","\n","\n","#######################\n","# Setup logging\n","#######################\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","handler = logging.StreamHandler()\n","handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n","logger.addHandler(handler)\n","\n","#######################\n","# Helper functions\n","#######################\n","\n","def get_checkpoint(training_args: SFTConfig):\n","    last_checkpoint = None\n","    if os.path.isdir(training_args.output_dir):\n","        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n","    return last_checkpoint\n","\n","def setup_model_for_spectrum(model, spectrum_config_path):\n","    unfrozen_parameters = []\n","    with open(spectrum_config_path, 'r') as fin:\n","        yaml_parameters = fin.read()\n","\n","    # get the unfrozen parameters from the yaml file\n","    for line in yaml_parameters.splitlines():\n","        if line.startswith('- '):\n","            unfrozen_parameters.append(line.split('- ')[1])\n","\n","    # freeze all parameters\n","    for param in model.parameters():\n","        param.requires_grad = False\n","    # unfreeze Spectrum parameters\n","    for name, param in model.named_parameters():\n","        if any(re.match(unfrozen_param, name) for unfrozen_param in unfrozen_parameters):\n","            param.requires_grad = True\n","\n","    # Sanity check and print the trainable parameters\n","    for name, param in model.named_parameters():\n","        if param.requires_grad:\n","            print(f\"Trainable parameter: {name}\")\n","\n","    return model\n","\n","\n","###############################################################################################\n","\n","def train_function(model_args: ModelConfig, script_args: ScriptArguments, training_args: SFTConfig):\n","    \"\"\"Main training function\"\"\"\n","    ###################\n","    # log parameters\n","    ###################\n","    logger.info(f'Model parameters {model_args}')\n","    logger.info(f'Script parameters {script_args}')\n","    logger.info(f'Training/evaluation parameters {training_args}')\n","\n","    ###################\n","    # load datasets\n","    ###################\n","    if script_args.dataset_id_or_path.endswith('.json'):\n","        train_dataset = load_dataset('json', data_files=script_args.dataset_id_or_path, split='train')\n","    else:\n","        train_dataset = load_dataset(script_args.dataset_id_or_path, split=script_args.dataset_splits)\n","\n","    train_dataset = train_dataset.select(range(10000))\n","\n","    logger.info(f'Loaded dataset with {len(train_dataset)} samples and the following features: {train_dataset.features}')\n","\n","    ##################\n","    # load tokenizer\n","    ##################\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        script_args.tokenizer_name_or_path if script_args.tokenizer_name_or_path else model_args.model_name_or_path,\n","        revision=model_args.model_revision,\n","        trust_remote_code=model_args.trust_remote_code,\n","    )\n","    if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","\n","    # If we use peft we need to make sure we use a chat template that is not using special tokens\n","    # because by default embedding layers will not be trainable\n","\n","    ##########################\n","    # load pretrained model\n","    ##########################\n","\n","    # define model kwargs\n","    model_kwargs = dict(\n","        revision=model_args.model_revision, # what revision from huggingface to use, defaults to main\n","        trust_remote_code=model_args.trust_remote_code, # whether to trust the remote code, this allows us to finetune custom architectures\n","        attn_implementation=model_args.attn_implementation, # what attention implementation to use, defaults to flash_attention_2\n","        torch_dtype=model_args.torch_dtype if model_args.torch_dtype in ['auto', None] else getattr(torch, model_args.torch_dtype), # what torch dtype to use, defaults to auto\n","        use_cache=False if training_args.gradient_checkpointing else True, # whether to use cache or not, defaults to False if gradient checkpointing is enabled\n","        low_cpu_mem_usage=True if not strtobool(os.environ.get('ACCELERATE_USE_DEEPSPEED', 'false')) else None, # reduce memory usage on CPU for loading the model\n","    )\n","\n","    # check which training method to use and if 4-bit quantization is needed\n","    if model_args.load_in_4bit:\n","        model_kwargs['quantization_config'] = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_use_double_quant=True,\n","            bnb_4bit_quant_type='nf4',\n","            bnb_4bit_compute_dtype=model_kwargs['torch_dtype'],\n","            bnb_4bit_quant_storage=model_kwargs['torch_dtype']\n","        )\n","    if model_args.use_peft:\n","        peft_config = get_peft_config(model_args)\n","    else:\n","        peft_config = None\n","\n","    # load the model with our kwargs\n","    if training_args.use_liger:\n","        model = AutoLigerKernelForCausalLM.from_pretrained(\n","            model_args.model_name_or_path,\n","            **model_kwargs\n","        )\n","    else:\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_args.model_name_or_path,\n","            **model_kwargs\n","        )\n","\n","    training_args.distributed_state.wait_for_everyone() # wait for all processes to load\n","\n","    if script_args.spectrum_config_path:\n","        model = setup_model_for_spectrum(model, script_args.spectrum_config_path)\n","\n","\n","    ##########################\n","    # initialize the Trainer\n","    ##########################\n","    trainer = SFTTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        tokenizer=tokenizer,\n","        peft_config=peft_config\n","    )\n","    if trainer.accelerator.is_main_process and peft_config:\n","        trainer.model.print_trainble_parameters()\n","\n","\n","    ######################\n","    # training loop\n","    ######################\n","    last_checkpoint = get_checkpoint(training_args)\n","    if last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n","        logger.info(f'Checkpoint detected, resuming training at {last_checkpoint}')\n","\n","    logger.info(f'*** Starting training {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} for {training_args.num_train_epochs} epochs ***')\n","    train_result = trainer.train(resume_from_checkpoint=last_checkpoint)\n","    # log metrics\n","    metrics = train_result.metrics\n","    metrics['train_samples'] = len(train_dataset)\n","    trainer.log_metrics('train', metrics)\n","    trainer.save_metrics('train', metrics)\n","    trainer.save_state()\n","\n","    ############################################\n","    # save model and create model card\n","    ############################################\n","    logger.info('*** Save model ***')\n","    if trainer.is_fsdp_enabled and peft_config:\n","        trainer.accelerator.state.fsdp_plugin.set_state_dict_type('FULL_STATE_DICT')\n","\n","    # restore k,v cache for fast inference\n","    trainer.model.config.use_cache = True\n","    trainer.save_model(training_args.output_dir)\n","    logger.info(f\"Model saved to {training_args.output_dir}\")\n","    training_args.distributed_state.wait_for_everyone() # wait for all processes to load\n","\n","    tokenizer.save_pretrained(training_args.output_dir)\n","    logger.info(f\"Tokenizer saved to {training_args.output_dir}\")\n","\n","    # Save everything else on main process\n","    if trainer.accelerator.is_main_process:\n","        trainer.create_model_card(\n","            {'tags': ['sft', 'tutorial']}\n","        )\n","\n","    # push to hub if necessary\n","    if training_args.push_to_hub:\n","        logger.info('Pushing to hub...')\n","        trainer.push_to_hub()\n","\n","    logger.info('*** Training complete ***')\n","\n","\n","def main():\n","    parser = TrlParser((ModelConfig, ScriptArguments, SFTConfig))\n","    model_args, script_args, training_args = parser.parse_args_and_config()\n","\n","    # set seed for reproducibility\n","    set_seed(training_args.seed)\n","\n","    # run the training loop\n","    train_function(model_args, script_args, training_args)\n","\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"xY6Qe5K00i2Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We use `dataclasses` definition for our arguments so that every argument can be provided either via the command line or a yaml configuration file:\n","```python\n","@dataclass\n","class ScriptArguments:\n","    dataset_id_or_path: str\n","    ...\n","\n","```\n","\n","Next, we can customize behavior for different training methods and use them in our script with `scirpt_args`. The training script is separated by `#######` blocks for the different parts of the script.\n","\n","The main training function:\n","1. logs all hyperparameters\n","2. loads the dataset from HuggingFace Hub or local disk\n","3. loads the tokenizer and model with our training strategy (e.g., Q-LoRA, Spectrum)\n","4. initializes the `SFTTrainer`\n","5. starts the training loop (optionally continue training from a checkpoint)\n","6. saves the model and optionally pushes it to the HuggingFace Hub"],"metadata":{"id":"wfHZS08MD2uj"}},{"cell_type":"markdown","source":["The following `llama-3-1-8b-qlora.yaml` is an example recipe to fine-tune a `Llama-3.1-8B` model with Q-LoRA:\n","```yaml\n","# Model arguments\n","model_name_or_path: Meta-Llama/Meta-Llama-3.1-8B\n","tokenizer_name_or_path: Meta-Llama/Meta-Llama-3.1-8B-Instruct\n","model_revision: main\n","torch_dtype: bfloat16\n","attn_implementation: flash_attention_2\n","use_liger: true\n","bf16: true\n","tf32: true\n","output_dir: runs/llama-3-1-8b-math-orca-qlora-10k-ep1\n","\n","# Dataset arguments\n","dataset_id_or_path: train_dataset.json\n","max_seq_length: 1024\n","packing: true\n","\n","# LoRA arguments\n","use_peft: true\n","load_in_4bit: true\n","lora_target_modules: 'all-linear'\n","# important as we need to train the special tokens for the caht template of llama\n","lora_modules_to_save: ['lm_head', 'embed_tokens'] # we may need to change this for qwen or other models\n","lora_r: 16\n","lora_alpha: 16\n","\n","# Training arguments\n","num_train_epochs: 1\n","per_device_train_batch_size: 8\n","gradient_accumulation_steps: 2\n","gradient_checkpointing: true\n","gradient_checkpointing_kwargs:\n","    use_reentrant: false\n","learning_rate: 2.0e-4\n","lr_scheduler_type: constant\n","warmup_ratio: 0.1\n","\n","# Logging arguments\n","logging_strategy: steps\n","logging_steps: 5\n","report_to:\n","    - tensorboard\n","save_strategy: epoch\n","seed: 42\n","\n","# HuggingFace Hub\n","push_to_hub: false\n","hub_strategy: every_save\n","hub_model_id: llama-3-1-8b-math-orca-qlora-10k-ep1 # if not defined the same as output_dir\n","```\n","\n","This config works for single-GPU training and for multi-GPU training with DeepSpeed.\n","\n","What we need to do is to run the following line in the terminal:\n","```bash\n","python run_sft.py --config llama-3-1-8b-qlora.yaml\n","```\n","\n","Notes:\n","* Q-LoRA includes trianing the embedding layer and the `lm_head`, as we use the Llama 3.1 chat template and in the base model the special tokens are not trained.\n","* For distributed training DeepSpeed with ZeRO3 and HuggingFace Accelerate was used.\n","* Spectrum with 30% SNR layers took slightly longer than Q-LoRA, but achieves higher accuracy on GSM8K dataset.\n","\n","Using Q-LoRA only saves the trained adapter weights. If we want to use the model as standalone model, e.g., for inference we may want to merge the adapter and base model."],"metadata":{"id":"9wHHGSe6zjWA"}},{"cell_type":"markdown","source":["## Test model and run inference"],"metadata":{"id":"-WNruPbb2Etq"}},{"cell_type":"markdown","source":["As we trained our model on solving math problems, we will evaluate the model on `GSM8K` (Grade School Math 8K) dataset, which is a dataset of 8.5K high quality linguistically diverse grade school math word problems. This dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning.\n","\n","In this example, we will use [`llm-evaluation-harness`](https://github.com/EleutherAI/lm-evaluation-harness), an open-source framework to evaluate language models on a wide range of tasks and benchmarks.\n","\n","We will use [`text-generation-inference` (TGI)](https://github.com/huggingface/text-generation-inference) for testing and deploying our model. TGI is a purpose-built solution for deploying and serving LLMs. TGI enables high-performance text generation using Tensor parallelism and continous batching.\n","\n","We will start with 1 GPU, but free to increase the `num_gpus` if available."],"metadata":{"id":"2qUJzjjE3OQD"}},{"cell_type":"code","source":["%%bash\n","\n","num_gpus=1\n","model_id=philschmid/llama-3-1-8b-math-orca-spectrum-10k-ep1 # replace with your model id\n","\n","docker run --name tgi --gpus ${num_gpus} -d -ti -p 8080:80 --shm-size=2GB \\\n","  -e HF_TOKEN=$(cat ~/.cache/huggingface/token) \\\n","  ghcr.io/huggingface/text-generation-inference:3.0.1 \\\n","  --model-id ${model_id} \\\n","  --num-shard ${num_gpus}\n"],"metadata":{"id":"ZtR-IuE62GKA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Our container will start in the background and download the model from HuggingFace Hub. We can check the logs to see the progress with\n","```bash\n","docker logs -f tgi\n","```\n","\n","Once our container is running we can send requests using the `openai` or `huggingface_hub` SDK. In this example, we will use the `openai` SDK to send a request to our inference server."],"metadata":{"id":"bZ8f2mHa4i9F"}},{"cell_type":"code","source":["from openai import OpenAI\n","\n","# create client\n","client = OpenAI(base_url='http://localhost:8080/v1', api_key='dummy')\n","\n","system_message = \"\"\"Solve the given high school math problem by providing a clear explanation of each step leading to the final solution.\n","\n","Provide a detailed breakdown of your calculations, beginning with an explanation of the problem and describing how you derive each formula, value, or conclusion. Use logical steps that build upon one another, to arrive at the final answer in a systematic manner.\n","\n","# Steps\n","\n","1. **Understand the Problem**: Restate the given math problem and clearly identify the main question and any important given values.\n","2. **Set Up**: Identify the key formulas or concepts that could help solve the problem (e.g., algebraic manipulation, geometry formulas, trigonometric identities).\n","3. **Solve Step-by-Step**: Iteratively progress through each step of the math problem, justifying why each consecutive operation brings you closer to the solution.\n","4. **Double Check**: If applicable, double check the work for accuracy and sense, and mention potential alternative approaches if any.\n","5. **Final Answer**: Provide the numerical or algebraic solution clearly, accompanied by appropriate units if relevant.\n","\n","# Notes\n","\n","- Always clearly define any variable or term used.\n","- Wherever applicable, include unit conversions or context to explain why each formula or step has been chosen.\n","- Assume the level of mathematics is suitable for high school, and avoid overly advanced math techniques unless they are common at that level.\n","\"\"\"\n","\n","messages = [\n","    {'role': 'system', 'content': system_message},\n","    {'role': 'user', 'content': \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\"}\n","]\n","expected_answer = '72'\n","\n","# Test an example question\n","response = client.chat.completions.create(\n","    model='orca',\n","    messages=messages,\n","    stream=False, # no streaming\n","    max_tokens=256\n",")\n","response = response.choices[0].message.content\n","\n","print(f\"Query:\\n{messages[1]['content']}\")\n","print(f\"Original Answer:\\n{expected_answer}\")\n","print(f\"Generated Answer:\\n{response}\")"],"metadata":{"id":"x_u32_It45fN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next we will evaluate our model with the `llm-evaluation-harness`:"],"metadata":{"id":"nvnqVVy_5xrZ"}},{"cell_type":"code","source":["!lm_eval --model local-chat-completions \\\n","  --tasks gsm8k_cot \\\n","  --model_args model=philschmid/llama-3-1-8b-math-orca-spectrum-10k-ep1,base_url=http://localhost:8080/v1/chat/completions,num_concurrent=8,max_retries=3,tokenized_requests=False \\\n","  --apply_chat_template \\\n","  --fewshot_as_multiturn"],"metadata":{"id":"MbN6jhI5543c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Stop our container before disconnection:"],"metadata":{"id":"eyEsWnda56zF"}},{"cell_type":"code","source":["!docker stop tgi\n","!docker rm tgi"],"metadata":{"id":"M-vtNWEX59ZD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Distributed training: DeepSpeed + Q-LoRA"],"metadata":{"id":"8_SPEM5J6DAV"}},{"cell_type":"markdown","source":["We can combine DeepSpeed and Q-LoRA. Don't forget to change the `num_processes` to the number of GPUs we want to use:\n","```bash\n","accelerate launch run_sft.py\n","    --config_file deepspeed_zero3.yaml\n","    --num_processes 8\n","    --config llama-3-1-8b-qlora.yaml\n","```"],"metadata":{"id":"vn-ITqC46LBU"}},{"cell_type":"markdown","source":["## Inference: vllm"],"metadata":{"id":"wezCsOaC6pS3"}},{"cell_type":"markdown","source":["For faster inference,\n","```bash\n","docker run --runtime nvidia --gpus all \\\n","    -p 8000:8000 \\\n","    vllm/vllm-openai ----model philschmid/llama-3-1-8b-math-orca-qlora-10k-ep1-merged\n","```"],"metadata":{"id":"TtEya2tI6sb7"}},{"cell_type":"markdown","source":["## Spectrum"],"metadata":{"id":"6wbXb9dJ62Fj"}},{"cell_type":"markdown","source":["Spectrum uses Signal-to-Noise Ratio (SNR) analysis to select the most useful layers for fine-tuning. It provides scripts and pre-run scanned for different models. If our model is not scanned, it will prompt us for the batch size for scanning. Batch size of 4 for 70B models requires 8xH100. Popular models like Llama 3.1 8B are already scanned.\n","\n","The script will generate a yaml configuration file in the `model_snr_results` with the name of the model and the top-precent, e.g., for `meta-llama/Llama-3.1-8B` and `30` , it will generate it at `snr_results_meta-llama-Meta-Llama-3.1-8B_unfrozenparameters_30percent.yaml.\n","* `--model_name`: specifies the local model path or the HuggingFace repository\n","* `--top-percent`: specifies the top percentage of SNR layers we want to retrieve\n","\n","For example,\n","```bash\n","git clone https://github.com/cognitivecomputations/spectrum.git\n","cd spectrum\n","\n","python3 spectrum.py\n","    --model-name meta-llama/Meta-Llama-3.1-8B\n","    --top-percent 30\n","cd ..\n","```\n","Then, the top 30% SNR layers are saved to `snr_results_meta-llama-Meta-Llama-3.1-8B_unfrozenparameters_30percent.yaml`\n","\n","Afte the yaml configuration is generated we can use it to fine-tune our model. We need to define the yaml configuration file in our train config yaml file and provide the path to the yaml file as `spectrum_config_path`."],"metadata":{"id":"xiescjVN652W"}},{"cell_type":"code","source":[],"metadata":{"id":"OExIGPxi6ral"},"execution_count":null,"outputs":[]}]}