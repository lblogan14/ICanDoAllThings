{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a10c02",
   "metadata": {},
   "source": [
    "# Project: Ambient Agents with LangGraph - Module 3: Agent Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cab9c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00603521",
   "metadata": {},
   "source": [
    "# Evaluating Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c934a8eb",
   "metadata": {},
   "source": [
    "In previous module, we have an email assistant that uses a router to triage emails and then passes the email to the agent for response generation. The next step is to evaluate how well it works in production. The testing process will guide our decisions about our agent architecture with quantifiable metrics like response quality, token usage, latency, or triage accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3adb324",
   "metadata": {},
   "source": [
    "# How to run evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0c7d45",
   "metadata": {},
   "source": [
    "## Pytest / Vitest\n",
    "\n",
    "Pytest and Vitest are popular testing frameworks for Python and JavaScript/TypeScript, respectively. \n",
    "\n",
    "LangSmith integrates with these frameworks to allow us to write and run tests that log results to LangSmith. In this module, we will use Pytest for Python examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cbaf06",
   "metadata": {},
   "source": [
    "## LangSmith Datasets\n",
    "\n",
    "We can also create a dataset in LangSmith and run our agent against the dataset using the LangSmith evaluate API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba3140",
   "metadata": {},
   "source": [
    "# Test cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ea3254",
   "metadata": {},
   "source": [
    "We will define a set of example emails we want to handle along with a few things to test. The test cases are in `src/email_assistant/eval/email_dataset.py` and contain the following:\n",
    "- **Input Emails**: A collection of diverse email examples\n",
    "- **Ground Truth Classifications**: `Respond`, `Notify`, `Ignore`\n",
    "- **Expected Tool Calls**: Tools called for each email that requires a reponse\n",
    "- **Response Criteria**: What makes a good response for emails requiring a response\n",
    "\n",
    "Note that we need to have both\n",
    "- End-to-end \"integration* tests (e.g., Input Emails -> Agent -> Final Ourput VS Response Criteria)\n",
    "- Tests for specific steps in our workflow (e.g., Input Emails -> Agent -> Classification VS Ground Truth Classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcff37da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email Input: {'author': 'Alice Smith <alice.smith@company.com>', 'to': 'Lance Martin <lance@company.com>', 'subject': 'Quick question about API documentation', 'email_thread': \"Hi Lance,\\n\\nI was reviewing the API documentation for the new authentication service and noticed a few endpoints seem to be missing from the specs. Could you help clarify if this was intentional or if we should update the docs?\\n\\nSpecifically, I'm looking at:\\n- /auth/refresh\\n- /auth/validate\\n\\nThanks!\\nAlice\"}\n",
      "Expected Triage Output: respond\n",
      "Expected Tool Calls: ['write_email', 'done']\n",
      "Response Criteria: \n",
      "â€¢ Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from email_assistant.eval.email_dataset import email_inputs, expected_tool_calls, triage_outputs_list, response_criteria_list\n",
    "\n",
    "test_case_idx = 0\n",
    "\n",
    "print(\"Email Input:\", email_inputs[test_case_idx])\n",
    "print(\"Expected Triage Output:\", triage_outputs_list[test_case_idx])\n",
    "print(\"Expected Tool Calls:\", expected_tool_calls[test_case_idx])\n",
    "print(\"Response Criteria:\", response_criteria_list[test_case_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a578ff",
   "metadata": {},
   "source": [
    "# Pytest Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a42c792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from email_assistant.eval.email_dataset import email_inputs, expected_tool_calls\n",
    "from email_assistant.utils import format_messages_string, extract_tool_calls\n",
    "from email_assistant.email_assistant import email_assistant\n",
    "\n",
    "from langsmith import testing as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e35f960",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.langsmith\n",
    "@pytest.mark.parametrize(\n",
    "    'email_input, expected_calls',\n",
    "    [\n",
    "        # Pick some examples with email reply expected\n",
    "        (email_inputs[0], expected_tool_calls[0]),\n",
    "        (email_inputs[3], expected_tool_calls[3]),\n",
    "    ]\n",
    ")\n",
    "def test_email_dataset_tool_calls(email_input, expected_calls):\n",
    "    \"\"\"Test if email processing contains expected tool calls.\n",
    "    \n",
    "    This test confirms that all expected tools are called during email processing,\n",
    "    but does not check the order of tool invocations or the number of invocations\n",
    "    per tool. Additional checks for these aspects could be added if desired.\n",
    "    \"\"\"\n",
    "    # Run the email assistant\n",
    "    messages = [{'role': 'user', 'content': str(email_input)}]\n",
    "    result = email_assistant.invoke({'messages': messages})\n",
    "\n",
    "    # Extract tool calls from messages list\n",
    "    extracted_tool_calls = extract_tool_calls(result['messages'])\n",
    "\n",
    "    # Check if all expected tool calls are in the extracted ones\n",
    "    missing_calls = [\n",
    "        call for call in expected_calls if call.lower() not in extracted_tool_calls\n",
    "    ]\n",
    "\n",
    "    t.log_outputs({\n",
    "        'missing_calls': missing_calls,\n",
    "        'extracted_tool_calls': extracted_tool_calls,\n",
    "        'response': format_messages_string(result['messages']),\n",
    "    })\n",
    "\n",
    "    # Test passes if no expected calls are missing\n",
    "    assert len(missing_calls) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99c1198",
   "metadata": {},
   "source": [
    "To run with Pytest and log test results to LangSmith, we only need to add the `@pytest.mark.langsmith` decorator to our function and place it in a file, named `test_tools.py` placed in the same directory as this notebook. This will log the test results to LangSmith.\n",
    "\n",
    "We can pass dataset examples to the test function as shown via `@py.mark.parametrize`.\n",
    "\n",
    "We can run the test from the command line with:\n",
    "```bash\n",
    "! LANGSMITH_TEST_SUITE='Email assistant: Test Tools For Interrupt' pytest ./test_tools.py\n",
    "```\n",
    "\n",
    "After that, we can view the results in the LangSmith UI. The `assert len(missing_calls) == 0` is logged to the `Pass` column in LangSmith."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb36f6",
   "metadata": {},
   "source": [
    "# LangSmith Datasets Example\n",
    "\n",
    "In the previous exmaple with Pytest, we evaluated the tool calling accuracy of the email assistant. Now, the dataset that we will evaluate is specifically for the triage step of the email assistant, in classifying whether an email requires a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74613317",
   "metadata": {},
   "source": [
    "## Dataset definition\n",
    "\n",
    "We can create a dataset in LangSmith with the LangSmith SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "352d78e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "from email_assistant.eval.email_dataset import examples_triage\n",
    "\n",
    "# Initialize LangSmith client\n",
    "client = Client()\n",
    "\n",
    "# Dataset name\n",
    "dataset_name = \"Email Triage Evaluation\"\n",
    "\n",
    "# Create dataset if it doesn't exist\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"A dataset of emails and their triage decisions.\"\n",
    "    )\n",
    "    # Add examples to the dataset\n",
    "    client.create_examples(\n",
    "        dataset_id=dataset.id,\n",
    "        examples=examples_triage\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5246ba23",
   "metadata": {},
   "source": [
    "## Target function\n",
    "\n",
    "The dataset has the following structure, with an email input and a ground truth triage classification for the email as output:\n",
    "\n",
    "```python\n",
    "    examples_triage = [\n",
    "        {\n",
    "            \"inputs\": {\"email_input\": email_input_1},\n",
    "            \"outputs\": {\"classification\": triage_output_1},   # NOTE: This becomes the reference_output in the created dataset\n",
    "        }, ...\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48c5ecae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Example Input (inputs): {'email_input': {'author': 'Alice Smith <alice.smith@company.com>', 'to': 'Lance Martin <lance@company.com>', 'subject': 'Quick question about API documentation', 'email_thread': \"Hi Lance,\\n\\nI was reviewing the API documentation for the new authentication service and noticed a few endpoints seem to be missing from the specs. Could you help clarify if this was intentional or if we should update the docs?\\n\\nSpecifically, I'm looking at:\\n- /auth/refresh\\n- /auth/validate\\n\\nThanks!\\nAlice\"}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Example Input (inputs):\", examples_triage[0]['inputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a44c60c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Example Reference Output (reference_outputs): {'classification': 'respond'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Example Reference Output (reference_outputs):\", examples_triage[0]['outputs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc055f60",
   "metadata": {},
   "source": [
    "We will define a function that takes the dataset inputs and passes them to our email assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe3884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_email_assistant(inputs: dict) -> dict:\n",
    "    \"\"\"Process an email through the workflow\"\"\"\n",
    "    response = email_assistant.nodes['triage_router'].invoke({\n",
    "        'email_input': inputs['email_input']\n",
    "    })\n",
    "    return {\n",
    "        'classification_decision': response.update['classification_decision'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f22d9fb",
   "metadata": {},
   "source": [
    "## Evaluator function\n",
    "\n",
    "We will create an evaluator function to compare\n",
    "- Reference outputs: `\"reference_outputs\": {'classification': triage_output_1} ...`\n",
    "- Agent outputs: `\"outputs\": {'classification_decision': agent_output_1} ...`\n",
    "\n",
    "We want to evaluate if the agent's output matches the reference output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9ed606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_evaluator(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Check if the answer exactly matches the reference output.\"\"\"\n",
    "    return outputs['classification_decision'].lower() == reference_outputs['classification'].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c1230b",
   "metadata": {},
   "source": [
    "## Running evaluation\n",
    "\n",
    "The evaluate API will take care of the rest. It passes the `inputs` dict from our dataset to the target function, and passes the `reference_outputs` dict from out dataset to the evaluator function. And it passes the `outputs` of our agent to the evaluator function.\n",
    "\n",
    "This is similar to what we did with Pytest. In Pytest, we passed in the dataset example inputs and references outputs to the test function with `@pytest.mark.parametrize`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa09c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to true if we want to kick off evaluation\n",
    "run_expt = True\n",
    "if run_expt:\n",
    "    experiment_results_workflow = client.evaluate(\n",
    "        # Run agent \n",
    "        target_email_assistant,\n",
    "        # Dataset name   \n",
    "        data=dataset_name,\n",
    "        # Evaluator\n",
    "        evaluators=[classification_evaluator],\n",
    "        # Name of the experiment\n",
    "        experiment_prefix=\"Email assistant workflow\", \n",
    "        # Number of concurrent evaluations\n",
    "        max_concurrency=2, \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf1d60",
   "metadata": {},
   "source": [
    "# LLM-as-Judge evaluation\n",
    "\n",
    "We have shown unit tests for the triage step (using evaluate()) and tool calling (using Pytest).\n",
    "\n",
    "Now, we will use an LLM as a judge to evaluate our agent's execution against a set of success criteria.\n",
    "\n",
    "First, we define a structured output schema for our LLM grader that contains a grade and justification for the grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653297e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "class CriteriaGrade(BaseModel):\n",
    "    \"\"\"Score the response against specific criteria.\"\"\"\n",
    "    justification: str = Field(\n",
    "        description:\"The justification for the grade and score, including specific examples from the response.\"\n",
    "    )\n",
    "    grade: bool = Field(\n",
    "        description=\"Does the response meet the provided criteria?\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Create a global LLM for evaluation to avoid recreating it for each test\n",
    "criteria_eval_llm = init_chat_model('openai:gpt-4o')\n",
    "criteria_eval_structured_llm = criteria_eval_llm.with_structured_output(CriteriaGrade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3aef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_input = email_inputs[0]\n",
    "print(\"Email Input:\", email_input)\n",
    "success_criteria = response_criteria_list[0]\n",
    "print(\"Success Criteria:\", success_criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0f4234",
   "metadata": {},
   "source": [
    "Our email assistant is invoked with the email input and the response is formatted into a string. These are all then passed to the LLM grader to receive a grade and justification for the grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c06475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = email_assistant.invoke({\"email_input\": email_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9561297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from email_assistant.eval.prompts import RESPONSE_CRITERIA_SYSTEM_PROMPT\n",
    "\n",
    "from email_assistant.eval.prompts import RESPONSE_CRITERIA_SYSTEM_PROMPT\n",
    "\n",
    "all_messages_str = format_messages_string(response['messages'])\n",
    "eval_result = criteria_eval_structured_llm.invoke([\n",
    "        {\"role\": \"system\",\n",
    "            \"content\": RESPONSE_CRITERIA_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",\n",
    "            \"content\": f\"\"\"\\n\\n Response criteria: {success_criteria} \\n\\n Assistant's response: \\n\\n {all_messages_str} \\n\\n Evaluate whether the assistant's response meets the criteria and provide justification for your evaluation.\"\"\"}\n",
    "    ])\n",
    "\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0d3dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_CRITERIA_SYSTEM_PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433b1aea",
   "metadata": {},
   "source": [
    "# Running against a larger test suite\n",
    "\n",
    "Now that we have sen how to evaluate our agent using Pytest and evaluate(), and seen an example of using an LLM as a judge, we can use evaluations over a bigger test suite to get a better sense of how our agent performs over a wider variety of examples.\n",
    "\n",
    "We can run our `email_assistant` against a larger test suite by running\n",
    "\n",
    "```bash\n",
    "    ! LANGSMITH_TEST_SUITE='Email assistant: Test Full Response Interrupt' \n",
    "    LANGSMITH_EXPERIMENT='email_assistant' pytest tests/test_response.py --agent-module email_assistant\n",
    "```\n",
    "\n",
    "In `test_response.py`, we pass our dataset examples into functions that will run pytest and log to our `LANGSMITH_TEST_SUITE`:\n",
    "\n",
    "```python\n",
    "    # Reference output key\n",
    "    @pytest.mark.langsmith(output_keys=[\"criteria\"])\n",
    "    # Variable names and a list of tuples with the test cases\n",
    "    # Each test case is (email_input, email_name, criteria, expected_calls)\n",
    "    @pytest.mark.parametrize(\"email_input,email_name,criteria,expected_calls\",create_response_test_cases())\n",
    "    def test_response_criteria_evaluation(email_input, email_name, criteria, expected_calls):\n",
    "```\n",
    "\n",
    "We will use LLM-as-judge with a grading schema:\n",
    "```python\n",
    "    class CriteriaGrade(BaseModel):\n",
    "        \"\"\"Score the response against specific criteria.\"\"\"\n",
    "        grade: bool = Field(description=\"Does the response meet the provided criteria?\")\n",
    "        justification: str = Field(description=\"The justification for the grade and score, including specific examples from the response.\")\n",
    "```\n",
    "\n",
    "We will evaluate the agent response relative to the criteria:\n",
    "\n",
    "```python\n",
    "    # Evaluate against criteria\n",
    "    eval_result = criteria_eval_structured_llm.invoke([\n",
    "        {\"role\": \"system\",\n",
    "            \"content\": RESPONSE_CRITERIA_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",\n",
    "            \"content\": f\"\"\"\\n\\n Response criteria: {criteria} \\n\\n Assistant's response: \\n\\n {all_messages_str} \\n\\n Evaluate whether the assistant's response meets the criteria and provide justification for your evaluation.\"\"\"}\n",
    "    ])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8e191",
   "metadata": {},
   "source": [
    "## Getting results\n",
    "\n",
    "We can get the results of our evaluation by reading the tracing project associated with our experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7c7aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy our experiment name here\n",
    "experiment_name = \"email_assistant:8286b3b8\"\n",
    "# Set this to load expt results\n",
    "load_expt = False\n",
    "if load_expt:\n",
    "    email_assistant_experiment_results = client.read_project(project_name=experiment_name, include_stats=True)\n",
    "    print(\"Latency p50:\", email_assistant_experiment_results.latency_p50)\n",
    "    print(\"Latency p99:\", email_assistant_experiment_results.latency_p99)\n",
    "    print(\"Token Usage:\", email_assistant_experiment_results.total_tokens)\n",
    "    print(\"Feedback Stats:\", email_assistant_experiment_results.feedback_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
