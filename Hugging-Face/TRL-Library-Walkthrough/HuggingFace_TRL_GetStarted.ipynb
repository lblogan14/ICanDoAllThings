{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNZRwt5DgIYQ1DXIgTwPy3H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# TRL - Transformer Reinforcement Learning"],"metadata":{"id":"xoNV5JRe_Hal"}},{"cell_type":"markdown","source":["TRL is a full stack library where we can apply a set of tools to train transformer language model with **Reinforcement Learning**, from the **Supervised Fine-tuning (SFT)** step, **Reward Modeling (RM)** step to the **Proximal Policy Optimization (PPO)** step. The TRL library is integrated with HuggingFace `transformers`."],"metadata":{"id":"QogKDacz_M-Z"}},{"cell_type":"markdown","source":["# Installation"],"metadata":{"id":"mZuRS-7o_251"}},{"cell_type":"markdown","source":["PyPI\n","```bash\n","pip install trl\n","```\n","\n","Source\n","```bash\n","git clone https://github.com/huggingface/trl.git\n","cd trl/\n","pip install -e .\n","```"],"metadata":{"id":"baQWk29fAAOa"}},{"cell_type":"markdown","source":["# Quickstart"],"metadata":{"id":"xTMoOlycAKmm"}},{"cell_type":"markdown","source":["Fine-tuning a language model via PPO consists of three steps:\n","1. **Rollout**: The langauge model generates a response or continuation based on a query which could be the start of a sentence.\n","2. **Evaluation**: The query and response are evaluated with a funciton, model, human feedback, or some combination of them. The important thing is that this process should yield a scalar value for each query/resopnse pair. The optimization will foucs on maximizing this value.\n","3. **Optimization**: [MAIN FOCUS] In the optimization step, the query/response pairs are used to calculate the log-probabilities of the tokens in the sequences. This is done with the **active model** that is trained and a **reference model**, which is usually the pre-trained model before fine-tuning. The KL-divergence between the two outputs is used as an additional reward signal to make sure the generated responses do not deviate too far from the reference language model. The **active lanuage model** is then trained with PPO.\n","\n","![PPO_steps](https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/trl_overview.png)"],"metadata":{"id":"PGBHe06OBEwQ"}},{"cell_type":"markdown","source":["## Minimal requirement setup"],"metadata":{"id":"wTjh7AJKCVg-"}},{"cell_type":"code","source":["!pip install -qU trl transformers accelerate dataset"],"metadata":{"id":"CRd_vrRRC7VC"},"execution_count":null,"outputs":[]}]}