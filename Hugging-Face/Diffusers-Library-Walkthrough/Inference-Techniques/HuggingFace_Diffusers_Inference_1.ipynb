{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMCBLoDJTa3t3mXXf3nIbhi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -qU diffusers accelerate transformers huggingface_hub"],"metadata":{"id":"V0O3J__QZ-hD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","notebook_login()"],"metadata":{"id":"5tcC4kCkaBJZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Distributed inference"],"metadata":{"id":"PCvbVjJkXTEK"}},{"cell_type":"markdown","source":["On distributed setups, we can run inference across multiple GPUs with HuggingFace Accelerate or PyTorch Distributed, which is useful for generating with multiple prompts in parallel."],"metadata":{"id":"gVSIl494ZpiW"}},{"cell_type":"markdown","source":["## HuggingFace Accelerate"],"metadata":{"id":"MFczqclXaNi4"}},{"cell_type":"markdown","source":["Accelerate is a library designed to make it easy to train or run inference across distributed setups.\n","\n","To begin, we need to create a Python file and initialize an `accelerate.PartialState` to create a distributed environment; our setup is automatically detected so we do not need to explicitly define the `rank` or `world_size`. Move the `DiffusionPipeline` to `distributed_state.device` to assign a GPU to each process.\n","\n","Now we can use the `split_between_processes` utility as a context manager to automatically distribute the prompts between the number of processes."],"metadata":{"id":"TF_GUqb3aQOO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2O_CTDcNXKB-"},"outputs":[],"source":["# the following snippet is saved as `run_distributed.py`\n","\n","import torch\n","from accelerate import PartialState\n","from diffusers import DiffusionPipeline\n","\n","pipeline = DiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True,\n",")\n","\n","distributed_state = PartialState()\n","pipeline.to(distributed_state.device)\n","\n","with distributed_state.split_between_processes([\"a dog\", 'a cat']) as prompt:\n","    result = pipeline(prompt).images[0]\n","    result.save(f\"result_{distributed_state.process_index}.png\")"]},{"cell_type":"markdown","source":["Use the `--num_process` argument to specify the number of GPUs to use, and call `accelerate launch` to run the script:\n","```bash\n","accelearte launch run_distributed.py --num_processes=2\n","```"],"metadata":{"id":"5kVZIxXIbQ4E"}},{"cell_type":"markdown","source":["## PyTorch Distributed"],"metadata":{"id":"tSCWWcB8bm01"}},{"cell_type":"markdown","source":["PyTorch supports `DistributedDataParallel` which enables data parallelism.\n","\n","To start, create a python file and import `torch.distributed` and `torch.multiprocessing` to set up the distributed process group and to spawn the processes for inference on each GPU."],"metadata":{"id":"qRoyF-uUdz3N"}},{"cell_type":"code","source":["# the following snippet is saved as `run_distributed.py`\n","import torch\n","import torch.distributed as dist\n","import torch.multiprocessing as mp\n","\n","from diffusers import DiffusionPipeline\n","\n","sd = DiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True,\n",")\n","\n","\n","def run_inference(rank, world_size):\n","    dist.init_process_group('nccl', rank=rank, world_size=world_size)\n","    # `init_process_group` handles creating a distributed envrionment with\n","    # the type of backend to use, the `rank` of the current process, and\n","    # the `world_size` or the number of processes participating.\n","\n","    sd.to(rank)\n","\n","    if torch.distributed.get_rank() == 0:\n","        prompt = 'a dog'\n","    elif torch.distributed.get_rank() == 1:\n","        prompt = 'a cat'\n","\n","    image = sd(prompt).images[0]\n","    image.save(f\"./{'_'.join(prompt)}.png\")\n","\n","\n","def main():\n","    # call `mp.spawn` to run the distributed inference\n","    world_size = 2 # 2 gpus\n","    mp.spawn(\n","        run_inference,\n","        args=(world_size,),\n","        nprocs=world_size,\n","        join=True,\n","    )\n","\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"h-FIt1Sjbod4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now the inference script is completed, we can use the `--nproc_per_node` argument to specify the number of GPUs to use and call `torchrun` to run the script:\n","```bash\n","torchrun run_distributed.py --nproc_per_node=2\n","```"],"metadata":{"id":"5oH1-87hfV_J"}},{"cell_type":"markdown","source":["## Model sharding"],"metadata":{"id":"He9kuwVefi7w"}},{"cell_type":"markdown","source":["Modern diffusion systems such as `Flux` are very large and have multiple models. For example, `Flux.1-Dev` is made up of two text encoders - `T5-XXL` and `CLIP-L`, a diffusion transformer, and a VAE. With a model this size, it can be challenging to run inference on consumer GPUs.\n","\n","Model sharding is a technique that distributes models aross GPUs when the models do not fit on a single GPU. The example below assumes two 16GB GPUs are available for inference.\n","\n","Start by computing the text embedings with the text encoders. Keep the text encoders on two GPUs by setting `device_map=\"balanced\"`. The `balanced` strategy evenly distributes the model on all available GPUs. Use the `max_memory` parameter to allocate the maximum amount of memory for each text encoder on each GPU."],"metadata":{"id":"fPNRx1uVmc0l"}},{"cell_type":"code","source":["from diffusers import FluxPipeline\n","import torch\n","\n","prompt = 'a photo of a dog with cat-like look'\n","\n","pipeline = FluxPipeline.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    transformers=None,\n","    vae=None,\n","    device_map='balanced',\n","    max_memory={0: \"16GB\", 1: \"16GB\"},\n","    torch_dtype=torch.float16,\n",")\n","\n","with torch.no_grad():\n","    print('Encoding prompts')\n","    prompt_embeds, pooled_prompt_embeds, text_ids = pipeline.encode_prompt(\n","        prompt=prompt,\n","        prompt_2=None,\n","        max_sequence_length=512,\n","    )"],"metadata":{"id":"a-MrSelQfd3B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once the text embeddings are computed, remove them from the GPU to make space for the diffusion transformer:"],"metadata":{"id":"pHbOzqBLnq28"}},{"cell_type":"code","source":["import gc\n","\n","def flush():\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    torch.cuda.reset_max_memory_allocated()\n","    torch.cuda.reset_peak_memory_stats()\n","\n","\n","del pipeline.text_encoder\n","del pipeline.text_encoder_2\n","del pipeline.tokenizer\n","del pipeline.tokenizer_2\n","del pipeline\n","\n","flush()"],"metadata":{"id":"m9SCe00fnwQS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the diffusion transformer next which has 12.5B parameters. This time, set `device_map=\"auto\"` to automatically distribute the model across two 16GB GPUs. The `auto` strategy is backed by Acclereate and available as a part of the Big Model Inference feature."],"metadata":{"id":"IJqMLOjPoAne"}},{"cell_type":"code","source":["from diffusers import FluxTransformer2DModel\n","import torch\n","\n","transformer = FluxTransformer2DModel.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    subfolder='transformer',\n","    device_map='audo',\n","    torch_dtype=torch.float16,\n",")"],"metadata":{"id":"rBJrT9E9oQCs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipeline.hf_device_map"],"metadata":{"id":"B0FpRpO2oc8e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transformer.hf_device_map"],"metadata":{"id":"jHGkOZIlof4X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Add the transformer model the pipeline for denoising, but set the other model-level components like the text encoders and VAE to `None` because we do not need them yet."],"metadata":{"id":"-HS8tevzohjH"}},{"cell_type":"code","source":["pipeline = FluxPipeline.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    text_encoder=None,\n","    text_encoder_2=None,\n","    tokenizer=None,\n","    tokenizer_2=None,\n","    vae=None,\n","    transformer=transformer,\n","    torch_dtype=torch.float16,\n",")\n","\n","print('Running denoising...')\n","height, width = 768, 1360\n","latents = pipeline(\n","    prompt_embeds=prompt_embeds,\n","    pooled_prompt_embeds=pooled_prompt_embeds,\n","    num_inference_steps=50,\n","    guidance_scale=3.5,\n","    height=height,\n","    width=width,\n","    output_type='latent',\n",").images"],"metadata":{"id":"CecqpiitopHd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Remove the pipeline and transformer from memory as they're no longer needed."],"metadata":{"id":"uG71ypD1pA0_"}},{"cell_type":"code","source":["del pipeline.transformer\n","del pipeline\n","\n","flush()"],"metadata":{"id":"Ac7HN0-MpEC-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, decode the latents with the VAE into an image. The VAE is typically small enough to be loaded on a single GPU."],"metadata":{"id":"99-kCZ0cpG65"}},{"cell_type":"code","source":["from diffusers import AutoencoderKL\n","from diffusers.image_processor import VaeImageProcessor\n","import torch\n","\n","vae = AutoencoderKL.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    subfolder='vae',\n","    torch_dtype=torch.bfloat16,\n",").to('cuda')\n","vae_scale_factor = 2 ** (len(vae.config.block_out_channels))\n","image_processor = VaeImageProcessor(vae_scale_factor=vae_scale_factor)\n","\n","with torch.no_grad():\n","    print('Running decoding...')\n","    latents = FluxPipeline._unpack_latents(\n","        latents,\n","        height,\n","        width,\n","        vae_scale_factor,\n","    )\n","    latents = (latents / vae.config.scaling_factor) + vae.config.shift_factor\n","\n","    image = vae.decode(latents, return_dict=False)[0]\n","    image = image_processor.postprocess(image, output_type='pil')\n","    image[0].save('split_transformer.png')"],"metadata":{"id":"bu6dCkLApNkb"},"execution_count":null,"outputs":[]}]}