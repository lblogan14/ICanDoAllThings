{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPtabdE5vVCDMiwNDKSMiaW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Wuerstchen"],"metadata":{"id":"UdsIzHqxuNWn"}},{"cell_type":"markdown","source":["The **Wuerstchen** model reduces computational costs by compressing the latent space by 42x, without compromising image quality and accelerating inference.\n","\n","During training, Wuerstchen uses two models (VQGAN + autoencoder) to compress the latents, and then a third model (text-conditioned latent diffusion model) is conditioned on this highly compressed space to generate an image.\n","\n","We will explores the [`train_text_to_image_prior.py`](https://github.com/huggingface/diffusers/tree/wuerschten-tests/examples/wuerstchen/text_to_image) script to train our custom Wuerstchen model.\n","\n","As always, make sure to install the `diffusers` library from source:\n","```bash\n","git clone https://github.com/huggingface/diffusers\n","git checkout wuerschten-tests\n","cd diffusers\n","pip install .\n","```\n","\n","We need to navigate to the following folder and install the corresponding dependencies:\n","```bash\n","cd examples/wuerstchen/text_to_image\n","pip install -r requirements.txt\n","```\n","HuggingFace Accelerate is also helpful to train on multiple GPUs with mixed-precision.\n","```bash\n","pip install accelerate\n","```\n","\n","Now we can initialize a HuggingFace Accelerate environment\n","```bash\n","accelerate config\n","```\n","To set up a default Acclerate environment without choosing any configurations:\n","```bash\n","accelerate config default\n","```\n","Or if our environment does not support an interactive shell like a notebook, we can use:\n","```python\n","from accelerate.utils import write_basic_config\n","write_basic_config()\n","```"],"metadata":{"id":"S2-o_uKiHjRU"}},{"cell_type":"markdown","source":["## Script parameters"],"metadata":{"id":"e4yJgV7JKU4c"}},{"cell_type":"markdown","source":["The training script provides many parameters to customize the training run. We can find all of the parameters and their descriptions in the `parse_args()` function.\n","\n","To speed up training with mixed precision using the `fp16` format, add the `--mixed_precision` parameter to the training command:\n","```bash\n","accelerate launch train_text_to_image.py --mixed_precision=\"fp16\"\n","```\n","Most of the parameters are identical to the parameters in the **Text-to-image** training guide."],"metadata":{"id":"GJlrXK_ZKcLA"}},{"cell_type":"markdown","source":["## Training script"],"metadata":{"id":"neu7LcsdKdCQ"}},{"cell_type":"markdown","source":["The training script is similar to the Text-to-image training guide, but it has been modified to support Wuerstchen.\n","\n","The `main()` function starts by initializing the image encoder - an **EfficientNet** -- in addition to the usual scheduler and tokenizer:\n","```python\n","    def deepspeed_zero_init_disabled_context_manager():\n","        \"\"\"\n","        returns either a context list that includes one that will disable zero.Init or an empty context list\n","        \"\"\"\n","        deepspeed_plugin = AcceleratorState().deepspeed_plugin if is_initialized() else None\n","        if deepspeed_plugin is None:\n","            return []\n","\n","        return [deepspeed_plugin.zero3_init_context_manager(enable=False)]\n","\n","    weight_dtype = torch.float32\n","    if accelerator.mixed_precision == \"fp16\":\n","        weight_dtype = torch.float16\n","    elif accelerator.mixed_precision == \"bf16\":\n","        weight_dtype = torch.bfloat16\n","    with ContextManagers(deepspeed_zero_init_disabled_context_manager()):\n","        pretrained_checkpoint_file = hf_hub_download(\"dome272/wuerstchen\", filename=\"model_v2_stage_b.pt\")\n","        state_dict = torch.load(pretrained_checkpoint_file, map_location=\"cpu\")\n","        image_encoder = EfficientNetEncoder()\n","        image_encoder.load_state_dict(state_dict[\"effnet_state_dict\"])\n","        image_encoder.eval()\n","\n","        text_encoder = CLIPTextModel.from_pretrained(\n","            args.pretrained_prior_model_name_or_path, subfolder=\"text_encoder\", torch_dtype=weight_dtype\n","        ).eval()\n","```\n","\n","We will also load the `WuerstchenPrior` model for optimization\n"," ```python\n","    # load prior model\n","    prior = WuerstchenPrior.from_pretrained(args.pretrained_prior_model_name_or_path, subfolder=\"prior\")\n","```\n","Then we apply some transforms to the images and tokenize the captions\n","```python\n","    # Preprocessing the datasets.\n","    # We need to tokenize input captions and transform the images\n","    def tokenize_captions(examples, is_train=True):\n","        captions = []\n","        for caption in examples[caption_column]:\n","            if isinstance(caption, str):\n","                captions.append(caption)\n","            elif isinstance(caption, (list, np.ndarray)):\n","                # take a random caption if there are multiple\n","                captions.append(random.choice(caption) if is_train else caption[0])\n","            else:\n","                raise ValueError(\n","                    f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n","                )\n","        inputs = tokenizer(\n","            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n","        )\n","        text_input_ids = inputs.input_ids\n","        text_mask = inputs.attention_mask.bool()\n","        return text_input_ids, text_mask\n","\n","    effnet_transforms = transforms.Compose(\n","        [\n","            transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),\n","            transforms.CenterCrop(args.resolution),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n","        ]\n","    )\n","\n","    def preprocess_train(examples):\n","        images = [image.convert(\"RGB\") for image in examples[image_column]]\n","        examples[\"effnet_pixel_values\"] = [effnet_transforms(image) for image in images]\n","        examples[\"text_input_ids\"], examples[\"text_mask\"] = tokenize_captions(examples)\n","        return examples\n","```\n","Finally, the training loop handles compressing the images to latent space with the `EfficientNetEncoder`, adding noise to the latents, and predicting the noise residual with the `WuerstchenPrior` model."],"metadata":{"id":"7xZYurGwKfs_"}},{"cell_type":"markdown","source":["## Launch the script"],"metadata":{"id":"iCCJBbw_LppJ"}},{"cell_type":"markdown","source":["```bash\n","export DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n","\n","accelerate launch  train_text_to_image_prior.py \\\n","  --mixed_precision=\"fp16\" \\\n","  --dataset_name=$DATASET_NAME \\\n","  --resolution=768 \\\n","  --train_batch_size=4 \\\n","  --gradient_accumulation_steps=4 \\\n","  --gradient_checkpointing \\\n","  --dataloader_num_workers=4 \\\n","  --max_train_steps=15000 \\\n","  --learning_rate=1e-05 \\\n","  --max_grad_norm=1 \\\n","  --checkpoints_total_limit=3 \\\n","  --lr_scheduler=\"constant\" \\\n","  --lr_warmup_steps=0 \\\n","  --validation_prompts=\"A robot naruto, 4k photo\" \\\n","  --report_to=\"wandb\" \\\n","  --push_to_hub \\\n","  --output_dir=\"wuerstchen-prior-naruto-model\"\n","```"],"metadata":{"id":"U2JqdIAfLtnf"}},{"cell_type":"markdown","source":["Once training is complete,"],"metadata":{"id":"M9jrnPdYLwfs"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQd4TYgVuLBV"},"outputs":[],"source":["from diffusers.pipelines.wuerstchen import DEFAULT_STAGE_C_TIMESTEPS\n","from diffusers import AutoPipelineForText2Image\n","import torch\n","\n","pipeline = AutoPipelineForText2Image.from_pretrained(\n","    'path/to/our/cusotm/wuerstchen/model',\n","    torch_dtype=torch.float16\n",")"]},{"cell_type":"code","source":["caption = 'a cute bird naruto holding a shield'\n","images = pipeline(\n","    caption,\n","    width=1024,\n","    height=1536,\n","    prior_timesteps=DEFAULT_STAGE_C_TIMESTEPS,\n","    prior_guidance_scale=4.0,\n","    num_images_per_prompt=2,\n",").images[0]\n","image"],"metadata":{"id":"Ft6Ub5EsMFX5"},"execution_count":null,"outputs":[]}]}