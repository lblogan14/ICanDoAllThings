{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOjQ1Vu88jkhiBfVZCi1cqO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# How to Fine-Tune Multimodal Models or VLMs with HuggingFace TRL"],"metadata":{"id":"1z4_OrEX3QEn"}},{"cell_type":"markdown","source":["The Vision-Language Models (VLMs) can handle a variety of multimodal tasks, including image captioning, visual question answering, and image-text matching without additional training. However, to customize a model for our specific application, we may need to fine-tune it on our data to achieve higher quality results or to create a more efficient model for our use case."],"metadata":{"id":"qJRAXIkf3Us6"}},{"cell_type":"markdown","source":["## Define our multimodal use case"],"metadata":{"id":"LMsuxqP66q71"}},{"cell_type":"markdown","source":["For most use case, fine-tuning may not be the first option. We should evaluate pretrained models or API-based solutions before committing to fine-tuning our own model.\n","\n","In this example, we want to fine-tune a model that can generate detailed product descriptions based on product images and basic metadata. This model will be integrated into our e-commerce platform to help sellers create more compelling listings. The goal is to reduce the time it takes to create product descriptions and improve their quality and consistency.\n","\n","Existing models may already be good for this use case, but we may want to tweak/tune it to our specific needs. This image-to-text generation task is well-suited for fine-tuning VLMs, as it requires understanding visual features and combining them with textual information to produce coherent and relevant descriptions.\n","\n","We will use the [`amazon-product-descriptions-vlm`](https://huggingface.co/datasets/philschmid/amazon-product-descriptions-vlm) dataset for fine-tuning."],"metadata":{"id":"cwk-wtzM97AS"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"DEGPLdfeAVqf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KIawr2EG3HVI"},"outputs":[],"source":["# Install Pytorch & other libraries\n","%pip install \"torch==2.4.0\" tensorboard pillow\n","\n","# Install Hugging Face libraries\n","%pip install  --upgrade \\\n","  \"transformers==4.45.1\" \\\n","  \"datasets==3.0.1\" \\\n","  \"accelerate==0.34.2\" \\\n","  \"evaluate==0.4.3\" \\\n","  \"bitsandbytes==0.44.0\" \\\n","  \"trl==0.11.1\" \\\n","  \"peft==0.13.0\" \\\n","  \"qwen-vl-utils\""]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","notebook_login()"],"metadata":{"id":"PJb_pJ34AYXS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create and prepare dataset"],"metadata":{"id":"QRqvNeIlAa25"}},{"cell_type":"markdown","source":["The [`amazon-product-description-vlm`](https://huggingface.co/datasets/philschmid/amazon-product-descriptions-vlm) dataset contains 1350 amazon products with title, images, and descriptions and metadata. We want to fine-tune our model to generate product descriptions based on the images, title, and metadata. Therefore, we need to create a prompt including the title, metadata, and image, and the completion is the description from the model.\n","\n","A typical conversational dataset format would look like:\n","```python\n","messages = [\n","    {'role': 'system', 'content': [{'type': 'text', 'text': 'You are a helpful..'}]},\n","    {'role': 'user', 'content': [\n","        {'type': 'text', 'text': 'How many dogs are in this image?',\n","        {'type': 'image', 'image': <PIL.Image>}\n","    ]},\n","    {'role': 'assistant', 'content': [{'type': 'text', 'text': 'There are 3 dogs in the image.'}]}\n","]\n","```"],"metadata":{"id":"cntvTH_HBlN6"}},{"cell_type":"code","source":["# Note the image is nor provided in the prompt, its included as part of the `processor`\n","prompt = \"\"\"Create a short product description based on the provided ##PRODUCT NAME## and ##CATEGORY## and image.\n","Only return description. The description should be SEO optimized and for a better mobile search experience.\n","\n","##PRODUCT NAME##: {product_name}\n","##CATEGORY##: {category}\"\"\"\n","\n","system_message = \"You are an expert product description writer for Amazon.\""],"metadata":{"id":"TMpTgShcAcZh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We also need to format our dataset to the conversational format:"],"metadata":{"id":"zGCMhbfWESbF"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","# convert dataset to OpenAI messages\n","def format_data(sample):\n","    return {\n","        \"messages\": [\n","            {\n","                'role': 'system',\n","                'content': [{'type': 'text', 'text': system_message}]\n","            },\n","            {\n","                'role': 'user',\n","                'content': [\n","                    {\n","                        'type': 'text',\n","                        'text': prompt.format(product_name=sample['Product Name'], category=sample['Category'])\n","                    },\n","                    {\n","                        'type': 'image',\n","                        'image': sample['image']\n","                    }\n","                ]\n","            },\n","            {\n","                'role': 'assistant',\n","                'content': [{'type': 'text', 'text': sample['description']}]\n","            }\n","        ]\n","    }\n","\n","\n","# load dataset\n","dataset_id = 'philschmid/amazon-product-descriptions-vlm'\n","dataset = load_dataset(\n","    dataset_id,\n","    split='train'\n",")\n","\n","# Convert format\n","# Need to use list comprehension to keep PIL.Image type; `.map()` convert image to bytes\n","dataset = [format_data(sample) for sample in dataset]"],"metadata":{"id":"ZyXb1FVOEWZs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset"],"metadata":{"id":"ECQdfG5VFcHJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset[0]"],"metadata":{"id":"KRD3cQi9FdWz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset[0]['messages']"],"metadata":{"id":"DYE99OShFfhG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fine-tune VLM using `trl` and the `SFTTrainer`"],"metadata":{"id":"fkoWac-iFhht"}},{"cell_type":"markdown","source":["The `SFTTrainer` is straightforward to supervise fine-tune open LLMs and VLMs.\n","\n","We will also use Q-LoRA to reduce the memory footprint during finetuning, without sacrificing performance by using quantization.\n","\n","We will use `Qwen-2-7B` model for fine-tuning in this example, but other models are welcomed as well."],"metadata":{"id":"qDKpilElFy7S"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoProcessor, AutoModelForVision2Seq, BitsAndBytesConfig\n","\n","model_id = 'Qwen/Qwen2-VL-7B-Instruct'\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type='nf4',\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","# load model and tokenizer\n","processor = AutoProcessor.from_pretrained(model_id)\n","model = AutoModelForVision2Seq.from_pretrained(\n","    model_id,\n","    device_map='auto',\n","    torch_dtype=torch.bfloat16,\n","    quantization_config=bnb_config\n",")"],"metadata":{"id":"GHnj8UyWFwNM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prepare for inference\n","text = processor.apply_chat_template(\n","    dataset[0]['messages'],\n","    tokenize=False,\n","    add_generation_prompt=False\n",")\n","text"],"metadata":{"id":"aarrDud_GuMs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from peft import LoraConfig\n","\n","peft_config = LoraConfig(\n","    lora_alpha=16,\n","    lora_dropout=0.05,\n","    r=8,\n","    bias='none',\n","    target_modules=['q_proj', 'v_proj'],\n","    task_type='CAUSAL_LM'\n",")"],"metadata":{"id":"VQoEWYMwG3tZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we need to create a custom `DataCollator` which formates the inputs correctly and include the image features. We will use the `process_vision_info` method from a utility package the Qwen2 team provides. If we want to use other models, we may need to check if this works."],"metadata":{"id":"ThWVyyVvIdft"}},{"cell_type":"code","source":["from trl import SFTConfig\n","from transformers import Qwen2VLProcessor\n","from qwen_vl_utils import process_vision_info\n","\n","\n","training_args = SFTConfig(\n","    output_dir='qwen2-7b-instruct-amazon-description',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=4, # batch size per device\n","    gradient_accumulation_steps=8, # number of steps before performing a backward pass\n","    gradient_checkpointing=True\n","    optim='adamw_torch_fused',\n","    logging_steps=5,\n","    save_strategy='epoch',\n","    learning_rate=2e-4,\n","    bf16=True,\n","    tf32=True,\n","    max_grad_norm=0.3,\n","    warmup_ratio=0.03,\n","    lr_scheduler_type='constant',\n","    push_to_hub=False,\n","    report_to='tensorboard',\n","    gradient_checkpointing_kwargs={'use_reentrant': False},  # use reentrant checkpointing\n","    dataset_text_field=\"\",  # need a dummy field for collator\n","    dataset_kwargs={'skip_prepare_dataset': True}\n","    remove_unused_columns=False\n",")"],"metadata":{"id":"kNG6bzu2IySz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a data collator to encode text and image pairs\n","def collate_fn(examples):\n","    # get the texts and images, and apply the chat template\n","    texts = [\n","        processor.apply_chat_template(example['message'], tokenize=False)\n","        for example in examples\n","    ]\n","    image_inputs = [\n","        process_vision_info(example['messages'])[0]\n","        for example in examples\n","    ]\n","\n","    # tokenize the texts and process the images\n","    batch = processor(\n","        text=texts,\n","        images=image_inputs,\n","        padding=True,\n","        return_tensors='pt'\n","    )\n","\n","    # The labels are the input_ids,\n","    # and we need to mask the padding tokens in the loss computation\n","    labels = batch['input_ids'].clone()\n","    labels[labels == processor.tokenizer.pad_token_id] = -100\n","\n","    # ignore the image token index in the loss computation (model specific)\n","    if isinstance(processor, Qwen2VLProcessor):\n","        image_tokens = [151652,151653,151655]\n","    else:\n","        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]\n","\n","    for image_token_id in image_tokens:\n","        labels[labels == image_token_id] = -100\n","\n","    batch['labels'] = labels\n","\n","    return batch"],"metadata":{"id":"BSOQZCPxJyB9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from trl import SFTTrainer\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset,\n","    data_collator=collate_fn,\n","    dataset_text_field=\"\", # need dummy value\n","    peft_config=peft_config,\n","    tokenizer=processor.tokenizer\n",")"],"metadata":{"id":"35b5w1hSKq0M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()\n","trainer.save_model(training_args.output_dir)"],"metadata":{"id":"kZKOktrFK1P7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["del model\n","del trainer\n","torch.cuda.empty_cache()"],"metadata":{"id":"CkgxxoujK5-D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test model and run inference"],"metadata":{"id":"f1j2wwb_K6az"}},{"cell_type":"markdown","source":["We will first load the base model and let it generate a description for a random amazon product, and then we will load our Q-LoRA adapted model and let it generate a description for the same product.\n","\n","Finally we can merge the adapter into the base model to make it more efficient and run inference on the same product again."],"metadata":{"id":"xnCa_09lM_DB"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoProcessor, AutoModelForVision2Seq\n","\n","adapter_path = './qwen2-7b-instruct-amazon-description'\n","\n","# load model\n","processor = AutoProcessor.from_pretrained(model_id)\n","model = AutoModelForVision2Seq.from_pretrained(\n","    model_id,\n","    device_map='auto',\n","    torch_dtype=torch.float16\n",")"],"metadata":{"id":"cAvJ9iB4K7yj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Select a random product from amazon and prepared a `generated_description` function to generate a description for the product"],"metadata":{"id":"ZLivLTwQNa_D"}},{"cell_type":"code","source":["from qwen_vl_utils import process_vision_info\n","\n","# sample from amazon\n","sample = {\n","  \"product_name\": \"Hasbro Marvel Avengers-Serie Marvel Assemble Titan-Held, Iron Man, 30,5 cm Actionfigur\",\n","  \"catergory\": \"Toys & Games | Toy Figures & Playsets | Action Figures\",\n","  \"image\": \"https://m.media-amazon.com/images/I/81+7Up7IWyL._AC_SY300_SX300_.jpg\"\n","}\n","\n","# prepare message\n","messages = [{\n","        \"role\": \"user\",\n","        \"content\": [\n","            {\n","                \"type\": \"image\",\n","                \"image\": sample[\"image\"],\n","            },\n","            {\"type\": \"text\", \"text\": prompt.format(product_name=sample[\"product_name\"], category=sample[\"catergory\"])},\n","        ],\n","    }\n","]"],"metadata":{"id":"KkcbhnIuNiwf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_description(sample, model, processor):\n","    messages = [\n","        {'role': 'system', 'content': [{'type': 'text', 'text': system_message}]},\n","        {'role': 'user', 'content': [\n","            {'type': 'text', 'text': prompt.format(product_name=sample['product_name'], category=sample['catergory'])},\n","            {'type': 'image', 'image': sample['image']}\n","        ]}\n","    ]\n","\n","    # prepare for inference\n","    text = processor.apply_chat_template(\n","        messages,\n","        tokenize=False,\n","        add_generation_prompt=True\n","    )\n","\n","    image_inputs, video_inputs = process_vision_info(messages)\n","    inputs = processor(\n","        text=[text],\n","        images=image_inputs,\n","        padding=True,\n","        return_tensors='pt'\n","    ).to(model.device)\n","\n","    # Inference: generation of the output\n","    generated_ids = model.generate(\n","        **inputs,\n","        max_new_tokens=256,\n","        top_p=1.0,\n","        do_sample=True,\n","        temperature=0.8\n","    )\n","    generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs['input_ids'], generated_ids)]\n","    output_text = processor.batch_decode(\n","        generated_ids_trimmed,\n","        skip_special_tokens=True,\n","        clean_up_tokenization_spaces=False\n","    )\n","    return output_text[0]"],"metadata":{"id":"m8OYKk_DNqIt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_description = generate_description(sample, model, processor)\n","base_description"],"metadata":{"id":"rVsI6SJtO57x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now load our adapter"],"metadata":{"id":"lNBkYYaGO8pE"}},{"cell_type":"code","source":["model.load_adapter(adapter_path)\n","\n","ft_description = generate_description(sample, model, processor)\n","ft_description"],"metadata":{"id":"sZ_ELGtnP5KQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from IPython.display import display, HTML\n","\n","def compare_generations(base_gen, ft_gen):\n","    df = pd.DataFrame({\n","        'Base Generation': [base_gen],\n","        'Fine-tuned Generation': [ft_gen]\n","    })\n","\n","    styled_df = df.style.set_properties(**{\n","        'text-align': 'left',\n","        'white-space': 'pre-wrap',\n","        'border': '1px solid black',\n","        'padding': '10px',\n","        'width': '250px',\n","        'overflow-wrap': 'break-word'\n","    })\n","\n","    display(HTML(styled_df.to_html()))\n","\n","\n","compare_generations(base_description, ft_description)"],"metadata":{"id":"rX53QYv1O-4C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Merge LoRA adapter into the original model"],"metadata":{"id":"JUoaVM1XQD0J"}},{"cell_type":"markdown","source":["When using Q-LoRA, we only train adapters and not the full model. This means when saving the model during training we only save the adapter weights and not the full model. If we want to save the full model, which makes it easier to use with text generation inference, we can merge the adapter weights into the model weights using the `merge_and_unload` method and then save the model with the `save_pretrained` method."],"metadata":{"id":"wGOIJivkQMFX"}},{"cell_type":"code","source":["from peft import PeftModel\n","from transformers import AutoProcessor, AutoModelForVision2Seq\n","\n","adapter_path = \"./qwen2-7b-instruct-amazon-description\"\n","base_model_id = \"Qwen/Qwen2-VL-7B-Instruct\"\n","merged_path = \"merged\"\n","\n","# load base model\n","processor = AutoProcessor.from_pretrained(base_model_id)\n","model = AutoModelForVision2Seq.from_pretrained(\n","    model_id,\n","    low_cpu_mem_usage=True\n",")\n","\n","# merge lora\n","peft_model = PeftModel.from_pretrained(\n","    model,\n","    adapter_path\n",")\n","merged_model = peft_model.merge_and_unload()\n","# save merged model\n","merged_model.save_pretrained(\n","    merged_path,\n","    safe_serialization=True,\n","    max_shard_size='2GB'\n",")\n","\n","# don't forget to save processor to the merged model path\n","processor.save_pretrained(merged_path)"],"metadata":{"id":"qBWY8hB5QGQq"},"execution_count":null,"outputs":[]}]}