{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN6XCoxLjFHwTs1RhXOmqEC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -qU datasets trl peft bitsandbytes sentencepiece wandb"],"metadata":{"id":"1SH4HB4d6uJD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fine-tune a Mistral-7B model with DPO"],"metadata":{"id":"2mAts0626dxR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"91SmdLi76T-h"},"outputs":[],"source":["import os\n","import gc\n","import torch\n","import transformers\n","from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n","from datasets import load_dataset\n","from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n","from trl import DPOTrainer\n","import bitsandbytes as bnb\n","import wandb\n","\n","from google.colab import userdata\n","\n","# setups\n","hf_token = userdata.get('HF_TOKEN')\n","wb_token = userdata.get('WB_TOKEN')\n","wandb.login(key=wb_token)\n","\n","model_name = 'teknium/OpenHermes-2.5-Mistral-7B'\n","new_model = 'NeuralHermes-2.5-Mistral-7B'"]},{"cell_type":"code","source":["# tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = 'left'\n","\n","# model to fine-tune\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    torch_dtype=torch.float16,\n","    load_in_4_bit=True\n",")"],"metadata":{"id":"O3LJmwsd9HX9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Format dataset"],"metadata":{"id":"av6ZOQfF76h7"}},{"cell_type":"code","source":["def chatml_format(example):\n","    # Format system\n","    if len(example['system']) > 0:\n","        message = {'role': 'system', 'content': example['system']}\n","        system = tokenizer.apply_chat_template([message], tokenize=False)\n","    else:\n","        system = \"\"\n","\n","    # Format instruction\n","    message = {'role': 'user', 'content': example['question']}\n","    prompt = tokenizer.apply_chat_template(\n","        [message],\n","        tokenize=False,\n","        add_generation_prompt=True # add `<|assistant|>` to indicate the start of a meesage\n","    )\n","\n","    # Format chosen answer\n","    chosen = example['chosen'] + \"<|im_end|>\\n\"\n","\n","    # Format rejected answer\n","    rejected = example['rejected'] + \"<|im_end|>\\n\"\n","\n","    return {\n","        'prompt': system + prompt,\n","        'chosen': chosen,\n","        'rejected': rejected\n","    }\n","\n","\n","# load dataset\n","dataset = load_dataset('Intel/orca_dpo_pairs')['train']\n","\n","# save columns\n","original_columns = dataset.column_names\n","\n","# Format dataset\n","dataset = dataset.map(\n","    chatml_format,\n","    remove_columns=original_columns\n",")"],"metadata":{"id":"B0xjjvk877u2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset[0]"],"metadata":{"id":"3llvavtY9XiH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train model with DPO"],"metadata":{"id":"beQMe1MI9ZOj"}},{"cell_type":"code","source":["# LoRA config\n","peft_config = LoraConfig(\n","    r=16,\n","    lora_alpha=16,\n","    lora_dropout=0.05,\n","    bias='none',\n","    task_type='CAUSAL_LM',\n","    target_modules=['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",")\n","\n","# Training arguments\n","training_args = TrainingArguments(\n","    per_device_train_batch_size=4,\n","    gradient_accumulation_steps=4,\n","    gradient_checkpointing=True,\n","    learning_rate=5e-5,\n","    lr_scheduler_type='cosine',\n","    max_steps=200,\n","    save_strategy='no',\n","    logging_steps=1,\n","    output_dir=new_model,\n","    optim='paged_adamw_32bit',\n","    warmup_steps=100,\n","    bf16=True,\n","    report_to='wandb'\n",")\n","\n","# Create DPO trainer\n","dpo_trainer = DPOTrainer(\n","    model,\n","    args=training_args,\n","    train_dataset=dataset,\n","    tokenizer=tokenizer,\n","    peft_config=peft_config,\n","    beta=0.1,\n","    max_prompt_length=1024\n","    max_length=1536\n",")"],"metadata":{"id":"23aIwOQu9ak3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dop_trainer.train()"],"metadata":{"id":"-qTVeGjl-U2o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Save model"],"metadata":{"id":"DKId0zAm-YY1"}},{"cell_type":"code","source":["dpo_trainer.model.save_pretrained('final_checkpoint')\n","tokenizer.save_pretrained('final_checkpoint')\n","\n","# flush memory\n","del dpo_trainer, model\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","# reload model in fp16 (instead of nf4)\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    return_dict=True,\n","    torch_dtype=torch.float16\n",")\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Merge base model with the adapter\n","model = PeftModel.from_pretrained(base_model, 'final_checkpoint')\n","model = model.merge_and_unload()\n","\n","# Save model and tokenizer\n","model.save_pretrained(new_model)\n","tokenizer.save_pretrained(new_model)\n","\n","# upload to the hub\n","model.push_to_hub(new_model, use_temp_dir=False, token=hf_token)\n","tokenizer.push_to_hub(new_model, use_temp_dir=False, token=hf_token)"],"metadata":{"id":"wxeBMOmZ-Z18"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Inference"],"metadata":{"id":"plipIHYX_BFX"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(new_model)\n","pipeline = transformers.pipeline(\n","    'text-generation',\n","    model=new_model,\n","    tokenizer=tokenizer\n",")"],"metadata":{"id":"iJLzO15Y_CD5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["message = [\n","    {'role': 'system', 'content': \"You are a helpful assistant chatbot.\"},\n","    {'role': 'user', 'content': 'What is a Large Language Model?'}\n","]\n","prompt = tokenizer.apply_chat_template(\n","    message,\n","    tokenize=False,\n","    add_generation_prompt=True\n",")\n","\n","sequences = pipeline(\n","    prompt,\n","    do_smaple=True,\n","    temperature=0.7,\n","    top_p=0.9,\n","    num_return_sequences=1,\n","    max_length=200\n",")\n","print(sequences[0]['generated_text'])"],"metadata":{"id":"2xTALKW9_OCv"},"execution_count":null,"outputs":[]}]}