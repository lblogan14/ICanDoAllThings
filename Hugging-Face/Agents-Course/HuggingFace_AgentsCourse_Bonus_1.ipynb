{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning an LLM for Function-Calling"
      ],
      "metadata": {
        "id": "RNYwwh8y1yHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rather than relying only on prompt-based approaches, function calling trains our model to **take actions and interpret observations during the training phase**, making our AI more robust.\n",
        "\n",
        "**Function-calling** is a way for **an LLM to take actions on its environment**. Just like the tools of an Agent, function-calling gives the model the capacity to **take an action on its environment**. However, the function calling capacity **is learned by the model**, and relies **less on prompting than other agent techniques**.\n",
        "\n",
        "For regular agent setup, the Agent did not learn to use the tools, we just provided the list, and we relied on the fact that the model **was able to generalize on defining a plan using these tools**.\n",
        "\n",
        "**With function-calling, the Agent is fine-tuned (trained) to use tools**.\n",
        "\n",
        "\n",
        "In general agent workflow, once the user has given some tools to the agent and prompted it with a query, the model will cycle through:\n",
        "- *Think* - What action(s) it needs to take in order to fulfill the objective.\n",
        "- *Act* - Format the action with the correct parameter and stop the generation.\n",
        "- *Observe* - Get back the result from the execution.\n",
        "\n",
        "\n",
        "In a typical conversation with a model through an API, the conversation will alternate between user and assistant messages like:\n",
        "```python\n",
        "conversation = [\n",
        "    {\"role\": \"user\", \"content\": \"I need help with my order\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"I'd be happy to help. Could you provide your order number?\"},\n",
        "    {\"role\": \"user\", \"content\": \"It's ORDER-123\"},\n",
        "]\n",
        "```\n",
        "\n",
        "Function-calling brings **new roles to the conversation**:\n",
        "- a new role for an **Action**\n",
        "- a new role for an **Observation**\n",
        "\n",
        "For example, in a case of Mistral API,\n",
        "```python\n",
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What's the status of my transaction T1001?\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"\",\n",
        "        \"function_call\": {\n",
        "            \"name\": \"retrieve_payment_status\",\n",
        "            \"arguments\": \"{\\\"transaction_id\\\": \\\"T1001\\\"}\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"tool\",\n",
        "        \"name\": \"retrieve_payment_status\",\n",
        "        \"content\": \"{\\\"status\\\": \\\"Paid\\\"}\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"Your transaction T1001 has been successfully paid.\"\n",
        "    }\n",
        "]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "6hbI29IFm2iY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fune-Tune Model for Function-Calling"
      ],
      "metadata": {
        "id": "2BJE8ghHqbHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A model training process can be divided into 3 steps:\n",
        "1. **The model is pretrained on a large quantity data.** The output of this is a **pretrained model**. For example, [`google/gemma-2-2b`](https://huggingface.co/google/gemma-2-2b) is a base model only knowing to predict the next token without strong instruction-following capabilities.\n",
        "2. To be useful in a chat context, the model needs to be **fine-tuned** to follow instructions. It can be trained by model creators, the open-source community, or anyone. For example, [`google/gemma-2-2b-it`](https://huggingface.co/google/gemma-2-2b-it) is an instruction-tuned model.\n",
        "3. The model can then be **aligned** to the creator's preferences. For example, a customer service chat model that must never be impolite to customers.\n",
        "\n",
        "\n",
        "In this example, we will fine-tune [`google/gemme-2-2b-it`](https://huggingface.co/google/gemma-2-2b-it) to build a function-calling model.\n",
        "\n",
        "Starting from a pretrained model instead of a fine-tuned model would require more training in order to learn instruction following, chat and function-calling."
      ],
      "metadata": {
        "id": "2_wZrp8QqfsU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVzRIX421spX"
      },
      "outputs": [],
      "source": [
        "!pip install -qU bitsandbytes peft trl tensorboardX wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning Gemma2-2b-it"
      ],
      "metadata": {
        "id": "Te82jTM-sTz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "from functools import partial\n",
        "import pandas as pd\n",
        "import torch\n",
        "import json\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
        "from datasets import load_dataset\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "from peft import LoraConfig, TaskType\n",
        "\n",
        "seed = 111\n",
        "set_seed(seed)\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "TrDoQRgCsQRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing the dataset"
      ],
      "metadata": {
        "id": "t1PGjcFnukDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the model, we need to format the inputs into what we want the model to learn.\n",
        "\n",
        "We will enhance a popular dataset for function calling, `\"NousResearch/hermes-function-calling-v1\"` by adding new **thinking** step computer from **deepseek-ai/DeepSeek-R1-Distill-Qwen-32B**. We also need to format the conversation correctly.\n",
        "\n",
        "The default chat template of gemma-2-2B does not contain tool calls, so we need to modify it.\n"
      ],
      "metadata": {
        "id": "fe0YiUIeumkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'google/gemma-2-2b-it'\n",
        "dataset_name = 'Jofthomas/hermes-function-calling-thinking-V1'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.chat_template = \"\"\"\n",
        "{{ bos_token }}\n",
        "{% if messages[0]['role'] == 'system' %}\n",
        "    {{ raise_exception('System role not supported') }}\n",
        "{% endif %}\n",
        "{% for message in messages %}\n",
        "    {{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}\n",
        "{% endfor %}\n",
        "{% if add_generation_prompt %}\n",
        "    {{'<start_of_turn>model\\n'}}\n",
        "{% endif %}\"\"\"\n",
        "\n",
        "def preprocess(sample):\n",
        "    messages = sample['messages']\n",
        "    first_message = messages[0]\n",
        "\n",
        "    # Instead of adding a system message, we merge the content into the first user message\n",
        "    if first_message['role'] == 'system':\n",
        "        system_message_content = first_message['content']\n",
        "        # Merge system content with the first user message\n",
        "        messages[1]['content'] = system_message_content + \"Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\n\" + messages[1][\"content\"]\n",
        "        # Remove the system message from the conversation\n",
        "        messages.pop(0)\n",
        "\n",
        "    return {'text': tokenizer.apply_chat_template(messages, tokenize=False)}\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(dataset_name)\n",
        "dataset = dataset.rename('conversation', 'messages')"
      ],
      "metadata": {
        "id": "8-FlEJZvumNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is why we need a custom dataset, `'Jofthomas/hermes-function-calling-thinking-V1'`, based on a reference dataset `\"NousResearch/hermes-function-calling-v1\"` because the original dataset does not have a \"**thinking**\" step.\n",
        "\n",
        "In function-calling, such a step is optional, but the deepseek model or the paper [\"Test-Time Compute\"](https://huggingface.co/papers/2408.03314) suggests that **giving an LLM time to \"think\" before it answers (or in this case, before taking an action) can significantly improve model performance**."
      ],
      "metadata": {
        "id": "ox-vI7QS1zhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "dxjZBGAUy04_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "8j7LBHD2y1F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(preprocess, remove_columns='messages')\n",
        "dataset = dataset['train'].train_test_split(0.1)\n",
        "dataset"
      ],
      "metadata": {
        "id": "gnPhG8sK2Hrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]['text']"
      ],
      "metadata": {
        "id": "GUmMaUML2PWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.pad_token)\n",
        "print(tokenizer.eos_token)"
      ],
      "metadata": {
        "id": "gZ23jQxM2ScF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see in this example, there are new tokens in our dataset, such as `<think>`, `<tool_call>`, and `<tool_response>`, which the tokenizer does not yet treat them as whole tokens. To ensure the model correctly interprets our new format, we must **add these tokens** to our tokenizer.\n",
        "\n",
        "In addition, we also need to change the `chat_template` to format conversations as messages within a prompt."
      ],
      "metadata": {
        "id": "rtRHZjbMy9UB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatmlSpecialTokens(str, Enum):\n",
        "    tools = \"<tools>\"\n",
        "    eotools = \"</tools>\"\n",
        "    think = \"<think>\"\n",
        "    eothink = \"</think>\"\n",
        "    tool_call = \"<tool_call>\"\n",
        "    eotool_call = \"</tool_call>\"\n",
        "    tool_response = \"<tool_response>\"\n",
        "    eotool_response = \"</tool_response>\"\n",
        "    pad_token = \"<pad>\"\n",
        "    eos_token = \"<eos>\"\n",
        "\n",
        "    @classmethod\n",
        "    def list(cls):\n",
        "        return [c.value for c in cls]\n",
        "\n",
        "\n",
        "# Load a tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    pad_token=ChatmlSpecialTokens.pad_token.value,\n",
        "    additional_special_tokens=ChatmlSpecialTokens.list(),\n",
        "\n",
        ")\n",
        "\n",
        "tokenizer.chat_template = \"\"\"\n",
        "{{ bos_token }}\n",
        "{% if messages[0]['role'] == 'system' %}\n",
        "    {{ raise_exception('System role not supported') }}\n",
        "{% endif %}\n",
        "{% for message in messages %}\n",
        "    {{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}\n",
        "{% endfor %}\n",
        "{% if add_generation_prompt %}\n",
        "    {{'<start_of_turn>model\\n'}}\n",
        "{% endif %}\"\"\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    attn_implementation='eager',\n",
        "    device_map='auto'\n",
        ")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.to(torch.bfloat16)"
      ],
      "metadata": {
        "id": "Ljj-Lv1mzhJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuring LoRA"
      ],
      "metadata": {
        "id": "ohqBnL-v2cC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can define the parameters of LoRA adapter.\n",
        "\n"
      ],
      "metadata": {
        "id": "opQxONWO01mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft improt LoraConfig\n",
        "\n",
        "# r - rank dimension for LoRA update matrices (smaller = more compression)\n",
        "rank_dimension = 16\n",
        "# lora_alpha - scaling factor for LoRA layers (higher = stronger adapation)\n",
        "lora_alpha = 64\n",
        "# lora_dropout - dropout probability for LoRA layers (helps prevent overfitting)\n",
        "lora_dropout = 0.05\n",
        "\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=rank_dimension,\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    target_modules=[\n",
        "        'gate_proj',\n",
        "        'q_proj',\n",
        "        'lm_head',\n",
        "        'o_proj',\n",
        "        'k_proj',\n",
        "        'embed_tokens',\n",
        "        'down_proj',\n",
        "        'up_proj',\n",
        "        'v_proj'\n",
        "    ],\n",
        "    task_type=TaskType.CAUSAL_lm\n",
        ")"
      ],
      "metadata": {
        "id": "YZY8VbUO03pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuring Hyperparameters"
      ],
      "metadata": {
        "id": "QcTIMAIb3Fs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "username = '<hf_username>'\n",
        "output_dir = 'gemma-2-2B-it-thinking-function_calling-v0'\n",
        "per_device_train_batch_size = 1\n",
        "per_device_eval_batch_size = 1\n",
        "gradient_accumulation_steps = 4\n",
        "logging_steps = 5\n",
        "learning_rate = 1e-4\n",
        "\n",
        "max_grad_norm = 1.0\n",
        "num_train_epochs = 1\n",
        "warmup_ratio = 0.1\n",
        "lr_scheduler_type = 'cosine'\n",
        "max_seq_length = 1500\n",
        "\n",
        "\n",
        "training_arguments = SFTConfig(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    save_strategy='no',\n",
        "    eval-strategy='epoch',\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    weight_decay=0.1,\n",
        "    wramup_ratio=warmup_ratio,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to='tensorboard',\n",
        "    bf16=True,\n",
        "    hub_private_repo=False,\n",
        "    push_to_hub=False,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={'use_reentraint': False},\n",
        "    packing=True,\n",
        "    max_seq_length=max_seq_length\n",
        ")"
      ],
      "metadata": {
        "id": "M2A376Oa3JPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test'],\n",
        "    processing_class=tokenizer,\n",
        "    peft_config=peft_config\n",
        ")"
      ],
      "metadata": {
        "id": "2RGnvsWw4TvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "_RdGsSF94bVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing model"
      ],
      "metadata": {
        "id": "uzPzn_1p4d6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "        )\n",
        "\n",
        "peft_model_id = f\"{username}/{output_dir}\" # replace with your newly trained adapter\n",
        "device = \"auto\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
        "                                             device_map=\"auto\",\n",
        "                                             )\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "model.to(torch.bfloat16)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "sXRjdnfm4fNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[\"test\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "9V2geFHH4iIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this prompt is a sub-sample of one of the test set examples. In this example we start the generation after the model generation starts.\n",
        "prompt=\"\"\"<bos><start_of_turn>human\n",
        "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'convert_currency', 'description': 'Convert from one currency to another', 'parameters': {'type': 'object', 'properties': {'amount': {'type': 'number', 'description': 'The amount to convert'}, 'from_currency': {'type': 'string', 'description': 'The currency to convert from'}, 'to_currency': {'type': 'string', 'description': 'The currency to convert to'}}, 'required': ['amount', 'from_currency', 'to_currency']}}}, {'type': 'function', 'function': {'name': 'calculate_distance', 'description': 'Calculate the distance between two locations', 'parameters': {'type': 'object', 'properties': {'start_location': {'type': 'string', 'description': 'The starting location'}, 'end_location': {'type': 'string', 'description': 'The ending location'}}, 'required': ['start_location', 'end_location']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
        "<tool_call>\n",
        "{tool_call}\n",
        "</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\n",
        "\n",
        "Hi, I need to convert 500 USD to Euros. Can you help me with that?<end_of_turn><eos>\n",
        "<start_of_turn>model\n",
        "<think>\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
        "inputs = {k: v.to(\"cuda\") for k,v in inputs.items()}\n",
        "outputs = model.generate(**inputs,\n",
        "                         max_new_tokens=300,# Adapt as necessary\n",
        "                         do_sample=True,\n",
        "                         top_p=0.95,\n",
        "                         temperature=0.01,\n",
        "                         repetition_penalty=1.0,\n",
        "                         eos_token_id=tokenizer.eos_token_id)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "0r3QTZ8Z4kph"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}