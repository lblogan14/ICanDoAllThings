{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPaRskHyVsFU2yoSrW2Umgl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Fine-tuning SmolVLM Using Direct Preference Optimization (DPO) with TRL on a consumer GPU"],"metadata":{"id":"zgzn6cLqPNoB"}},{"cell_type":"markdown","source":["In this example, we will finetune a smol **Vision Language Model (VLM)** with **Direct Preference Optimization (DPO)** using the Transformer Reinforcement Learning (TRL) library on consumer-grade GPUs.\n","\n","We will finetune [`SmolVLM`](https://huggingface.co/blog/smolvlm) using a **preference dataset** to help the model align with desired outputs. The dataset we will use is [`HuggingFaceH4/rlaif-v_formatted`](https://huggingface.co/datasets/HuggingFaceH4/rlaif-v_formatted), which contains pairs of `prompt + image` along with a `chosen` and `rejected` answer for each pair. The goal of this finetuning process is to make the model consistently prefer the `chosen` answer from the dataset, reducing hullucinations."],"metadata":{"id":"2jCnVqsERMfa"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"MjYSzQUqR5Zz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8wXZgDRRPJPJ"},"outputs":[],"source":["!pip install  -U -q transformers trl datasets bitsandbytes peft accelerate\n","# Tested with transformers==4.46.3, trl==0.12.2, datasets==3.2.0, bitsandbytes==0.45.0, peft==0.14.0, accelerate==1.2.0"]},{"cell_type":"code","source":["!pip install -q flash-attn --no-build-isolation"],"metadata":{"id":"QKD4YxF1R8Oz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load dataset"],"metadata":{"id":"9723KfXVSD97"}},{"cell_type":"markdown","source":["We will load the [`HuggingFaceH4/rlaif-v_formatted`](https://huggingface.co/datasets/HuggingFaceH4/rlaif-v_formatted) dataset."],"metadata":{"id":"Ffnc0bS9YoEH"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset_id = \"HuggingFaceH4/rlaif-v_formatted\"\n","train_dataset, test_dataset = load_dataset(\n","    dataset_id,\n","    split=['train[:6%]', 'test[:1%]']\n",")"],"metadata":{"id":"e_lfqWKKSFHy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We also need to ensure all the images are RGB formatted:"],"metadata":{"id":"sHgCFfmnY5fi"}},{"cell_type":"code","source":["from PIL import Image\n","\n","def ensure_rgb(example):\n","    image = example['images'][0]\n","    if isinstance(image, Image.Image):\n","        if image.mode != 'RGB':\n","            image = image.convert('RGB')\n","        example['images'] = [image]\n","\n","    return example"],"metadata":{"id":"qU_Y09aLY83B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = train_dataset.map(ensure_rgb, num_proc=4)\n","test_dataset = test_dataset.map(ensure_rgb, num_proc=4)"],"metadata":{"id":"Ah5TS_NlZIDt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset[0]"],"metadata":{"id":"AZZUgf17ZMLP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset[0]['images'][0]"],"metadata":{"id":"5kSeywRqZOE4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Finetune the model using TRL"],"metadata":{"id":"ga_SPgCDZP3T"}},{"cell_type":"markdown","source":["### Load the quantized model for training"],"metadata":{"id":"LThdkbFxZST8"}},{"cell_type":"code","source":["from transformers import AutoProcessor, Idefics3ForConditionalGeneration, BitsAndBytesConfig\n","import torch\n","\n","model_id = 'HuggingFaceTB/SmolVLM-Instruct'\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type='nf4',\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","processor = AutoProcessor.from_pretrained(model_id)\n","model = Idefics3ForConditionalGeneration.from_pretrained(\n","    model_id,\n","    device_map='auto',\n","    torch_dtype=torch.bfloat16,\n","    quantization_config=bnb_config,\n","    _attn_implementation='flash_attention_2'\n",")"],"metadata":{"id":"g1zy0D8xZRol"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Set up Q-LoRA and DPOConfig"],"metadata":{"id":"Pk20PVwKZpJA"}},{"cell_type":"code","source":["from peft import LoraConfig, get_peft_model\n","\n","peft_config = LoraConfig(\n","    r=8,\n","    lora_alpha=8,\n","    lora_dropout=0.1,\n","    target_modules=['down_proj', 'up_proj', 'gate_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj'],\n","    use_dora=True,\n","    init_lora_weights='gaussian'\n",")\n","\n","peft_model = get_peft_model(model, peft_config)\n","\n","peft_model.print_trainable_parameters()"],"metadata":{"id":"G_F6LaeDZrpZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we will configure the training options using `DPOConfig`."],"metadata":{"id":"KZ004HBkaBT1"}},{"cell_type":"code","source":["from trl import DPOConfig\n","\n","training_args = DPOConfig(\n","    output_dir='smolvlm-instruct-trl-dpo-rlaif-v',\n","    bf16=True,\n","    gradient_checkpointing=True,\n","    per_device_train_batch_size=1,\n","    per_device_eval_batch_size=1,\n","    gradient_accumulation_steps=32,\n","    num_train_epochs=5,\n","    dataset_num_proc=8, # tokenization will use 8 processes\n","    dataloader_num_workers=8, # dataloading will use 8 workers\n","    logging_steps=10,\n","    report_to='tensorboard',\n","    push_to_hub=False,\n","    save_strategy='steps',\n","    save_steps=10,\n","    save_total_limit=1,\n","    eval_steps=10,\n","    eval_strategy='steps'\n",")"],"metadata":{"id":"1UpSWPW6aFJf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["NExt, we will define the `DPOTrainer`.\n","\n","**DPO** uses labeled preference data to guide the model toward generating responses that align with preferences. TRL's `DPOTrainer` will **tokenize the dataset** before training and save it to disk. This process can consume significant disk space, depending on the amount of data used for training."],"metadata":{"id":"azWoCZtfahls"}},{"cell_type":"code","source":["from trl import DPOTrainer\n","\n","trainer = DPOTrainer(\n","    model=model,\n","    ref_model=None,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n","    peft_config=peft_config,\n","    tokenizer=processor\n",")"],"metadata":{"id":"Q6Ts_P5_ayVm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"TY_guvxDa9g_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.save_model(training_args.output_dir)"],"metadata":{"id":"02as_lQPa-Xv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test the finetuned model"],"metadata":{"id":"EiX6KjBTbAzX"}},{"cell_type":"code","source":["import gc\n","import time\n","\n","\n","def clear_memory():\n","    # Delete variables if they exist in the current global scope\n","    if \"inputs\" in globals():\n","        del globals()[\"inputs\"]\n","    if \"model\" in globals():\n","        del globals()[\"model\"]\n","    if \"processor\" in globals():\n","        del globals()[\"processor\"]\n","    if \"trainer\" in globals():\n","        del globals()[\"trainer\"]\n","    if \"peft_model\" in globals():\n","        del globals()[\"peft_model\"]\n","    if \"bnb_config\" in globals():\n","        del globals()[\"bnb_config\"]\n","    time.sleep(2)\n","\n","    # Garbage collection and clearing CUDA memory\n","    gc.collect()\n","    time.sleep(2)\n","    torch.cuda.empty_cache()\n","    torch.cuda.synchronize()\n","    time.sleep(2)\n","    gc.collect()\n","    time.sleep(2)\n","\n","    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n","    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n","\n","\n","clear_memory()"],"metadata":{"id":"5XvMY7BKbCQQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will reload the base model."],"metadata":{"id":"uYwzm9hGbFMK"}},{"cell_type":"code","source":["processor = AutoProcessor.from_pretrained(model_id)\n","model = Idefics3ForConditionalGeneration.from_pretrained(\n","    model_id,\n","    device_map='auto',\n","    torch_dtype=torch.bfloat16,\n","    _attn_implementation='flash_attention_2'\n",")"],"metadata":{"id":"e0HHl1A-bGw_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we will attach the trained adapter to the pretrained model."],"metadata":{"id":"rL-eyzhEbOR5"}},{"cell_type":"code","source":["adapter_path = \"sergiopaniego/smolvlm-instruct-trl-dpo-rlaif-v\"\n","model.load_adapter(adapter_path)"],"metadata":{"id":"UbWfRCsebVMr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test\n","sample = test_dataset[0]\n","sample"],"metadata":{"id":"2OGk6aApbWSF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample['images'][0]"],"metadata":{"id":"kfOeAFzpbYpC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We need to create a function to streamline the test process."],"metadata":{"id":"He4fh1NKbblT"}},{"cell_type":"code","source":["def generate_text_from_sample(model, processor, sample, max_new_tokens=1024, device='cuda'):\n","    text_input = processor.apply_chat_template(\n","        sample['prompt'],\n","        add_generation_prompt=True\n","    )\n","\n","    image_inputs = []\n","    image = sample['images'][0]\n","    if image.mode != 'RGB':\n","        image = image.convert('RGB')\n","    image_inputs.append([image])\n","\n","    # Prepare the inputs for the model\n","    model_inputs = processor(\n","        text=text_input,\n","        images=image_inputs,\n","        return_tensors='pt'\n","    ).to(device)\n","\n","    # Generate ids\n","    generated_ids = model.generate(\n","        **model_inputs,\n","        max_new_tokens=max_new_tokens\n","    )\n","\n","    # Trim the generated ids\n","    generated_ids_trimmed = [\n","        out_ids[len(in_ids) :]\n","        for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)\n","    ]\n","\n","    # Decode the output text\n","    output_text = processor.batch_decode(\n","        generated_ids_trimmed,\n","        skip_special_tokens=True,\n","        clean_up_tokenization_spaces=False\n","    )\n","\n","    return output_text[0]"],"metadata":{"id":"ywnXpaTQbepa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = generate_text_from_sample(model, processor, sample)\n","output"],"metadata":{"id":"hQmtfRJscMXC"},"execution_count":null,"outputs":[]}]}