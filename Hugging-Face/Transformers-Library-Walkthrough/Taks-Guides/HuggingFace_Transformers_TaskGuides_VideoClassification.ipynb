{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 213197,
     "status": "ok",
     "timestamp": 1740752976478,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "PHwWKb0wWuz3",
    "outputId": "11527a92-3135-4e75-b388-88c35f397d58"
   },
   "outputs": [],
   "source": [
    "!pip install -qU pytorchvideo transformers evaluate torchvision==0.16.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29wPkOx2WzhE"
   },
   "source": [
    "# Video Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Va6hTo9jwSj8"
   },
   "source": [
    "**Video classification** is the task of assigning a label or class to an entire video. Videos are expected to have only one class for each video.\n",
    "\n",
    "Video classification models take a video as input and return a prediction about which class the video belongs to. One of applications is action/activity recognition, for fitness application.\n",
    "\n",
    "We will fine-tune `VideoMAE` on a subset of the `UCF101` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6zghyAXxBe2"
   },
   "source": [
    "## Load UCF101 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2487,
     "status": "ok",
     "timestamp": 1740751984822,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "9PGi0jiRW023"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import tarfile\n",
    "\n",
    "hf_dataset_identifier = 'sayakpaul/ucf101-subset'\n",
    "filename = 'UCF101_subset.tar.gz'\n",
    "file_path = hf_hub_download(\n",
    "    repo_id=hf_dataset_identifier,\n",
    "    filename=filename,\n",
    "    repo_type='dataset'\n",
    ")\n",
    "\n",
    "with tarfile.open(file_path) as t:\n",
    "    t.extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740751984846,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "Pp8h7JduxePO"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "dataset_root_path = 'UCF101_subset'\n",
    "dataset_root_path = pathlib.Path(dataset_root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1740751984878,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "TCHLoKLsxqrG",
    "outputId": "4472d247-d044-4cef-8b6d-483fd7901ae1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 300, Val: 30, Test: 75, Total: 405\n"
     ]
    }
   ],
   "source": [
    "video_count_train = len(list(dataset_root_path.glob('train/*/*.avi')))\n",
    "video_count_val = len(list(dataset_root_path.glob('val/*/*.avi')))\n",
    "video_count_test = len(list(dataset_root_path.glob('test/*/*.avi')))\n",
    "video_total = video_count_train + video_count_val + video_count_test\n",
    "print(f'Train: {video_count_train}, Val: {video_count_val}, Test: {video_count_test}, Total: {video_total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1740751984960,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "IUmDIlj5x4HG",
    "outputId": "0a38d730-8e98-40ca-a64c-769be132f95c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('UCF101_subset/train/BabyCrawling/v_BabyCrawling_g11_c01.avi'),\n",
       " PosixPath('UCF101_subset/train/BabyCrawling/v_BabyCrawling_g23_c01.avi'),\n",
       " PosixPath('UCF101_subset/train/BabyCrawling/v_BabyCrawling_g02_c01.avi'),\n",
       " PosixPath('UCF101_subset/train/BabyCrawling/v_BabyCrawling_g20_c05.avi'),\n",
       " PosixPath('UCF101_subset/train/BabyCrawling/v_BabyCrawling_g20_c07.avi')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_video_file_paths = (\n",
    "    list(dataset_root_path.glob('train/*/*.avi'))\n",
    "    + list(dataset_root_path.glob('val/*/*.avi'))\n",
    "    + list(dataset_root_path.glob('test/*/*.avi'))\n",
    ")\n",
    "all_video_file_paths[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4uXCZuaz44G"
   },
   "source": [
    "In the training, the video clips belonging to the same group/scene are denoted by `g` in the video file paths.\n",
    "\n",
    "For the validation and evaluation splits, we will not want video clips from the same group/scene to prevent data leakage.\n",
    "\n",
    "We also need to create two dictionaries:\n",
    "* `label2id` maps the class names to integers\n",
    "* `id2label` maps the integers to class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1740751984983,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "DBV537FlyBG8",
    "outputId": "29027868-3716-44b3-c0d2-f35dd741ae85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress'].\n"
     ]
    }
   ],
   "source": [
    "class_labels = sorted({str(path).split('/')[2] for path in all_video_file_paths})\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "print(f'Unique classes: {list(label2id.keys())}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAB4ZPR-1z99"
   },
   "source": [
    "## Load a model to fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAydFpbf17d5"
   },
   "source": [
    "We will instantiate a video classification model from a pretrained checkpoint and its associated image processor.\n",
    "\n",
    "The model's encoder comes with pretrained parameters, and the classification head is randomly initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23497,
     "status": "ok",
     "timestamp": 1740752008478,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "bzMbNo0v1v7W",
    "outputId": "64d29abb-2db8-4f71-ae3f-255ffcf5f31e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
    "\n",
    "model_ckpt = 'MCG-NJU/videomae-base'\n",
    "image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tb_wbjqO2zGY"
   },
   "source": [
    "## Prepare the datasets for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WViV4Yr122Xk"
   },
   "source": [
    "For preprocessing the videos, we will leverage the `PyTorchVideo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6843,
     "status": "ok",
     "timestamp": 1740753208494,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "5NGzlf712o0x",
    "outputId": "7b5f30f2-832b-47a1-96a2-18c78297335a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pytorchvideo.data\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m74zsFvb5tvF"
   },
   "source": [
    "For the training dataset transformations, we use a combination of uniform temporal subsampling, pixel normalization, random cropping, and random horizontal flipping.\n",
    "\n",
    "For the validation and evaluation dataset transformations, we keep the same transformation chain except for random cropping and horizontal fliiping.\n",
    "\n",
    "We will use the `image_processor` associated with the pretrained model to obtain\n",
    "* image mean and standard deviation with which the video frame pixels will be normalized\n",
    "* spatial resotluion to which the video frames will be resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1740752509588,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "35e5zbdV5tW8"
   },
   "outputs": [],
   "source": [
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "\n",
    "if 'shortest_edge' in image_processor.size:\n",
    "    height = width = image_processor.size['shortest_edge']\n",
    "else:\n",
    "    height = image_processor.size['height']\n",
    "    width = image_processor.size['width']\n",
    "\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = model.config.num_frames\n",
    "sample_rate = 4\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ak6DTYds63QM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key='video',\n",
    "        transform=Compose([\n",
    "            UniformTemporalSubsample(num_frames_to_sample),\n",
    "            Lambda(lambda x: x / 255.0),\n",
    "            Normalize(mean, std),\n",
    "            RandomShortSideScale(min_size=256, max_size=320),\n",
    "            RandomCrop(resize_to),\n",
    "            RandomHorizontalFlip(p=0.5)\n",
    "        ])\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, 'train'),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler('random', clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=train_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUTnTuMN82vi"
   },
   "outputs": [],
   "source": [
    "val_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key='video',\n",
    "        transform=Compose([\n",
    "            UniformTemporalSubsample(num_frames_to_sample),\n",
    "            Lambda(lambda x: x / 255.0),\n",
    "            Normalize(mean, std),\n",
    "            Resize(resize_to),\n",
    "        ])\n",
    "    )\n",
    "])\n",
    "\n",
    "val_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, 'val'),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler('uniform', clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "test_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, 'test'),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler('uniform', clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSXM5EGP-n7t"
   },
   "source": [
    "We use the `pytrochvideo.data.Ucf101()` because it is tailored for the UCF-101 dataset. If we want to use a custom dataset, we can extend the `LabeledVideoDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iuQ0IvZe-1Un"
   },
   "outputs": [],
   "source": [
    "print(train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hk9p19c0-6i4"
   },
   "source": [
    "## Visualize the preprocessed video for better debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5E7ZckZp--Bz"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "\n",
    "def unnormalize_img(img):\n",
    "    img = (img * std) + mean\n",
    "    img = (img * 255).astype('uint8')\n",
    "    return img.clip(0, 255)\n",
    "\n",
    "def create_gif(video_tensor, filename='sample.gif'):\n",
    "    \"\"\"Prepares a GIF from a video tensor\n",
    "\n",
    "    The video tensor is expected to have the following shape:\n",
    "    (num_frames, num_channels, height, width)\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for video_frame in video_tensor:\n",
    "        frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\n",
    "        frames.append(frame_unnormalized)\n",
    "\n",
    "    kwargs = {'duration': 0.25}\n",
    "    imageio.mimsave(filename, frames, 'GIF', **kwargs)\n",
    "    return filename\n",
    "\n",
    "def display_gif(video_tensor, gif_name='sample.gif'):\n",
    "    video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
    "    gif_filename = create_gif(video_tensor, gif_name)\n",
    "    return Image(filename=gif_filename)\n",
    "\n",
    "\n",
    "\n",
    "sample_video = next(iter(train_dataset))\n",
    "video_tensor = sample_video['video']\n",
    "display_gif(video_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ML02oh8PCbJZ"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuV5m3WiCb0y"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "\n",
    "    return metric.compute(\n",
    "        predictions=predictions,\n",
    "        references=eval_pred.label_ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MM_pA_CiDnUx"
   },
   "source": [
    "In the VideoMAE paper, the authors evaluate the model on several clips from test videos and apply different crops to those clips and report the aggregate score. Due to simplicity, we will not consider that in this guide.\n",
    "\n",
    "We also need to define a `collate_fn` to batch examples together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPuzb3oUD5rx"
   },
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    # permute to (num_frames, num_channels, height, width)\n",
    "    pixel_values = torch.stack(\n",
    "        [example['video'].permute(1, 0, 2, 3) for example in examples]\n",
    "    )\n",
    "\n",
    "    labels = torch.tensor(\n",
    "        [example['label'] for example in examples]\n",
    "    )\n",
    "\n",
    "    return {'pixel_values': pixel_values, 'labels': labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZL74yLjCppS"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZmzc9QMCqcF"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model_name = model_ckpt.split('/')[-1]\n",
    "new_model_name = f\"{model_name}-finetuned-ucf101-subset\"\n",
    "num_epochs = 4\n",
    "batch_size = 4\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    new_model_name,\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_step=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    push_to_hub=False,\n",
    "    max_steps=(train_dataset.num_videos // batch_size) * num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8Z09DkiENko"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn\n",
    ")\n",
    "\n",
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTJmXgRfGNms"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shc4u9bJGPWA"
   },
   "outputs": [],
   "source": [
    "sample_test_video = next(iter(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JftmPG_UGVKO"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "video_cls = pipeline(model='my_awesome_video_cls_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O0VvkF-rGbF_"
   },
   "outputs": [],
   "source": [
    "video_cls(\"https://huggingface.co/datasets/sayakpaul/ucf101-subset/resolve/main/v_BasketballDunk_g14_c06.avi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KBp1DJ6Gg9S"
   },
   "source": [
    "We can also manually do the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Com42NgaGiqx"
   },
   "outputs": [],
   "source": [
    "def run_inference(model, video):\n",
    "    # (num_frames, num_channels, height, width)\n",
    "    permuted_sample_test_video = video.permute(1, 0, 2, 3)\n",
    "    inputs = {\n",
    "        'pixel_values': permuted_sample_test_video.unsqueeze(0),\n",
    "        'labels': torch.tensor(\n",
    "            [sample_test_video['label']]\n",
    "            # skip if no label availble\n",
    "        )\n",
    "    }\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "logits = run_inference(model, sample_test_video['video'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7P-ZLJkHCns"
   },
   "outputs": [],
   "source": [
    "predicted_cls_idx = logits.argmax(-1).item()\n",
    "print('Predicted class:', model.config.id2label[predicted_cls_idx])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOikuHJDaCohtsWmjte8KkG",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
