{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM8l3pJO4DEWLuQslhPkymX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Prompt Tuning with PEFT"],"metadata":{"id":"ziqwWewW8mqb"}},{"cell_type":"markdown","source":["In this example, we will aply prompt tuning with the PEFT library to a pretrained model."],"metadata":{"id":"8Bdy56-dASV1"}},{"cell_type":"markdown","source":["## Introduction to prompt tuning"],"metadata":{"id":"YX1VzYRJAZtX"}},{"cell_type":"markdown","source":["Prompt tuning is an *additive fine-tuning* technique for models, which means that we **WILL NOT MODIFY ANY WEIGHTS OF THE ORIGINAL MODEL**. Instead, we will train **additional layers** that are added to the model, which is why it is called an additive technique.\n","\n","We are creating a type of superprompt by enabling a model to enhance a portion of the prompt with its acquired knowledge. However, this particular section of the prompt cannot be translated into natural language. **It is as if we have mastered expressing ourselves in embeddings and generating highly effective prompts.**\n","\n","In each training cycle, the only weights that can be modified to minimize the loss function are those integrated into the prompt. Since we do not modify the weights of the pretrained model, **it does not alter its behavior or forget any information it has previously learned**.\n","\n","The training is faster and more cost-effective. Moreover, we can train various models, and during inference time, we only need to load one foundational model along with the new smaller trained models because the weights of the original model have not been altered."],"metadata":{"id":"DcLHBdbpAcuO"}},{"cell_type":"markdown","source":["## Load the PEFT library"],"metadata":{"id":"J1Yzht32BmGK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYsA83My8kps"},"outputs":[],"source":["!pip install -qU peft datasets transformers"]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer"],"metadata":{"id":"WBEdy6a1BsYZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Bloom is one of the smallest and smartest models available for training with the PEFT library using prompt tuning. We can choose any model from the Bloom family."],"metadata":{"id":"RSHZNbaaBwQK"}},{"cell_type":"code","source":["model_name = 'bigscience/bloomz-560m' # 'bigscience/bloomz-1b1'\n","\n","NUM_VIRTUAL_TOKENS = 4\n","NUM_EPOCHS = 6"],"metadata":{"id":"M9DB_QERB5Ib"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(model_name)\n","foundational_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    trust_remote_code=True\n",")"],"metadata":{"id":"OUUrQM-FCA9d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Inference with the pretrained Bloom model"],"metadata":{"id":"GvtFs3cnCIzK"}},{"cell_type":"markdown","source":["If we want more varied generations, we need to uncomment the following paramters: `temperature`, `top_p`, `do_sample` below:"],"metadata":{"id":"q4Zd2bLSCZkp"}},{"cell_type":"code","source":["def get_outputs(model, inputs, max_new_tokens=100):\n","    outputs = model.generate(\n","        input_ids=inputs['input_ids'],\n","        attention_mask=inputs['attention_mask'],\n","        max_new_tokens=max_new_tokens,\n","        #temperature=0.2,\n","        #top_p=0.95,\n","        #do_sample=True,\n","        repetition_penalty=1.5, # avoid repetition\n","        early_stopping=True,\n","        eos_token_id=tokenizer.eos_token_id\n","    )\n","    return outputs"],"metadata":{"id":"uOCug8ynCNQk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this example, we want to have two different trained models, so we will create two distinct prompts.\n","* The first model will be trained with a dataset comtaining prompts, such as \"I want you to act as a motivational coach\", and\n","* The second one with a dataset of movitaional sentences, such as \"There are two nice things that should matter to you:\"\n","\n","Before doing this, we will collect some results from the model without fine-tuning."],"metadata":{"id":"p2P61OaLCuRA"}},{"cell_type":"code","source":["input_prompt = tokenizer(\n","    'I want you to act as a motivational coach',\n","    return_tensors='pt'\n",")\n","foundational_outputs_prompt = get_outputs(\n","    foundational_model,\n","    input_prompt,\n","    max_new_tokens=50\n",")\n","print(tokenizer.batch_decode(foundational_outputs_prompt, skip_special_tokens=True))"],"metadata":{"id":"LtSLYwTMEBbM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_prompt = tokenizer(\n","    'There are two nice things that should matter to you:',\n","    return_tensors='pt'\n",")\n","foundational_outputs_prompt = get_outputs(\n","    foundational_model,\n","    input_prompt,\n","    max_new_tokens=50\n",")\n","print(tokenizer.batch_decode(foundational_outputs_prompt, skip_special_tokens=True))"],"metadata":{"id":"M9_LTy_HESXj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prepra the datasets"],"metadata":{"id":"KFq9naJMEW_E"}},{"cell_type":"markdown","source":["We will use the following datasets:\n","* [`awesome-chatgpt-prompts`](https://huggingface.co/datasets/fka/awesome-chatgpt-prompts)\n","* [`english-quotes`](https://huggingface.co/datasets/Abirate/english_quotes)"],"metadata":{"id":"VKzmuW7ZEZII"}},{"cell_type":"code","source":["import os\n","from datasets import load_dataset\n","\n","dataset_prompt = 'fka/awesome-chatgpt-prompts'\n","data_prompt = load_dataset(dataset_prompt)\n","data_prompt = data_prompt.map(lambda x: tokenizer(x['prompt']), batched=True)\n","train_sample_prompt = data_prompt['train'].select(range(50))"],"metadata":{"id":"9yuYf1uPEYRH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_sample_prompt"],"metadata":{"id":"YTSWnvNcE7aP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_sample_prompt[0]"],"metadata":{"id":"o6s-6jaNE_HU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_sentences = load_dataset('Abirate/english_quotes')\n","dataset_sentences = dataset_sentences.map(lambda x: tokenizer(x['quote']), batched=True)\n","train_sample_sentences = dataset_sentences['train'].select(range(25))\n","train_sample_sentences = train_sample_sentences.remove_columns(['author', 'tags'])"],"metadata":{"id":"ZQcBqiu7FAvJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_sample_sentences"],"metadata":{"id":"X5gClz9NFL9P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_sample_sentences[0]"],"metadata":{"id":"AVAUBvytFM9c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fine-tuning"],"metadata":{"id":"3qpBB8fWFOgE"}},{"cell_type":"markdown","source":["We can use the same configuration for both models to be trained."],"metadata":{"id":"k3BTFP0FFSBD"}},{"cell_type":"code","source":["from peft import get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n","\n","generation_config = PromptTuningConfig(\n","    task_type=TaskType.CAUSAL_LM, # this type incidates the applied model will generate text\n","    prompt_tuning_init=PromptTuningInit.RANDOM, # the added virtual tokens are initialized with random numbers\n","    num_virtual_tokens=NUM_VIRTUAL_TOKENS, # number of virtual tokens to be added and trained\n","    tokenizer_name_or_path=model_name, # the pretrained model\n",")"],"metadata":{"id":"Lc1BFYJnFQJW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we will create two identical prompt tuning models using the same pretrained model and the same configuration."],"metadata":{"id":"DCqp71vpF0_X"}},{"cell_type":"code","source":["peft_model_prompt = get_peft_model(foundational_model, generation_config)\n","peft_model_prompt.print_trainable_parameters()"],"metadata":{"id":"4w6FTbv-F5nS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["peft_model_sentences = get_peft_model(foundational_model, generation_config)\n","peft_model_sentences.print_trainable_parameters()"],"metadata":{"id":"rQdiussCGBK5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We see that the trained parameters is about 0.001% of the available parameters in the model.\n","\n","Next, we will create the training arguments, and use the same configuration in both trainings:"],"metadata":{"id":"W-JpCnN_GFHi"}},{"cell_type":"code","source":["from transformers import TrainingArguments\n","\n","def create_training_arguments(path, learning_rate=0.0035, epochs=6):\n","    training_args = TrainingArguments(\n","        output_dir=path,\n","        use_cpu=True, # necessary for CPU clusters\n","        auto_find_batch_size=True, # find a suitable batch size that will fit into memory automatically\n","        learning_rate=learning_rate, # higher lr than full fine-tuning\n","        num_train_epochs=epochs\n","    )\n","    return training_args"],"metadata":{"id":"-bJLCh1YGeR9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["working_dir = './'\n","\n","# Create the name of the directories where to store the models\n","output_dir_prompt = os.path.join(working_dir, 'peft_outputs_prompt')\n","output_dir_sentences = os.path.join(working_dir, 'peft_outputs_sentences')\n","\n","if not os.path.exists(working_dir):\n","    os.mkdir(working_dir)\n","if not os.path.exists(output_dir_prompt):\n","    os.mkdir(output_dir_prompt)\n","if not os.path.exists(output_dir_sentences):\n","    os.mkdir(output_dir_sentences)"],"metadata":{"id":"oNLS7UFtG08h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args_prompt = create_training_arguments(\n","    output_dir_prompt,\n","    0.003,\n","    epochs=NUM_EPOCHS\n",")\n","training_args_sentences = create_training_arguments(\n","    output_dir_sentences,\n","    0.003,\n","    epochs=NUM_EPOCHS\n",")"],"metadata":{"id":"paH3oUPqHE8U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train"],"metadata":{"id":"tB7JnBaPHOG6"}},{"cell_type":"code","source":["from transformers import Trainer, DataCollatorForLanguageModeling\n","\n","def create_trainer(model, training_args, train_dataset):\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n","        # mlm=False indicates not ot use masked language modeling\n","    )\n","    return trainer"],"metadata":{"id":"lmYNVaBbHPA1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training first model\n","trainer_prompt = create_trainer(\n","    peft_model_prompt,\n","    training_args_prompt,\n","    train_sample_prompt\n",")\n","trainer_prompt.train()"],"metadata":{"id":"e1IehtKRHhx4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training second model\n","trainer_sentences = create_trainer(\n","    peft_model_sentences,\n","    training_args_sentences,\n","    train_sample_sentences\n",")\n","trainer_sentences.train()"],"metadata":{"id":"-qdhv3koHn2D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Save models"],"metadata":{"id":"hYCTLOsgHqrm"}},{"cell_type":"code","source":["trainer_prompt.model.save_pretrained(output_directory_prompt)\n","trainer_sentences.model.save_pretrained(output_directory_sentences)"],"metadata":{"id":"HXm2xY_iHrqE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Inference"],"metadata":{"id":"mzC5xbaMHtjI"}},{"cell_type":"code","source":["from peft improt PeftModel\n","\n","loaded_model_prompt = PeftModel.from_pretrained(\n","    foundational_model,\n","    output_directory_prompt,\n","    device_map='auto',\n","    is_trainable=False\n",")"],"metadata":{"id":"6kt8hORzHuav"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_model_prompt_outputs = get_outputs(\n","    loaded_model_prompt,\n","    input_prompt\n",")\n","tokenizer.batch_decode(\n","    loaded_model_prompt_outputs,\n","    skip_special_tokens=True\n",")"],"metadata":{"id":"Ax6141WKH5IR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_model_prompt.load_adapter(\n","    output_directory_sentences,\n","    adapter_name='quotes'\n",")\n","loaded_model_prompt.set_adapter('quotes')"],"metadata":{"id":"lRt4DiGfIEXS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_model_sentences_outputs = get_outputs(\n","    loaded_model_prompt,\n","    input_sentences\n",")\n","tokenizer.batch_decode(\n","    loaded_model_sentences_outputs,\n","    skip_special_tokens=True\n",")"],"metadata":{"id":"RSNZObI4IT0g"},"execution_count":null,"outputs":[]}]}