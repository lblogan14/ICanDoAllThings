{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Use Case for Agentic RAG"
      ],
      "metadata": {
        "id": "XeoptqQBZCGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Agentic RAG"
      ],
      "metadata": {
        "id": "rAkaM6tSZGAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLMs are trained on enormous bodies of data to learn general knowledge. However, the world knowledge model of LLMs may not always be relevant and up-to-date information. RAG solves this problem by finding and retrieving relevant information from our data and forwarding that to the LLM. Agentic RAG is a powerful way to use agents to answer questions about our data."
      ],
      "metadata": {
        "id": "GkPeESieGyHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each step will be implemented using `smolagents`, `llama-index`, and `langgraph`, respectively."
      ],
      "metadata": {
        "id": "5zDn85DUHO3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a RAG Tool for Guest Stories"
      ],
      "metadata": {
        "id": "Q_neiZSXHJjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will implement an agent within a HF Space, as a structured Python project..\n",
        "\n",
        "Project structure\n",
        "- `tools.py` - provides auxiliary tools for the agent\n",
        "- `retriever.py` - implements retrieval functions to support knowledge access\n",
        "- `app.py` - integrates all components into a fully functional agent.\n",
        "\n",
        "The complete development is live [HERE](https://huggingface.co/spaces/agents-course/Unit_3_Agentic_RAG)"
      ],
      "metadata": {
        "id": "xNqNfQWTJrCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "Q9uJXZrrKPGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset as external knowledge base [`agents-course/unit3-invitees`](https://huggingface.co/datasets/agents-course/unit3-invitees) contains\n",
        "- Name - Guest's full name\n",
        "- Relation - How the guest is related to the host\n",
        "- Description - A brief biography or interesting facts about the guest\n",
        "- Email address - Contact information for sending invitations or follow-ups"
      ],
      "metadata": {
        "id": "ur33E5FNKTIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the Guestbook Tool"
      ],
      "metadata": {
        "id": "iksgIvy4Ku9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will create a custom tool that the agent can use to quickly retrieve guest information. There are 3 steps to complete:\n",
        "1. Load and prepare the dataset\n",
        "2. Create the Retriever Tool\n",
        "3. Integrate the Tool with agent"
      ],
      "metadata": {
        "id": "MLW0ZA6wKxZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1 - load and prepare the dataset\n",
        "\n",
        "We need to transform our raw guest data into a format that is optimized for retrieval."
      ],
      "metadata": {
        "id": "V0fnQELjK-Og"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`smolagent`"
      ],
      "metadata": {
        "id": "FUqLz_NhLOEB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX6xI6dMQkLu"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# Load the dataset\n",
        "guest_dataset = datasets.load_dataset(\n",
        "    'agents-course/unit3-invitees',\n",
        "    split='train'\n",
        ")\n",
        "\n",
        "# Convert dataset entries into Document objects\n",
        "docs = [\n",
        "    Document(\n",
        "        page_content='\\n'.join([\n",
        "            f\"Name: {guest['name']}\",\n",
        "            f\"Relation: {guest['relation']}\",\n",
        "            f\"Description: {guest['description']}\",\n",
        "            f\"Email: {guest['email']}\"\n",
        "        ]),\n",
        "        metadata={'name': guest['name']}\n",
        "    )\n",
        "    for guest in guest_dataset\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`llama-index`"
      ],
      "metadata": {
        "id": "qEo1_hWJLqQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from llama_index.core.schema import Document\n",
        "\n",
        "# Load the dataset\n",
        "guest_dataset = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\")\n",
        "\n",
        "# Convert dataset entries into Document objects\n",
        "docs = [\n",
        "    Document(\n",
        "        text=\"\\n\".join([\n",
        "            f\"Name: {guest_dataset['name'][i]}\",\n",
        "            f\"Relation: {guest_dataset['relation'][i]}\",\n",
        "            f\"Description: {guest_dataset['description'][i]}\",\n",
        "            f\"Email: {guest_dataset['email'][i]}\"\n",
        "        ]),\n",
        "        metadata={\"name\": guest_dataset['name'][i]}\n",
        "    )\n",
        "    for i in range(len(guest_dataset))\n",
        "]"
      ],
      "metadata": {
        "id": "GJagoY2PLrX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`langgraph`"
      ],
      "metadata": {
        "id": "-DgwnRq5LtFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# Load the dataset\n",
        "guest_dataset = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\")\n",
        "\n",
        "# Convert dataset entries into Document objects\n",
        "docs = [\n",
        "    Document(\n",
        "        page_content=\"\\n\".join([\n",
        "            f\"Name: {guest['name']}\",\n",
        "            f\"Relation: {guest['relation']}\",\n",
        "            f\"Description: {guest['description']}\",\n",
        "            f\"Email: {guest['email']}\"\n",
        "        ]),\n",
        "        metadata={\"name\": guest[\"name\"]}\n",
        "    )\n",
        "    for guest in guest_dataset\n",
        "]"
      ],
      "metadata": {
        "id": "BhrxsZW5Lt8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2 - create the Retriever tool"
      ],
      "metadata": {
        "id": "SwsZc6J2LxWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`smolagents`"
      ],
      "metadata": {
        "id": "62b-61z5L5dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import Tool\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "class GuestInfoRetrieverTool(Tool):\n",
        "    name = \"guest_info_retriever\"\n",
        "    description = \"Retrieves detailed information about gala guests based on their name or relation.\"\n",
        "    inputs = {\n",
        "        'query': {\n",
        "            'type': 'string',\n",
        "            'description': 'The name or relation of the guest you want information about.'\n",
        "        }\n",
        "    }\n",
        "    output_type = 'string'\n",
        "\n",
        "    def __init__(self, docs):\n",
        "        self.is_initialized = False\n",
        "        self.retriever = BM25Retriever.from_documents(docs)\n",
        "\n",
        "    def forward(self, query: str):\n",
        "        results = self.retriever.get_relevant_documents(query)\n",
        "        if results:\n",
        "            return \"\\n\\n\".join([doc.page_content for doc in reuslts[:3]])\n",
        "        else:\n",
        "            return \"No matching guest information found.\"\n",
        "\n",
        "\n",
        "# Initialize the tool\n",
        "guest_info_tool = GuestInfoRetrieverTool(docs)"
      ],
      "metadata": {
        "id": "I0TTdBraL0c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`llama-index`"
      ],
      "metadata": {
        "id": "K9MVhEytMmZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_defaults(nodes=docs)\n",
        "\n",
        "def get_guest_info_retriever(query: str) -> str:\n",
        "    \"\"\"Retrieves detailed information about gala guests based on their name or relation.\"\"\"\n",
        "    results = bm25_retriever.retrieve(query)\n",
        "    if results:\n",
        "        return '\\n\\n'.join([doc.text for doc in results[:3]])\n",
        "    else:\n",
        "        return 'No matching guest information found.'\n",
        "\n",
        "# Initialize the tool\n",
        "guest_info_tool = FunctionTool.from_defaults(get_guest_info_retriever)"
      ],
      "metadata": {
        "id": "d7cYLMotMnSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`langgraph`"
      ],
      "metadata": {
        "id": "mGsGpJ3jNG0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.tools import Tool\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(docs)\n",
        "\n",
        "def extract_text(query: str) -> str:\n",
        "    \"\"\"Retreives detailed information about gala guests based on their name or relation.\"\"\"\n",
        "    results = bm25_retriever.invoke(query)\n",
        "    if results:\n",
        "        return '\\n\\n'.join([doc.page_content for doc in results[:3]])\n",
        "    else:\n",
        "        return 'No matching guest information found.'\n",
        "\n",
        "\n",
        "# Initialzie the tool\n",
        "guest_info_tool = Tool(\n",
        "    func=extract_text,\n",
        "    name='guest_info_retriever',\n",
        "    description='Retrieves detailed information about gala guests based on their name or relation.'\n",
        ")"
      ],
      "metadata": {
        "id": "HRBwbJpINISI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3 - integrate the tool with agent"
      ],
      "metadata": {
        "id": "Eey37W7qN30i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`smolagents`"
      ],
      "metadata": {
        "id": "oRXquNU_N9b2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import CodeAgent, InferenceClientModel\n",
        "\n",
        "# Initialize the HuggingFace model\n",
        "model = InferenceClientModel()\n",
        "\n",
        "# Create agent\n",
        "agent = CodeAgent(\n",
        "    model=model,\n",
        "    tools=[guest_info_tool]\n",
        ")\n",
        "\n",
        "# Test example\n",
        "response = agent.run(\n",
        "    \"Tell me about our guest named 'Lady Ada Lovelace'.\"\n",
        ")\n",
        "print(\"Agent's Response:\")\n",
        "print(repsonse)"
      ],
      "metadata": {
        "id": "JzXYSVVlN7Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`llama-index`"
      ],
      "metadata": {
        "id": "N7sVwJAgOeuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent.workflow import AgentWorkflow\n",
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "\n",
        "# Initialize the HuggingFace model\n",
        "llm = HuggingFaceInferenceAPI(model_name='Qwen/Qwen2.5-Coder-32B-Instruct')\n",
        "\n",
        "# Create agent\n",
        "agent = AgentWorkflow.from_tools_or_functions(\n",
        "    [guest_info_tool],\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "# Test example\n",
        "response = agent.run(\n",
        "    \"Tell me about our guest named 'Lady Ada Lovelace'.\"\n",
        ")\n",
        "print(\"Agent's Response:\")\n",
        "print(repsonse)"
      ],
      "metadata": {
        "id": "WmwNy_7COf2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`langgraph`"
      ],
      "metadata": {
        "id": "v3eFlLNrO21g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langgraph.graph import START, StateGraph\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "from google.colab import userdata\n",
        "HFHUB_API_TOKEN = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Initialize the HuggingFace model\n",
        "\n",
        "# Generate the chat interface, including the tools\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id='Qwen/Qwen2.5-Coder-32B-Instruct',\n",
        "    huggingfacehub_api=HUGGINGFACEHUB_API_TOKEN\n",
        ")\n",
        "\n",
        "chat = ChatHuggingFace(llm=llm, verbose=True)\n",
        "tools = [guest_info_tool]\n",
        "chat_with_tools = chat.bind_tools(tools)\n",
        "\n",
        "# Generate the AgentState and Agent graph\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add_messages]\n",
        "\n",
        "\n",
        "def assistant(state: AgentState):\n",
        "    return {\n",
        "        'messages': [chat_with_tools.invoke(state['messages'])]\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# The graph\n",
        "builder = StateGraph(AgentState)\n",
        "\n",
        "# Define nodes\n",
        "builder.add_node('assistant', assistant)\n",
        "builder.add_node('tools', ToolNode(tools))\n",
        "\n",
        "# Define edges\n",
        "builder.add_edge(START, 'assistant')\n",
        "builder.add_conditional_edges(\n",
        "    'assistant',\n",
        "    # If the latest message requires a tool, route to tools,\n",
        "    # Otherwise, provide a direct response\n",
        "    tools_condition\n",
        ")\n",
        "builder.add_edge('tools', 'assistant')\n",
        "\n",
        "agent = builder.compile()\n",
        "\n",
        "\n",
        "# Test example\n",
        "messages = [\n",
        "    HumanMessage(content='Tell me about our guest named \"Lady Ada Lovelace\".')\n",
        "]\n",
        "response = agent.invoke({'messages': messages})\n",
        "print(\"Agent's Response:\")\n",
        "print(response['messages'][-1].content)"
      ],
      "metadata": {
        "id": "cNfwk4p4O3uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building and Integrating Tools for Agent"
      ],
      "metadata": {
        "id": "4FBpvu8CRf8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we will grant the agent access to the web, enabling it to find the latest news and global updates. In addition, the agent will have access to weather data and HuggingFace hub model download statistics, so that it can make relevant conversation about fresh topics."
      ],
      "metadata": {
        "id": "tbCotvj7kfOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Give agent access to the web"
      ],
      "metadata": {
        "id": "_aTGfNa4kvqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`smolagents`"
      ],
      "metadata": {
        "id": "yEioSEY2k0Hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import DuckDuckGoSearchTool\n",
        "\n",
        "# Initialize the DuckDuckGo search tool\n",
        "search_tool = DuckDuckGoSearchTool()\n",
        "\n",
        "# Example test\n",
        "results = search_tool(\"Who's the current General Manager of Golden State Warriors?\")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "4o1v-dMbRmKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`llama-index`"
      ],
      "metadata": {
        "id": "bFrEJiULlEbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.tools.duckduckgo import DuckDuckGoSearchToolSpec\n",
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "# Initialize the DuckDuckGo search tool\n",
        "tool_spec = DuckDuckGoSearchToolSpec()\n",
        "search_tool = FunctionTool.from_defaults(tool_spec.duckduckgo_full_search)\n",
        "\n",
        "# Example test\n",
        "response = search_tool(\"Who's the current General Manager of Golden State Warriors?\")\n",
        "print(response.raw_output[-1]['body'])"
      ],
      "metadata": {
        "id": "Z71yiHd7lFie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`langgraph`"
      ],
      "metadata": {
        "id": "Lz4RbzCUlsy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "\n",
        "search_tool = DuckDuckGoSearchRun()\n",
        "\n",
        "# Example test\n",
        "results = search_tool.invoke(\"Who's the current General Manager of Golden State Warriors?\")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "W7_6Xp1cltlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a custom tool for weather information"
      ],
      "metadata": {
        "id": "NSYcLD0Zl4At"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`smolagents`"
      ],
      "metadata": {
        "id": "2cOjGiBomCkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import Tool\n",
        "import random\n",
        "\n",
        "\n",
        "class WeatherInfoTool(Tool):\n",
        "    name: \"weather_info\"\n",
        "    description = \"Fetches dummy weather information for a given location.\"\n",
        "    inputs = {\n",
        "        'location': {\n",
        "            'type': 'string',\n",
        "            'description': 'The location to get weather information for.'\n",
        "        }\n",
        "    }\n",
        "    output_type = 'string'\n",
        "\n",
        "    def forward(self, location: str):\n",
        "        # Dummyh weather data\n",
        "        weather_conditions = [\n",
        "            {'condition': 'Rainy', 'temp_c': 15},\n",
        "            {'condition': 'Clear', 'temp_c': 25},\n",
        "            {'condition': 'Windy', 'temp_c': 20}\n",
        "        ]\n",
        "\n",
        "        # Randomly select a weather condition\n",
        "        data = random.choice(weather_conditions)\n",
        "        return f\"Weather in {location}: {data['condition']}, {data['temp_c']} degrees Celcius\"\n",
        "\n",
        "\n",
        "# Initialize the tool\n",
        "weather_info_tool = WeatherInfoTool()"
      ],
      "metadata": {
        "id": "4wlATVcwl_10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`llama-index`"
      ],
      "metadata": {
        "id": "xp9AyVxcmt1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from llama_index.core.tools improt FunctionTool\n",
        "\n",
        "\n",
        "def get_weather_info(location: str) -> str:\n",
        "    \"\"\"Fetches dummy weather information for a given location.\"\"\"\n",
        "    # Dummy weather data\n",
        "    weather_conditions = [\n",
        "        {\"condition\": \"Rainy\", \"temp_c\": 15},\n",
        "        {\"condition\": \"Clear\", \"temp_c\": 25},\n",
        "        {\"condition\": \"Windy\", \"temp_c\": 20}\n",
        "    ]\n",
        "    # Randomly select a weather condition\n",
        "    data = random.choice(weather_conditions)\n",
        "    return f\"Weather in {location}: {data['condition']}, {data['temp_c']} degrees Celcius\"\n",
        "\n",
        "\n",
        "# Initialize the tool\n",
        "weather_info_tool = FunctionTool.from_defaults(get_weather_info)"
      ],
      "metadata": {
        "id": "eizKueUemu70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`langgraph`"
      ],
      "metadata": {
        "id": "dzMCquaTm8NJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from langchain.tools import Tool\n",
        "\n",
        "\n",
        "def get_weather_info(location: str) -> str:\n",
        "    \"\"\"Fetches dummy weather information for a given location.\"\"\"\n",
        "    # Dummy weather data\n",
        "    weather_conditions = [\n",
        "        {\"condition\": \"Rainy\", \"temp_c\": 15},\n",
        "        {\"condition\": \"Clear\", \"temp_c\": 25},\n",
        "        {\"condition\": \"Windy\", \"temp_c\": 20}\n",
        "    ]\n",
        "    # Randomly select a weather condition\n",
        "    data = random.choice(weather_conditions)\n",
        "    return f\"Weather in {location}: {data['condition']}, {data['temp_c']} degrees Celcius\"\n",
        "\n",
        "\n",
        "# Initialize the tool\n",
        "weather_info_tool = Tool(\n",
        "    name=\"get_weather_info\",\n",
        "    func=get_weather_info,\n",
        "    description=\"Fetches dummy weather information for a given location.\"\n",
        ")"
      ],
      "metadata": {
        "id": "fhA5s2i2m82l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a hug stats tool for influential AI builders"
      ],
      "metadata": {
        "id": "LnOawy5dn1sF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also create a tool to fetch model statistics from the HuggingFace Hub based on a username."
      ],
      "metadata": {
        "id": "QmNAvtUSpJHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`smolagents`"
      ],
      "metadata": {
        "id": "L4Yrw813pOgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import Tool\n",
        "from huggingface_hub import list_models\n",
        "\n",
        "\n",
        "class HubStatsTool(Tool):\n",
        "    name: 'hub_stats'\n",
        "    description = 'Fetches the most downloaded model from a specific author on the HuggingFace Hub.'\n",
        "    inputs = {\n",
        "        'author': {\n",
        "            'type': 'string',\n",
        "            'description': 'The username of the model author/organization to find models from.'\n",
        "        }\n",
        "    }\n",
        "    output_type = 'string'\n",
        "\n",
        "    def forward(self, author: str):\n",
        "        try:\n",
        "            # List models from the specified author, sorted by downloads\n",
        "            models = list(list_models(\n",
        "                author=author,\n",
        "                sort='downloads',\n",
        "                direction=-1,\n",
        "                limit=1\n",
        "            ))\n",
        "\n",
        "            if models:\n",
        "                model = models[0]\n",
        "                return f\"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads.\"'\n",
        "            else:\n",
        "                return f\"No models found for author {author}.\"\n",
        "        except Exception as e:\n",
        "            return f\"Error fetching models for {author}: {str(e)}\"\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the tool\n",
        "hub_stats_tool = HubStatsTool()\n",
        "\n",
        "# Example test\n",
        "print(hub_stats_tool('facebook'))"
      ],
      "metadata": {
        "id": "gNVnUQfBn4xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`llama-index`"
      ],
      "metadata": {
        "id": "VN0Vd8DVqECs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "from huggingface_hub import list_models\n",
        "\n",
        "\n",
        "def get_hub_stats(author: str) -> str:\n",
        "    \"\"\"Fetches the most downloaded model from a specific author on the HuggingFace Hub.\"\"\"\n",
        "    try:\n",
        "        # List models from the specified author, sorted by downloads\n",
        "        models = list(list_models(\n",
        "            author=author,\n",
        "            sort='downloads',\n",
        "            direction=-1,\n",
        "            limit=1\n",
        "        ))\n",
        "\n",
        "        if models:\n",
        "            model = models[0]\n",
        "            return f\"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads.\"'\n",
        "        else:\n",
        "            return f\"No models found for author {author}.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching models for {author}: {str(e)}\"\n",
        "\n",
        "\n",
        "# Initialize the tool\n",
        "hub_stats_tool = FunctionTool.from_defaults(get_hub_stats)\n",
        "\n",
        "# Example test\n",
        "print(hub_stats_tool('facebook'))"
      ],
      "metadata": {
        "id": "10FxeL5mqE5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`langgraph`"
      ],
      "metadata": {
        "id": "ffktjTCrsmpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import Tool\n",
        "from huggingface_hub import list_models\n",
        "\n",
        "\n",
        "def get_hub_stats(author: str) -> str:\n",
        "    \"\"\"Fetches the most downloaded model from a specific author on the HuggingFace Hub.\"\"\"\n",
        "    try:\n",
        "        # List models from the specified author, sorted by downloads\n",
        "        models = list(list_models(\n",
        "            author=author,\n",
        "            sort='downloads',\n",
        "            direction=-1,\n",
        "            limit=1\n",
        "        ))\n",
        "\n",
        "        if models:\n",
        "            model = models[0]\n",
        "            return f\"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads.\"'\n",
        "        else:\n",
        "            return f\"No models found for author {author}.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching models for {author}: {str(e)}\"\n",
        "\n",
        "\n",
        "# Initialize the tool\n",
        "hub_stats_tool = Tool(\n",
        "    name='get_hub_stats',\n",
        "    func=get_hub_stats,\n",
        "    description='Fetches the most downloaded model from a specific author on the HuggingFace Hub.'\n",
        ")\n",
        "\n",
        "# Example test\n",
        "print(hub_stats_tool('facebook'))"
      ],
      "metadata": {
        "id": "QbD_dvI1tHgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Integrating tools with agents"
      ],
      "metadata": {
        "id": "baZ4M8D_tjjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have all the tools, we can integrate them into agents."
      ],
      "metadata": {
        "id": "PF8qLucitmW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`smolagents`"
      ],
      "metadata": {
        "id": "9Ex1ack9tqFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import CodeAgent, InferenceClientModel\n",
        "\n",
        "# Initialize the HuggingFace model\n",
        "model = InferenceClientModel()\n",
        "\n",
        "# Create agent\n",
        "agent = CodeAgent(\n",
        "    model=model,\n",
        "    tools=[\n",
        "        search_tool,\n",
        "        weather_info_tool,\n",
        "        hub_stats_tool\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Example test\n",
        "response = agent.run(\n",
        "    \"What is Facebook and what's their most popular model?\"\n",
        ")\n",
        "print(\"Agent's Response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "gzVKYBkZtmCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`llama-index`"
      ],
      "metadata": {
        "id": "nwasAxFjt8_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent.workflow import AgentWorkflow\n",
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "\n",
        "# Initialize the HuggingFace model\n",
        "llm = HuggingFaceInferenceAPI(model_name='Qwen/Qwen2.5-Coder-32B-Instruct')\n",
        "\n",
        "# Create agent\n",
        "agent = AgentWorkflow.from_tools_or_functions(\n",
        "    [\n",
        "        search_tool,\n",
        "        weather_info_tool,\n",
        "        hub_stats_tool\n",
        "    ],\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "# Example test\n",
        "response = await agent.run(\n",
        "    \"What is Facebook and what's their most popular model?\"\n",
        ")\n",
        "print(\"Agent's Response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "OvcqMnF_t-GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`langgraph`"
      ],
      "metadata": {
        "id": "cnge4YgTuNVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langgraph.graph import START, StateGraph\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Initialize the HuggingFace model\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id='Qwen/Qwen2.5-Coder-32B-Instruct',\n",
        "    huggingfacehub_api=HF_TOKEN\n",
        ")\n",
        "chat = ChatHuggingFace(llm=llm, verbose=True)\n",
        "tools = [search_tool, weather_info_tool, hub_stats_tool]\n",
        "chat_with_tools = chat.bind_tools(tools)\n",
        "\n",
        "# Generate AgentState and Agent graph\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add_messages]\n",
        "\n",
        "def assistant(state: AgentState):\n",
        "    return {\n",
        "        \"messages\": [chat_with_tools.invoke(state[\"messages\"])],\n",
        "    }\n",
        "\n",
        "## The graph\n",
        "builder = StateGraph(AgentState)\n",
        "# Define nodes\n",
        "builder.add_node(\"assistant\", assistant)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# Define edges\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant\",\n",
        "    # If the latest message requires a tool, route to tools\n",
        "    # Otherwise, provide a direct response\n",
        "    tools_condition,\n",
        ")\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "agent = builder.compile()\n",
        "\n",
        "\n",
        "# Example test\n",
        "messages = [\n",
        "    HumanMessage(content=\"What is Facebook and what's their most popular model?\")\n",
        "]\n",
        "response = agent.invoke({'messages': messages})\n",
        "print(\"Agent's Response:\")\n",
        "print(response['messages'][-1].content)"
      ],
      "metadata": {
        "id": "-fz2Cp8fuOgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Agent"
      ],
      "metadata": {
        "id": "TOzHMYOmvArS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have built all the necessary components for our agents, it is time to bring everything together into a complete agent."
      ],
      "metadata": {
        "id": "0570aOBivCs5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assembling agent"
      ],
      "metadata": {
        "id": "NaGx7uVgvayb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`smolagents`"
      ],
      "metadata": {
        "id": "ccSUi-f_vdgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from smolagents import CodeAgent, InferenceClientModel\n",
        "\n",
        "# Import our custom tools from previous secitons\n",
        "# Assume they are saved under `tools.py` and `retriever.py`\n",
        "from tools import DuckDuckGoSearchTool, WeatherInfoTool, HubStatsTool\n",
        "from retriever import guest_info_tool"
      ],
      "metadata": {
        "id": "YdWaFYxovZHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the HuggingFace model\n",
        "model = InferenceClientModel()\n",
        "\n",
        "# Initialize the web search tool\n",
        "search_tool = DuckDuckGoSearchTool()\n",
        "\n",
        "# Initialize the weather tool\n",
        "weather_info_tool = WeatherInfoTool()\n",
        "\n",
        "# Initialize the Hub stats tool\n",
        "hub_stats_tool = HubStatsTool()\n",
        "\n",
        "# Create agent with all the tools\n",
        "agent = CodeAgent(\n",
        "    model=model,\n",
        "    tools=[\n",
        "        guest_info_tool,\n",
        "        weather_info_tool,\n",
        "        search_tool,\n",
        "        hub_stats_tool\n",
        "    ],\n",
        "    add_base_tools=True,  # add any additional base tools\n",
        "    planning_intervals=3, # enable planning every 3 steps\n",
        ")"
      ],
      "metadata": {
        "id": "5w9xsX6dvCG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`llama-index`"
      ],
      "metadata": {
        "id": "4UukFPucw8sZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent.workflow import AgentWorkflow\n",
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "\n",
        "from tools import search_tool, weather_info_tool, hub_stats_tool\n",
        "from retriever import guest_info_tool"
      ],
      "metadata": {
        "id": "yR3fm_CJw-FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the HuggingFace model\n",
        "llm = HuggingFaceInferenceAPI(model_name='Qwen/Qwen2.5-COder-32B-Instruct')\n",
        "\n",
        "# Create agent with all the tools\n",
        "agent = AgentWorkflow.from_tools_or_functions(\n",
        "    [\n",
        "        guest_info_tool,\n",
        "        weather_info_tool,\n",
        "        search_tool,\n",
        "        hub_stats_tool\n",
        "    ],\n",
        "    llm=llm\n",
        ")"
      ],
      "metadata": {
        "id": "g5Uy0QisxHG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`langgraph`"
      ],
      "metadata": {
        "id": "5FEw8hBExUHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langgraph.graph import START, StateGraph\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "\n",
        "from tools import DuckDuckGoSearchRun, weather_info_tool, hub_stats_tool\n",
        "from retriever import guest_info_tool"
      ],
      "metadata": {
        "id": "Wyt8mxIUxU8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the web search tool\n",
        "search_tool = DuckDuckGoSearchRun()\n",
        "\n",
        "# Initialize the HuggingFace model\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id='Qwen/Qwen2.5-Coder-32B-Instruct',\n",
        "    huggingfacehub_api=HF_TOKEN\n",
        ")\n",
        "chat = ChatHuggingFace(llm=llm, verbose=True)\n",
        "tools = [search_tool, weather_info_tool, hub_stats_tool, guest_info__tool]\n",
        "chat_with_tools = chat.bind_tools(tools)\n",
        "\n",
        "# Generate AgentState and Agent graph\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add_messages]\n",
        "\n",
        "def assistant(state: AgentState):\n",
        "    return {\n",
        "        \"messages\": [chat_with_tools.invoke(state[\"messages\"])],\n",
        "    }\n",
        "\n",
        "\n",
        "# The graph\n",
        "builder = StateGraph(AgentState)\n",
        "# Define nodes\n",
        "builder.add_node(\"assistant\", assistant)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# Define edges\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant\",\n",
        "    # If the latest message requires a tool, route to tools\n",
        "    # Otherwise, provide a direct response\n",
        "    tools_condition,\n",
        ")\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "agent = builder.compile()"
      ],
      "metadata": {
        "id": "4a_wUPzmxhM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### End-to-end examples"
      ],
      "metadata": {
        "id": "yxpixIpQyArm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 1: Finding guest information"
      ],
      "metadata": {
        "id": "Q4bPko5nyHK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`smolagents`"
      ],
      "metadata": {
        "id": "nyD3Z9aCyK2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Tell me about 'Lady Ada Lovelace'\"\n",
        "response = agent.run(query)\n",
        "\n",
        "print(\"Agent's Response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "ztY99tftyE0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`llama-index`"
      ],
      "metadata": {
        "id": "SLGfygk5yReL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Tell me about 'Lady Ada Lovelace'\"\n",
        "response = await agent.run(query)\n",
        "\n",
        "print(\"Agent's Response:\")\n",
        "print(response.response.blocks[0].text)"
      ],
      "metadata": {
        "id": "ukoGwFGeySj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`langgraph`"
      ],
      "metadata": {
        "id": "9yYISTtxyfvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Tell me about 'Lady Ada Lovelace'\"\n",
        "messages = {\"messages\": query}\n",
        "response = agent.invoke(messages)\n",
        "\n",
        "print(\"Agent's Response:\")\n",
        "print(response['messages'][-1].content)"
      ],
      "metadata": {
        "id": "X-C361uaygUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 2: Checking the weather"
      ],
      "metadata": {
        "id": "jqP2C_yTysMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`smolagents`"
      ],
      "metadata": {
        "id": "fVMEhKWYyxlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What's the weather like in Houston tonight? Will it be suitable for our fireworks display?\"\n",
        "response = agent.run(query)\n",
        "\n",
        "print(\"Agent's Response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "1WG4OfZ8yvXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`llama-index`"
      ],
      "metadata": {
        "id": "MB6SoVkey1aF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What's the weather like in Houston tonight? Will it be suitable for our fireworks display?\"\n",
        "response = await agent.run(query)\n",
        "\n",
        "print(\"Agent's Response:\")\n",
        "print(response.response.blocks[0].text)"
      ],
      "metadata": {
        "id": "BHswcE2by2Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`langgraph`"
      ],
      "metadata": {
        "id": "IVGTVIvQy6Ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What's the weather like in Houston tonight? Will it be suitable for our fireworks display?\"\n",
        "messages = {\"messages\": query}\n",
        "response = agent.invoke(messages)\n",
        "\n",
        "print(\"ðŸŽ© Alfred's Response:\")\n",
        "print(response['messages'][-1].content)"
      ],
      "metadata": {
        "id": "HX452PL-y60c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 3: Impressing AI reseaerchers"
      ],
      "metadata": {
        "id": "8XOoGXWezFYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`smolagents`"
      ],
      "metadata": {
        "id": "4wSFsyzPzLTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"One of our guests is from Qwen. What can you tell me about their most popular model?\"\n",
        "response = agent.run(query)\n",
        "\n",
        "print(\"Agent's Response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "PQmTBKhhzJAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`llama-index`"
      ],
      "metadata": {
        "id": "y9o0y91KzPMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"One of our guests is from Qwen. What can you tell me about their most popular model?\"\n",
        "response = await agent.run(query)\n",
        "\n",
        "print(\"Agent's Response:\")\n",
        "print(response.response.blocks[0].text)"
      ],
      "metadata": {
        "id": "U2axbHA-zRFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`langgraph`"
      ],
      "metadata": {
        "id": "8OTVDy0FzVdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"One of our guests is from Qwen. What can you tell me about their most popular model?\"\n",
        "messages = {\"messages\": query}\n",
        "response = agent.invoke(messages)\n",
        "\n",
        "print(\"ðŸŽ© Alfred's Response:\")\n",
        "print(response['messages'][-1].content)"
      ],
      "metadata": {
        "id": "UQ7gAKqZzWLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 4: Combining multiple tools"
      ],
      "metadata": {
        "id": "k3rTO43Hzjjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`smolagents`"
      ],
      "metadata": {
        "id": "y3InEUtBzmQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I need to speak with Dr. Nikola Tesla about recent advancements in wireless energy. Can you help me prepare for this conversation?\"\n",
        "response = agent.run(query)\n",
        "\n",
        "print(\"Agent's Response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "ul_CaYxbzl5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`llama-index`"
      ],
      "metadata": {
        "id": "KB2vvwfhzqNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I need to speak with Dr. Nikola Tesla about recent advancements in wireless energy. Can you help me prepare for this conversation?\"\n",
        "response = await agent.run(query)\n",
        "\n",
        "print(\"Agent's Response:\")\n",
        "print(response.response.blocks[0].text)"
      ],
      "metadata": {
        "id": "fDYNrAwLzrD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`langgraph`"
      ],
      "metadata": {
        "id": "__doJSs2zvnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I need to speak with Dr. Nikola Tesla about recent advancements in wireless energy. Can you help me prepare for this conversation?\"\n",
        "messages = {\"messages\": query}\n",
        "response = agent.invoke(messages)\n",
        "\n",
        "print(\"ðŸŽ© Alfred's Response:\")\n",
        "print(response['messages'][-1].content)"
      ],
      "metadata": {
        "id": "Ln7LqewBzwYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advanced features: conversation memory"
      ],
      "metadata": {
        "id": "GPJSJKuez2d-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`smolagents`"
      ],
      "metadata": {
        "id": "1U5ka7nO0BXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create agent with conversation memory\n",
        "agent_with_memory = CodeAgent(\n",
        "    model=model,\n",
        "    tools=[\n",
        "        guest_info_tool,\n",
        "        weather_info_tool,\n",
        "        search_tool,\n",
        "        hub_stats_tool\n",
        "    ],\n",
        "    add_base_tools=True,\n",
        "    planning_intervals=3,\n",
        ")\n",
        "\n",
        "# First interaction\n",
        "response1 = agent_with_memory.run(\n",
        "    \"Tell me about Lady Ada Lovelace.\"\n",
        ")\n",
        "print(\"Agent's First Response:\")\n",
        "print(response1)\n",
        "\n",
        "# Second interaction (referencing the first)\n",
        "response2 = agent_with_memory.run(\n",
        "    \"What projects is she currently working on?\",\n",
        "    reset=False     # IMPORTANT\n",
        ")\n",
        "print(\"Agent's Second Response:\")\n",
        "print(response2)"
      ],
      "metadata": {
        "id": "T9-mm70u0AaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`llama-index`"
      ],
      "metadata": {
        "id": "LG0Rk2240hB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import Context\n",
        "\n",
        "agent = AgentWorkflow.from_tools_or_functions(\n",
        "    [\n",
        "        guest_info_tool,\n",
        "        weather_info_tool,\n",
        "        search_tool,\n",
        "        hub_stats_tool\n",
        "    ],\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "# Remembering state (IMPORTANT)\n",
        "ctx = Context(agent)\n",
        "\n",
        "# First interaction\n",
        "response1 = await agent.run(\n",
        "    \"Tell me about Lady Ada Lovelace.\",\n",
        "    ctx=ctx\n",
        ")\n",
        "print(\"Agent's First Response:\")\n",
        "print(response1)\n",
        "\n",
        "# Second interaction (referencing the first)\n",
        "response2 = await agent.run(\n",
        "    \"What projects is she currently working on?\",\n",
        "    ctx=ctx\"\n",
        ")\n",
        "print(\"Agent's Second Response:\")\n",
        "print(response2)"
      ],
      "metadata": {
        "id": "ez1TnhU70iHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`langgraph`"
      ],
      "metadata": {
        "id": "Ukki5naz07hV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First interaction\n",
        "response = agent.invoke(\n",
        "    {'messages': [\n",
        "        HumanMessage(content=\"Tell me about Lady Ada Lovelace.\")\n",
        "    ]}\n",
        ")\n",
        "print(\"Agent's First Response:\")\n",
        "print(response['messages'][-1].content)\n",
        "print()\n",
        "\n",
        "# Second interaction (referencing the first)\n",
        "response = agent.invoke(\n",
        "    {'messages':\n",
        "        response['messages'] + [\n",
        "            HumanMessage(content=\"What projects is she currently working on?\")\n",
        "        ]\n",
        "    }\n",
        ")\n",
        "print(\"Agent's Second Response:\")\n",
        "print(response['messages'][-1].content)"
      ],
      "metadata": {
        "id": "peKrE1DY08rI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}