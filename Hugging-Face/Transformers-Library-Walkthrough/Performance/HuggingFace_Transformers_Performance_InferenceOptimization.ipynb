{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMz4m6Z8bfv4Ki0d0KptxNF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# LLM Inference Optimization"],"metadata":{"id":"BdVJHEnki68v"}},{"cell_type":"markdown","source":["Basic inference is slow because LLMs have to be called repeatedly to generate the next token. The input sequence increases as generation progresses, which takes longer and longer for the LLM to process. LLMs also have billions of parameters, making it a challenge to store and handle all those weights in memory.\n","\n","HuggingFace also provides **Text Generation Inference (TGI)** dedicated to deploying and serving highly optimized LLMs for inference."],"metadata":{"id":"YLQvJzStlsu8"}},{"cell_type":"markdown","source":["## Static kv-cache and torch.compile"],"metadata":{"id":"TE_BWYU9mKhF"}},{"cell_type":"markdown","source":["During decoding, a LLM computes the key-value (kv) values for each input token and since it is autoregressive, it computes the same kv values each time because the generated output becomes part of the input now.\n","\n","To optimize this process, we can use a kv-cache to store the past keys and values instead of recomputing them each time. However, since the kv-cache grows with each generation step and is dynamic, it prevents us from taking advantage of `torch.compile`.\n","\n","The *static kv-cache* solves this issue by pre-allocating the kv-cache size to a maximum value which allows us to combine it with `torch.compile` for up to a 4x speed-up.\n","\n","There are three flavors of static kv-cache usage:\n","1. Basic usage\n","2. Advanced usage: control Static Cache\n","3. Advanced usage: end-to-end generate compilation"],"metadata":{"id":"sTVGQhKHTcS7"}},{"cell_type":"markdown","source":["##### basic usage: generation_config"],"metadata":{"id":"1wpL4cqLVue3"}},{"cell_type":"markdown","source":["1. Access the model's `generation_config` attribute and set the `cache_implementation` to `\"static\"`;\n","2. Call `torch.compile` on the model to compile the forward pass with the static kv-cache."],"metadata":{"id":"Yf3YIvoVVy4X"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5N1xtBJOi2er"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","import os\n","\n","os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n","\n","model_name = 'google/gemma-2b'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    torch_dtype='auto',\n","    device_map='auto'\n",")"]},{"cell_type":"code","source":["model.generation_config.cache_implementation = 'static'\n","model.forward = torch.compile(\n","    model.forward,\n","    mode='reduce-overhead',\n","    fullgraph=True\n",")"],"metadata":{"id":"YbBC3IVGWUdZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_text = \"The theory of special relativity states \"\n","input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device.type)\n","outputs = model.generate(**input_ids)\n","print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"],"metadata":{"id":"FskiFbeTWcsW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### advanced usage: control Static Cache"],"metadata":{"id":"8gTCl5MWWi1Z"}},{"cell_type":"markdown","source":["A `StaticCache` object can be passed to the model's `generate()` under the `past_key_values` argument."],"metadata":{"id":"-8wqVlu1Wl9_"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM, StaticCache\n","import torch\n","import os\n","\n","os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n","\n","model_name = 'google/gemma-2b'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    torch_dtype='auto',\n","    device_map='auto'\n",")"],"metadata":{"id":"89g1ieEoWekf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.forward = torch.compile(\n","    model.forward,\n","    mode='reduce-overhead',\n","    fullgraph=True\n",")\n","\n","input_text = \"The theory of special relativity states \"\n","input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device.type)\n","prompt_length = input_ids.input_ids.shape[1]\n","model.generation_config.max_new_tokens = 16\n","\n","past_key_values = StaticCache(\n","    config=model.config,\n","    batch_size=1,\n","    # If the cache is re-used, make sure the cache length is large enough for all cases\n","    max_cache_len=prompt_length + model.generation_config.max_new_tokens*2,\n","    device=model.device,\n","    dtype=model.dtype\n",")\n","\n","outputs = model.generate(\n","    **input_ids,\n","    past_key_values=past_key_values\n",")\n","print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n","\n","# pass the generated text and the same cache object to continue generation from\n","# where it left off.\n","# Optionally, in a multi-turn conversation, append the new user input to the generated text.\n","new_input_ids = outputs\n","outputs = model.generate(\n","    new_input_ids,\n","    past_key_values=past_key_values\n",")\n","print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"],"metadata":{"id":"fI8_QqG8cxBk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If we want to reuse the same `StaticCache` object on a new prompt, make sure to reset its contents with the `.reset()` method between calls.\n","\n","The `StaticCache` object can also be passed to the model's forward pass under the same `past_key_values`. We can write our own function to decode the next token given the current token and position and cache position of previously generated tokens."],"metadata":{"id":"x3-bcvufWeTK"}},{"cell_type":"code","source":["from transformers import LlamaTokenizer, LlamaForCausalLM, StaticCache, logging\n","from transformers.testing_utils import CaptureLogger\n","import torch\n","from accelerate.test_utils.testing import get_backend\n","\n","prompts = [\n","    \"Simply put, the theory of relativity states that \",\n","    \"My favorite all time favorite condiment is ketchup.\",\n","]\n","\n","NUM_TOKENS_TO_GENERATE = 40\n","torch_device, _, _ = get_backend()"],"metadata":{"id":"JoR7VC4wj9lb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = 'meta-llama/Llama-2-7b-hf'\n","tokenizer = LlamaTokenizer.from_pretrained(\n","    model_name,\n","    pad_token='</s>',\n","    padding_side='right'\n",")\n","model = LlamaForCausalLM.from_pretrained(\n","    model_name,\n","    device_map='sequential'\n",")"],"metadata":{"id":"62Kc1JwhlFGO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = tokenizer(prompts, return_tensors='pt', padding=True).to(model.device)\n","\n","def decode_one_token(model, cur_token, input_pos, cache_position, past_key_values):\n","    logits = model(\n","        cur_token,\n","        position_ids=input_pos,\n","        cache_position=cache_position,\n","        past_key_values=past_key_values,\n","        return_dict=False,\n","        use_cache=True\n","    )[0]\n","\n","    new_token = torch.argmax(logits[:, -1], dim=-1)[:, None]\n","    return new_token"],"metadata":{"id":"SkXG2ltMlaQI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To enable static kv-cache and `torch.compile` with the `StaticCache` method, we must\n","1. initialize the `StaticCache` instance before using the model for inference.\n","2. call `torch.compile` on the model to compile the forward pass with the static kv-cache.\n","3. use `SDPBackend.MATH` in the `torch.nn.attention.sdpa_kernel` context mananger to enable the native PyTorch C++ implementation of scaled dot product attention to speed up inference even more."],"metadata":{"id":"j8ygMlkzl1-2"}},{"cell_type":"code","source":["from torch.nn.attention import SDPBackend, sdpa_kernel\n","\n","batch_size, seq_length = inputs['input_ids'].shape\n","\n","with torch.no_grad():\n","    past_key_values = StaticCache(\n","        config=model.config,\n","        batch_size=2,\n","        max_cache_len=4096,\n","        device=torch_device,\n","        dtype=model.dtype\n","    )\n","    cache_position = torch.arange(seq_length, device=torch_device)\n","    generated_ids = torch.zeros(\n","        batch_size,\n","        seq_length + NUM_TOKENS_TO_GENERATE + 1,\n","        dtype=torch.int,\n","        device=torch_device\n","    )\n","    generated_ids[:, cache_position] = inputs['input_ids'].to(torch_device).tor(torch.int)\n","\n","    logits = model(\n","        **inputs,\n","        cache_position=cache_position,\n","        past_key_values=past_key_values,\n","        return_dict=False,\n","        use_cache=True\n","    )[0]\n","    next_token = torch.argmax(logits[:, -1], dim=-1)[:, None]\n","    generated_ids[:, seq_length] = next_token[:, 0]\n","\n","    decode_one_token = torch.compile(\n","        decode_one_token,\n","        mode='reduce-overhead',\n","        fullgraph=True\n","    )\n","    cache_position = torch.tensor([seq_length + 1], device=torch_device)\n","\n","    for _ in range(1, NUM_TOKENS_TO_GENERATE):\n","        with sdpa_kernel(SDPBackend.MATH):\n","            next_token = decode_one_token(\n","                model,\n","                next_token.clone(),\n","                None,\n","                cache_position,\n","                past_key_values\n","            )\n","            generated_ids[:, cache_position] = next_token.int()\n","        cache_position += 1\n","\n","text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n","print(text)"],"metadata":{"id":"RTjwFpn5mQr6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### advanced usage: end-to-end generate compilation"],"metadata":{"id":"rvAReK8nnciG"}},{"cell_type":"markdown","source":["Compiling the entire `generate` function only needs to call `torch.compile` on `generate` to compile the entire function."],"metadata":{"id":"VWQRfD1-nfwf"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","import os\n","\n","os.envrion['TOKENIZERS_PARALLELISM'] = 'false'\n","\n","model_name = 'google/gemma-2b'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    torch_dtype='auto',\n","    device_map='auto'\n",")"],"metadata":{"id":"OCQbrFlxnfHC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.generate = torch.compile(\n","    model.generate,\n","    mode='reduce-overhead',\n","    fullgraph=True\n",")\n","\n","input_text = \"The theory of special relativity states \"\n","input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device.type)\n","\n","outputs = model.generate(**input_ids)\n","print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"],"metadata":{"id":"sKM7YnRspoED"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Speculative decoding"],"metadata":{"id":"_YLJVfaap4xP"}},{"cell_type":"markdown","source":["For each input token generated by an autoregressive model, we need to load the model weights each time during the forward pass.\n","\n","Speculative decoding alleviates this slowdown by using a second smaller and faster assistant model to generate candidate tokens that are verified by the larger LLM in a single forward pass. If the verified tokens are correct, the LLM essentially gets them for \"free\" without having to generate them itself. There is no degradation in accuracy because the verification forward pass ensures the same outputs are generated as if the LLM had generated them on its own."],"metadata":{"id":"7oKVin0Np7Dw"}},{"cell_type":"markdown","source":["##### greedy search"],"metadata":{"id":"VmqPin4RrVZo"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","from accelerate.test_utils.testing import get_backend\n","\n","device, _, _ = get_backend()\n","\n","model_name = 'facebook/opt-1.3b'\n","assistant_model_name = 'facebook/opt-125m'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name).to(device)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    torch_dtype='auto'\n",").to(device)\n","assistant_model = AutoModelForCausalLM.from_pretrained(\n","    assistant_model_name,\n",").to(device)"],"metadata":{"id":"F-vT_F8sp6F4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = tokenizer(\"Einstein's theory of relativity states\", return_tensors='pt').to(device)\n","\n","outputs = model.generate(\n","    **inputs,\n","    assistant_model=assistant_model\n",")\n","print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"],"metadata":{"id":"mxTI83mTr1bx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### sampling"],"metadata":{"id":"9lO5JF_dsIYN"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","from accelerate.test_utils.testing import get_backend\n","\n","device, _, _ = get_backend()\n","\n","model_name = 'facebook/opt-1.3b'\n","assistant_model_name = 'facebook/opt-125m'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name).to(device)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    torch_dtype='auto'\n",").to(device)\n","assistant_model = AutoModelForCausalLM.from_pretrained(\n","    assistant_model_name,\n",").to(device)"],"metadata":{"id":"nImQsrV8sJL1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = tokenizer(\"Einstein's theory of relativity states\", return_tensors='pt').to(device)\n","\n","outputs = model.generate(\n","    **inputs,\n","    assistant_model=assistant_model,\n","    do_sample=True,\n","    temperature=0.7\n",")\n","print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"],"metadata":{"id":"pjGLBe-nsT_N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Prompt lookup decoding"],"metadata":{"id":"-oNHIvWwsdFi"}},{"cell_type":"markdown","source":["**Prompt lookup decoding** is a variant of speculative decoding that is also compatible with greedy search and sampling.\n","\n","Prompt lookup works especially well for input-grounded tasks - such as summarization - where there is often overlapping words between the prompt and output. These overlapping n-grams are used as the LLM candidate tokens."],"metadata":{"id":"SpvhP0z5sf21"}},{"cell_type":"markdown","source":["##### greedy decoding"],"metadata":{"id":"7CTGTl7EwT2z"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","from accelerate.test_utils.testing import get_backend\n","\n","device, _, _ = get_backend()\n","\n","model_name = 'facebook/opt-1.3b'\n","assistant_model_name = 'facebook/opt-125m'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name).to(device)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    torch_dtype='auto'\n",").to(device)\n","assistant_model = AutoModelForCausalLM.from_pretrained(\n","    assistant_model_name,\n",").to(device)"],"metadata":{"id":"JTSN_DLqsfcH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = tokenizer(\"The second law of thermodynamics states\", return_tensors=\"pt\").to(device)\n","outputs = model.generate(\n","    **inputs,\n","    prompt_lookup_num_tokens=3 # add here\n",")\n","print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"],"metadata":{"id":"QYIDvjeRweRK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### sampling"],"metadata":{"id":"NsY3bsrTwq_n"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","from accelerate.test_utils.testing import get_backend\n","\n","device, _, _ = get_backend()\n","\n","model_name = 'facebook/opt-1.3b'\n","assistant_model_name = 'facebook/opt-125m'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name).to(device)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    torch_dtype='auto'\n",").to(device)\n","assistant_model = AutoModelForCausalLM.from_pretrained(\n","    assistant_model_name,\n",").to(device)"],"metadata":{"id":"tXqBlzKiwr8g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = tokenizer(\"The second law of thermodynamics states\", return_tensors=\"pt\").to(device)\n","outputs = model.generate(\n","    **inputs,\n","    prompt_lookup_num_tokens=3,\n","    do_sample=True, # add here\n","    temperature=0.7 # and here\n",")\n","print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"],"metadata":{"id":"gGJW-LyXwv5R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Attention optimization"],"metadata":{"id":"EhamR1cgw8cV"}},{"cell_type":"markdown","source":["The self-attention mechanism in the transformer model grows quadratically in compute and memory with the number of input tokens. This limitation is only magnified in LLMs which handles much longer sequences."],"metadata":{"id":"VdEpUflHw-13"}},{"cell_type":"markdown","source":["### FlashAttention-2"],"metadata":{"id":"xaWMFMnMxM2K"}},{"cell_type":"markdown","source":["**FlashAttention** and **FlashAttention-2** break up the attention computation into smaller chunks and reduces the number of intermediate read/write operations to GPU memory to speed up inference.\n","\n","FlashAttention-2 improves on the original FlashAttention algorithm by also parallelizing over sequence length dimension and better partitioning work on the hardware to reduce synchronization and communication overhead."],"metadata":{"id":"sboiJjOlxPty"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n","\n","quant_config = BitsAndBytesConfig(load_in_8bit=True)\n","model = AutoModelForCausalLM.from_pretrained(\n","    'google/gemma-2b',\n","    quantization_config=quant_config,\n","    torch_dtype=torch.bfloat16,\n","    attn_implementation='flash_attention_2' # add here\n",")"],"metadata":{"id":"oob2iX6Gw986"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Fine-tuning with `torch.compile` and padding-free data collation"],"metadata":{"id":"UpLeHxE3yK7i"}},{"cell_type":"markdown","source":["We can enhance the training efficiency of large language models by leveraging `torch.compile` during fine-tuning and using a padding-free data collator."],"metadata":{"id":"Az2vZmCSyPIi"}},{"cell_type":"code","source":["import math\n","import datasets\n","import dataclasses\n","from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n","from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n","\n","model_name = 'meta-llama/Llama-3.2-1B'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=true)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    attn_implementation='flash_attention_2', # enable FlashAttention-2\n",")"],"metadata":{"id":"pQgS6VZcyObT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response_template = \"\\n### Label:\"\n","response_template_ids = tokenizer.encode(\n","    response_template,\n","    add_special_tokens=False\n",")[2:] # exclude special tokens\n","\n","data_collator = DataCollatorForCompletionOnlyLM(\n","    response_template_ids=response_template_ids,\n","    tokenizer=tokenizer,\n","    ignore_index=-100,\n","    padding_free=True # enable padding-free collation\n",")\n","\n","def format_dataset(example):\n","    return {\n","        'output': example['output'] + tokenizer.eos_token\n","    }\n","\n","\n","data_files = {'train': 'path/to/dataset'}\n","json_dataset = datasets.load_dataset('json', data_files=data_files)\n","formatted_train_dataset = json_dataset['train'].map(format_dataset)"],"metadata":{"id":"NOvVDJe1Dh_I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_args = TrainingArguments(\n","    num_train_epochs=5,\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    gradient_accumulation_steps=4,\n","    learning_rate=1e-5,\n","    weight_decay=0.0,\n","    warmup_ratio=0.03,\n","    lr_scheduler_type=\"cosine\",\n","    logging_steps=1,\n","    include_tokens_per_second=True,\n","    save_strategy=\"epoch\",\n","    output_dir=\"output\",\n","    torch_compile=True,  # enable torch.compile\n","    torch_compile_backend=\"inductor\",\n","    torch_compile_mode=\"default\"\n",")"],"metadata":{"id":"tqxUPng2EFNe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# convert TrainingArguments to SFTConfig\n","transformer_train_arg_fields = [x.name for x in dataclasses.fields(SFTConfig)]\n","transformer_kwargs = {\n","    k: v\n","    for k, v in train_args.to_dict().items()\n","    if k in transformer_train_arg_fields\n","}\n","training_args = SFTConfig(**transformer_kwargs)"],"metadata":{"id":"yKr0JCugEJLT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = SFTTrainer(\n","    model=model,\n","    tokenizer=tokenizer,\n","    train_dataset=formatted_train_dataset,\n","    data_collator=data_collator,\n","    dataset_text_field=\"output\",\n","    args=training_args,\n",")\n","trainer.train()"],"metadata":{"id":"N0HPp7WOEaoP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### PyTorch scaled dot product attention"],"metadata":{"id":"5G5_MkGVEdH-"}},{"cell_type":"markdown","source":["**Scaled dot product attention (SDPA)** is automatically enabled in PyTorch 2.0 and it supports FlashAttention, xFormers, and PyTorch C++ implementation.\n","\n","We can use the `torch.nn.attention.sdpa_kernel` context manager to explicitly enable or disable any of the attention algorithms."],"metadata":{"id":"uRZu0ewaEfoM"}},{"cell_type":"code","source":["import torch\n","from torch.nn.attention import SDPBackend, sdpa_kernel\n","from transformers import AutoModelForCausalLM\n","\n","tokenizer = AutoTokenizer.from_pretrained('google/gemma-2b')\n","model = AutoModelForCausalLM.from_pretrained(\n","    'google/gemma-2b',\n","    torch_dtype=torch.bfloat16\n",")"],"metadata":{"id":"C1ljzrf2EfOc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = tokenizer(\"The second law of thermodynamics states\", return_tensors=\"pt\").to(device)\n","\n","with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n","    outputs = model.generate(**inputs)\n","\n","print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"],"metadata":{"id":"-vZBvHWgFBEA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Quantization"],"metadata":{"id":"_mYDWrTYFKK7"}},{"cell_type":"markdown","source":["Quantization reduces the size of the LLM weights by storing them in a lower precision. This translates to lower memory usage and makes loading LLMs for inference more accessible if we are constrained by our GPU's memory."],"metadata":{"id":"plZxzaRYFOSW"}},{"cell_type":"code","source":["# load `Mistral-7B-v0.1` in half-precision\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"mistralai/Mistral-7B-v0.1\",\n","    torch_dtype=torch.bfloat16,\n","    device_map=\"auto\",\n",")"],"metadata":{"id":"fXNZdCA5FLGs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load a quantized model\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","import torch\n","\n","quant_config = BitsAndBytesConfig(load_in_8bit=True)\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"mistralai/Mistral-7B-v0.1\", quantization_config=quant_config, device_map=\"auto\"\n",")"],"metadata":{"id":"w-jz9vzxFWza"},"execution_count":null,"outputs":[]}]}