{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPDd4O0WluLv4sFe+aANYH1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ControlNet"],"metadata":{"id":"a6_N4e-nMWxK"}},{"cell_type":"markdown","source":["**ControlNet** models are adapters trained on top of another pretrained model. It allows for a greater degree of control over image generation by conditioning the model with an additional input image.\n","\n","We will explore the [`train_controlnet.py`](https://github.com/huggingface/diffusers/blob/main/examples/controlnet/train_controlnet.py) script.\n","\n","As always, make sure to install the `diffusers` library from source:\n","```bash\n","git clone https://github.com/huggingface/diffusers\n","cd diffusers\n","pip install .\n","```\n","\n","We need to navigate to the following folder and install the corresponding dependencies:\n","```bash\n","cd examples/controlnet\n","pip install -r requirements.txt\n","```\n","HuggingFace Accelerate is also helpful to train on multiple GPUs with mixed-precision.\n","```bash\n","pip install accelerate\n","```\n","\n","Now we can initialize a HuggingFace Accelerate environment\n","```bash\n","accelerate config\n","```\n","To set up a default Acclerate environment without choosing any configurations:\n","```bash\n","accelerate config default\n","```\n","Or if our environment does not support an interactive shell like a notebook, we can use:\n","```python\n","from accelerate.utils import write_basic_config\n","write_basic_config()\n","```"],"metadata":{"id":"4bVzCorz0s5K"}},{"cell_type":"markdown","source":["## Script parameters"],"metadata":{"id":"guyTSs1o1Qpw"}},{"cell_type":"markdown","source":["The training script provides many parameters to customize the training run. We can find all of the parameters and their descriptions in the `parse_args()` function.\n","\n","To speed up training with mixed precision using the `fp16` format, add the `--mixed_precision` parameter to the training command:\n","```bash\n","accelerate launch train_text_to_image.py --mixed_precision=\"fp16\"\n","```\n","Most of the parameters are identical to the parameters in the **Text-to-image** training guide. In addition to those, we have the following parameters to focus on the ControlNet:\n","* `--max_train_samples`: the number of training samples; this can be lowered for faster training, but if we want to stream really large datasets, we will need to include this parameter and the `--streaming` parmaeter in the training command\n","* `--gradient_accumulation_steps`: number of update steps to accumulate before the backward pass; this allows us to train with a bigger batch size than our GPU memory can typically handgle"],"metadata":{"id":"2vgMrthU2txF"}},{"cell_type":"markdown","source":["### Min-SNR weighting"],"metadata":{"id":"g2Xv6qiB3PiQ"}},{"cell_type":"markdown","source":["The `Min-SNR` weighting strategy can help with training by rebalancing the loss to achieve faster convergence. The training script supports predicting `epsilon` (noise) or `v_prediction`, but Min-SNR is compatible with both prediction types.\n","\n","We can add the `--snr_gamma` and set it to the recommended value of 5.0:\n","```bash\n","accelerate launch train_text_to_image.py --snr_gamma=5.0\n","```"],"metadata":{"id":"00VyonBR38iW"}},{"cell_type":"markdown","source":["# Training script"],"metadata":{"id":"3_h2aOJ341QK"}},{"cell_type":"markdown","source":["The general training script is provided in the **Text-to-image** training guide. This guide is specifically about ControlNet part.\n","\n","The training script has a `make_train_dataset` function for preprocessing the dataset with image transforms and caption tokenization. In addition to the usual caption tokenization and image transforms, the script also includes transforms for the conditioning image.\n","\n","In the `make_train_dataset`,\n","```python\n","    def tokenize_captions(examples, is_train=True):\n","        captions = []\n","        for caption in examples[caption_column]:\n","            if random.random() < args.proportion_empty_prompts:\n","                captions.append(\"\")\n","            elif isinstance(caption, str):\n","                captions.append(caption)\n","            elif isinstance(caption, (list, np.ndarray)):\n","                # take a random caption if there are multiple\n","                captions.append(random.choice(caption) if is_train else caption[0])\n","            else:\n","                raise ValueError(\n","                    f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n","                )\n","        inputs = tokenizer(\n","            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n","        )\n","        return inputs.input_ids\n","\n","    image_transforms = transforms.Compose(\n","        [\n","            transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n","            transforms.CenterCrop(args.resolution),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.5], [0.5]),\n","        ]\n","    )\n","\n","    conditioning_image_transforms = transforms.Compose(\n","        [\n","            transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n","            transforms.CenterCrop(args.resolution),\n","            transforms.ToTensor(),\n","        ]\n","    )\n","```\n","Within the `main()` function, we will find the code for loading the tokenizer, text encoder, scheduler, and models. This is also where the ControlNet model is loaded either from existing weights or randomly initialized from a UNet:\n","```python\n","    if args.controlnet_model_name_or_path:\n","        logger.info(\"Loading existing controlnet weights\")\n","        controlnet = ControlNetModel.from_pretrained(args.controlnet_model_name_or_path)\n","    else:\n","        logger.info(\"Initializing controlnet weights from unet\")\n","        controlnet = ControlNetModel.from_unet(unet)\n","```\n","The optimizer is set up to update the ControlNet parameters:\n","```python\n","    # Optimizer creation\n","    params_to_optimize = controlnet.parameters()\n","    optimizer = optimizer_class(\n","        params_to_optimize,\n","        lr=args.learning_rate,\n","        betas=(args.adam_beta1, args.adam_beta2),\n","        weight_decay=args.adam_weight_decay,\n","        eps=args.adam_epsilon,\n","    )\n","```\n","Finally, in the training loop, the conditioning text embeddings and image are passed to the down and mid-blocks of the ControlNet model."],"metadata":{"id":"jCYg7dSa5Aoq"}},{"cell_type":"markdown","source":["## Launch the script"],"metadata":{"id":"cmdKyZE5_d4M"}},{"cell_type":"markdown","source":["We will use the [`fusing/fill50k`](https://huggingface.co/datasets/fusing/fill50k) dataset to train our custom ControlNet as an example.\n","\n","We will set the environment variable `MODEL_NAME` to a model id on the Hub or a path to a local model and `OUTPUT_DIR` to where we want to save the model.\n","\n","Before training, we also need to download the following iamges to condition our training:\n","```bash\n","wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png\n","wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png\n","```\n","\n","Run the script\n","```bash\n","export MODEL_DIR=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n","export OUTPUT_DIR=\"path/to/save/model\"\n","\n","accelerate launch train_controlnet.py \\\n"," --pretrained_model_name_or_path=$MODEL_DIR \\\n"," --output_dir=$OUTPUT_DIR \\\n"," --dataset_name=fusing/fill50k \\\n"," --resolution=512 \\\n"," --learning_rate=1e-5 \\\n"," --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\" \\\n"," --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\n"," --train_batch_size=1 \\\n"," --gradient_accumulation_steps=4 \\\n"," --push_to_hub\n","```\n","\n","If we have a 12GB GPU, we will need `bitsandbytes` 8-bit optimizer, gradient checkpointing, xFormers, and set the gradients to `None` instead of zero to reduce our memory-usage:\n","```bash\n","export MODEL_DIR=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n","export OUTPUT_DIR=\"path/to/save/model\"\n","\n","accelerate launch train_controlnet.py \\\n"," --pretrained_model_name_or_path=$MODEL_DIR \\\n"," --output_dir=$OUTPUT_DIR \\\n"," --dataset_name=fusing/fill50k \\\n"," --resolution=512 \\\n"," --learning_rate=1e-5 \\\n"," --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\" \\\n"," --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\n"," --train_batch_size=1 \\\n"," --gradient_accumulation_steps=4 \\\n"," --use_8bit_adam \\\n"," --gradient_checkpointing \\\n"," --enable_xformers_memory_efficient_attention \\\n"," --set_grads_to_none \\\n"," --push_to_hub\n","```\n","\n","If we have a 16GB GPU, we can use bitsandbytes 8-bit optimizer and gradient checkpointing to optimize our training run.\n","```bash\n","export MODEL_DIR=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n","export OUTPUT_DIR=\"path/to/save/model\"\n","\n","accelerate launch train_controlnet.py \\\n"," --pretrained_model_name_or_path=$MODEL_DIR \\\n"," --output_dir=$OUTPUT_DIR \\\n"," --dataset_name=fusing/fill50k \\\n"," --resolution=512 \\\n"," --learning_rate=1e-5 \\\n"," --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\" \\\n"," --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\n"," --train_batch_size=1 \\\n"," --gradient_accumulation_steps=4 \\\n"," --use_8bit_adam \\\n"," --gradient_checkpointing \\\n"," --push_to_hub\n","```"],"metadata":{"id":"Sl01XQlf_ikV"}},{"cell_type":"markdown","source":["Once training is complete,"],"metadata":{"id":"50xTMZIYBdz8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jc5Mb8FsMUn8"},"outputs":[],"source":["from diffusers improt StableDiffusionControlNetPipeline, ControlNetModel\n","from diffusers.utils import load_image\n","import torch\n","\n","controlnet = ControlNetModel.from_pretrained(\n","    'path/to/our/custom/controlnet',\n","    torch_dtype=torch.float16\n",")\n","\n","pipeline = StableDiffusionControlNetPipeline.from_pretrained(\n","    'path/to/our/base/model',\n","    controlnet=controlnet,\n","    torch_dtype=torch.float16\n",").to('cuda')"]},{"cell_type":"code","source":["control_image = load_image(\"./conditioning_image_1.png\")\n","prompt = \"pale golden rod circle with old lace background\"\n","generator = torch.manual_seed(1111)\n","\n","image = pipeline(\n","    prompt,\n","    image=control_image,\n","    num_inference_steps=20,\n","    generator=generator,\n",").images[0]\n","image"],"metadata":{"id":"SWCIupPYBy8_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# SDXL\n","For SDXL models, we will use the [`train_controlnet_sdxl.py`](https://github.com/huggingface/diffusers/blob/main/examples/controlnet/train_controlnet_sdxl.py) script to train a ControlNet adapter for the SDXL model.\n","\n","We need to explore the SDXL training script in the SDXL training guide."],"metadata":{"id":"4vZVegHNB76x"}},{"cell_type":"code","source":[],"metadata":{"id":"8Jw3Xt69CM8r"},"execution_count":null,"outputs":[]}]}