{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOEwFbK1Ic3es8uG2qyAXK/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introduction to LangGraph - Module 6: Deployment"],"metadata":{"id":"bI49gtliSUpP"}},{"cell_type":"markdown","source":["# Pre-requisites"],"metadata":{"id":"DuVClgeFmsbv"}},{"cell_type":"code","source":["!pip install -qU langgraph_sdk langchain_openai langgraph langchain_core"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AZ-Kf2mRmuwr","executionInfo":{"status":"ok","timestamp":1756131798102,"user_tz":300,"elapsed":9492,"user":{"displayName":"Bin Liu","userId":"03585165976699804089"}},"outputId":"e7c37cda-6550-495f-b110-474fcbefebd7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["# Deployment Concepts"],"metadata":{"id":"COUB4v_NU1SW"}},{"cell_type":"markdown","source":["**Productionize existing LangGraph features**:\n","- Streaming\n","- Checkpoints / short-term memory\n","- Human-in-the-Loop\n","- Long-term memory\n","\n","**Additional features for agent productionisation**:\n","- Agent scheduling: Cron jobs to run agent on a schedule\n","- Long-running agents:\n","  - Background runs / task queue\n","- Better chat UX: support for double texting\n","- Agent configuration: versioning."],"metadata":{"id":"FHQrQNnuU33t"}},{"cell_type":"markdown","source":["# Creating a deployment"],"metadata":{"id":"j7cXvwd_0xRy"}},{"cell_type":"markdown","source":["## Code structure\n","\n","We want to create a deployment for our `task_mAIstro` agent we built in Module 5.\n","\n","To create a LangGraph Platform deployment, we need\n","- a LangGraph API configuration file - `langgraph.json`\n","- the graphs that implement the logic of the application - e.g., `task_maistro.py`\n","- a file that specifies dependencies required to run the application - `requirements.txt`\n","- environment variables needed for the application to run - `.env` file or `docker-compose.yml` file."],"metadata":{"id":"WZTFh35b01Lr"}},{"cell_type":"markdown","source":["### langgraph.json\n","\n","The `langgraph.json` is needed for the CLI to create a deployment.\n","\n","```json\n","{\n","    \"dockerfile_lines\": [],\n","    \"graphs\": {\n","      \"task_maistro\": \"./task_maistro.py:graph\"\n","    },\n","    \"python_version\": \"3.11\",\n","    \"dependencies\": [\n","      \".\"\n","    ]\n","  }\n","```"],"metadata":{"id":"cuRfPxtb35NT"}},{"cell_type":"markdown","source":["`graph` is the agent graph we want to work with, and we will call this agent `task_maistro`, which points the our `task_maistro.py` code and directly to the `graph` object"],"metadata":{"id":"jncTvci24qdl"}},{"cell_type":"markdown","source":["### task_maistro.py\n","\n","```python\n","import uuid\n","from datetime import datetime\n","\n","from pydantic import BaseModel, Field\n","\n","from trustcall import create_extractor\n","\n","from typing import Literal, Optional, TypedDict\n","\n","from langchain_core.runnables import RunnableConfig\n","from langchain_core.messages import merge_message_runs\n","from langchain_core.messages import SystemMessage, HumanMessage\n","\n","from langchain_openai import ChatOpenAI\n","\n","from langgraph.checkpoint.memory import MemorySaver\n","from langgraph.graph import StateGraph, MessagesState, START, END\n","from langgraph.store.base import BaseStore\n","from langgraph.store.memory import InMemoryStore\n","\n","import configuration\n","\n","## Utilities\n","\n","# Inspect the tool calls for Trustcall\n","class Spy:\n","    def __init__(self):\n","        self.called_tools = []\n","\n","    def __call__(self, run):\n","        q = [run]\n","        while q:\n","            r = q.pop()\n","            if r.child_runs:\n","                q.extend(r.child_runs)\n","            if r.run_type == \"chat_model\":\n","                self.called_tools.append(\n","                    r.outputs[\"generations\"][0][0][\"message\"][\"kwargs\"][\"tool_calls\"]\n","                )\n","\n","# Extract information from tool calls for both patches and new memories in Trustcall\n","def extract_tool_info(tool_calls, schema_name=\"Memory\"):\n","    \"\"\"Extract information from tool calls for both patches and new memories.\n","    \n","    Args:\n","        tool_calls: List of tool calls from the model\n","        schema_name: Name of the schema tool (e.g., \"Memory\", \"ToDo\", \"Profile\")\n","    \"\"\"\n","    # Initialize list of changes\n","    changes = []\n","    \n","    for call_group in tool_calls:\n","        for call in call_group:\n","            if call['name'] == 'PatchDoc':\n","                # Check if there are any patches\n","                if call['args']['patches']:\n","                    changes.append({\n","                        'type': 'update',\n","                        'doc_id': call['args']['json_doc_id'],\n","                        'planned_edits': call['args']['planned_edits'],\n","                        'value': call['args']['patches'][0]['value']\n","                    })\n","                else:\n","                    # Handle case where no changes were needed\n","                    changes.append({\n","                        'type': 'no_update',\n","                        'doc_id': call['args']['json_doc_id'],\n","                        'planned_edits': call['args']['planned_edits']\n","                    })\n","            elif call['name'] == schema_name:\n","                changes.append({\n","                    'type': 'new',\n","                    'value': call['args']\n","                })\n","\n","    # Format results as a single string\n","    result_parts = []\n","    for change in changes:\n","        if change['type'] == 'update':\n","            result_parts.append(\n","                f\"Document {change['doc_id']} updated:\\n\"\n","                f\"Plan: {change['planned_edits']}\\n\"\n","                f\"Added content: {change['value']}\"\n","            )\n","        elif change['type'] == 'no_update':\n","            result_parts.append(\n","                f\"Document {change['doc_id']} unchanged:\\n\"\n","                f\"{change['planned_edits']}\"\n","            )\n","        else:\n","            result_parts.append(\n","                f\"New {schema_name} created:\\n\"\n","                f\"Content: {change['value']}\"\n","            )\n","    \n","    return \"\\n\\n\".join(result_parts)\n","\n","## Schema definitions\n","\n","# User profile schema\n","class Profile(BaseModel):\n","    \"\"\"This is the profile of the user you are chatting with\"\"\"\n","    name: Optional[str] = Field(description=\"The user's name\", default=None)\n","    location: Optional[str] = Field(description=\"The user's location\", default=None)\n","    job: Optional[str] = Field(description=\"The user's job\", default=None)\n","    connections: list[str] = Field(\n","        description=\"Personal connection of the user, such as family members, friends, or coworkers\",\n","        default_factory=list\n","    )\n","    interests: list[str] = Field(\n","        description=\"Interests that the user has\",\n","        default_factory=list\n","    )\n","\n","# ToDo schema\n","class ToDo(BaseModel):\n","    task: str = Field(description=\"The task to be completed.\")\n","    time_to_complete: Optional[int] = Field(description=\"Estimated time to complete the task (minutes).\")\n","    deadline: Optional[datetime] = Field(\n","        description=\"When the task needs to be completed by (if applicable)\",\n","        default=None\n","    )\n","    solutions: list[str] = Field(\n","        description=\"List of specific, actionable solutions (e.g., specific ideas, service providers, or concrete options relevant to completing the task)\",\n","        min_items=1,\n","        default_factory=list\n","    )\n","    status: Literal[\"not started\", \"in progress\", \"done\", \"archived\"] = Field(\n","        description=\"Current status of the task\",\n","        default=\"not started\"\n","    )\n","\n","## Initialize the model and tools\n","\n","# Update memory tool\n","class UpdateMemory(TypedDict):\n","    \"\"\" Decision on what memory type to update \"\"\"\n","    update_type: Literal['user', 'todo', 'instructions']\n","\n","# Initialize the model\n","model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n","\n","## Create the Trustcall extractors for updating the user profile and ToDo list\n","profile_extractor = create_extractor(\n","    model,\n","    tools=[Profile],\n","    tool_choice=\"Profile\",\n",")\n","\n","## Prompts\n","\n","# Chatbot instruction for choosing what to update and what tools to call\n","MODEL_SYSTEM_MESSAGE = \"\"\"{task_maistro_role}\n","\n","You have a long term memory which keeps track of three things:\n","1. The user's profile (general information about them)\n","2. The user's ToDo list\n","3. General instructions for updating the ToDo list\n","\n","Here is the current User Profile (may be empty if no information has been collected yet):\n","<user_profile>\n","{user_profile}\n","</user_profile>\n","\n","Here is the current ToDo List (may be empty if no tasks have been added yet):\n","<todo>\n","{todo}\n","</todo>\n","\n","Here are the current user-specified preferences for updating the ToDo list (may be empty if no preferences have been specified yet):\n","<instructions>\n","{instructions}\n","</instructions>\n","\n","Here are your instructions for reasoning about the user's messages:\n","\n","1. Reason carefully about the user's messages as presented below.\n","\n","2. Decide whether any of the your long-term memory should be updated:\n","- If personal information was provided about the user, update the user's profile by calling UpdateMemory tool with type `user`\n","- If tasks are mentioned, update the ToDo list by calling UpdateMemory tool with type `todo`\n","- If the user has specified preferences for how to update the ToDo list, update the instructions by calling UpdateMemory tool with type `instructions`\n","\n","3. Tell the user that you have updated your memory, if appropriate:\n","- Do not tell the user you have updated the user's profile\n","- Tell the user them when you update the todo list\n","- Do not tell the user that you have updated instructions\n","\n","4. Err on the side of updating the todo list. No need to ask for explicit permission.\n","\n","5. Respond naturally to user user after a tool call was made to save memories, or if no tool call was made.\"\"\"\n","\n","# Trustcall instruction\n","TRUSTCALL_INSTRUCTION = \"\"\"Reflect on following interaction.\n","\n","Use the provided tools to retain any necessary memories about the user.\n","\n","Use parallel tool calling to handle updates and insertions simultaneously.\n","\n","System Time: {time}\"\"\"\n","\n","# Instructions for updating the ToDo list\n","CREATE_INSTRUCTIONS = \"\"\"Reflect on the following interaction.\n","\n","Based on this interaction, update your instructions for how to update ToDo list items. Use any feedback from the user to update how they like to have items added, etc.\n","\n","Your current instructions are:\n","\n","<current_instructions>\n","{current_instructions}\n","</current_instructions>\"\"\"\n","\n","## Node definitions\n","\n","def task_mAIstro(state: MessagesState, config: RunnableConfig, store: BaseStore):\n","\n","    \"\"\"Load memories from the store and use them to personalize the chatbot's response.\"\"\"\n","    \n","    # Get the user ID from the config\n","    configurable = configuration.Configuration.from_runnable_config(config)\n","    user_id = configurable.user_id\n","    todo_category = configurable.todo_category\n","    task_maistro_role = configurable.task_maistro_role\n","\n","   # Retrieve profile memory from the store\n","    namespace = (\"profile\", todo_category, user_id)\n","    memories = store.search(namespace)\n","    if memories:\n","        user_profile = memories[0].value\n","    else:\n","        user_profile = None\n","\n","    # Retrieve people memory from the store\n","    namespace = (\"todo\", todo_category, user_id)\n","    memories = store.search(namespace)\n","    todo = \"\\n\".join(f\"{mem.value}\" for mem in memories)\n","\n","    # Retrieve custom instructions\n","    namespace = (\"instructions\", todo_category, user_id)\n","    memories = store.search(namespace)\n","    if memories:\n","        instructions = memories[0].value\n","    else:\n","        instructions = \"\"\n","    \n","    system_msg = MODEL_SYSTEM_MESSAGE.format(task_maistro_role=task_maistro_role, user_profile=user_profile, todo=todo, instructions=instructions)\n","\n","    # Respond using memory as well as the chat history\n","    response = model.bind_tools([UpdateMemory], parallel_tool_calls=False).invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n","\n","    return {\"messages\": [response]}\n","\n","def update_profile(state: MessagesState, config: RunnableConfig, store: BaseStore):\n","\n","    \"\"\"Reflect on the chat history and update the memory collection.\"\"\"\n","    \n","    # Get the user ID from the config\n","    configurable = configuration.Configuration.from_runnable_config(config)\n","    user_id = configurable.user_id\n","    todo_category = configurable.todo_category\n","\n","    # Define the namespace for the memories\n","    namespace = (\"profile\", todo_category, user_id)\n","\n","    # Retrieve the most recent memories for context\n","    existing_items = store.search(namespace)\n","\n","    # Format the existing memories for the Trustcall extractor\n","    tool_name = \"Profile\"\n","    existing_memories = ([(existing_item.key, tool_name, existing_item.value)\n","                          for existing_item in existing_items]\n","                          if existing_items\n","                          else None\n","                        )\n","\n","    # Merge the chat history and the instruction\n","    TRUSTCALL_INSTRUCTION_FORMATTED=TRUSTCALL_INSTRUCTION.format(time=datetime.now().isoformat())\n","    updated_messages=list(merge_message_runs(messages=[SystemMessage(content=TRUSTCALL_INSTRUCTION_FORMATTED)] + state[\"messages\"][:-1]))\n","\n","    # Invoke the extractor\n","    result = profile_extractor.invoke({\"messages\": updated_messages,\n","                                         \"existing\": existing_memories})\n","\n","    # Save save the memories from Trustcall to the store\n","    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n","        store.put(namespace,\n","                  rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n","                  r.model_dump(mode=\"json\"),\n","            )\n","    tool_calls = state['messages'][-1].tool_calls\n","    # Return tool message with update verification\n","    return {\"messages\": [{\"role\": \"tool\", \"content\": \"updated profile\", \"tool_call_id\":tool_calls[0]['id']}]}\n","\n","def update_todos(state: MessagesState, config: RunnableConfig, store: BaseStore):\n","\n","    \"\"\"Reflect on the chat history and update the memory collection.\"\"\"\n","    \n","    # Get the user ID from the config\n","    configurable = configuration.Configuration.from_runnable_config(config)\n","    user_id = configurable.user_id\n","    todo_category = configurable.todo_category\n","\n","    # Define the namespace for the memories\n","    namespace = (\"todo\", todo_category, user_id)\n","\n","    # Retrieve the most recent memories for context\n","    existing_items = store.search(namespace)\n","\n","    # Format the existing memories for the Trustcall extractor\n","    tool_name = \"ToDo\"\n","    existing_memories = ([(existing_item.key, tool_name, existing_item.value)\n","                          for existing_item in existing_items]\n","                          if existing_items\n","                          else None\n","                        )\n","\n","    # Merge the chat history and the instruction\n","    TRUSTCALL_INSTRUCTION_FORMATTED=TRUSTCALL_INSTRUCTION.format(time=datetime.now().isoformat())\n","    updated_messages=list(merge_message_runs(messages=[SystemMessage(content=TRUSTCALL_INSTRUCTION_FORMATTED)] + state[\"messages\"][:-1]))\n","\n","    # Initialize the spy for visibility into the tool calls made by Trustcall\n","    spy = Spy()\n","    \n","    # Create the Trustcall extractor for updating the ToDo list\n","    todo_extractor = create_extractor(\n","    model,\n","    tools=[ToDo],\n","    tool_choice=tool_name,\n","    enable_inserts=True\n","    ).with_listeners(on_end=spy)\n","\n","    # Invoke the extractor\n","    result = todo_extractor.invoke({\"messages\": updated_messages,\n","                                         \"existing\": existing_memories})\n","\n","    # Save save the memories from Trustcall to the store\n","    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n","        store.put(namespace,\n","                  rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n","                  r.model_dump(mode=\"json\"),\n","            )\n","        \n","    # Respond to the tool call made in task_mAIstro, confirming the update    \n","    tool_calls = state['messages'][-1].tool_calls\n","\n","    # Extract the changes made by Trustcall and add the the ToolMessage returned to task_mAIstro\n","    todo_update_msg = extract_tool_info(spy.called_tools, tool_name)\n","    return {\"messages\": [{\"role\": \"tool\", \"content\": todo_update_msg, \"tool_call_id\":tool_calls[0]['id']}]}\n","\n","def update_instructions(state: MessagesState, config: RunnableConfig, store: BaseStore):\n","\n","    \"\"\"Reflect on the chat history and update the memory collection.\"\"\"\n","    \n","    # Get the user ID from the config\n","    configurable = configuration.Configuration.from_runnable_config(config)\n","    user_id = configurable.user_id\n","    todo_category = configurable.todo_category\n","    \n","    namespace = (\"instructions\", todo_category, user_id)\n","\n","    existing_memory = store.get(namespace, \"user_instructions\")\n","        \n","    # Format the memory in the system prompt\n","    system_msg = CREATE_INSTRUCTIONS.format(current_instructions=existing_memory.value if existing_memory else None)\n","    new_memory = model.invoke([SystemMessage(content=system_msg)]+state['messages'][:-1] + [HumanMessage(content=\"Please update the instructions based on the conversation\")])\n","\n","    # Overwrite the existing memory in the store\n","    key = \"user_instructions\"\n","    store.put(namespace, key, {\"memory\": new_memory.content})\n","    tool_calls = state['messages'][-1].tool_calls\n","    # Return tool message with update verification\n","    return {\"messages\": [{\"role\": \"tool\", \"content\": \"updated instructions\", \"tool_call_id\":tool_calls[0]['id']}]}\n","\n","# Conditional edge\n","def route_message(state: MessagesState, config: RunnableConfig, store: BaseStore) -> Literal[END, \"update_todos\", \"update_instructions\", \"update_profile\"]:\n","\n","    \"\"\"Reflect on the memories and chat history to decide whether to update the memory collection.\"\"\"\n","    message = state['messages'][-1]\n","    if len(message.tool_calls) ==0:\n","        return END\n","    else:\n","        tool_call = message.tool_calls[0]\n","        if tool_call['args']['update_type'] == \"user\":\n","            return \"update_profile\"\n","        elif tool_call['args']['update_type'] == \"todo\":\n","            return \"update_todos\"\n","        elif tool_call['args']['update_type'] == \"instructions\":\n","            return \"update_instructions\"\n","        else:\n","            raise ValueError\n","\n","# Create the graph + all nodes\n","builder = StateGraph(MessagesState, config_schema=configuration.Configuration)\n","\n","# Define the flow of the memory extraction process\n","builder.add_node(task_mAIstro)\n","builder.add_node(update_todos)\n","builder.add_node(update_profile)\n","builder.add_node(update_instructions)\n","\n","# Define the flow\n","builder.add_edge(START, \"task_mAIstro\")\n","builder.add_conditional_edges(\"task_mAIstro\", route_message)\n","builder.add_edge(\"update_todos\", \"task_mAIstro\")\n","builder.add_edge(\"update_profile\", \"task_mAIstro\")\n","builder.add_edge(\"update_instructions\", \"task_mAIstro\")\n","\n","# Compile the graph\n","graph = builder.compile()\n","```"],"metadata":{"id":"8b1S2Szm4Col"}},{"cell_type":"markdown","source":["### requirements.txt\n","\n","```python\n","langgraph\n","langchain-core\n","langchain-community\n","langchain-openai\n","trustcall\n","```"],"metadata":{"id":"DMDrzjbu4Oub"}},{"cell_type":"markdown","source":["### docker-compose-example.yml\n","\n","```yaml\n","volumes:\n","    langgraph-data:\n","        driver: local\n","services:\n","    langgraph-redis:\n","        image: redis:6\n","        healthcheck:\n","            test: redis-cli ping\n","            interval: 5s\n","            timeout: 1s\n","            retries: 5\n","        ports:\n","            - \"6379:6379\"\n","    langgraph-postgres:\n","        image: postgres:16\n","        ports:\n","            - \"5432:5432\"\n","        environment:\n","            POSTGRES_DB: postgres\n","            POSTGRES_USER: postgres\n","            POSTGRES_PASSWORD: postgres\n","        volumes:\n","            - langgraph-data:/var/lib/postgresql/data\n","        healthcheck:\n","            test: pg_isready -U postgres\n","            start_period: 10s\n","            timeout: 1s\n","            retries: 5\n","            interval: 5s\n","    langgraph-api:\n","        image: \"my-image\"\n","        ports:\n","            - \"8123:8000\"\n","        depends_on:\n","            langgraph-redis:\n","                condition: service_healthy\n","            langgraph-postgres:\n","                condition: service_healthy\n","        environment:\n","            REDIS_URI: redis://langgraph-redis:6379\n","            OPENAI_API_KEY: \"your_openai_api_key\"\n","            LANGSMITH_API_KEY: \"your_langsmith_api_key\"\n","            POSTGRES_URI: postgres://postgres:postgres@langgraph-postgres:5432/postgres?sslmode=disable\n","```"],"metadata":{"id":"Pt70zy1i4Yu3"}},{"cell_type":"markdown","source":["## CLI\n","\n","The LangGraph CLI is a command-line interface for creating a LangGraph Platform deployemnt.\n","\n","```bash\n","pip install -U langgraph-cli\n","```"],"metadata":{"id":"Aod2lsDB1cP0"}},{"cell_type":"markdown","source":["## Build Docker image for LangGraph server\n","\n","We will first use the LangGraph CLI to create a Docker image for the LangGraph server. This will package our graph and dependencies into a Docker image.\n","\n","A *Docker image* is a template for a Docker container that contains the code and dependencies required to run the application.\n","\n","Ensure that Docker is installed and then run the following command to create the Docker image, for example, here we called `my-image`:\n","```bash\n","cd ./deployment\n","langgraph build -t my-image\n","```"],"metadata":{"id":"9U5mRow-1wQt"}},{"cell_type":"markdown","source":["## Set up Redis and PostreSQL\n","\n","If we already have Redis and PostgreSQL running (e.g., locally or on other servers), then create and run the LangGraph server container by itself with the URIs for Redis and PostgreSQL:\n","```bash\n","docker run \\\n","    --env-file .env \\\n","    -p 8123:8000 \\\n","    -e REDIS_URI=\"<redis_uri>\" \\\n","    -e LANGSMITH_API_KEY=\"<langsmith_api_key>\" \\\n","    my-image\n","```\n","\n","\n","Alternatively, we can use a `docker-compose.yml` file to create three separate containers based on the services defined:\n","- `langgraph-redis`: create a new container using the official Redis image.\n","- `langgraph-postgres`: create a new container using the official Postgres image.\n","- `langgraph-api`: create a new container using our pre-built image.\n","\n","Then we can copy the `docker-compose-example.yml` and add the following environment variables to run the deployed `task_maistro` app:\n","- `IMAGE_NAME` (e.g., `my-image`)\n","- `LANGSMITH_API_KEY`\n","- `OPENAI_API_KEY`\n","\n","Finally, we can launch the deployment:\n","```bash\n","cd ./deployment\n","docker compose up\n","```"],"metadata":{"id":"VfVjsVzj2Sey"}},{"cell_type":"markdown","source":["# Connecting to a LangGraph Platform Deployment\n","\n","We have used the provided `docker-compose.yml` file to create three separate containers based on the services defined:\n","- `langgraph-redis`: create a new container using the official Redis image.\n","- `langgraph-postres`: create a new container using the official Postgres image.\n","- `langgraph-api`: create a new ocntainer using our pre-built `task_maistro` Docker image.\n","\n","Finally, we run\n","```bash\n","cd ./deployment\n","docker compose up\n","```\n","\n","Once running, we can access the deployment through:\n","- API: http://localhost:8123\n","- Docs: http://localhost:8123/docs\n","- LangGraph Studio: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:8123"],"metadata":{"id":"ECZjek8Ihuad"}},{"cell_type":"markdown","source":["## Using the API\n","\n","LangGraph Server exposes many API endpoints for interacting with the deployed agent. We can group these endpoints into a few common agent needs:\n","- **Runs**: atomic agent executions\n","- **Threads**: multi-turn interactions or humna-in-the-loop\n","- **Store**: long-term memory\n","\n","We can test requests directly in the API docs: http://localhost:8123/docs#tag/thread-runs (make sure our agent is deployed and running)"],"metadata":{"id":"Op187TiQkucb"}},{"cell_type":"markdown","source":["## LangGraph SDK\n","\n","The LangGraph SDKs (Python and JavaScript) provide a developer-firendly interface to interact with the LangGraph Server API presented above."],"metadata":{"id":"-FARXi5Emf1h"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"u7ek3r3JSSTj"},"outputs":[],"source":["from langgraph_sdk import get_client\n","\n","# Connect via SDK\n","url_for_cli_deployment = \"http://localhost:8123\"\n","client = get_client(url=url_for_cli_deployment)"]},{"cell_type":"markdown","source":["## Remote Graph\n","\n","If we are working in the LangGraph library, *Remote Graph* is also a useful way to connect directly to the graph."],"metadata":{"id":"sDJMPW2xm-Il"}},{"cell_type":"code","source":["from langgraph.pregel.remote import RemoteGraph\n","from langchain_core.messages import convert_to_messages, HumanMessage, SystemMessage\n","\n","# Connect via remote graph\n","url = \"http://localhost:8123\"\n","graph_name = \"task_maistro\"\n","remote_graph = RemoteGraph(graph_name, url=url)"],"metadata":{"id":"7R9X1q-MnFEJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Runs\n","\n","A *run* represents a single execution of our graph. Each time a client makes a request:\n","1. The HTTP worker generates a unique run ID\n","2. This run and its results are stored in PostgreSQL\n","3. We can query these runs to:\n","  - check their status\n","  - get their results\n","  - track execution history"],"metadata":{"id":"m0Oq_VUrnZcI"}},{"cell_type":"markdown","source":["### Background Runs\n","\n","The LangGraph server supports two types of runs:\n","- **Fire and forget** - launch a run in the backround, but do not wait for it to finish\n","- **Waiting on a reply (blocking or polling) - launch a run and wait/stream its output\n","\n","Background runs and polling are quite useful when working with long-running agents."],"metadata":{"id":"td5jxDZgoPCT"}},{"cell_type":"markdown","source":["We can see how the background run works:"],"metadata":{"id":"zaEpMTlqopXk"}},{"cell_type":"code","source":["# Create a thread\n","thread = await client.threads.create()\n","thread"],"metadata":{"id":"-JJwrTjUoOwI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check any existing runs on a thread\n","thread = await client.threads.create()\n","runs = await client.runs.list(thread['thread_id'])\n","print(runs)"],"metadata":{"id":"tS6YJff8ov0U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ensure we've created some ToDos and saved them to my user_id\n","user_input = \"Add a ToDo to finish booking travel to Hong Kong by end of next week. Also, add a ToDo to call parents back about Thanksgiving plans.\"\n","config = {\"configurable\": {\"user_id\": \"Test\"}}\n","graph_name = \"task_maistro\"\n","run = await client.runs.create(thread[\"thread_id\"], graph_name, input={\"messages\": [HumanMessage(content=user_input)]}, config=config)"],"metadata":{"id":"ADA5FNoRo6fc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the run status\n","print(await client.runs.get(thread[\"thread_id\"], run[\"run_id\"]))"],"metadata":{"id":"EJamGu5so8WB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The returned response has `'status': 'pending'` because it is still running.\n","\n","If we want to wait until the run completes, making it a blocking run, then we can use `client.run.join` to wait until the run completes.\n","\n","This ensures that no new runs are started until the current run completes on the thread."],"metadata":{"id":"kM7OjprOo-Mi"}},{"cell_type":"code","source":["# Wait until the run completes\n","await client.runs.join(thread[\"thread_id\"], run[\"run_id\"])\n","print(await client.runs.get(thread[\"thread_id\"], run[\"run_id\"]))"],"metadata":{"id":"htZm_oHlpxJW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The run response `'status': 'success'` means the run has completed."],"metadata":{"id":"RFNeWi-epztj"}},{"cell_type":"markdown","source":["### Streaming Runs\n","\n","Each time a client makes a streaming request:\n","1. The HTTP worker generates a unique run ID\n","2. The Queue worker begins work on the run\n","3. During execution, the Queue worker publishes update to Redis\n","4. The HTTP worker subscribes to updates from Redis for this run, and returns them to the client\n","\n","We will use the streaming tokens to highlight this streaming run.\n","\n","Streaming tokens back to the client is especially useful when working with production agents that may take a while to complete.\n","\n","We will stream tokens using `stream_mode=\"messages-tuple\"`"],"metadata":{"id":"ibdtNftfqGYu"}},{"cell_type":"code","source":["user_input = \"What ToDo should I focus on first.\"\n","\n","async for chunk in client.runs.stream(\n","    thread[\"thread_id\"],\n","    graph_name,\n","    input={\"messages\": [HumanMessage(content=user_input)]},\n","    config=config,\n","    stream_mode=\"messages-tuple\"\n","):\n","\n","    if chunk.event == \"messages\":\n","        print(\"\".join(data_item['content'] for data_item in chunk.data if 'content' in data_item), end=\"\", flush=True)"],"metadata":{"id":"6feXU9jTqEy6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Threads\n","\n","A **thread** supports *multi-turn* interactions, whereas a run is only a single execution of the graph.\n","\n","When the client makes a graph execution with a `thread_id`, the server will save all checkpoints (steps) in the run to the thread in the Postgres database.\n","\n","The server allows us to check the status of created threads.\n"],"metadata":{"id":"_DF-3tljuCqp"}},{"cell_type":"markdown","source":["### Check thread state\n","\n","We can easily access the state checkpoints saved to any specific thread."],"metadata":{"id":"njMrlHkeufax"}},{"cell_type":"code","source":["thread_state = await client.threads.get_state(thread['thread_id'])\n","for m in convert_to_messages(thread_state['values']['messages']):\n","    m.pretty_print()"],"metadata":{"id":"ZICEcrmbuerf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Copy threads\n","\n","We can also copy (i.e., \"fork\") an existing thread.\n","\n","This will keep the existing thread's history, but allow us to create independent runs that do not affect the original thread."],"metadata":{"id":"OG-hOOROul69"}},{"cell_type":"code","source":["# Copy the thread\n","copied_thread = await client.threads.copy(thread['thread_id'])"],"metadata":{"id":"sMjGwZ4luzkz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the state of the copied thread\n","copied_thread_state = await client.threads.get_state(copied_thread['thread_id'])\n","for m in convert_to_messages(copied_thread_state['values']['messages']):\n","    m.pretty_print()"],"metadata":{"id":"BLg5py6pu2Xq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Human-in-the-loop\n","\n","We can search, edit, and continue graph execution from any prior checkpoint."],"metadata":{"id":"8ago7hP2u49N"}},{"cell_type":"code","source":["# Get the history of the thread\n","states = await client.threads.get_history(thread['thread_id'])\n","\n","# Pick a state update to fork\n","to_fork = states[-2]\n","to_fork['values']"],"metadata":{"id":"mQivUM-FvAHC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["to_fork['values']['messages'][0]['id']"],"metadata":{"id":"SxRTMSWAvBrx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["to_fork['next']"],"metadata":{"id":"Z3lCtZCSvCZH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["to_fork['checkpoint_id']"],"metadata":{"id":"ougFXhOtvDFh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To edit the state, we need to use our reducer in the `messages` object:\n","- It will append, unless we supply a message ID.\n","- We supply the message ID to overwrite the message, rather than appending to state."],"metadata":{"id":"Di7NGzJGvEOe"}},{"cell_type":"code","source":["forked_input = {\"messages\": HumanMessage(\n","                                content=\"Give me a summary of all ToDos that need to be done in the next week.\",\n","                                id=to_fork['values']['messages'][0]['id']\n","                            )}\n","\n","# Update the state, creating a new checkpoint in the thread\n","forked_config = await client.threads.update_state(\n","    thread[\"thread_id\"],\n","    forked_input,\n","    checkpoint_id=to_fork['checkpoint_id']\n",")"],"metadata":{"id":"nZnf3KWHvRle"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run the graph from the new checkpoint in the thread\n","async for chunk in client.runs.stream(\n","    thread[\"thread_id\"],\n","    graph_name,\n","    input=None,\n","    config=config,\n","    checkpoint_id=forked_config['checkpoint_id'],\n","    stream_mode=\"messages-tuple\"\n","):\n","\n","    if chunk.event == \"messages\":\n","        print(\"\".join(data_item['content'] for data_item in chunk.data if 'content' in data_item), end=\"\", flush=True)"],"metadata":{"id":"X3mxNeuPvYLR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Across-thread memory\n","\n","The LangGraph memory Store can be used to save information across threads.\n","\n","Our deployed graph, `task_maistro`, uses the `store` to save information - such as ToDos - namespaced to the `user_id`.\n","\n","Our deployment includes a Postgres database, which stores these long-term (across-thread) memories.\n","\n","There are several methods available for interacting with the store in our deployment uisng the LangGraph SDK."],"metadata":{"id":"uhxMp2Pwvc7e"}},{"cell_type":"markdown","source":["### Search items\n","\n","The `task_maistro` graph uses the `store` to save ToDos namespaced by default to (`todo`, `todo_category`, `user_id`).\n","\n","The `todo_category` is by default set to `general` (seen in `deployment/configuration.py`)\n","\n","We can supply this tuple to search for all ToDos:"],"metadata":{"id":"so29PhJiv2Cp"}},{"cell_type":"code","source":["items = await client.store.search_items(\n","    (\"todo\", \"general\", \"Test\"),\n","    limit=5,\n","    offset=0\n",")\n","items['items']"],"metadata":{"id":"alKfneCBv0xv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Add items\n","\n","In our graph, we call `put` to add items to the store.\n","\n","We can use `put` with the SDK if we want to directly add items to the store outside our graph."],"metadata":{"id":"YAz3DEadwJoh"}},{"cell_type":"code","source":["from uuid import uuid4\n","await client.store.put_item(\n","    (\"testing\", \"Test\"),\n","    key=str(uuid4()),\n","    value={\"todo\": \"Test SDK put_item\"},\n",")"],"metadata":{"id":"GQd53L4zwR5v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["items = await client.store.search_items(\n","    (\"testing\", \"Test\"),\n","    limit=5,\n","    offset=0\n",")\n","items['items']"],"metadata":{"id":"ZSNtEqeGwVPt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Delete items\n","\n","We can use the SDK to delete items from the store by key."],"metadata":{"id":"vMCmmUhuwWNH"}},{"cell_type":"code","source":["[item['key'] for item in items['items']]"],"metadata":{"id":"cjnsgfcowZd4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["await client.store.delete_item(\n","       (\"testing\", \"Test\"),\n","        key='3de441ba-8c79-4beb-8f52-00e4dcba68d4',\n","    )"],"metadata":{"id":"uUTwe7nIwa6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["items = await client.store.search_items(\n","    (\"testing\", \"Test\"),\n","    limit=5,\n","    offset=0\n",")\n","items['items']"],"metadata":{"id":"NFHS2TWswb7w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Double Texting\n","\n","Seamless handling of double texting is important for handling real-world usage scenarios, especially in chat applications.\n","\n","Users can send multiple messages in a row before the prior run(s) complete, and we want to ensure that we handle this gracefully."],"metadata":{"id":"1c5TNayMx8wR"}},{"cell_type":"markdown","source":["## Reject\n","\n","A simple approach is to **reject** any new runs until the current run completes."],"metadata":{"id":"gUjj1Mv_ykgL"}},{"cell_type":"code","source":["from langgraph_sdk import get_client\n","\n","url_for_cli_deployment = \"http://localhost:8123\"\n","client = get_client(url=url_for_cli_deployment)"],"metadata":{"id":"LkxbOfoDyore"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import httpx\n","from langchain_core.messages import HumanMessage\n","\n","# Create a thread\n","thread = await client.threads.create()\n","\n","# Create to dos\n","user_input_1 = \"Add a ToDo to follow-up with DI Repairs.\"\n","user_input_2 = \"Add a ToDo to mount dresser to the wall.\"\n","config = {\"configurable\": {\"user_id\": \"Test-Double-Texting\"}}\n","graph_name = \"task_maistro\"\n","\n","\n","# Create a run\n","run = await client.runs.create(\n","    thread['thread_id'],\n","    graph_name,\n","    input={'messages': [HumanMessage(content=user_input_1)]},\n","    config=config\n",")\n","\n","\n","# What happens if we pass another run immediately\n","try:\n","    await client.runs.create(\n","        thread['thread_id'],\n","        graph_name,\n","        input={'messages': [HumanMessage(content=user_input_2)]},\n","        config=config,\n","        multitask_strategy='reject',  # Here we use reject strategy\n","    )\n","except httpx.HTTPStatusError as e:\n","    print(\"Failed to start concurrent run\", e)"],"metadata":{"id":"AgnT9E_s0CQs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This triggers a `Client error '409 Conflict'`."],"metadata":{"id":"x36ntLHl1_KI"}},{"cell_type":"code","source":["from langchain_core.messages import convert_to_messages\n","\n","# Wait until the original run completes\n","await client.runs.join(thread[\"thread_id\"], run[\"run_id\"])\n","\n","# Get the state of the thread\n","state = await client.threads.get_state(thread[\"thread_id\"])\n","for m in convert_to_messages(state[\"values\"][\"messages\"]):\n","    m.pretty_print()"],"metadata":{"id":"tm9sN-lU2GyL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see we only finsihed the first run in the chat history."],"metadata":{"id":"2ZPufjAD2MNH"}},{"cell_type":"markdown","source":["## Enqueue\n","\n","We can **enqueue** any new runs until the current run completes."],"metadata":{"id":"SVu2eREH2S-v"}},{"cell_type":"code","source":["# Create a new thread\n","thread = await client.threads.create()\n","\n","# Create new ToDos\n","user_input_1 = \"Send Erik his t-shirt gift this weekend.\"\n","user_input_2 = \"Get cash and pay nanny for 2 weeks. Do this by Friday.\"\n","config = {\"configurable\": {\"user_id\": \"Test-Double-Texting\"}}\n","graph_name = \"task_maistro\"\n","\n","\n","# Create two runs\n","first_run = await client.runs.create(\n","    thread[\"thread_id\"],\n","    graph_name,\n","    input={\"messages\": [HumanMessage(content=user_input_1)]},\n","    config=config,\n",")\n","second_run = await client.runs.create(\n","    thread[\"thread_id\"],\n","    graph_name,\n","    input={\"messages\": [HumanMessage(content=user_input_2)]},\n","    config=config,\n","    multitask_strategy=\"enqueue\", # Here we use enqueue strategy\n",")"],"metadata":{"id":"vXOCw7d32RTi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Wait until the second run completes\n","await client.runs.join(thread[\"thread_id\"], second_run[\"run_id\"])\n","\n","# Get the state of the thread\n","state = await client.threads.get_state(thread[\"thread_id\"])\n","for m in convert_to_messages(state[\"values\"][\"messages\"]):\n","    m.pretty_print()"],"metadata":{"id":"qVu1zmJz2mRv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can see the second run was processed after the first run was finished."],"metadata":{"id":"quATzSZ62p-J"}},{"cell_type":"markdown","source":["## Interrupt\n","\n","We can **interrupt** the current run, but save all the work that has been done so far up to that point."],"metadata":{"id":"B5M9-eEL2uiD"}},{"cell_type":"code","source":["import asyncio\n","\n","# Create a new thread\n","thread = await client.threads.create()\n","\n","# Create new ToDos\n","user_input_1 = \"Give me a summary of my ToDos due tomrrow.\"\n","user_input_2 = \"Never mind, create a ToDo to Order Ham for Thanksgiving by next Friday.\"\n","config = {\"configurable\": {\"user_id\": \"Test-Double-Texting\"}}\n","graph_name = \"task_maistro\""],"metadata":{"id":"-44No3e321xk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a run used to be interrupted\n","interrupted_run = await client.runs.create(\n","    thread[\"thread_id\"],\n","    graph_name,\n","    input={\"messages\": [HumanMessage(content=user_input_1)]},\n","    config=config,\n",")\n","\n","# Wait for some of run 1 to complete so that we can see it in the thread\n","await asyncio.sleep(1)\n","\n","# Create a second run to interrupt the first run\n","second_run = await client.runs.create(\n","    thread[\"thread_id\"],\n","    graph_name,\n","    input={\"messages\": [HumanMessage(content=user_input_2)]},\n","    config=config,\n","    multitask_strategy=\"interrupt\",   # here we use interrupt strategy\n",")"],"metadata":{"id":"qKg1555426mS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Wait until the second run completes\n","await client.runs.join(thread[\"thread_id\"], second_run[\"run_id\"])\n","\n","# Get the state of the thread\n","state = await client.threads.get_state(thread[\"thread_id\"])\n","for m in convert_to_messages(state[\"values\"][\"messages\"]):\n","    m.pretty_print()"],"metadata":{"id":"GQFU9pme3F8P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see the initial run is saved with a status `interrupted`."],"metadata":{"id":"N871Y2Dc3G-m"}},{"cell_type":"code","source":["# Confirm that the first run was interrupted\n","print((await client.runs.get(thread[\"thread_id\"], interrupted_run[\"run_id\"]))[\"status\"])"],"metadata":{"id":"fDQMml213Mph"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Rollback\n","\n","We can **rollback** to interrupt the prior run of the graph, delete it, and start a new run."],"metadata":{"id":"VD1-JQw73NMB"}},{"cell_type":"code","source":["# Create a new thread\n","thread = await client.threads.create()\n","\n","# Create new ToDos\n","user_input_1 = \"Add a ToDo to call to make appointment at Yoga.\"\n","user_input_2 = \"Actually, add a ToDo to drop by Yoga in person on Sunday.\"\n","config = {\"configurable\": {\"user_id\": \"Test-Double-Texting\"}}\n","graph_name = \"task_maistro\""],"metadata":{"id":"UogNLgKz3UB4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rolled_back_run = await client.runs.create(\n","    thread[\"thread_id\"],\n","    graph_name,\n","    input={\"messages\": [HumanMessage(content=user_input_1)]},\n","    config=config,\n",")\n","\n","second_run = await client.runs.create(\n","    thread[\"thread_id\"],\n","    graph_name,\n","    input={\"messages\": [HumanMessage(content=user_input_2)]},\n","    config=config,\n","    multitask_strategy=\"rollback\",\n",")"],"metadata":{"id":"Hsly3sQI4wsS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Wait until the second run completes\n","await client.runs.join(thread[\"thread_id\"], second_run[\"run_id\"])\n","\n","# Get the state of the thread\n","state = await client.threads.get_state(thread[\"thread_id\"])\n","for m in convert_to_messages(state[\"values\"][\"messages\"]):\n","    m.pretty_print()"],"metadata":{"id":"j-hDuWId4yfY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see that the initial run was deleted in the chat history"],"metadata":{"id":"oBVy1lCQ40bL"}},{"cell_type":"code","source":["# Confirm that the original run was deleted\n","try:\n","    await client.runs.get(thread[\"thread_id\"], rolled_back_run[\"run_id\"])\n","except httpx.HTTPStatusError as _:\n","    print(\"Original run was correctly deleted\")"],"metadata":{"id":"wgEmIIH_4_yu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For double texting in details, we can check [Docs](https://docs.langchain.com/langgraph-platform/double-texting)"],"metadata":{"id":"aOnlQaDl5DBz"}},{"cell_type":"markdown","source":["# Assistants\n","\n","**Assistants** give developers a quick and easy way to modify and version agents for experimentation."],"metadata":{"id":"pbmmbskw5LWZ"}},{"cell_type":"markdown","source":["## Supplying configuration to the graph\n","\n","The `task_maistro` graph is already set up to use assitants. It has a `configuration.py` file defined and loaded in the graph.\n","\n","\n","We can access configurable fields (`user_id`, `todo_category`, `task_maistro_role`) inside the graph nodes."],"metadata":{"id":"TLEioLVO5S3Q"}},{"cell_type":"markdown","source":["### configuration.py\n","\n","```python\n","import os\n","from dataclasses import dataclass, field, fields\n","from typing import Any, Optional\n","\n","from langchain_core.runnables import RunnableConfig\n","from typing_extensions import Annotated\n","from dataclasses import dataclass\n","\n","@dataclass(kw_only=True)\n","class Configuration:\n","    \"\"\"The configurable fields for the chatbot.\"\"\"\n","    user_id: str = \"default-user\"\n","    todo_category: str = \"general\"\n","    task_maistro_role: str = \"You are a helpful task management assistant. You help you create, organize, and manage the user's ToDo list.\"\n","\n","    @classmethod\n","    def from_runnable_config(\n","        cls, config: Optional[RunnableConfig] = None\n","    ) -> \"Configuration\":\n","        \"\"\"Create a Configuration instance from a RunnableConfig.\"\"\"\n","        configurable = (\n","            config[\"configurable\"] if config and \"configurable\" in config else {}\n","        )\n","        values: dict[str, Any] = {\n","            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))\n","            for f in fields(cls)\n","            if f.init\n","        }\n","        return cls(**{k: v for k, v in values.items() if v})\n","```"],"metadata":{"id":"_LDRupnfsiyu"}},{"cell_type":"markdown","source":["## Creating assistants\n","\n","Within the `task_maistro` app, we have separate ToDo lists for different categories of tasks. For example, we have one assistant for our personal tasks and another for our work tasks.\n","\n","These are easily configurable using the `todo_category` and `task_maistro_role` configurable fields.\n","\n","Just like before, we first need to connect to our deployment:"],"metadata":{"id":"oJaX-XEC5owl"}},{"cell_type":"code","source":["from langgraph_sdk import get_client\n","\n","url_for_cli_deployment = \"http://localhost:8123\"\n","client = get_client(url=url_for_cli_deployment)"],"metadata":{"id":"N66P2-uX5JaG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Personal assistant\n","\n","The personal assistant will manage our personal tasks."],"metadata":{"id":"msrnWcMuuXhR"}},{"cell_type":"code","source":["personal_assistant = await client.assistants.create(\n","    # \"task_maistro\" is the name of a graph we deployed\n","    \"task_maistro\",\n","    config = {'configurable': {'todo_category': 'personal'}}\n",")\n","\n","print(personal_assistant)"],"metadata":{"id":"WYViB3a0ubXS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we will update this assistant to include our `user_id` for convenience, creating a new version of it."],"metadata":{"id":"J4-vXR1kurV1"}},{"cell_type":"code","source":["task_maistro_role = \"\"\"You are a friendly and organized personal task assistant. Your main focus is helping users stay on top of their personal tasks and commitments. Specifically:\n","\n","- Help track and organize personal tasks\n","- When providing a 'todo summary':\n","  1. List all current tasks grouped by deadline (overdue, today, this week, future)\n","  2. Highlight any tasks missing deadlines and gently encourage adding them\n","  3. Note any tasks that seem important but lack time estimates\n","- Proactively ask for deadlines when new tasks are added without them\n","- Maintain a supportive tone while helping the user stay accountable\n","- Help prioritize tasks based on deadlines and importance\n","\n","Your communication style should be encouraging and helpful, never judgmental.\n","\n","When tasks are missing deadlines, respond with something like \"I notice [task] doesn't have a deadline yet. Would you like to add one to help us track it better?\"\"\"\n","\n","\n","# Consistent with the Configuration class\n","configurations = {\n","    'todo_cateogry': 'personal',\n","    'user_id': \"Bin\",\n","    'task_maistro_role': task_maistro_role\n","}\n","\n","personal_assistant = await client.assistants.update(\n","    personal_assistant['assistant_id'],\n","    config={'configurable': configurations}\n",")\n","print(personal_assistant)"],"metadata":{"id":"cEp32PNHuwd4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Work assistant\n","\n","Then we create a work assistant for our work tasks."],"metadata":{"id":"4JK0YhOSvLQA"}},{"cell_type":"code","source":["task_maistro_role = \"\"\"You are a focused and efficient work task assistant.\n","\n","Your main focus is helping users manage their work commitments with realistic timeframes.\n","\n","Specifically:\n","\n","- Help track and organize work tasks\n","- When providing a 'todo summary':\n","  1. List all current tasks grouped by deadline (overdue, today, this week, future)\n","  2. Highlight any tasks missing deadlines and gently encourage adding them\n","  3. Note any tasks that seem important but lack time estimates\n","- When discussing new tasks, suggest that the user provide realistic time-frames based on task type:\n","  • Developer Relations features: typically 1 day\n","  • Course lesson reviews/feedback: typically 2 days\n","  • Documentation sprints: typically 3 days\n","- Help prioritize tasks based on deadlines and team dependencies\n","- Maintain a professional tone while helping the user stay accountable\n","\n","Your communication style should be supportive but practical.\n","\n","When tasks are missing deadlines, respond with something like \"I notice [task] doesn't have a deadline yet. Based on similar tasks, this might take [suggested timeframe]. Would you like to set a deadline with this in mind?\"\"\"\n","\n","configurations = {\n","    'todo_cateogry': 'work',\n","    'user_id': \"Bin\",\n","    'task_maistro_role': task_maistro_role\n","}\n","\n","configurations = {\n","    'todo_category': 'work',\n","    'user_id': 'Bin',\n","    'task_maistro_role': task_maistro_role\n","}\n","\n","work_assistant = await client.assistants.create(\n","    # \"task_maistro\" is the name of a graph we deployed\n","    \"task_maistro\",\n","    config={\"configurable\": configurations}\n",")\n","print(work_assistant)"],"metadata":{"id":"OoYpJCKSvPHh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice how different the `task_maistro_role` prompts for personal assistant are from the work assistant."],"metadata":{"id":"asI_DV0rviOm"}},{"cell_type":"markdown","source":["## Using assistants\n","\n","Assistants will be saved to `Postgres` in our deployment, which allows us easily search for assistants with the SDK."],"metadata":{"id":"_Oeg31Xmvp2z"}},{"cell_type":"code","source":["assistants = await client.assistants.search()\n","for assistant in assistants:\n","    print({\n","        'assistant_id': assistant['assistant_id'],\n","        'version': assistant['version'],\n","        'config': assistant['config']\n","    })"],"metadata":{"id":"dmE8JDLZvwqW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can manage them easily with the SDK. For example, we can delete assistants that we are no longer using."],"metadata":{"id":"LC-7sIHuwKdT"}},{"cell_type":"code","source":["# Create a tempoaray assistant\n","temp_assistant = await client.assistants.create(\n","    'task_maistro',\n","    config={'configurable': configurations}\n",")\n","\n","assistants = await client.assistants.search()\n","for assistant in assistants:\n","    print(f\"before delete: {{'assistant_id': {assistant['assistant_id']}}}\")\n","\n","# delete our temporary assistant\n","await client.assistants.delete(assistants[-1][\"assistant_id\"])\n","print()\n","\n","assistants = await client.assistants.search()\n","for assistant in assistants:\n","    print(f\"after delete: {{'assistant_id': {assistant['assistant_id']} }}\")"],"metadata":{"id":"s7HdBswWwRIz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can set the assistant IDs for the `personal` and `work` assistants that we will work with."],"metadata":{"id":"S4LYeaQZwhFW"}},{"cell_type":"code","source":["personal_assistant_id = assistants[0]['assistant_id']\n","work_assistant_id = assistants[1]['assistant_id']"],"metadata":{"id":"dAlIEL-1wrHz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Work assistant"],"metadata":{"id":"lK8oqX_sxKhx"}},{"cell_type":"code","source":["from langchain_core.messages import HumanMessage\n","from langchain_core.messages import convert_to_messages\n","\n","user_input = \"Create or update few ToDos: 1) Re-film Module 6, lesson 5 by end of day today. 2) Update audioUX by next Monday.\"\n","thread = await client.threads.create()\n","async for chunk in client.runs.stream(thread[\"thread_id\"],\n","                                      work_assistant_id,\n","                                      input={\"messages\": [HumanMessage(content=user_input)]},\n","                                      stream_mode=\"values\"):\n","\n","    if chunk.event == 'values':\n","        state = chunk.data\n","        convert_to_messages(state[\"messages\"])[-1].pretty_print()"],"metadata":{"id":"oP_8eM4AxLsO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["user_input = \"Create another ToDo: Finalize set of report generation tutorials.\"\n","thread = await client.threads.create()\n","async for chunk in client.runs.stream(thread[\"thread_id\"],\n","                                      work_assistant_id,\n","                                      input={\"messages\": [HumanMessage(content=user_input)]},\n","                                      stream_mode=\"values\"):\n","\n","    if chunk.event == 'values':\n","        state = chunk.data\n","        convert_to_messages(state[\"messages\"])[-1].pretty_print()"],"metadata":{"id":"lMlyRd8sxNzg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["user_input = \"OK, for this task let's get it done by next Tuesday.\"\n","async for chunk in client.runs.stream(thread[\"thread_id\"],\n","                                      work_assistant_id,\n","                                      input={\"messages\": [HumanMessage(content=user_input)]},\n","                                      stream_mode=\"values\"):\n","\n","    if chunk.event == 'values':\n","        state = chunk.data\n","        convert_to_messages(state[\"messages\"])[-1].pretty_print()"],"metadata":{"id":"8UT7i0Z8xWN4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Personal assistant"],"metadata":{"id":"SO5h8sgOxW3G"}},{"cell_type":"code","source":["user_input = \"Create ToDos: 1) Check on swim lessons for the baby this weekend. 2) For winter travel, check AmEx points.\"\n","thread = await client.threads.create()\n","async for chunk in client.runs.stream(thread[\"thread_id\"],\n","                                      personal_assistant_id,\n","                                      input={\"messages\": [HumanMessage(content=user_input)]},\n","                                      stream_mode=\"values\"):\n","\n","    if chunk.event == 'values':\n","        state = chunk.data\n","        convert_to_messages(state[\"messages\"])[-1].pretty_print()"],"metadata":{"id":"d8gp_ZGlxYOR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["user_input = \"Give me a todo summary.\"\n","thread = await client.threads.create()\n","async for chunk in client.runs.stream(thread[\"thread_id\"],\n","                                      personal_assistant_id,\n","                                      input={\"messages\": [HumanMessage(content=user_input)]},\n","                                      stream_mode=\"values\"):\n","\n","    if chunk.event == 'values':\n","        state = chunk.data\n","        convert_to_messages(state[\"messages\"])[-1].pretty_print()"],"metadata":{"id":"o8G1jtsOxZ16"},"execution_count":null,"outputs":[]}]}