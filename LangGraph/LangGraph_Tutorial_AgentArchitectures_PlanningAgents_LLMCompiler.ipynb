{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13937,
     "status": "ok",
     "timestamp": 1733154378877,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "lUMMpFhGZ0hQ",
    "outputId": "324f6f0c-f41e-4138-ee5d-646cca63537f"
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain_openai langsmith langgraph langchain numexpr langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1733154333447,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "R9EWlCapZ-2J"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'YOUR_OPENAI_API_KEY'\n",
    "os.environ['TAVILY_API_KEY'] = 'YOUR_TAVILY_API_KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mv4CngxEZ_vi"
   },
   "source": [
    "# LLMCompiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8jJltM9aBst"
   },
   "source": [
    "**LLMCompiler** is an agent architecture designed to **speed up** the execution of agentic tasks by eagerly-executed tasks within a DAG. It also saves costs on redundant token usage by reducing the number of calls to the LLM.\n",
    "\n",
    "It has 3 main components:\n",
    "1. **Planner**: stream a DAG of tasks.\n",
    "2. **Task Fetching Unit**: schedules and executes the tasks as soon as they are executable.\n",
    "3. **Joiner**: responds to the user or trigers a second plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53oC6Gryadzt"
   },
   "source": [
    "# Helper Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h17LJRr7Ogmj"
   },
   "source": [
    "## Math Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 8036,
     "status": "ok",
     "timestamp": 1733151996783,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "q9wPlWknaAxK"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "import numexpr\n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import StructuredTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1733151996783,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "MlBRm7WEPZ5F"
   },
   "outputs": [],
   "source": [
    "_MATH_DESCRIPTION = (\n",
    "    \"math(problem: str, context: Optional[list[str]]) -> float:\\n\"\n",
    "    \" - Solves the provided math problem.\\n\"\n",
    "    ' - `problem` can be either a simple math problem (e.g. \"1 + 3\") or a word problem (e.g. \"how many apples are there if there are 3 apples and 2 apples\").\\n'\n",
    "    \" - You cannot calculate multiple expressions in one call. For instance, `math('1 + 3, 2 + 4')` does not work. \"\n",
    "    \"If you need to calculate multiple expressions, you need to call them separately like `math('1 + 3')` and then `math('2 + 4')`\\n\"\n",
    "    \" - Minimize the number of `math` actions as much as possible. For instance, instead of calling \"\n",
    "    '2. math(\"what is the 10% of $1\") and then call 3. math(\"$1 + $2\"), '\n",
    "    'you MUST call 2. math(\"what is the 110% of $1\") instead, which will reduce the number of math actions.\\n'\n",
    "    # Context specific rules below\n",
    "    \" - You can optionally provide a list of strings as `context` to help the agent solve the problem. \"\n",
    "    \"If there are multiple contexts you need to answer the question, you can provide them as a list of strings.\\n\"\n",
    "    \" - `math` action will not see the output of the previous actions unless you provide it as `context`. \"\n",
    "    \"You MUST provide the output of the previous actions as `context` if you need to do math on it.\\n\"\n",
    "    \" - You MUST NEVER provide `search` type action's outputs as a variable in the `problem` argument. \"\n",
    "    \"This is because `search` returns a text blob that contains the information about the entity, not a number or value. \"\n",
    "    \"Therefore, when you need to provide an output of `search` action, you MUST provide it as a `context` argument to `math` action. \"\n",
    "    'For example, 1. search(\"Barack Obama\") and then 2. math(\"age of $1\") is NEVER allowed. '\n",
    "    'Use 2. math(\"age of Barack Obama\", context=[\"$1\"]) instead.\\n'\n",
    "    \" - When you ask a question about `context`, specify the units. \"\n",
    "    'For instance, \"what is xx in height?\" or \"what is xx in millions?\" instead of \"what is xx?\"\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1733151996784,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "KESjoUcIPg24"
   },
   "outputs": [],
   "source": [
    "_SYSTEM_PROMPT = \"\"\"\n",
    "Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
    "\n",
    "Question: ${{Question with math problem.}}\n",
    "```text\n",
    "${{single line mathematical expression that solves the problem}}\n",
    "```\n",
    "...numexpr.evaluate(text)...\n",
    "```output\n",
    "${{Output of running the code}}\n",
    "```\n",
    "Answer: ${{Answer}}\n",
    "\n",
    "Begin.\n",
    "\n",
    "Question: What is 37593 * 67?\n",
    "ExecuteCode({{code: \"37593 * 67\"}})\n",
    "...numexpr.evaluate(\"37593 * 67\")...\n",
    "```output\n",
    "2518731\n",
    "```\n",
    "Answer: 2518731\n",
    "\n",
    "Question: 37593^(1/5)\n",
    "ExecuteCode({{code: \"37593**(1/5)\"}})\n",
    "...numexpr.evaluate(\"37593**(1/5)\")...\n",
    "```output\n",
    "8.222831614237718\n",
    "```\n",
    "Answer: 8.222831614237718\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1733151996784,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "Uv1WUXUUPn5e"
   },
   "outputs": [],
   "source": [
    "_ADDITIONAL_CONTEXT_PROMPT = \"\"\"\n",
    "The following additional context is provided from other functions.\\\n",
    "    Use it to substitute into any ${{#}} variables or other words in the problem.\\\n",
    "    \\n\\n${context}\\n\\nNote that context variables are not defined in code yet.\\\n",
    "You must extract the relevant numbers and directly put them in code.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1733151996784,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "_QVKV31fQZjR"
   },
   "outputs": [],
   "source": [
    "class ExecuteCode(BaseModel):\n",
    "    \"\"\"The input to the numexpr.evaluate() function\"\"\"\n",
    "\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"The reasoning behind the code expression, including how context is included, if applicable.\",\n",
    "    )\n",
    "\n",
    "    code: str = Field(\n",
    "        ...,\n",
    "        description=\"The simple code expression to execute by numexpr.evaluate().\",\n",
    "    )\n",
    "\n",
    "\n",
    "def _evaluate_expression(expression: str) -> str:\n",
    "    try:\n",
    "        local_dict = {'pi': math.pi, 'e': math.e}\n",
    "\n",
    "        output = str(\n",
    "            numexpr.evaluate(\n",
    "                expression.strip(),\n",
    "                global_dict={}, # restrict access to globals\n",
    "                local_dict=local_dict, # add common mathematical function\n",
    "            )\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise ValueError(\n",
    "            f'Failed to evaluate \"{expression}\". Raise error: {repr(e)}.'\n",
    "            \" Please try again with a valid numerical expression.\"\n",
    "        )\n",
    "\n",
    "    # Remove any leading and trailing brackets from the output\n",
    "    return re.sub(r\"^\\[|\\]$\", \"\", output)\n",
    "\n",
    "\n",
    "def get_math_tool(llm: ChatOpenAI):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            ('system', _SYSTEM_PROMPT),\n",
    "            ('user', \"{problem}\"),\n",
    "            MessagesPlaceholder(variable_name='context', optional=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    extractor = prompt | llm.with_structured_output(ExecuteCode)\n",
    "\n",
    "    def calculate_expression(\n",
    "            problem: str,\n",
    "            context: Optional[List[str]] = None,\n",
    "            config: Optional[RunnableConfig] = None,\n",
    "    ):\n",
    "        chain_input = {'problem': problem}\n",
    "        if context:\n",
    "            context_str = \"\\n\".join(context)\n",
    "            if context_str.strip():\n",
    "                context_str = _ADDITIONAL_CONTEXT_PROMPT.format(\n",
    "                    context=context_str.strip()\n",
    "                )\n",
    "                chain_input['context'] = [SystemMessage(content=context_str)]\n",
    "\n",
    "        code_model = extractor.invoke(chain_input, config)\n",
    "        try:\n",
    "            return _evaluate_expression(code_model.code)\n",
    "        except Exception as e:\n",
    "            return repr(e)\n",
    "\n",
    "    return StructuredTool.from_function(\n",
    "        name='math',\n",
    "        func=calculate_expression,\n",
    "        description=_MATH_DESCRIPTION,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pllpuUPkTBZ1"
   },
   "source": [
    "## Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1733151996784,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "W6eJuOehS_mX"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "from typing import Any, Dict, Iterator, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.output_parsers.transform import BaseTransformOutputParser\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 145,
     "status": "ok",
     "timestamp": 1733153425135,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "b48IiYywTfbg"
   },
   "outputs": [],
   "source": [
    "THOUGHT_PATTERN = r\"Thought: ([^\\n]*)\"\n",
    "ACTION_PATTERN = r\"\\n*(\\d+)\\. (\\w+)\\((.*)\\)(\\s*#\\w+\\n)?\"\n",
    "# $1 or ${1} -> 1\n",
    "ID_PATTERN = r\"\\$\\{?(\\d+)\\}?\"\n",
    "END_OF_PLAN = \"<END_OF_PLAN>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 110,
     "status": "ok",
     "timestamp": 1733153524252,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "0C83GHxCTqIr"
   },
   "outputs": [],
   "source": [
    "def _ast_parse(arg: str) -> Any:\n",
    "    try:\n",
    "        return ast.literal_eval(arg)\n",
    "    except:\n",
    "        return arg\n",
    "\n",
    "\n",
    "def _parse_llm_compiler_action_args(args: str, tool: Union[str, BaseTool]) -> List[Any]:\n",
    "    \"\"\"Parse arguments from a string\"\"\"\n",
    "\n",
    "    if args == \"\":\n",
    "        return ()\n",
    "    if isinstance(tool, str):\n",
    "        return ()\n",
    "\n",
    "    extracted_args = {}\n",
    "    tool_key = None\n",
    "    prev_idx = None\n",
    "    for key in tool.args.keys():\n",
    "        # Split if present\n",
    "        if f\"{key}=\" in args:\n",
    "            idx = args.index(f\"{key}=\")\n",
    "            if prev_idx is not None:\n",
    "                extracted_args[tool_key] = _ast_parse(args[prev_idx:idx].strip().rstrip(\",\"))\n",
    "\n",
    "            args = args.split(f\"{key}=\", 1)[1]\n",
    "            tool_key = key\n",
    "            prev_idx = 0\n",
    "\n",
    "    if prev_idx is not None:\n",
    "        extracted_args[tool_key] = _ast_parse(args[prev_idx:].strip().rstrip(\",\").rstrip(\")\"))\n",
    "\n",
    "    return extracted_args\n",
    "\n",
    "\n",
    "def default_dependency_rule(idx, args: str):\n",
    "    matches = re.findall(ID_PATTERN, args)\n",
    "    numbers = [int(match) for match in matches]\n",
    "    return idx in numbers\n",
    "\n",
    "\n",
    "def _get_dependencies_from_graph(\n",
    "        idx: int, tool_name: str, args: Dict[str, Any]\n",
    ") -> Dict[str, list[str]]:\n",
    "    \"\"\"Get dependencies from a graph\"\"\"\n",
    "    if tool_name == 'join':\n",
    "        return list(range(1, idx))\n",
    "\n",
    "    return [i for i in range(1, idx) if default_dependency_rule(i, str(args))]\n",
    "\n",
    "\n",
    "\n",
    "class Task(TypedDict):\n",
    "    idx: int\n",
    "    tool: BaseTool\n",
    "    args: list\n",
    "    dependencies: Dict[str, list]\n",
    "    thought: Optional[str]\n",
    "\n",
    "\n",
    "\n",
    "def instantiate_task(\n",
    "        tools: Sequence[BaseTool],\n",
    "        idx: int,\n",
    "        tool_name: str,\n",
    "        args: Union[str, Any],\n",
    "        thought: Optional[str] = None,\n",
    ") -> Task:\n",
    "    if tool_name == 'join':\n",
    "        tool = 'join'\n",
    "    else:\n",
    "        try:\n",
    "            tool = tools[[tool.name for tool in tools].index(tool_name)]\n",
    "        except ValueError as e:\n",
    "            raise OutputParserException(f\"Tool {tool_name} not found.\") from e\n",
    "\n",
    "    tool_args = _parse_llm_compiler_action_args(args, tool)\n",
    "    dependencies = _get_dependencies_from_graph(idx, tool_name, tool_args)\n",
    "\n",
    "    return Task(\n",
    "        idx=idx,\n",
    "        tool=tool,\n",
    "        args=tool_args,\n",
    "        dependencies=dependencies,\n",
    "        thought=thought,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "class LLMCompilerPlanParser(BaseTransformOutputParser[dict], extra=\"allow\"):\n",
    "    \"\"\"Planning output parser\"\"\"\n",
    "\n",
    "    tools: List[BaseTool]\n",
    "\n",
    "    def _transform(self, input: Iterator[Union[str, BaseMessage]]) -> Iterator[Task]:\n",
    "        texts = []\n",
    "        # TODO: Cleanup tuple state tracking here\n",
    "        thought = None\n",
    "        for chunk in input:\n",
    "            # Assume input is str. TODO: support vision/other formats\n",
    "            text = chunk if isinstance(chunk, str) else str(chunk.content)\n",
    "            for task, thought in self.ingest_token(text, texts, thought):\n",
    "                yield task\n",
    "\n",
    "        # Final possible task\n",
    "        if texts:\n",
    "            task, _ = self._parse_task(\"\".join(texts), thought)\n",
    "            if task:\n",
    "                yield task\n",
    "\n",
    "    def parse(self, text: str) -> List[Task]:\n",
    "        return list(self._transform([text]))\n",
    "\n",
    "    def stream(\n",
    "            self,\n",
    "            input: str | BaseMessage,\n",
    "            config: RunnableConfig | None = None,\n",
    "            **kwargs: Any | None,\n",
    "    ) -> Iterator[Task]:\n",
    "        yield from self.transform([input], config, **kwargs)\n",
    "\n",
    "    def ingest_token(\n",
    "            self,\n",
    "            token: str,\n",
    "            buffer: List[str],\n",
    "            thought: Optional[str],\n",
    "    ) -> Iterator[Tuple[Optional[Task], str]]:\n",
    "        buffer.append(token)\n",
    "\n",
    "        if \"\\n\" in token:\n",
    "            buffer_ = \"\".join(buffer).split('\\n')\n",
    "            suffix = buffer_[-1]\n",
    "            for line in buffer_[:-1]:\n",
    "                task, thought = self._parse_task(line, thought)\n",
    "                if task:\n",
    "                    yield task, thought\n",
    "\n",
    "            buffer.clear()\n",
    "            buffer.append(suffix)\n",
    "\n",
    "    def _parse_task(self, line: str, thought: Optional[str] = None):\n",
    "        task = None\n",
    "\n",
    "        if match := re.match(THOUGHT_PATTERN, line):\n",
    "            # Optionally, action can be preceded by a thought\n",
    "            thought = match.group(1)\n",
    "        elif match := re.match(ACTION_PATTERN, line):\n",
    "            # if action is parsed, return the task, and clear the buffer\n",
    "            idx, tool_name, args, _ = match.groups()\n",
    "            idx = int(idx)\n",
    "            task = instantiate_task(\n",
    "                tools=self.tools,\n",
    "                idx=idx,\n",
    "                tool_name=tool_name,\n",
    "                args=args,\n",
    "                thought=thought,\n",
    "            )\n",
    "            thought = None\n",
    "\n",
    "        return task, thought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaU72JoqABWt"
   },
   "source": [
    "# Define Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X67SY60OC9zm"
   },
   "source": [
    "We will give the agent the search engine and calculator tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 429,
     "status": "ok",
     "timestamp": 1733154482312,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "mq-xM8elAAka"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# calculator\n",
    "calculate = get_math_tool(ChatOpenAI(model='gpt-3.5-turbo'))\n",
    "# search engine\n",
    "search = TavilySearchResults(\n",
    "    max_results=1,\n",
    "    description='tavily_search_results_json(query=\"the search query\") - a search engine.',\n",
    ")\n",
    "\n",
    "tools = [search, calculate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 1545,
     "status": "ok",
     "timestamp": 1733154561701,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "yy6otOPIDqZJ",
    "outputId": "dc024f74-e4f5-4a09-8c32-d7cd58f4f7a4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'77'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if the calculator works\n",
    "calculate.invoke(\n",
    "    {\n",
    "        'problem': \"What's the temperature in Raleigh + 5?\",\n",
    "        'context': ['The tempearature in Raleigh is 72 degrees Fahrenheit.'],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AL3dC-RyD_Eg"
   },
   "source": [
    "# Planner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzT8wZVGJJhu"
   },
   "source": [
    "The planner accepts the input question and generates a task list to execute.\n",
    "\n",
    "If it is provided with a previous plan, it is instructed to re-plan, which is useful if, upon completion of the first batch of tasks, the agent must take more actions.\n",
    "\n",
    "The output parser processes a task list in the following form:\n",
    "```\n",
    "1. tool_1(arg1=\"arg1\", arg2=3.5, ...)\n",
    "Thought: I then want to find out Y by using tool_2\n",
    "2. tool_2(arg1=\"\", args2=\"${1}\")\n",
    "3. join()<END_OF_PLAN>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1733156086646,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "MxWbRF3zD9gH"
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2779,
     "status": "ok",
     "timestamp": 1733156110521,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "uvA2oGpbJyJI",
    "outputId": "9f62c688-a767-4557-c0a3-137b369c4854"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an action from the following \u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m types:\n",
      "\u001b[33;1m\u001b[1;3m{tool_descriptions}\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m. join(): Collects and combines results from prior actions.\n",
      "\n",
      " - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\n",
      " - join should always be the last action in the plan, and will be called in two scenarios:\n",
      "   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\n",
      "   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\n",
      " - Each action described above contains input/output types and description.\n",
      "    - You must strictly adhere to the input and output types for each action.\n",
      "    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\n",
      " - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\n",
      " - Each action MUST have a unique ID, which is strictly increasing.\n",
      " - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\n",
      " - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\n",
      " - Ensure the plan maximizes parallelizability.\n",
      " - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\n",
      " - Never introduce new actions other than the ones provided.\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{messages}\u001b[0m\n",
      "\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Remember, ONLY respond with the task list in the correct format! E.g.:\n",
      "idx. tool(arg_name=args)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull('wfh/llm-compiler')\n",
    "print(prompt.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1733156635788,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "EBgBgeIUJ3VI"
   },
   "outputs": [],
   "source": [
    "def create_planner(\n",
    "        llm: BaseChatModel,\n",
    "        tools: Sequence[BaseTool],\n",
    "        base_prompt: ChatPromptTemplate,\n",
    "):\n",
    "    tool_descriptions = \"\\n\".join(\n",
    "        f\"{i+1}. {tool.description}\\n\" for i, tool in enumerate(tools)\n",
    "    )\n",
    "\n",
    "    planner_prompt = base_prompt.partial(\n",
    "        replan=\"\",\n",
    "        num_tools=len(tools)\n",
    "                    +1, # because we add the 'join()' tool at the end\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "\n",
    "    replanner_prompt = base_prompt.partial(\n",
    "        replan=' - You are given \"Previous Plan\" which is the plan that the previous agent created along with the execution results '\n",
    "        \"(given as Observation) of each plan and a general thought (given as Thought) about the executed results.\"\n",
    "        'You MUST use these information to create the next plan under \"Current Plan\".\\n'\n",
    "        ' - When starting the Current Plan, you should start with \"Thought\" that outlines the strategy for the next plan.\\n'\n",
    "        \" - In the Current Plan, you should NEVER repeat the actions that are already executed in the Previous Plan.\\n\"\n",
    "        \" - You must continue the task index from the end of the previous one. Do not repeat task indices.\",\n",
    "        num_tools=len(tools) + 1,\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "\n",
    "    def should_replan(state: list):\n",
    "        # Context is passed as a system message\n",
    "        return isinstance(state[-1], SystemMessage)\n",
    "\n",
    "    def wrap_messages(state: list):\n",
    "        return {'messages': state}\n",
    "\n",
    "    def wrap_and_get_last_index(state: list):\n",
    "        next_task = 0\n",
    "\n",
    "        for message in state[::-1]:\n",
    "            if isinstance(message, FunctionMessage):\n",
    "                next_task = message.additional_kwargs['idx'] + 1\n",
    "                break\n",
    "\n",
    "        state[-1].content = state[-1].content + f\" - Begin counting at: {next_task}\"\n",
    "        return {'messages': state}\n",
    "\n",
    "\n",
    "    return (\n",
    "        RunnableBranch(\n",
    "        (should_replan, wrap_and_get_last_index | replanner_prompt),\n",
    "        wrap_messages | planner_prompt,\n",
    "        )\n",
    "        | llm\n",
    "        | LLMCompilerPlanParser(tools=tools)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 94,
     "status": "ok",
     "timestamp": 1733156656056,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "Gzy51_uML4Np"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-3.5-turbo')\n",
    "# This is the primary agent in our application\n",
    "planner = create_planner(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1092,
     "status": "ok",
     "timestamp": 1733156706221,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "o9NiaXsML9K3",
    "outputId": "6623cd08-6f62-40f5-ab64-e4a77b662562"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description='tavily_search_results_json(query=\"the search query\") - a search engine.' max_results=1 api_wrapper=TavilySearchAPIWrapper(tavily_api_key=SecretStr('**********')) {'query': 'temperature in Raleigh'}\n",
      "--------\n",
      "name='math' description='math(problem: str, context: Optional[list[str]]) -> float:\\n - Solves the provided math problem.\\n - `problem` can be either a simple math problem (e.g. \"1 + 3\") or a word problem (e.g. \"how many apples are there if there are 3 apples and 2 apples\").\\n - You cannot calculate multiple expressions in one call. For instance, `math(\\'1 + 3, 2 + 4\\')` does not work. If you need to calculate multiple expressions, you need to call them separately like `math(\\'1 + 3\\')` and then `math(\\'2 + 4\\')`\\n - Minimize the number of `math` actions as much as possible. For instance, instead of calling 2. math(\"what is the 10% of $1\") and then call 3. math(\"$1 + $2\"), you MUST call 2. math(\"what is the 110% of $1\") instead, which will reduce the number of math actions.\\n - You can optionally provide a list of strings as `context` to help the agent solve the problem. If there are multiple contexts you need to answer the question, you can provide them as a list of strings.\\n - `math` action will not see the output of the previous actions unless you provide it as `context`. You MUST provide the output of the previous actions as `context` if you need to do math on it.\\n - You MUST NEVER provide `search` type action\\'s outputs as a variable in the `problem` argument. This is because `search` returns a text blob that contains the information about the entity, not a number or value. Therefore, when you need to provide an output of `search` action, you MUST provide it as a `context` argument to `math` action. For example, 1. search(\"Barack Obama\") and then 2. math(\"age of $1\") is NEVER allowed. Use 2. math(\"age of Barack Obama\", context=[\"$1\"]) instead.\\n - When you ask a question about `context`, specify the units. For instance, \"what is xx in height?\" or \"what is xx in millions?\" instead of \"what is xx?\"' args_schema=<class 'langchain_core.utils.pydantic.math'> func=<function get_math_tool.<locals>.calculate_expression at 0x7caf5ea4eb90> {'context': ['$1']}\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "example_question = \"What's the temperature in Raleigh raised to the 3rd power?\"\n",
    "\n",
    "for task in planner.stream([HumanMessage(content=example_question)]):\n",
    "    print(task['tool'], task['args'])\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rwzjcIdMNB3"
   },
   "source": [
    "# Task Fetching Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3rxcM0PMPVm"
   },
   "source": [
    "This component schedules the tasks. It receives the stream of tools of the following format:\n",
    "```\n",
    "{\n",
    "    tool: BaseTool,\n",
    "    dependencies: number[],\n",
    "}\n",
    "```\n",
    "\n",
    "The basic idea is to begin executing tools as soon as their dependencies are met. This is done through multi-threading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 96,
     "status": "ok",
     "timestamp": 1733158357201,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "L6qUq7Q9MJLE"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from typing import Any, Dict, Iterable, List, Union\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.runnables import chain as as_runnable\n",
    "\n",
    "\n",
    "def _get_observations(messages: List[BaseMessage]) -> Dict[int, Any]:\n",
    "    # Get all previous tool responses\n",
    "    results = {}\n",
    "    for message in messages[::-1]:\n",
    "        if isinstance(message, FunctionMessage):\n",
    "            results[int(message.additional_kwargs['idx'])] = message.content\n",
    "    return results\n",
    "\n",
    "\n",
    "class SchedulerInput(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    tasks: Iterable[Task]\n",
    "\n",
    "\n",
    "def _execute_task(task, observations, config):\n",
    "    tool_to_use = task['tool']\n",
    "    if isinstance(tool_to_use, str):\n",
    "        return tool_to_use\n",
    "\n",
    "    args = task['args']\n",
    "    try:\n",
    "        if isinstance(args, str):\n",
    "            resolved_args = _resolve_arg(args, observations)\n",
    "        elif isinstance(args, dict):\n",
    "            resolved_args = {\n",
    "                key: _resolve_arg(val, observations) for key, val in args.items()\n",
    "            }\n",
    "        else:\n",
    "            resolved_args = args\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR (Failed to call {tool_to_use.name} with args {args}.)\"\n",
    "            f\" Args could not be resolved. Error: {repr(e)}\"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        return tool_to_use.invoke(resolved_args, config)\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.\"\n",
    "            + f\" Args resolved to {resolved_args}. Error: {repr(e)})\"\n",
    "        )\n",
    "\n",
    "def _resolve_arg(arg: Union[str, Any], observations: Dict[int, Any]):\n",
    "    # $1 or ${1} -> 1\n",
    "    ID_PATTERN = r\"\\$\\{?(\\d+)\\}?\"\n",
    "\n",
    "    def replace_match(match):\n",
    "        # If the string is ${123}, match.group(0) is ${123}, and match.group(1) is 123.\n",
    "\n",
    "        # Return the match group, in this case the index, from the string. This is the index\n",
    "        # number we get back.\n",
    "        idx = int(match.group(1))\n",
    "        return str(observations.get(idx, match.group(0)))\n",
    "\n",
    "    # For dependencies on other tasks\n",
    "    if isinstance(arg, str):\n",
    "        return re.sub(ID_PATTERN, replace_match, arg)\n",
    "    elif isinstance(arg, list):\n",
    "        return [_resolve_arg(a, observations) for a in arg]\n",
    "    else:\n",
    "        return str(arg)\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def schedule_task(task_inputs, config):\n",
    "    task: Task = task_inputs[\"task\"]\n",
    "    observations: Dict[int, Any] = task_inputs[\"observations\"]\n",
    "    try:\n",
    "        observation = _execute_task(task, observations, config)\n",
    "    except Exception:\n",
    "        import traceback\n",
    "\n",
    "        observation = traceback.format_exception()  # repr(e) +\n",
    "    observations[task[\"idx\"]] = observation\n",
    "\n",
    "\n",
    "def schedule_pending_task(\n",
    "    task: Task, observations: Dict[int, Any], retry_after: float = 0.2\n",
    "):\n",
    "    while True:\n",
    "        deps = task[\"dependencies\"]\n",
    "        if deps and (any([dep not in observations for dep in deps])):\n",
    "            # Dependencies not yet satisfied\n",
    "            time.sleep(retry_after)\n",
    "            continue\n",
    "        schedule_task.invoke({\"task\": task, \"observations\": observations})\n",
    "        break\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def schedule_tasks(scheduler_input: SchedulerInput) -> List[FunctionMessage]:\n",
    "    \"\"\"Group the tasks into a DAG schedule.\"\"\"\n",
    "    # For streaming, we are making a few simplifying assumption:\n",
    "    # 1. The LLM does not create cyclic dependencies\n",
    "    # 2. That the LLM will not generate tasks with future deps\n",
    "    # If this ceases to be a good assumption, you can either\n",
    "    # adjust to do a proper topological sort (not-stream)\n",
    "    # or use a more complicated data structure\n",
    "    tasks = scheduler_input[\"tasks\"]\n",
    "    args_for_tasks = {}\n",
    "    messages = scheduler_input[\"messages\"]\n",
    "    # If we are re-planning, we may have calls that depend on previous\n",
    "    # plans. Start with those.\n",
    "    observations = _get_observations(messages)\n",
    "    task_names = {}\n",
    "    originals = set(observations)\n",
    "    # ^^ We assume each task inserts a different key above to\n",
    "    # avoid race conditions...\n",
    "    futures = []\n",
    "    retry_after = 0.25  # Retry every quarter second\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for task in tasks:\n",
    "            deps = task[\"dependencies\"]\n",
    "            task_names[task[\"idx\"]] = (\n",
    "                task[\"tool\"] if isinstance(task[\"tool\"], str) else task[\"tool\"].name\n",
    "            )\n",
    "            args_for_tasks[task[\"idx\"]] = task[\"args\"]\n",
    "            if (\n",
    "                # Depends on other tasks\n",
    "                deps and (any([dep not in observations for dep in deps]))\n",
    "            ):\n",
    "                futures.append(\n",
    "                    executor.submit(\n",
    "                        schedule_pending_task, task, observations, retry_after\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # No deps or all deps satisfied\n",
    "                # can schedule now\n",
    "                schedule_task.invoke(dict(task=task, observations=observations))\n",
    "                # futures.append(executor.submit(schedule_task.invoke dict(task=task, observations=observations)))\n",
    "\n",
    "        # All tasks have been submitted or enqueued\n",
    "        # Wait for them to complete\n",
    "        wait(futures)\n",
    "    # Convert observations to new tool messages to add to the state\n",
    "    new_observations = {\n",
    "        k: (task_names[k], args_for_tasks[k], observations[k])\n",
    "        for k in sorted(observations.keys() - originals)\n",
    "    }\n",
    "    tool_messages = [\n",
    "        FunctionMessage(\n",
    "            name=name,\n",
    "            content=str(obs),\n",
    "            additional_kwargs={\"idx\": k, \"args\": task_args},\n",
    "            tool_call_id=k,\n",
    "        )\n",
    "        for k, (name, task_args, obs) in new_observations.items()\n",
    "    ]\n",
    "    return tool_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 95,
     "status": "ok",
     "timestamp": 1733158363736,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "UtsiWxYOScfY"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def plan_and_schedule(state):\n",
    "    messages = state[\"messages\"]\n",
    "    tasks = planner.stream(messages)\n",
    "    # Begin executing the planner immediately\n",
    "    try:\n",
    "        tasks = itertools.chain([next(tasks)], tasks)\n",
    "    except StopIteration:\n",
    "        # Handle the case where tasks is empty.\n",
    "        tasks = iter([])\n",
    "    scheduled_tasks = schedule_tasks.invoke(\n",
    "        {\n",
    "            \"messages\": messages,\n",
    "            \"tasks\": tasks,\n",
    "        }\n",
    "    )\n",
    "    return {\"messages\": scheduled_tasks}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLsKCwfCSfn_"
   },
   "source": [
    "# Example Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 3010,
     "status": "ok",
     "timestamp": 1733158441256,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "kLTxCjbgSeFj"
   },
   "outputs": [],
   "source": [
    "tool_messages = plan_and_schedule.invoke(\n",
    "    {\n",
    "        'messages': [HumanMessage(content=example_question)]\n",
    "    }\n",
    ")['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1733158452044,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "i1w9hRPlSya8",
    "outputId": "cbb9bd4a-4fc4-4853-e63b-a80737ced8e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FunctionMessage(content=\"[{'url': 'https://forecast.weather.gov/MapClick.php?site=RAH&lat=35.822&lon=-78.6588', 'content': 'NWS Raleigh Winter Weather SKYWARN virtual training - Tuesday December 10, 2024 at 7pm - Free to attend - open to public ... Hazardous Weather Conditions. Hazardous Weather Outlook ; Current conditions at Raleigh / Durham, Raleigh-Durham International Airport (KRDU) Lat: 35.89°NLon: 78.78°WElev: 394ft. Fair. 30°F-1°C. Humidity: 66%: Wind'}]\", additional_kwargs={'idx': 1, 'args': {'query': 'temperature in Raleigh'}}, response_metadata={}, name='tavily_search_results_json', tool_call_id=1),\n",
       " FunctionMessage(content='27000', additional_kwargs={'idx': 2, 'args': {'problem': 'what is $1 raised to the 3rd power', 'context': ['$1']}}, response_metadata={}, name='math', tool_call_id=2)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_36WtDwSxKa"
   },
   "source": [
    "# Joiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoS7x67dS2tz"
   },
   "source": [
    "Now we have the planning and initial execution done. We need a component to process these outputs and either:\n",
    "1. Respond with the correct answer, or\n",
    "2. Loop with a new plan.\n",
    "\n",
    "\n",
    "In the original paper, this is referred to as the **\"joiner\"**, which is another LLM call. We will use function calling to improve parsing reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 913,
     "status": "ok",
     "timestamp": 1733158740711,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "7vTncD8GSwTL",
    "outputId": "b52f0ec3-e57a-4b56-9e58-0378fff6f8df"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class FinalResponse(BaseModel):\n",
    "    \"\"\"The final response/answer\"\"\"\n",
    "\n",
    "    response: str = Field(description=\"The final response/answer\")\n",
    "\n",
    "\n",
    "class Replan(BaseModel):\n",
    "    feedback: str = Field(\n",
    "        description=\"Analysis of the previous attempts and recommendations on what needs to be fixed.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class JoinOutputs(BaseModel):\n",
    "    \"\"\"Decide whether to replan or whether you can return the final response\"\"\"\n",
    "    thought: str = Field(\n",
    "        description=\"The chain of thought reasoning for the selected action\"\n",
    "    )\n",
    "    action: Union[FinalResponse, Replan]\n",
    "\n",
    "\n",
    "\n",
    "joiner_prompt = hub.pull('wfh/llm-compiler-joiner').partial(\n",
    "    examples=\"\"\n",
    ") # we can optionally add examples\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo')\n",
    "\n",
    "runnable = joiner_prompt | llm.with_structured_output(JoinOutputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTLqAG0LT6uN"
   },
   "source": [
    "We will select only the most recent messages in the state, and format the output to be more useful for the planner, should the agent need to loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 98,
     "status": "ok",
     "timestamp": 1733158775977,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "0-fPWlI4T57G"
   },
   "outputs": [],
   "source": [
    "def _parse_joiner_output(decision: JoinOutputs) -> List[BaseMessage]:\n",
    "    response = [AIMessage(content=f\"Thought: {decision.thought}\")]\n",
    "    if isinstance(decision.action, Replan):\n",
    "        return {\n",
    "            \"messages\": response\n",
    "            + [\n",
    "                SystemMessage(\n",
    "                    content=f\"Context from last attempt: {decision.action.feedback}\"\n",
    "                )\n",
    "            ]\n",
    "        }\n",
    "    else:\n",
    "        return {\"messages\": response + [AIMessage(content=decision.action.response)]}\n",
    "\n",
    "\n",
    "def select_recent_messages(state) -> dict:\n",
    "    messages = state[\"messages\"]\n",
    "    selected = []\n",
    "    for msg in messages[::-1]:\n",
    "        selected.append(msg)\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            break\n",
    "    return {\"messages\": selected[::-1]}\n",
    "\n",
    "\n",
    "joiner = select_recent_messages | runnable | _parse_joiner_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 145,
     "status": "ok",
     "timestamp": 1733158782718,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "WExUiYEhUCuq"
   },
   "outputs": [],
   "source": [
    "input_messages = [HumanMessage(content=example_question)] + tool_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 841,
     "status": "ok",
     "timestamp": 1733158786812,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "0PrneNoCUEXN",
    "outputId": "6091cd79-798b-4e38-ce65-c9d1db75b15a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [AIMessage(content=\"Thought: I have the necessary information to answer the user's question.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The temperature in Raleigh raised to the 3rd power is 27,000.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joiner.invoke({\"messages\": input_messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpBRSSjNUF7k"
   },
   "source": [
    "# Compose using LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixphnAXsUIJG"
   },
   "source": [
    "We will define the agent as a stateful graph, with the main nodes being:\n",
    "1. Plan and execute (the DAG from the first step above)\n",
    "2. Join: determine if we should finish or replan\n",
    "3. Recontextualize: update the graph state based on the output from the joiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1733158858519,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "6OZVijOvUFMR"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 1.  Define vertices\n",
    "# We defined plan_and_schedule above already\n",
    "# Assign each node to a state variable to update\n",
    "graph_builder.add_node(\"plan_and_schedule\", plan_and_schedule)\n",
    "graph_builder.add_node(\"join\", joiner)\n",
    "\n",
    "\n",
    "## Define edges\n",
    "graph_builder.add_edge(\"plan_and_schedule\", \"join\")\n",
    "\n",
    "### This condition determines looping logic\n",
    "\n",
    "\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    if isinstance(messages[-1], AIMessage):\n",
    "        return END\n",
    "    return \"plan_and_schedule\"\n",
    "\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"join\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "graph_builder.add_edge(START, \"plan_and_schedule\")\n",
    "chain = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "executionInfo": {
     "elapsed": 870,
     "status": "ok",
     "timestamp": 1733158870268,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "flUeOQ6aUW4A",
    "outputId": "408dbcb6-865c-4897-bf59-6696b7b16faf"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALcAAAFNCAIAAADNcmkDAAAAAXNSR0IArs4c6QAAIABJREFUeJztnXdAU9fix8/NIHsQIAFZgqAMB1qcda8q7lk3+itaR7G2ttW+2tc+bV31afVVq9YtaivFiritOLFuqSKKgkyZSSCDhOzfH7FoIOGCJtwTcz5/hZt7z/0mfHLuufeecy5mMpkAAtEgJKIDIJwAZAkCH2QJAh9kCQIfZAkCH2QJAh8K0QGaTGWZRlFlUCn0NUqjVmMkOk6jcKNhJArG5FCYHJIogEGmYEQnahqYs1wvKclV59yvzntY7elL06gMTA6Fzac4y9ftxiDJKnQqhV6l1Jfla3xDGMFt2W06c9xozlGXO4ElFUWaayliNp8i8HZrGclyF7oRnehNKXisepahLMmtCW7L6jrUg+g4+MBuydWjFUXZ6h4jPAPaMInOYn9unpHe+bNy0DRhSAcO0VkaAl5L9FrjoR8Ke47yDGrLIjqLA9HrjJeTxEwuuVsMvJUKpJbodcZfvsqd/Lk/38vpjy+N4dZZqdFggvboA2PrSaM27Pw6d97aVi6iCACg82ABwMDZ/aVEB7EOjJYcWls4ZUkA0Smam65DPFg8yt3USqKDWAE6Sy7+Xj5gspDjTiU6CAG8O9JTLtUVZKmIDlIXuCwpfKKqKtf5t34LT2caSYde/MtHKohOURe4LLmWIukxAtIWXPPgLnLzDqQ/uiknOogFEFnyLEPpG8IQ+tOJDkIwPUZ6ZP+tJDqFBRBZ8vSuUuhPa7bdZWRkaDSa19vWYDCkp6fbO9ELmGyKRmUsyVU7qPzXACJLcjOqm+0CWkpKysyZM9Xq1/xPrFixYuXKlfYO9ZLgdqxnD6odV35TgcWSwixVqyg21a2Z8rx2LWK+CPnamzeS4PYsSbHWobtoErD0HKgS66hUh9zgzc/PX7VqVUZGBpfL7dmz59KlS0+cOLF69WoAwMCBAwEA33zzzYgRI9LT03fs2GE+jkRGRi5atCg8PBwAUFVVNXDgwI8//jgrK+vixYthYWF+fn7nzp0DAERHRwMAjh071qJFC/tm5nlQoTofhsUSlVzP5DokzIoVK/Ly8hYvXlxdXX379m0SifTuu+9OmzYtISHhxx9/ZLPZAQEBAIDi4mKNRhMXF0cikRITExcuXJiSkkKnv2hK79y5c8KECVu3biWTySwWq6ys7Pnz58uXLwcAeHp62j0zhmEMNlml0DM5UPyDoAgBAFDJDV4BDmm6FhcXh4WFjRkzBgAwbdo0AIBAIPDz8wMAtG3bls/nm1cbOnRoTEyM+XVERMTcuXPT09O7detmXtKuXbsFCxbUlsnn8yUSSVRUlCMCm2HxyNUyA7LEAowEHHTEiYmJ2bNnz9q1a+Pi4gQCgc0AGHbhwoWEhITc3FwmkwkAkEgkte926dLFEdkagM4kGw2w3IiFpfVKY5AVVXpHlLxgwYJPP/307NmzI0eOPHz4sK3VduzY8fnnn0dERKxfv37RokUAAKPxZXdJBoPhiGwNUFmuZfFg+Q3DYgmTS1bJDY4oGcOwKVOmJCcn9+nTZ+3ata9e56jtNaHRaHbv3j169OjFixdHRUW1a9cOt1hH97hQyQ1MLtmhu2g8sFjC9aAAx/RhNZ+1slisuXPnAgAeP35cWzdUVLy4Y6JWqzUajfmkxnxeU6cuqQODwZBIJA2s8IZUy/UB4UwSCZZevbDUaYHhrJM7S3uP8bJ7yUuWLGGz2d26dbt69SoAwKxChw4dyGTyunXrRo4cqdFoxo0bFxIS8uuvv3p4eCiVyu3bt5NIpOzsbFtldurU6dixYytXroyKiuJyub1797Zv5twH1Ww+LP8aAAD522+/JToDMB8XyvJraAyS3XseFRUVXb169fTp02q1Oj4+vm/fvgAALpcrEonOnTt35coVuVw+fPjwTp06paWlHT58OD8/Pz4+PjAwMCkpaerUqTqdbt++fT179oyIiKgtMyQkRCaTnT59+u7du3w+3+5t2+unpOGdOfB0woKoR+PjW3KZWAdtr75mw2g0Hd38fGy8H9FBXgJRtRbWmbv729zI7jxblW1mZub8+fPrL+dwOAqFwuomH3/8sflKiUOJi4uzengSiURlZWX1l48bNy4+Pt5WaddPSgLD4eoQDlFdAgDIuqPIz6wePN3b6rtarVYsFjepQB6Px2I5/BuvqKjQ6XT1l+t0OirVSqc7FovF4/GsFqVRG/Yuz5+zKtgBMV8fuCwBAJzeW9pliLtA1HxdCKDixikJz5Ma1plLdBALYDkTrmXQVNGvPxQSnYIYHv4lq5YbYFMERkvIFGz8Qr9DPxQQHaS5ycuszrwu7/++kOggVoDuiGNGLtWe2Fk6+XNXGW+Rc1/x6KZieJydeyDYC+jqEjNcgVv/94VbPsuWlkHUGcdB3E2VZt1WQqsIvHWJGYPedO5AGZmM9RjpwXJM7xNiyf5beS1FHN6V23mQzZvVMAC1JWYe35ZfOyaJ7MH1DqTDdiHh9VBW6XMfVuc/qiaTsR4jPHmesA9RcwJLzDy6KX96T1n0RNWuFw8DGItHZvOpZMd0SbE7ZDKmrNJVyw0qhb4sX6NS6IMiWWFdON6Bzd0h4fVwGkvMGAym/MxqmURXLTPUqAwalZ3vyqpUqvz8/Nqbw/aCzaMYDCYWl8ziUoQBNKcbc+RkljiaR48eff/99wkJCUQHgQtIz3EQUIEsQeCDLLEAwzDzwAvEqyBLLDCZTAUFLndzABdkSV3YbDbREaADWVIXpRKuWSFgAFliAYZhDYzsclmQJRaYTCapVEp0CuhAllhAIpGCgoKITgEdyBILjEZjbm4u0SmgA1lSl9rZKBC1IEvqUlNTQ3QE6ECWIPBBlliAWq9WQZZYgFqvVkGWIPBBlliAYRiXC92gKcJBllhgMpnkcrjmgIcBZIkFGIb5+/sTnQI6kCUWmEymwkIXHaXcAMgSBD7IkrqgXkj1QZbUBfVCqg+yBIEPsgSBD7LEAhKJ1LJlS6JTQAeyxAKj0ZiXl0d0CuhAliDwQZYg8EGW1AVdL6kPsqQu6HpJfZAlCHyQJRZgGObtbX1+c1cGWWKByWQqLS0lOgV0IEsQ+CBLLMAwjEJ5CyeWfUOQJRaYTCa93iGPInVqkCUWkMlkNB6nPsgSCwwGAxqPUx9kiQWod7RV0KzAAAAwadIklUplfuibTCbz8vIyP4j4zJkzREeDAlSXAADAsGHDysrKiouLxWKxTqcrLi4uLi7mcDhE54IFZAkAAIwfP77ONK8YhvXr14+4RHCBLAHmJ9KPGDGCTCbXLvH39584cSKhoSACWfKCCRMm+Pr6ml9jGDZw4EBz6wSBLHkJg8EYM2aMuTrx9/cfP3480YkgAlnykokTJ/r6+mIYNmDAAKEQxmdxEgX+PQudxigp0aqUhmbJQzAjBsRdunSpR9SYZxnVRGdxOCQSxvei8L3ccNfEuV5y+UhFdrqSxaMw2Oge2NsGm08peqLielA69XcPCGM2sGZDlpzaXeLuQ4/s7u6YkAgo0GqN5/c/7zHcw6+1TVFsWnLuQBlfRAvrzHdkQgQsHN9WMGCSUBhgfa5b663XssKaGrURKeI6dB8hvHO+0ta71i2RlmgpVHT640LwvNzyHqlsvWtdhWq5nu+J3/RFvDVQqCSBD61aZv1M1rolRgMw6NG9YteiulKH2XiINzqsIPBBliDwQZYg8EGWIPBBliDwQZYg8EGWIPBBliDwQZYg8EGWIPBBliDwaQ5LZn0wcfmKL5thR6/BiZNH+w2IlkjE9ipwxKi+P2/98U1KKHpe2G9A9PlUnGGFGzetGTt+8JvsqPGgugSBD7IEgY/d+jyPGNU3rE2kukadnZ3F4/HfGzx8xvTZ9ecV0mq1+/b/kpp6pryizMPDc/CgYTNjPzSPghkxqu+ij7+8evXC9RtXWSz2iOHjYmfMbninDx6k70/Y8SAjHQAQ1iZy7txFbVqHAwB+TzqYeuHshPFTd+7cLJGKQ0PDPvt0WUDAiwnmn2Zn/e+nH7KyMj0Env7+gbgfraam5sdNq69duwwAaN++40fzP/P29gEAnDyVfOSPXwsK8thsTo/uvT/4v/nu7gIAgFKp+H7V12lpF3lc/qRJsaNGjq8tZ8fOzedTT2u1Gn+/wIkTp/fv9+KQUVVVuXnLf9OuXXJzo3WMiq7d9c5dW347vP/s6b/Mfz7Oypw3f8bqVZu6dulRP6Stwt8ce/aMLyjMmzf3E08Pr7+uXzlwcLdSqVgY/0Wddchk8p07N7r36N3Cxy87OyvhwC4OhztxwjTzu6vXfDMz9sNJk2IvXjy3Z++2Nq3Du3Xr2cAeS0uLNVrN9GlxJBIpOTlx6ZcLDx1IodPpAIBHjzIOH96/ePEyvV6/fv33q9Z88/PmvQCAgoK8Tz6dw+PyZ8d9RCZT9u3/BfdzHTy0+8yZ47NmzvXw8Dxz9jiDwQAA7Nm7be++X/r2GThh3NTKKumtW39RqFTz+qdOH3tv8PBPFv0r9cKZHzeuDmrZqn37jkaj8atln5SWFk+dMovPF6Sn317x3b9qatQxQ0dptdrPvpj//HnhxAnTvL1bJCcnNvWbb6DwphZlFXta0rfPoL59BgIA2rbtIJfLUo4fiY39kMflvboOmUzesnkv9k93l+KSostXUmstiRk6auqUWQCAkFatT5w8evP2Xw1bMnDg0EGDYsyv27SJ+HTx3AcZ6Z2ju5mXfP/dBoHAAwAwduykLT9vkMllPC5v6/aNJIy0+ac9fL67+SEWP25c3fDnKiktZjAYUybPpFAow2JGAwAqKsoTDuwaNCjmX0uXm9eZ9P6M2vUHDxq25ItvAAC9evab+P7Qi5fOtW/f8fKV1PsP7h06kOLp6QUAGDhgiFqtSjpyKGboqKPJh3Nynv6wdnP0O10BAJER7WNnNW1kYQOFN6kcWzhqlE2XLj2On/jj6dPH5k/+KpWV0n37f7l1+7pCIQcAcNgvJ4Cg0xnmF2Qy2ctLKBFXNLwXDMOuXL1wODEhPz+XyWQCACqlkvqliUQ+AACJuILmRrt166+RI8ebFQEANGauvYEDhp4/f3rJ0vgF8xcHB4cAAO7cvWEwGEaNsP6/5PH4/wSgt2jhV15RBgC4fv2qXq+fMm1k7WoGg4HFYgMArly9EBwcUvtFkV4Z1N5IGijcLjjKEjabAwBQq+t2uJVKJXPmTmUwmP83a16LFn67dm0pLMq3noxMMRhxBhTu279j956t48ZOnhMXL5GK/7N8qdFkrL8alUIFABiMBolUrNfrfbxbNOmzdO3SY9XKjVu3/fjB7EnDYkYv+nipVCoBAHh5iXC3JZHJBoMBAFBZKfHw8Fy/buur75IpFABAeXlpaGhYkyLVoYHC7YKjLBFXlFv9Ho+lJFVWSjf/b49I5A0AEAq9bVmCi0ajOXho97CY0R8tWAwAKC8vw92Ez3M3V2ZN3VfXLj06R3dLOnJoy88bRCIfc20hrZQIhfiimOFwuFVVlSKRD41Gq5/KViTMVk/URhduFxxyJmwymU6dPsZhcwIDggAAblQ388EFACCXV/H57mZFAAAyedVrT9lVU6PWaDStW4fXFmVuxzWwCYvF8vX1v3jpT51O1/gdabVacwtmwvipnp5eT58+Np+GnDx5tHYd3Pk/O3XqYjAYjqX8XrtErVabX4SGhmVlZRYWWvm18HjuOp1OJpeZ/ywtLa59i0p1U6tV5v02ULhdsGddcuHiWQ8PTxqNfunSn/fSb384Z6H5dCAkpM3JU8mbt6yfMzs+Kir6j6OHd+3+OTKyw5UrqTdupBmNRpmsqvZY3nh4PH5wcMiRP34VCDyqlcq9+7aTSKRnz7Ib3ip2xpyVq77+KH7WkCEjSSRS0pFDuDs68sevadcuDRoYI5FUiMUVbdpE+PsHDh82JuX4Eblc1rlzd5msKiUlaf36bQ0cywYNjEk5fmTrto0lpcWtQ8Oys59cTbuwZ9fvdDp98uSZZ8+d+PiT2ePHTfEQeJ5PPV27VfQ7XTEM+2nzuvHjpuTl5mz7ZVPtW6EhbWpqar5dvmTe3E8aKLzRX2dD2LMu8fQUnjl7fPOW/5aXl8798OPaZn/cBwt69ex3+vQxjUbTu1f/GdPjjiYnfv/9Vzq9bvNPewICWv5x9LfX2+PXX61k0BnLV3z5W+L+efM+mT7tgzNnUhquJwYNHLow/gu5XLZt+8ZTp5IjItrh7qVFCz+dVvvz1g0nTh4dO3bS+xOnAwA+WfRl3AcLsrIyf9y4+vjxI507d6eQG/rJUanUH9ZsHj5sTGrqmfUbVt69d3PkiPHmtrNvC781q//n5Sncs3fb/oQdwcGhtVsFBgYt/eLbR5kPPl4Udz719IezF9a+NWDAkIkTpj1+/DAvN6eBwu2C9XHCN89ItTWgQ19B4wsaMapvzNDR8+YuslcyRDOT+N/cSZ8FMLlWzrBgn2/i+vWr369aZvWtnzbtDgy02zzPCxfF5eZaOVr16NHnyyX/sddenBTYLYmKit6+7aDVt7w87Tld0b+XrdLprRyqGP9cdHFl7GZJSvJFexX1KnQ6vamXN14P81VLhFXQPWEEPsgSBD7IEgQ+yBIEPsgSBD7IEgQ+yBIEPsgSBD7IEgQ+yBIEPtav0NOZZKOhoe48iLcPd28aZqPHrfW6hOdJKcmzZ2cnBOQoq3QysZbBsq6JdUv8QplatUs86gRhpjRP3aaTzadZWreETMG6DhGc3ffckcEQsFCap3qYVtl9uIetFRp68snzHPWZfaVRfQR8EY3Jgb0nCqLJYEBaolFW6Z7elU/+3J9EttlfH+cpSsoq/d3UytK8GpXi7TwAGY1GvV7v5mZz0n21Wm3u4/32IfBxwwDwb8OM6oPXNd3k2iQlJX333Xe23t21a1enTp3WrFnTvKGgw9Wvl2RmZkZERNh6Ny0tDQBw6tSp48ePN28uuHB1Sx49ehQeHm71LalUWlFRgWGYQqHYtm1bTk5Os6eDBZe2xGQysdnssDDrY3QfPnxYVVVlfl1cXPzFF3Vn2XAdXNqSR48eqVQ2HzB1/fp1pVJpfo1hWF5e3pIlS5oxHUS4tCXPnj3r1q2brXfT09NfHcyNYdhff/21e/fu5koHES5tyd9//+3j42PrXYVC8eqfbm5uTCZz1qxZzRINLlz6WplKpWrgBEcqlYpEohMnTkilUhqNxmKxmjcdROBcVXu76dq1a1paGu6o68TExJycnKVLlzZXLuhw3SPOs2fP+vTp05iB+d27d3flisSlLXny5An1n1kVG8bPzy8+Pt7xieDFdS15+vRpaGhoI1YEAIB79+7VnhW7IK5rSWlpaeMtuXHjxuXLlx2cCF5c15KMjIzAQPyJo81ER0c3aSq2twwXPRPWarUCgcDPz6+R60dHR0dHRzdixbcTF61LCgoKGrg2Xx+9Xp+QkODIRFDjupYEBAQ0fn0KhXLo0KHS0lJHhoIXF7VEIpE0cNXVKlOmTKmpqXFYIqhx0XZJbm5u45uuZqZOneqwOLDjonWJQqHw9fVt0ib37t27f/++wxJBjYtakp+f7+7u3qRNHj58eP78eYclghoXPeJIJBIPD5vDT6zSp08fiUTSiBXfQlzUEqFQ2NS6xN/f39/f32GJoMZFjziZmZnkJj6s6MGDB8nJyQ5LBDUuaonBYGjqZP45OTku23p1xSOOXq9vakUCAAgMDGSz7faMM+fCFS3R6XSdOnVq6lYdO3Z0TBwnwBWPOAwG4/bt2w0/las+GRkZ2dk4j2h6W3FFSwAAHA6nThd5XA4ePOiyw/uQJY3lnXfeiYyMdFgiqHHFdgkAICIiQi6XN2mTcePGOSwO7LhoXUKhUPLy8pq0Cepf4nIEBAQUFBQ0fv2ioqLExERHJoIaF7UkJCSkSZ1FtFrt9OnTHZkIalzUkqCgoCtXrjR+/eDg4PHjxzsyEdS4qCUtW7YsKSnRaDSNXP/vv/9uajvmbcJFLQEA9OrV68mTJ41cedOmTbUz3rggrmuJl5dXRkZGI1cOCgqyNbGWK+C6lnTs2LGkpKSRKy9btoxGozk4Eby4riWtWrUyT8GIS3l5+Z07dxyfCF5c15KWLVvqdDq1Gv+hDMeOHbt582azhIIUF71Cb8bb23v06NF6vb6qqiogIOCPP/6wupqPj0+bNm2aPR1EuKIlvXv3VqlU5kmgaufXa6DHybBhw5oxHYy44hGnZ8+eZj9qFaHRaLYma9Tr9X/++WfzBoQOV7Rk5cqVwcHBr/ZCcnd3t9Ur4O7du0lJSc2YDkZc0RIAwKpVq14d2ycSiVq0aGF1TQ6HM2/evGaMBiMuakmrVq3mzp3L4XDMDz+JioqytWZ4eHj79u2bNx10uKgl5jbpkCFDSCQSh8Pp0qWLrdU2b94sk8maNxp0vD3nOHKpHrP5sCjrzJ+z+NmTksrKyiD/CEWl3kqZcvnp45dmTPnQ6rsNQ3Uj0VlvyY/Q6WcFrpbrrx2X5KQrfUOZkuLG3uOtxWQyYbblMplMRqOBTH6d3xKNSdaoDJE9uJ0HCV5jc6hwbktkUl3i+qL+k334QjeqG3Q/XGWVLveBUi7RDIn1JjrLG+HElqiVhoSVeZOWtCI6CA6PblSJn9fEzHJiUaD7/TWetBRx/8nWT1+hIrwrn8Ei5z504kmFndiSZw+qeV42n90JFVQ6uSy/yW0meHBWS1RyvdCfTmM0eVA4IQh8aBp10wacQoWzWgIw7DXOaIjCqDep5E0+l4YHp7UE0YwgSxD4IEsQ+CBLEPggSxD4IEsQ+CBLEPggSxD4IEsQ+CBLEPggSxD4uJYlBoNhxsxxm7esx13z5Knk0WMHlpW56CPY6uBalmAYxmZz6HQ67ppubjQWi00iudb3Y4u3p3d0YyCRSFt+2tOYNQcOGDJwwBDHJ3IOXOi3cu7cyX4DovsNiP5w7rTahWfPnoidNX7Qe90mTRm+P2GnecDf6rXfmtfU6/UAgN+TDs7/aOaFi+emTR89dFjPhYviCgpca/YsF7KkQ4d3VixfFxb2cqTnmTPHV635JjQ07OtlK/v2GbRr988HDu4GAIwdM2nQoJhXt330KOPw4f2LFy9b/p91FeVlq9Z8Q8QnIAwXOuIIhSKhUHT6TEpFeZl5FMWOXZvbtYta9q/vAAC9e/VXKOS//rZ33NjJrUPDWgYG19n8++82CAQeAICxYydt+XmDTC7jcXkEfZTmxoXqkjoUFRWIxRW9e/WvXdK5c3eVSlX03PpswXQ6w/xCJPIBAEjEFc2VlHhc1xJltRIAwOe/HFLF4XABAOKK8oY3pFKoAACD0eD4jLDgupYIvUQAAJns5fyclZXSWlcQr+K6lnh4eHqLfG7efDkB36VLf9Lp9JAQl54cyyou1Hqtz8zYD1ev/faHdSs6d+5+9+7Nq2kXY2fMYTAYROeCDpezxGAwkP55tON77w2v0dQk/n7g7LkTnh5ec2bHT3p/BtEBYcRZxwmrFIZDawsmfhbUpK10Ot302DGhIWErlq9zWDQr5GcqCx8rhs7yac6d2hFXqUsqK6Upx49cv3G1rKw0fsHnRMdxMlyl9VpaVvLb4X1kMvmbf69+990+RMdxMlylLgkPizyRcpnoFM6Kq9QliDcBWYLAB1mCwAdZgsAHWYLAB1mCwAdZgsAHWYLAB1mCwAdZgsDHaS0xAS8//MFXkEAiYyyeE98McVZLmFxyeaG6pto5Op+Ki2voLOeYmtYqzmoJAKBVB3ZluXNM+aqrMYoCnfih1U5sSe8xXn8mFBOdAp97FyQAmALDWEQHeX2cta+aGXW1fufXeYOm+vC83Fg8KtFx6iIpqcm9ryBTQO+xXkRneSOc2xIAgNFguposzrlf7S50Ky+secPSTMBkNJrI9phqgMEiU+mkyO6cdu/y37w0YnF6S2qpURkaeGpWI3ny5Mm6deu2b9/+5nnc6KQ3jgMLTnx6Vgc60w4nEUJv90Hv9aUxnLi55gjenroE4TjQj8YCqVSamppKdAroQJZYUFZWtmvXLqJTQAeyxAIfH5+4uDiiU0AHapcg8EF1iQUSieTkyZNEp4AOZIkF5eXlBw8eJDoFdCBLLBAKhZMmTSI6BXSgdgkCH1SXWCAWi5OTk4lOAR3IEgsqKioSExOJTgEdyBILBALBkCFoYvG6oHYJAh9Ul1hQVVV18eJFolNAB7LEgpKSkh07dhCdAjqQJRZwudzu3bsTnQI6ULsEgQ+qSyxQKBS3bt0iOgV0IEssKCoq2rhxI9EpoANZYgGfz+/bty/RKaADtUsQ+KC6xAKlUnn//n2iU0AHssSCwsLCtWvXEp0COpAlFlCpVC8v5x6t6QhQuwSBD6pLLNBqtSUlJUSngA5kiQU5OTmff46ei1IXZIkFHA6nS5cuRKeADtQuQeCD6hIL0H0cqyBLLED3cayCLLGAyWSGh4cTnQI6ULsEgQ+qSyyQyWRpaWmNWNG1QJZYUFpaikaT1wdZYgGLxRKJRESngA7ULkHgg+oSC5RKZXp6OtEpoANZYkFhYeG6deuITgEdyBIL0H0cq6B2CQAALFu27NSpUxj24tswz0FtNBrv3r1LdDQoQHUJAADExsaaT20wDKudphwN8qsFWQIAAKGhoe+8886rS7hc7qxZs4hLBBfIkhdMnz699kqJyWRq27ZtdHQ00aFgAVnygtatW3fq1MncLvHw8EAVyasgS14yY8aMgIAAk8kUERHRsWNHouNABLLkJebWCZfLjY2NJToLXDjrmXBNtSHnQXVJnkZaolUr9XQmpbLCDk96NJlMBoOBQrHDY4NIZIxEAgwWhcEhe/nRgiOZviGMNy+WEJzPkux05b1LMkmxhuPF5HgySRQShUamuFEwyKpFDACD3qjTGPQag16rV5RXq2SasC68LoP5LK6TPbzKmSwpfKK6dEQCMLJ7AI/Fd5pHTtdi0BuVYlXpE2mrDux+EzzIZMi8to1zWGI0grMHxeJirSCAz+Q58YMKyiZ9AAAFvklEQVR5zYjzZepKVa/RXi3DneOzOIclv296Dqh0z5ZO/zDNV3l283mXwfy23blEB8HHCSxJ3laC0VlcoRM/tdkWBemlvUa6B0UyiQ6CA+yHxt//9xzQ3k5FAAABUd5pxytzHiiJDoID1JZcTKrAqHSe6O1UxIxfe+/U3yrkUi3RQRoCXksKs1TFeTqPwLeqLWKVgI7ep3aXE52iIeC15PIfEnfft18RAACN6WbEKJk35EQHsQmkljy9pzCRyAznP+ltJF7BgrRkCdEpbAKpJX9fkQsCeESnsIJYUvjZ113v3T9r32IpNDLHi5l1B9LqBEZL1EqDpETD5Dnf1dU3gcGnP7mrIjqFdWC05NkDJVcI+yUEu8MRsgofQ3pKDONtp/JCDVPgKEuu3Uy6lHZQJi8XuLfo2H5w33enUam058VZP+2Y/cH0DSfPbikufeLO9xk2+KO24b3NmyirK5NPbnj4+DKVQmsV9A7eHl4TEgnzDGCX5Kp9gqC7dQyjJeJiLdPLIZacTf3lUtrBnt3fF3kFlYvzL15JEIsLJ4//FgCg02kSfvtq9LDF7nyfM6nbDyZ+/dXiZBaLr9Nrt+2Jl0gKe787VeDuc+1GkiOCmdHrjNUyg+PKf21gtESlMHD9yHYvViavOH95z9TxK9q37W9ewuN4JqWsGRXzqfnP0cMWR7UbBACIGTT/x59jc/LutY/sl3Y9saT06ZzY/7UO6QIAaOnfbu2m9+2ezQyZSqmW6x1U+JsAoyU0JplCs78lT3NuGgz6A7//+8Dv//5nmQkAIFO8uKLlRn1R1bvzfQAAckUFACDj0SUfUYhZEQAAiWT/YLVQGRRtDapLGodKrjfojGSKnf8fcoUYAPDBtPV8nvDV5R4Cv9KynFeXUMhUAIDRaAAAVMlKfX3a2DeJLbQ1BrI9usnZHRgzMdhkvcbgxqDauVjGi3v0Qq+Wjd+KzXJXVlfaN4ktjDo9kwvjhUQYz4RZPIpea/+KNzQ4GsOwqzcO1y7RaNW4W/n6tCl8nllekW/3PPXRaw0srgOPaK8NjJb4tKTVKOzQ1bkOnh7+Pbu9n/n4yq6ExTfuHPvz4q7VG8YVFT9ueKt+vWZgGGnLrrmpl/fevnfiyPEf7B6sFlWlRugP47VEGI84we1YGddKQSuB3UseOXQRnye8ej0xK/s6l+PZNqIvjytseBNPD7/ZMzYeP7PpTOovfJ6oXXjfJ9k37B4MAFAtVQt8aDQGjHUJpH3Vdn2T59fe241p56YJzJQ+kbRuT+3Uz53oIFaAsS4BALTtwc3LqRYG2ew5cOrPrWk3Eusv9/MJKyqxfhCJn71DJAyyV8KT57Zcu2nlChuDzlHXKKxu8un8/QL3FrYK1Chqwjt72CuefYG0LgEA/Px5TpveASSK9ZZTtUqm0VTXX147B0l9eFwhmWy3X4WtACYT+GdqiyYEEOdViXxMvUZ72iuefYHXkvRLVY/vabzbQPrF2ZeMs7kfbQghOoVNYDzHMRPVh08l61XyGqKDOJzybPGAKTiNaGKB1xIAwLh437xbpUaDkeggDkSSXynyJYV3hnpUDtSWAABmLAt8/qCU6BSOQpxbxReY+oyF/XmSsFvC5lPGfeSTcS63Rgn1WITXQJxb6UbRDpgIuyJQt15fxWg07V9ZwBZyPfyhrpkbiUalk5fI/YLJ3WMgPfWtg3NYYubKUXHmdbkwRODuyyE6y2ui1xoqcirVcnX/CZ4tI9lEx2kszmSJueP0xSRxcbaazqOzPZlsD7rdOxg4Ao1KpyhXVUuqGWxSeGd2u3dhHB7QAE5miZkalSHvYXXW3WpllV5WoXVjkLlChkapIzqXBRgJ6DVGbY1BqzaIWjKEfrTQKFaLVtD1aW0MTmnJq+i0RpXcoFIYjAbIPggGqDSMxaU43cxH9XF6SxDNAOxnwggYQJYg8EGWIPBBliDwQZYg8EGWIPD5fwgEj7b+xKiQAAAAAElFTkSuQmCC",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(\n",
    "    chain.get_graph(xray=True).draw_mermaid_png(),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_1_L6LNUbkB"
   },
   "source": [
    "## Simple question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3374,
     "status": "ok",
     "timestamp": 1733158893782,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "x4ORSxcyUZhy",
    "outputId": "9d771b7c-0df5-46e9-a84d-ffea1c14bd62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': [FunctionMessage(content=\"[{'url': 'https://usafacts.org/metrics/gross-domestic-product-gdp-by-state-new-york/', 'content': 'Home New York Gross domestic product (GDP) state — New York (dollars) Adjustment Frequency Yearly In 2022 (most recent), Gross domestic product (GDP) was $2,053,179,700,000 in the United States for New York (state). This increased by $151,883,200,000 or 7.99% from 2021. Highest: $2,053,179,700,000 in 2022 Lowest: $718,814,300,000 in 1997 DOWNLOAD Data table Year    New York Sources Agency US Bureau of Economic Analysis Source Regional Economic Accounts Last updated at USAFacts     August 25, 2023 Suggested citation By state New York Search USAFacts Population Mental health Government spending Population change USAFacts is a not-for-profit, nonpartisan civic initiative making government data easy for all. Stay informed with unbiased, data-driven insights sent to your inbox weekly. Health Population Government spending Data sources © 2024 USAFacts.'}]\", additional_kwargs={'idx': 1, 'args': {'query': 'GDP of New York'}}, response_metadata={}, name='tavily_search_results_json', id='a4aca0fa-0da2-4993-8f89-4c06ac9fb25f', tool_call_id=1), FunctionMessage(content='ERROR(Failed to call math with args {\\'context\\': [\\'$1\\']}. Args resolved to {\\'context\\': [\"[{\\'url\\': \\'https://usafacts.org/metrics/gross-domestic-product-gdp-by-state-new-york/\\', \\'content\\': \\'Home New York Gross domestic product (GDP) state — New York (dollars) Adjustment Frequency Yearly In 2022 (most recent), Gross domestic product (GDP) was $2,053,179,700,000 in the United States for New York (state). This increased by $151,883,200,000 or 7.99% from 2021. Highest: $2,053,179,700,000 in 2022 Lowest: $718,814,300,000 in 1997 DOWNLOAD Data table Year    New York Sources Agency US Bureau of Economic Analysis Source Regional Economic Accounts Last updated at USAFacts     August 25, 2023 Suggested citation By state New York Search USAFacts Population Mental health Government spending Population change USAFacts is a not-for-profit, nonpartisan civic initiative making government data easy for all. Stay informed with unbiased, data-driven insights sent to your inbox weekly. Health Population Government spending Data sources © 2024 USAFacts.\\'}]\"]}. Error: 1 validation error for math\\nproblem\\n  Field required [type=missing, input_value={\\'context\\': [\"[{\\'url\\': \\'h... © 2024 USAFacts.\\'}]\"]}, input_type=dict]\\n    For further information visit https://errors.pydantic.dev/2.9/v/missing)', additional_kwargs={'idx': 2, 'args': {'context': ['$1']}}, response_metadata={}, name='math', id='12bae6b1-a719-4426-b7cc-de81a1e40d78', tool_call_id=2)]}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content='Thought: I have found the GDP of New York, which was $2,053,179,700,000 in 2022.', additional_kwargs={}, response_metadata={}, id='574ef014-47fc-417b-b9ed-b3677200a1e8'), AIMessage(content='The GDP of New York was $2,053,179,700,000 in 2022.', additional_kwargs={}, response_metadata={}, id='d738acc3-29cd-491c-bfb6-5eef956a078f')]}}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for step in chain.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"What's the GDP of New York?\")]}\n",
    "):\n",
    "    print(step)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 172,
     "status": "ok",
     "timestamp": 1733158895090,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "r1DqB1BFUesW",
    "outputId": "43dc1b84-8e97-461d-de83-de4c5eeb71e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GDP of New York was $2,053,179,700,000 in 2022.\n"
     ]
    }
   ],
   "source": [
    "# Final answer\n",
    "print(step[\"join\"][\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5JRPHY-UhEw"
   },
   "source": [
    "## Multi-hop question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Suk72wiUjf5"
   },
   "source": [
    "Ask a question that requires the agent to perform multiple searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44317,
     "status": "ok",
     "timestamp": 1733158980313,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "5y46QKUtUfyr",
    "outputId": "1a4ca790-5b51-4934-e6bc-d1793809651a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': [FunctionMessage(content=\"[{'url': 'https://www.oldest.org/animals/parrots/', 'content': '9 Oldest Birds in the World\\\\n10 Rarest Parrots in the World\\\\n10 Rarest Birds in the World\\\\n7 Largest Birds of Prey in the World\\\\n9 Largest Flying Birds in the World\\\\nRelated Post\\\\n8 Oldest Elephants in the World\\\\n10 Oldest Living Dogs\\\\n10 Oldest Rabbits in the World\\\\n10 Oldest Animal Lifespans On Earth\\\\n8 of the Oldest Known Puggles Ever\\\\nLeave a comment Cancel reply\\\\nYour email address will not be published. It should be noted that there are no certificates that prove Charlie’s age — but given how well-known she is and how her life has been tracked throughout history, we think it’s safe to assume that there’s some credibility to her age, even if she isn’t as old as records say.\\\\n Brookfield Zoo\\\\nphoto source: Wikipedia\\\\nCookie was one of the more beloved and known parrots in the world thanks to his adorable antics at Chicagoland’s zoo. 3 Oldest Parrots in The World\\\\nParrots are known to be some of the most talented birds in the words, being both intelligent and snarky. Pet and Exotics (Originally owned by Birds and Animals Unlimited)\\\\nphoto source: Twitter\\\\nMost birds of Poncho’s breed live to be around 50 or 60, though some can reach 80 years of age with a good, healthy diet.'}]\", additional_kwargs={'idx': 1, 'args': {'query': 'oldest living parrot'}}, response_metadata={}, name='tavily_search_results_json', id='1b03ac9f-39f8-4471-82ba-57a6edf92b6e', tool_call_id=1), FunctionMessage(content=\"ERROR(Failed to call math with args {'context': ['average lifespan of parrots']}. Args resolved to {'context': ['average lifespan of parrots']}. Error: 1 validation error for math\\nproblem\\n  Field required [type=missing, input_value={'context': ['average lifespan of parrots']}, input_type=dict]\\n    For further information visit https://errors.pydantic.dev/2.9/v/missing)\", additional_kwargs={'idx': 2, 'args': {'context': ['average lifespan of parrots']}}, response_metadata={}, name='math', id='4a72c1db-723a-486a-b941-59a175beaf97', tool_call_id=2), FunctionMessage(content='ERROR(Failed to call math with args {\\'context\\': [\\'$1\\']}. Args resolved to {\\'context\\': [\"[{\\'url\\': \\'https://www.oldest.org/animals/parrots/\\', \\'content\\': \\'9 Oldest Birds in the World\\\\\\\\n10 Rarest Parrots in the World\\\\\\\\n10 Rarest Birds in the World\\\\\\\\n7 Largest Birds of Prey in the World\\\\\\\\n9 Largest Flying Birds in the World\\\\\\\\nRelated Post\\\\\\\\n8 Oldest Elephants in the World\\\\\\\\n10 Oldest Living Dogs\\\\\\\\n10 Oldest Rabbits in the World\\\\\\\\n10 Oldest Animal Lifespans On Earth\\\\\\\\n8 of the Oldest Known Puggles Ever\\\\\\\\nLeave a comment Cancel reply\\\\\\\\nYour email address will not be published. It should be noted that there are no certificates that prove Charlie’s age — but given how well-known she is and how her life has been tracked throughout history, we think it’s safe to assume that there’s some credibility to her age, even if she isn’t as old as records say.\\\\\\\\n Brookfield Zoo\\\\\\\\nphoto source: Wikipedia\\\\\\\\nCookie was one of the more beloved and known parrots in the world thanks to his adorable antics at Chicagoland’s zoo. 3 Oldest Parrots in The World\\\\\\\\nParrots are known to be some of the most talented birds in the words, being both intelligent and snarky. Pet and Exotics (Originally owned by Birds and Animals Unlimited)\\\\\\\\nphoto source: Twitter\\\\\\\\nMost birds of Poncho’s breed live to be around 50 or 60, though some can reach 80 years of age with a good, healthy diet.\\'}]\"]}. Error: 1 validation error for math\\nproblem\\n  Field required [type=missing, input_value={\\'context\\': [\"[{\\'url\\': \\'h...ood, healthy diet.\\'}]\"]}, input_type=dict]\\n    For further information visit https://errors.pydantic.dev/2.9/v/missing)', additional_kwargs={'idx': 3, 'args': {'context': ['$1']}}, response_metadata={}, name='math', id='ae0ea240-e7b8-47f9-979e-5941a4348530', tool_call_id=3)]}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content='Thought: I have not been able to retrieve the specific information about the oldest parrot alive and the average lifespan of parrots.', additional_kwargs={}, response_metadata={}, id='2b7ff5d7-52ab-4fcc-8218-21b0fb2e6a2c'), SystemMessage(content='Context from last attempt: I was unable to find the specific details about the oldest parrot alive and the average lifespan of parrots. I recommend conducting a direct search on reliable websites or contacting avian experts for accurate information.', additional_kwargs={}, response_metadata={}, id='dc25c2e8-91cb-4ee4-bb1b-444ed0522bbb')]}}\n",
      "---\n",
      "{'plan_and_schedule': {'messages': [FunctionMessage(content='join', additional_kwargs={'idx': 4, 'args': ()}, response_metadata={}, name='join', id='53f04c0c-9731-4331-bc56-4f22e510808b', tool_call_id=4)]}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content='Thought: I have not been able to retrieve the specific information about the oldest parrot alive and the average lifespan of parrots.', additional_kwargs={}, response_metadata={}, id='316a32af-fbd3-4472-a563-49e87b89ff20'), SystemMessage(content='Context from last attempt: I recommend conducting a direct search on reliable websites or contacting avian experts for accurate information.', additional_kwargs={}, response_metadata={}, id='cf57c5d8-4cfd-4e8c-baeb-001a640f319e')]}}\n",
      "---\n",
      "{'plan_and_schedule': {'messages': []}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content='Thought: I have not been able to retrieve the specific information about the oldest parrot alive and the average lifespan of parrots.', additional_kwargs={}, response_metadata={}, id='3e5aa345-48e0-4223-8885-9127debdb561'), SystemMessage(content='Context from last attempt: I have not been able to retrieve the specific information about the oldest parrot alive and the average lifespan of parrots.', additional_kwargs={}, response_metadata={}, id='67911ce1-321d-498f-aa48-80e35615bc29')]}}\n",
      "---\n",
      "{'plan_and_schedule': {'messages': []}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content='Thought: I have not been able to retrieve the specific information about the oldest parrot alive and the average lifespan of parrots.', additional_kwargs={}, response_metadata={}, id='c87b63a8-f9e1-48b9-831e-2f1fddabeb09'), SystemMessage(content='Context from last attempt: I recommend conducting a direct search on reliable websites or contacting avian experts for accurate information.', additional_kwargs={}, response_metadata={}, id='6e05f060-9180-430b-9e6b-23071ac7b14c')]}}\n",
      "---\n",
      "{'plan_and_schedule': {'messages': []}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content='Thought: I have not been able to retrieve the specific information about the oldest parrot alive and the average lifespan of parrots.', additional_kwargs={}, response_metadata={}, id='2adc3799-6aea-4ab9-8c53-09eff663f4c9'), SystemMessage(content='Context from last attempt: I have not been able to find the specific information after multiple attempts. I recommend seeking information from reliable websites or contacting avian experts for accurate details.', additional_kwargs={}, response_metadata={}, id='7aab8a5e-472c-436c-bebe-c5e2f252b50b')]}}\n",
      "---\n",
      "{'plan_and_schedule': {'messages': []}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content='Thought: I have not been able to retrieve the specific information about the oldest parrot alive and the average lifespan of parrots.', additional_kwargs={}, response_metadata={}, id='c53abf45-1592-4db1-88c3-1b7c2d3f1e2a'), SystemMessage(content='Context from last attempt: I have not been able to retrieve the specific information after multiple attempts. I recommend seeking information from reliable websites or contacting avian experts for accurate details.', additional_kwargs={}, response_metadata={}, id='611ca999-fdf2-4bed-9597-ba989b053802')]}}\n",
      "---\n",
      "{'plan_and_schedule': {'messages': []}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content='Thought: I have not been able to retrieve the specific information about the oldest parrot alive and the average lifespan of parrots.', additional_kwargs={}, response_metadata={}, id='30324c18-22f6-4ab8-be60-f8e3a7338402'), SystemMessage(content='Context from last attempt: I have not been able to find the specific information after multiple attempts. It is recommended to seek information from reliable websites or contact avian experts for accurate details.', additional_kwargs={}, response_metadata={}, id='6716e29e-96fa-4c41-af53-eee3480f9961')]}}\n",
      "---\n",
      "{'plan_and_schedule': {'messages': [FunctionMessage(content='[{\\'url\\': \\'https://en.wikipedia.org/wiki/Cookie_(cockatoo)\\', \\'content\\': \\'He was one of the longest-lived birds on record[4] and was recognised by the Guinness World Records as the oldest living parrot in the world.[5]\\\\nThe next-oldest pink cockatoo to be found in a zoological setting was a 31-year-old female bird located at Paradise Wildlife Sanctuary, England.[3] Information published by the World Parrot Trust states longevity for Cookie\\\\\\'s species in captivity is on average 40–60 years.[6]\\\\nLife[edit]\\\\nCookie was Brookfield Zoo\\\\\\'s oldest resident and the last surviving member of the animal collection from the time of the zoo\\\\\\'s opening in 1934, having arrived from Taronga Zoo of Sydney, New South Wales, Australia, in the same year and judged to be one year old at the time.[7]\\\\nIn the 1950s an attempt was made to introduce Cookie to a female pink cockatoo, but Cookie rejected her as \"she was not nice to him\".[8]\\\\n In 2007, Cookie was diagnosed with, and placed on medication and nutritional supplements for, osteoarthritis and osteoporosis\\\\xa0– medical conditions which occur commonly in aging animals and humans alike,[7] although it is believed that the latter may also have been brought on as a result of being fed a seed-only diet for the first 40 years of his life, in the years before the dietary requirements of his species were fully understood.[9]\\\\nCookie was \"retired\" from exhibition at the zoo in 2009 (following a few months of weekend-only appearances) in order to preserve his health, after it was noticed by staff that his appetite, demeanor and stress levels improved markedly when not on public display. age.[11] A memorial at the zoo was unveiled in September 2017.[12]\\\\nIn 2020, Cookie became the subject of a poetry collection by Barbara Gregorich entitled Cookie the Cockatoo: Everything Changes.[13]\\\\nSee also[edit]\\\\nReferences[edit]\\\\nExternal links[edit] He was believed to be the oldest member of his species alive in captivity, at the age of 82 in June 2015,[1][2] having significantly exceeded the average lifespan for his kind.[3] He was moved to a permanent residence in the keepers\\\\\\' office of the zoo\\\\\\'s Perching Bird House, although he made occasional appearances for special events, such as his birthday celebration, which was held each June.[3]\\'}]', additional_kwargs={'idx': 5, 'args': {'query': 'oldest living parrot in the world'}}, response_metadata={}, name='tavily_search_results_json', id='412417a5-ad2a-4507-a978-d056666c52a6', tool_call_id=5), FunctionMessage(content='[{\\'url\\': \\'https://www.thesprucepets.com/how-long-do-parrots-and-other-pet-birds-live-1238433\\', \\'content\\': \"It\\'s possible that a pet bird can outlive its owners\\\\nThe Spruce / Adrienne Legault\\\\nParrots and other birds can live up to 10 to 50 years or more depending on the type and the conditions they live in. They vary in size from small birds that can fit in the palm of your hand to large birds the size of a cat and their lifespans are just as variable.\\\\n Also, for birds who live longer some owners have to make a plan of where the bird is going in the circumstance the bird outlives the owner.\\\\n In reality, there is a wide range in the age that pet birds might reach and certainly, some will live longer (or shorter amounts of time) than the ages listed.\\\\n Potential owners need to be aware of the longevity of their bird so they can be prepared to provide proper care for them for as long as they live.\\\\n\"}]', additional_kwargs={'idx': 6, 'args': {'query': 'average lifespan of parrots'}}, response_metadata={}, name='tavily_search_results_json', id='0d61f67f-5c41-440a-92f1-2f66686ee266', tool_call_id=6)]}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content=\"Thought: I have found information about Cookie, the oldest parrot alive, who was 82 years old in June 2015. The average lifespan of Cookie's species in captivity is around 40-60 years. Parrots in general can live up to 10 to 50 years or more depending on the type and conditions they live in.\", additional_kwargs={}, response_metadata={}, id='66012c4a-5587-4960-bb45-0fbc9837203a'), AIMessage(content=\"Cookie, a parrot, was believed to be the oldest member of his species alive in captivity at the age of 82 in June 2015. The average lifespan of Cookie's species in captivity is around 40-60 years. Parrots, in general, can live up to 10 to 50 years or more depending on the type and conditions they live in.\", additional_kwargs={}, response_metadata={}, id='7529daf4-027a-4bf7-a792-8a214090b275')]}}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "steps = chain.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"What's the oldest parrot alive, and how much longer is that than the average?\"\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"recursion_limit\": 100,\n",
    "    },\n",
    ")\n",
    "for step in steps:\n",
    "    print(step)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1733158980313,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "cwm0-PMQUp0E",
    "outputId": "aafa98d0-faaa-49bd-9bc5-e24fbaa04ac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie, a parrot, was believed to be the oldest member of his species alive in captivity at the age of 82 in June 2015. The average lifespan of Cookie's species in captivity is around 40-60 years. Parrots, in general, can live up to 10 to 50 years or more depending on the type and conditions they live in.\n"
     ]
    }
   ],
   "source": [
    "# Final answer\n",
    "print(step[\"join\"][\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNwvZ6tLUsK_"
   },
   "source": [
    "## Multi-step math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3179,
     "status": "ok",
     "timestamp": 1733158983490,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "YkkaMzvzUq7J",
    "outputId": "13f4b975-527f-43bf-cafe-0efcb885565a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': [FunctionMessage(content='3307.0', additional_kwargs={'idx': 1, 'args': {'problem': '((3*(4+5)/0.5)+3245) + 8'}}, response_metadata={}, name='math', id='cf3cbfcb-466f-4e94-ba44-125c9957ebe7', tool_call_id=1), FunctionMessage(content='7.565011820330969', additional_kwargs={'idx': 2, 'args': {'problem': '32/4.23'}}, response_metadata={}, name='math', id='8db202fe-d9cf-4837-a261-cd773dfc15fd', tool_call_id=2), FunctionMessage(content='join', additional_kwargs={'idx': 3, 'args': ()}, response_metadata={}, name='join', id='b9716615-0682-400c-9fa9-a555fa917708', tool_call_id=3)]}}\n",
      "{'join': {'messages': [AIMessage(content=\"Thought: I have all the necessary information to answer the user's question.\", additional_kwargs={}, response_metadata={}, id='aaa9b005-84a6-421d-be2c-6bd11fea9fe2'), AIMessage(content='The sum of ((3*(4+5)/0.5)+3245) + 8 is 3307.0, and the result of 32/4.23 is approximately 7.565. The sum of these two values is 3314.565.', additional_kwargs={}, response_metadata={}, id='eb473239-80ff-42dc-8d05-992ad1939640')]}}\n"
     ]
    }
   ],
   "source": [
    "for step in chain.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"What's ((3*(4+5)/0.5)+3245) + 8? What's 32/4.23? What's the sum of those two values?\"\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1733158983490,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "kM4rTOTUUu7C",
    "outputId": "c4415a2f-d8c6-4385-92a9-376d68f4b949"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of ((3*(4+5)/0.5)+3245) + 8 is 3307.0, and the result of 32/4.23 is approximately 7.565. The sum of these two values is 3314.565.\n"
     ]
    }
   ],
   "source": [
    "# Final answer\n",
    "print(step[\"join\"][\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j21afDWnUw7U"
   },
   "source": [
    "## Complex Replanning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4150,
     "status": "ok",
     "timestamp": 1733158992510,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "aD4mWH0hUwHB",
    "outputId": "beaaf70f-3264-40fb-9293-cf8593ece4b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': [FunctionMessage(content=\"[{'url': 'https://www.timeanddate.com/weather/japan/tokyo', 'content': 'Home \\\\xa0 Weather \\\\xa0 Japan \\\\xa0 Tokyo Weather in Tokyo, Japan Weather 50\\\\xa0°F Forecast: 56 / 44\\\\xa0°F Wind: 10 mph ↑ from North Current Time:   Nov 19, 2024 at 2:55:59 am See more hour-by-hour weather Forecast for the next 48 hours Forecast                           ↑   N * Updated Monday, November 18, 2024 10:02:53 pm Tokyo time - Weather by CustomWeather, © 2024 66 / 53\\\\xa0°F Wind: 8 mph ↑ from Northwest More weather last week 52\\\\xa0°F 46\\\\xa0°F 45\\\\xa0°F More weather in Japan 56 / 44\\\\xa0°F 50 / 43\\\\xa0°F 62 / 49\\\\xa0°F 64 / 46\\\\xa0°F 56 / 45\\\\xa0°F 57 / 43\\\\xa0°F 57 / 44\\\\xa0°F Sun & Moon times precise to the second. Privacy Policy Weather'}]\", additional_kwargs={'idx': 1, 'args': {'query': 'current temperature in Tokyo'}}, response_metadata={}, name='tavily_search_results_json', id='5bda7ed3-374e-4422-ae22-4b7047816844', tool_call_id=1), FunctionMessage(content='50', additional_kwargs={'idx': 2, 'args': {'problem': 'temperature in Tokyo', 'context': ['$1']}}, response_metadata={}, name='math', id='1cbc3dde-3d6c-44e8-8d0e-2c47244628e0', tool_call_id=2), FunctionMessage(content='join', additional_kwargs={'idx': 3, 'args': ()}, response_metadata={}, name='join', id='68815054-9bb9-400e-a229-38cea79db1c1', tool_call_id=3)]}}\n",
      "{'join': {'messages': [AIMessage(content='Thought: I have found the current temperature in Tokyo, which is 50°F.', additional_kwargs={}, response_metadata={}, id='1cd03ebd-428b-4fab-9c45-26eff3ef7547'), AIMessage(content='The current temperature in Tokyo is 50°F.', additional_kwargs={}, response_metadata={}, id='b7fc7547-5ce0-42d9-a489-169175c049e7')]}}\n"
     ]
    }
   ],
   "source": [
    "for step in chain.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Find the current temperature in Tokyo, then, respond with a flashcard summarizing this information\"\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIp2kehAU2ml"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOpobAV5alwKfMdpTo2bI1d",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
