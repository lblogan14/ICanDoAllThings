{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM/OKj6GyLX/L3s5qLlFWGD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"SMPpQuNhjGKp"},"outputs":[],"source":["!pip install -qU diffusers transformers accelerate"]},{"cell_type":"markdown","source":["# Stable Video Diffusion"],"metadata":{"id":"jghwVinMjIJJ"}},{"cell_type":"markdown","source":["**Stable Video Diffusion (SVD)** is an image-to-video generation model that can generate 2-4 second high resotlution (576x1024) videos conditioned on an input image.\n","\n","There are two variants of this model, `SVD` and `SVD-XT`. The SVD checkpoint is trained to generate 14 frames and the SVD-XT checkpoint is further finetuned to generate 25 frames."],"metadata":{"id":"SlB8MJSkpA9C"}},{"cell_type":"code","source":["import torch\n","from diffusers import StableVideoDiffusionPipeline\n","from diffusers.utils import load_image, export_to_video\n","\n","pipe = StableVideoDiffusionPipeline.from_pretrained(\n","    'stabilityai/stable-video-diffusion-img2vid-xt',\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",")\n","pipe.enable_model_cpu_offload()"],"metadata":{"id":"B6fH-mkPjH_M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the conditioning image\n","image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png\")\n","image = image.resize((1024, 576))\n","generator = torch.manual_seed(111)\n","\n","frames = pipe(\n","    image,\n","    decode_chunk_size=8,\n","    generator=generator\n",").frames[0]\n","\n","export_to_video(frames, 'generated.mp4', fps=7)"],"metadata":{"id":"23oSWDxtpqKA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## `torch.compile`"],"metadata":{"id":"eqpP5z_np44F"}},{"cell_type":"markdown","source":["We can gain a 20-25% speedup at the expense of slightly increased memory by compiling the UNet"],"metadata":{"id":"ttqbiOzZp6k6"}},{"cell_type":"code","source":["pipe = StableVideoDiffusionPipeline.from_pretrained(\n","    'stabilityai/stable-video-diffusion-img2vid-xt',\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",")\n","#pipe.enable_model_cpu_offload()\n","pipe.to('cuda')\n","pipe.unet = torch.compile(\n","    pipe.unet,\n","    mode='reduce-overhead',\n","    fullgraph=True\n",")"],"metadata":{"id":"5Hpp-fpop6Kj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the conditioning image\n","image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png\")\n","image = image.resize((1024, 576))\n","generator = torch.manual_seed(111)\n","\n","frames = pipe(\n","    image,\n","    decode_chunk_size=8,\n","    generator=generator\n",").frames[0]\n","\n","export_to_video(frames, 'generated.mp4', fps=7)"],"metadata":{"id":"TaVL8RXCqKHb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Reduce memory usage"],"metadata":{"id":"iepKpCn9qKqa"}},{"cell_type":"markdown","source":["Video generation is very memory intensive because we are generating `num_frames` all at once, similar to text-to-image generation with a high batch size.\n","\n","To reduce the memory requirement, we can\n","* enable model offloading: each component of the pipeline is offloaded to the CPU once it is not needed anymore.\n","* enable feed-forward chunking: the feed-forward layer runs in a loop instead of running a single feed-forward with a huge batch size.\n","* reduce `decode_chunk_size`: the VAE decodes frames in chunks instead of decoding them all together. Setting `decode_chunk_size=1` decodes one frame at a time and uses the least amount of memory"],"metadata":{"id":"LOE2a4JHqMql"}},{"cell_type":"code","source":["pipe = StableVideoDiffusionPipeline.from_pretrained(\n","    'stabilityai/stable-video-diffusion-img2vid-xt',\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",")\n","pipe.enable_model_cpu_offload()\n","pipe.unet.enable_forward_chunking()"],"metadata":{"id":"OTyiOIR-q1yp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the conditioning image\n","image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png\")\n","image = image.resize((1024, 576))\n","generator = torch.manual_seed(111)\n","\n","frames = pipe(\n","    image,\n","    decode_chunk_size=2,\n","    num_frames=25,\n","    generator=generator\n",").frames[0]\n","\n","export_to_video(frames, 'generated.mp4', fps=7)"],"metadata":{"id":"OBlkZOJ0q_I6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Micro-conditioning"],"metadata":{"id":"rxm8D6-ArD63"}},{"cell_type":"markdown","source":["* `fps`: the frames per second of the generated video\n","* `motion_bucket_id`: the motion bucket id to use for the generated video. This can be used to control the motion of the generated video. Increasing the motion bucket id increases the motion of the generated video.\n","* `noise_aug_strength`: the amount of noise added to the conditioning image. The higher the values the less the video resembles the conditioning image. Increasing this value also increases the motion of the generated video."],"metadata":{"id":"tXQrRR3KrHi5"}},{"cell_type":"code","source":["import torch\n","from diffusers import StableVideoDiffusionPipeline\n","from diffusers.utils import load_image, export_to_video\n","\n","pipe = StableVideoDiffusionPipeline.from_pretrained(\n","  \"stabilityai/stable-video-diffusion-img2vid-xt\",\n","  torch_dtype=torch.float16,\n","  variant=\"fp16\"\n",")\n","pipe.enable_model_cpu_offload()"],"metadata":{"id":"JRkv4j1lrf3E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the conditioning image\n","image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png\")\n","image = image.resize((1024, 576))\n","generator = torch.manual_seed(42)\n","\n","frames = pipe(\n","    image,\n","    decode_chunk_size=8,\n","    generator=generator,\n","    motion_bucket_id=180,\n","    noise_aug_strength=0.1\n",").frames[0]\n","export_to_video(frames, \"generated.mp4\", fps=7)"],"metadata":{"id":"d0bpt6PTrjS1"},"execution_count":null,"outputs":[]}]}