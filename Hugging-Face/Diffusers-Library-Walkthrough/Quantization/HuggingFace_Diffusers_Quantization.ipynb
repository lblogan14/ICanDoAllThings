{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNbuemcsE1j0NrR4xHVa/h5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -qU diffusers transformers accelerate bitsandbytes gguf torchao"],"metadata":{"id":"HPtoLoEjsgBd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Quantization"],"metadata":{"id":"YRqmMU_gqqc8"}},{"cell_type":"markdown","source":["**Quantization** techniques focus on representing data with less information while also trying not to lose too much accuracy. This often means converting a data type to represent the same information with fewer bits."],"metadata":{"id":"yjDKMlrJqsN0"}},{"cell_type":"markdown","source":["# bitsandbytes"],"metadata":{"id":"nFKdRAxUrGeP"}},{"cell_type":"markdown","source":["**bitsandbytes** is the easiest option for quantizing a model to 8-bit or 4-bit.\n","\n","8-bit quantization multiplies outliers in fp16 with non-outlier in int8, converts the non-outlier values back to fp16, and then adds them together to return the weights in fp16. This reduces the degradative effect outlier values have on a model's performance.\n","\n","4-bit quantization compresses a model even further, and it is commonly used with **QLoRA** to finetune quantized LLMs."],"metadata":{"id":"9ncVdzrQruNW"}},{"cell_type":"markdown","source":["##### 8-bit"],"metadata":{"id":"PTS6MTAEsqqZ"}},{"cell_type":"markdown","source":["Quantizing a model in 8-bit halves the memory-usage.\n","\n","bitsandbytes is supported in both `transformers` and `diffusers`, so we can quantize both the `FluxTransformer2DModel` and `T5EncoderModel`.\n","\n","For Ada and higher-series GPUs, we can change `torch_dtype` to `torch.bfloat16`.\n","\n","The `CLIPTextModel` and `AutoencoderKL` are not quantized because they are already small in size and because `AutoencoderKL` only has a few `torch.nn.Linear` layers."],"metadata":{"id":"S5EXvp8ossO0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TLXeYcM6qnkP"},"outputs":[],"source":["from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\n","from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig\n","\n","from diffusers import FluxTransformer2DModel\n","from transformers import T5EncoderModel\n","\n","quant_config = TransformersBitsAndBytesConfig(load_in_8bit=True)\n","\n","text_encoder_2_8bit = T5EncoderModel.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    subfolder='text_encoder_2',\n","    quantization_config=quant_config,\n","    torch_dtype=torch.float16,\n",")\n","\n","quant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\n","\n","transformer_8bit = FluxTransformer2DModel.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    subfolder='transformer',\n","    quantization_config=quant_config,\n","    torch_dtype=torch.float16,\n",")"]},{"cell_type":"markdown","source":["By default, all the other modules such as `torch.nn.LayerNorm` are converted to `torch.float16`. We can change the data type of these modules with the `torch_dtype` parameter:"],"metadata":{"id":"GUGOCWjlzUS4"}},{"cell_type":"code","source":["transformer_8bit = FluxTransformer2DModel.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    subfolder='transformer',\n","    quantization_config=quant_config,\n","    torch_dtype=torch.float32, # new here\n",")"],"metadata":{"id":"SuVFuhlezd3t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can generate an image using our quantized models. Setting `device_map=\"auto\"` automatically fills all available space on the GPUs first, then the CPU, and finally, the hard drive if there is still not enough memory."],"metadata":{"id":"QI6BV0bozrU1"}},{"cell_type":"code","source":["from diffusers import FluxPipeline\n","\n","pipe = FluxPipeline.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    transformer=transformer_8bit,\n","    text_encoder_2=text_encoder_2_8bit,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n",")"],"metadata":{"id":"MijXaVefz2y7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipe_kwargs = {\n","    'prompt': 'a cat holding a sign that says hello world',\n","    'height': 1024,\n","    'width': 1024,\n","    'guidance_scale': 3.5,\n","    'num_inference_steps': 50,\n","    'max_sequence_length': 512,\n","}\n","\n","image = pipe(\n","    **pipe_kwargs,\n","    generator=torch.manual_sed(111),\n",").images[0]\n","image"],"metadata":{"id":"SaT60nzl0MRr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When there is enough memory, we can also directly move the pipeline to the GPU with `.to('cuda')` and apply `enable_model_cpu_offload()` to optimize GPU memory usage.\n","\n","Once a model is quantized, we can push the model to the Hub with the `push_to_hub()` method. The quantization `config.json` file is pushed first, followed by the quantized model weights. We can also save the serialized 8-bit models locally with `save_pretrained()`.\n","\n","We can check our memory footprint with the `get_memory_footprint` method:"],"metadata":{"id":"3r9wctAhpzig"}},{"cell_type":"code","source":["print(model.get_memory_footprint())"],"metadata":{"id":"R7e63V7tqWqS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Quantized models can be loaded from the `from_pretrained()` method without needing to specify the `quantization_config` parameters:"],"metadata":{"id":"OftzJnjtqY6q"}},{"cell_type":"code","source":["from diffusers import FluxTransformer2DModel, BitsAndBytesConfig\n","\n","quant_config = BitsAndBytesConfig(load_in_8bit=True)\n","\n","model_8bit = FluxTransformer2DModel.from_pretrained(\n","    'hf-internal-testing/flux.1-dev-int8-pkg',\n","    subfolder='transformer'\n",")"],"metadata":{"id":"o_g0svKRqeq0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 4-bit"],"metadata":{"id":"Q1_zN3C4q7p-"}},{"cell_type":"markdown","source":["Quantizing a model in 4-bit reduces our memory-usage by 4 times:"],"metadata":{"id":"Ugl__NdSq9W9"}},{"cell_type":"code","source":["from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\n","from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig\n","\n","from diffusers import FluxTransformer2DModel\n","from transformers import T5EncoderModel\n","\n","quant_config = TransformersBitsAndBytesConfig(load_in_4bit=True)\n","\n","text_encoder_2_4bit = T5EncoderModel.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    subfolder='text_encoder_2',\n","    quantization_config=quant_config,\n","    torch_dtype=torch.float16,\n",")\n","\n","quant_config = DiffusersBitsAndBytesConfig(load_in_4bit=True)\n","\n","transformer_4bit = FluxTransformer2DModel.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    subfolder='transformer',\n","    quantization_config=quant_config,\n","    torch_dtype=torch.float16,\n",")"],"metadata":{"id":"CKM1Zo2rq8wK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Same as 8-bit, we can change the data of all other modules such as `torch.nn.LayerNorm` with the `torch_dtype` parameter:"],"metadata":{"id":"r6AS5P5zrYSf"}},{"cell_type":"code","source":["transformer_4bit = FluxTransformer2DModel.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    subfolder='transformer',\n","    quantization_config=quant_config,\n","    torch_dtype=torch.float32, # new here\n",")"],"metadata":{"id":"_QH511X-ri1C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Generate an image using our 4-bit quantized models:"],"metadata":{"id":"MHfsY--3rqac"}},{"cell_type":"code","source":["from diffusers import FluxPipeline\n","\n","pipe = FluxPipeline.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    transformer=transformer_4bit,\n","    text_encoder_2=text_encoder_2_4bit,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n",")"],"metadata":{"id":"qGskCgJSrtQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipe_kwargs = {\n","    \"prompt\": \"A cat holding a sign that says hello world\",\n","    \"height\": 1024,\n","    \"width\": 1024,\n","    \"guidance_scale\": 3.5,\n","    \"num_inference_steps\": 50,\n","    \"max_sequence_length\": 512,\n","}\n","\n","image = pipe(\n","    **pipe_kwargs,\n","    generator=torch.manual_seed(111),\n",").images[0]\n","image"],"metadata":{"id":"pF_8viAlr0BI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Same as 8-bit, we can load quantized models from the `from_pretrained()` without specifying the `quantization_config` parameters:"],"metadata":{"id":"zBhVs-hsr6MY"}},{"cell_type":"code","source":["from diffusers import FluxTransformer2DModel, BitsAndBytesConfig\n","\n","quant_config = BitsAndBytesConfig(load_in_4bit=True)\n","\n","model_4bit = FluxTransformer2DModel.from_pretrained(\n","    'hf-internal-testing/flux.1-dev-nf4-pkg',\n","    subfolder='transformer'\n",")"],"metadata":{"id":"E7EynbzpsCGj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 8-bit (LLM.int8() algorithm)"],"metadata":{"id":"wFd6Zng7sJzy"}},{"cell_type":"markdown","source":["### Outlier threshold"],"metadata":{"id":"-UUlr8bTtME2"}},{"cell_type":"markdown","source":["An **\"outlier\"** is a hidden state value greater than a certain threshold, and these values are computed in fp16. While the values are usually normally distributed (`[-3.5, 3.5]`), this distribution can be very different for large models (`[-60, 6]` or `[6, 60]`). 8-bit quantization works well for value around 5, but beyond that, there is a significant performance penalty. A good default threshold value is 6, but a lower threshold may be needed for more unstable models (small models or finetuning).\n","\n","To find the best threshold for a model, we can experiment with the `llm_int8_threshold` parameter in `BitsAndBytesConfig`:"],"metadata":{"id":"B7vAoZyntO47"}},{"cell_type":"code","source":["from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\n","from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig\n","\n","from diffusers import FluxTransformer2DModel\n","from transformers import T5EncoderModel\n","\n","\n","quant_config = TransformersBitsAndBytesConfig(load_in_8bit=True)\n","\n","text_encoder_2_8bit = T5EncoderModel.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    subfolder='text_encoder_2',\n","    quantization_config=quant_config,\n","    torch_dtype=torch.float16,\n",")\n","\n","quantization_config = DiffusersBitsAndBytesConfig(\n","    load_in_8bit=True,\n","    llm_int8_threshold=10, # change here\n",")\n","\n","model_8bit = FluxTransformer2DModel.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    subfolder='transformer',\n","    quantization_config=quantization_config,\n",")"],"metadata":{"id":"bJ1d0D0MsNVZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from diffusers import FluxPipeline\n","\n","pipe = FluxPipeline.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    transformer=transformer_8bit,\n","    text_encoder_2=text_encoder_2_8bit,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n",")"],"metadata":{"id":"Ci1Vs4xCu3Vx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipe_kwargs = {\n","    \"prompt\": \"A cat holding a sign that says hello world\",\n","    \"height\": 1024,\n","    \"width\": 1024,\n","    \"guidance_scale\": 3.5,\n","    \"num_inference_steps\": 50,\n","    \"max_sequence_length\": 512,\n","}\n","\n","image = pipe(\n","    **pipe_kwargs,\n","    generator=torch.manual_seed(111),\n",").images[0]\n","image"],"metadata":{"id":"H54yTLeku65k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Skip module conversion"],"metadata":{"id":"3tiBS8RQuI2A"}},{"cell_type":"markdown","source":["For some models, we do not need to quantize every module to 8-bit which can actually cause instability. For example, for diffusion models like **Stable Diffusion 3**, the `proj_out` module can be skipped using the `llm_int8_skip_modules` parameter in `BitsAndBytesConfig`:"],"metadata":{"id":"7ysCiJgnu_do"}},{"cell_type":"code","source":["from diffusers import SD3Transformer2DModel, BitsAndBytesConfig\n","\n","quantization_config = BitsAndBytesConfig(\n","    load_in_8bit=True,\n","    llm_int8_skip_modules=['proj_out']\n",")\n","\n","model_8bit = SD3Transformer2DModel.from_pretrained(\n","    'stabilityai/stable-diffusion-3-medium-diffusers',\n","    subfolder='transformer',\n","    quantization_config=quantization_config,\n",")"],"metadata":{"id":"2l5MSus0u-G9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4-bit (QLoRA algorithm)"],"metadata":{"id":"MrkW-GegvfsV"}},{"cell_type":"markdown","source":["### Compute data type"],"metadata":{"id":"iVU9Mn9fvjr9"}},{"cell_type":"markdown","source":["To speed up computation, we can change the data type from float32 (the default value) to bf16 using the `bnb_4bit_compute_dtype` parameter in `BitsAndBytesConfig`:"],"metadata":{"id":"5BpqW0Lxz-F8"}},{"cell_type":"code","source":["import torch\n","from diffusers import BitsAndBytesConfig\n","\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")"],"metadata":{"id":"Abok9sf4viMX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Normal Float 4 (NF4)"],"metadata":{"id":"e3aUzvLj0PjT"}},{"cell_type":"markdown","source":["NF4 is a 4-bit data type from the [**QLoRA**](https://hf.co/papers/2305.14314), adapted for weights initialized from a normal distribution. We should use NF4 for training 4-bit base models. This can be configured with the `bnb_4bit_quant_type` parameter:"],"metadata":{"id":"lFom6uCP0S4f"}},{"cell_type":"code","source":["from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\n","from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig\n","\n","from diffusers import FluxTransformer2DModel\n","from transformers import T5EncoderModel\n","\n","quant_config = TransformersBitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type='nf4',\n",")\n","\n","text_encoder_2_4bit = T5EncoderModel.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    subfolder='text_encoder_2',\n","    quantization_config=quant_config,\n","    torch_dtype=torch.float16,\n",")\n","\n","quant_config = DiffusersBitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type='nf4'\n",")\n","\n","transformer_4bit = FluxTransformer2DModel.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    subfolder='transformer',\n","    quantization_config=quant_config,\n","    torch_dtype=torch.float16,\n",")"],"metadata":{"id":"0caA6Eae0SVc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For inference, the `bnb_4bit_quant_type` does not have a huge impact on performance. However, to remain consistent with the model weights, we should use the `bnb_4bit_compute_dtype` and `torch_dtype` values."],"metadata":{"id":"LmvuA8mT0_nv"}},{"cell_type":"code","source":["from diffusers import FluxPipeline\n","\n","pipe = FluxPipeline.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    transformer=transformer_4bit,\n","    text_encoder_2=text_encoder_2_4bit,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n",")"],"metadata":{"id":"LSP4BRgO1J_k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipe_kwargs = {\n","    \"prompt\": \"A cat holding a sign that says hello world\",\n","    \"height\": 1024,\n","    \"width\": 1024,\n","    \"guidance_scale\": 3.5,\n","    \"num_inference_steps\": 50,\n","    \"max_sequence_length\": 512,\n","}\n","\n","image = pipe(\n","    **pipe_kwargs,\n","    generator=torch.manual_seed(111),\n",").images[0]\n","image"],"metadata":{"id":"AXeNXCtg1Mx_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Nested quantization"],"metadata":{"id":"9yolqLrt1P-9"}},{"cell_type":"markdown","source":["Nested quantization is a technique that can save additional memory at no additional performance cost. This feature performs a second quantization of the already quantized weights to save an additional 0.4 bits/parameter:"],"metadata":{"id":"1_YtIqvp1SEo"}},{"cell_type":"code","source":["from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\n","from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig\n","\n","from diffusers import FluxTransformer2DModel\n","from transformers import T5EncoderModel\n","\n","quant_config = TransformersBitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True, # new here\n",")\n","\n","text_encoder_2_4bit = T5EncoderModel.from_pretrained(\n","    \"black-forest-labs/FLUX.1-dev\",\n","    subfolder=\"text_encoder_2\",\n","    quantization_config=quant_config,\n","    torch_dtype=torch.float16,\n",")\n","\n","quant_config = DiffusersBitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True, # new here\n",")\n","\n","transformer_4bit = FluxTransformer2DModel.from_pretrained(\n","    \"black-forest-labs/FLUX.1-dev\",\n","    subfolder=\"transformer\",\n","    quantization_config=quant_config,\n","    torch_dtype=torch.float16,\n",")"],"metadata":{"id":"tiX1f0Sf1RWA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from diffusers import FluxPipeline\n","\n","pipe = FluxPipeline.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    transformer=transformer_4bit,\n","    text_encoder_2=text_encoder_2_4bit,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n",")"],"metadata":{"id":"MdvvYiXD14nw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipe_kwargs = {\n","    \"prompt\": \"A cat holding a sign that says hello world\",\n","    \"height\": 1024,\n","    \"width\": 1024,\n","    \"guidance_scale\": 3.5,\n","    \"num_inference_steps\": 50,\n","    \"max_sequence_length\": 512,\n","}\n","\n","image = pipe(\n","    **pipe_kwargs,\n","    generator=torch.manual_seed(111),\n",").images[0]\n","image"],"metadata":{"id":"-zrAh86315ts"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dequantizing bitsandbytes models"],"metadata":{"id":"ChkHYnsb2JRC"}},{"cell_type":"markdown","source":["Once quantized, we can dequantize a model to its original precision, but this may result in a small loss of quality."],"metadata":{"id":"geTPNYiD2MTT"}},{"cell_type":"code","source":["from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\n","from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig\n","\n","from diffusers import FluxTransformer2DModel\n","from transformers import T5EncoderModel\n","\n","quant_config = TransformersBitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n",")\n","\n","text_encoder_2_4bit = T5EncoderModel.from_pretrained(\n","    \"black-forest-labs/FLUX.1-dev\",\n","    subfolder=\"text_encoder_2\",\n","    quantization_config=quant_config,\n","    torch_dtype=torch.float16,\n",")\n","\n","quant_config = DiffusersBitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n",")\n","\n","transformer_4bit = FluxTransformer2DModel.from_pretrained(\n","    \"black-forest-labs/FLUX.1-dev\",\n","    subfolder=\"transformer\",\n","    quantization_config=quant_config,\n","    torch_dtype=torch.float16,\n",")\n","\n","\n","# Dequantize model\n","text_encoder_2_4bit.dequantize()\n","transformer_4bit.dequantize()"],"metadata":{"id":"zxpW6oU62L8-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from diffusers import FluxPipeline\n","\n","pipe = FluxPipeline.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    transformer=transformer_4bit,\n","    text_encoder_2=text_encoder_2_4bit,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n",")"],"metadata":{"id":"eq0S-TbP2fTI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipe_kwargs = {\n","    \"prompt\": \"A cat holding a sign that says hello world\",\n","    \"height\": 1024,\n","    \"width\": 1024,\n","    \"guidance_scale\": 3.5,\n","    \"num_inference_steps\": 50,\n","    \"max_sequence_length\": 512,\n","}\n","\n","image = pipe(\n","    **pipe_kwargs,\n","    generator=torch.manual_seed(111),\n",").images[0]\n","image"],"metadata":{"id":"eFZVahE62gTI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# GGUF"],"metadata":{"id":"M_-sWD3R2i5m"}},{"cell_type":"markdown","source":["The **GGUF** file format is typically used to store models for inference with [GGML](https://github.com/ggml-org/ggml) and supports a variety of block wise quantization options.\n","\n","Diffusers supports loading checkpoints prequantized and saved in the GGUF format via `from_single_file` loading with Model classes.\n","\n","Since GGUF is a single file format, we will use `from_single_file` to load the model and pass in the `GGUFQuantizationConfig`.\n","\n","When using GGUF checkpoints, the quantized weights remain in a low memory `dtype` (typically `torch.uint8`) and are dynamically dequantized and cast to the configured `compute_dtype` during each module's forward pass through the model. The `GGUFQuantizationConfig` allows us to set the `compute_dtype`."],"metadata":{"id":"FeVghPVN2kDo"}},{"cell_type":"code","source":["import torch\n","from diffusers import FluxPipeline, FluxTransformer2DModel, GGUFQuantizationConfig\n","\n","ckpt_path = \"https://huggingface.co/city96/FLUX.1-dev-gguf/blob/main/flux1-dev-Q2_K.gguf\"\n","\n","quant_config = GGUFQuantizationConfig(compute_dtype=torch.bfloat16)\n","\n","transformer = FluxTransformer2DModel.from_single_file(\n","    ckpt_path,\n","    quantization_config=quant_config,\n","    torch_dtype=torch.bfloat16,\n",")\n","\n","pipe = FluxPipeline.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    transformer=transformer,\n","    torch_dtype=torch.bfloat16,\n",")\n","pipe.enable_model_cpu_offload()"],"metadata":{"id":"1bt9dM8e2jjI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"A cat holding a sign that says hello world\"\n","image = pipe(\n","    prompt,\n","    generator=torch.manual_seed(111)\n",").images[0]\n","image"],"metadata":{"id":"8P-vikA532mX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# torchao"],"metadata":{"id":"rcECNLjc38oF"}},{"cell_type":"markdown","source":["[**TorchAO**](https://github.com/pytorch/ao) is an architecture optimization library for PyTorch. It provides high-performance dtypes, optimization techniques, and kernels for inference and training, featuring composability with native PyTorch features like `torch.compile`, `FullyShardedDataParallel` (FSDP), and more.\n","\n","Quantize a model by passing `TorchAoConfig` to `from_pretrained()`. This works for any model in anyu modality, as long as it supports loading with HuggingFace Accelerate library and contains `torch.nn.Linear` layers."],"metadata":{"id":"UIUD3eZN3-LB"}},{"cell_type":"code","source":["import torch\n","from diffusers import FluxPipeline, FluxTransformer2DModel, TorchAoConfig\n","\n","model_id = 'black-forest-labs/FLUX.1-dev'\n","dtype = torch.bfloat16\n","\n","quant_config = TorchAoConfig('int8wo')\n","\n","transformer = FluxTransformer2DModel.from_pretrained(\n","    model_id,\n","    subfolder='transformer',\n","    quantization_config=quant_config,\n","    torch_dtype=dtype,\n",")\n","\n","pipe = FluxPipeline.from_pretrained(\n","    model_id,\n","    transformer=transformer,\n","    torch_dtype=dtype,\n",")\n","pipe.to('cuda')\n","\n","# Without quantization: ~31.447 GB\n","# With quantization: ~20.40 GB\n","print(f\"Pipeline memory usage: {torch.cuda.max_memory_reserved() / 1024**3:.3f} GB\")"],"metadata":{"id":"8Xvp9Zn-39pC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"A cat holding a sign that says hello world\"\n","image = pipe(\n","    prompt,\n","    num_inference_steps=50,\n","    guidance_scale=4.5,\n","    max_sequence_length=512\n",").images[0]\n","image"],"metadata":{"id":"mq0WyqaE5YUM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["TorchAO is fully compatible with `torch.compile`, setting it apart from other quantization methods. This makes it easy to speed up inference with just one line of code:"],"metadata":{"id":"zu6hhrB55dF_"}},{"cell_type":"code","source":["import torch\n","from diffusers import FluxPipeline, FluxTransformer2DModel, TorchAoConfig\n","\n","model_id = 'black-forest-labs/FLUX.1-dev'\n","dtype = torch.bfloat16\n","\n","quant_config = TorchAoConfig('int8wo')\n","\n","transformer = FluxTransformer2DModel.from_pretrained(\n","    model_id,\n","    subfolder='transformer',\n","    quantization_config=quant_config,\n","    torch_dtype=dtype,\n",")\n","# apply torch.compile\n","transformer = torch.compile(\n","    transformer,\n","    mode='max-autotune',\n","    fullgraph=True,\n",")\n","\n","pipe = FluxPipeline.from_pretrained(\n","    model_id,\n","    transformer=transformer,\n","    torch_dtype=dtype,\n",")\n","pipe.to('cuda')"],"metadata":{"id":"Y3PdbN6u5lpU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"A cat holding a sign that says hello world\"\n","image = pipe(\n","    prompt,\n","    num_inference_steps=50,\n","    guidance_scale=4.5,\n","    max_sequence_length=512\n",").images[0]\n","image"],"metadata":{"id":"ChsyjBXL5vLT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["TorchAO also supports an automatic quantization API through [autoquant](https://github.com/pytorch/ao/blob/main/torchao/quantization/README.md#autoquantization). Autoquantization determines the best quantization strategy applicable to a model by comparing the performance of each technique on chosen input types and shapes.\n","\n","The `TorchAoConfig` class accepts:\n","* `quant_type`: a string value mentioning one of the quantization types below.\n","* `modules_to_not_convert`: a list of module full/partial module names for which quantization should not be performed.\n","* `kwargs`: a dictionary of keyword arguments to pass to the underlying quantization method which will be invoked based on `quant_type`."],"metadata":{"id":"MTG-XFud5zMT"}},{"cell_type":"markdown","source":["## Supported quantization types"],"metadata":{"id":"AZUTuX6s6aig"}},{"cell_type":"markdown","source":["TorchAO supports\n","* **weight-only quantization**, which stores the model weights in a specific low-bit data type but performs computation with a higher-precision data type, like `bfloat16`. This lowers the memory requirements from model weights but retains the memory peaks for activation computation.\n","* **dynamic-activation quantization**, which stores the model weights in a low-bit dtype, while also quantizing the activations on-the-fly to save additional memory. This lowers the memory requirements from model weights, while also lowering the memory overhead from activation computations. However, this may come at a quality tradeoff at times, so it is recommended totest different models thoroughly."],"metadata":{"id":"Zv0cQPY_6fSc"}},{"cell_type":"markdown","source":["The supported quantization methods:\n","\n","| **Category** | **Full Function Names** | **Shorthands** |\n","|--------------|-------------------------|----------------|\n","| **Integer quantization** | `int4_weight_only`, `int8_dynamic_activation_int4_weight`, `int8_weight_only`, `int8_dynamic_activation_int8_weight` | `int4wo`, `int4dq`, `int8wo`, `int8dq` |\n","| **Floating point 8-bit quantization** | `float8_weight_only`, `float8_dynamic_activation_float8_weight`, `float8_static_activation_float8_weight` | `float8wo`, `float8wo_e5m2`, `float8wo_e4m3`, `float8dq`, `float8dq_e4m3`, `float8_e4m3_tensor`, `float8_e4m3_row` |\n","| **Floating point X-bit quantization** | `fpx_weight_only` | `fpX_eAwB` where `X` is the number of bits (1-7), `A` is exponent bits, and `B` is mantissa bits. Constraint: `X == A + B + 1` |\n","| **Unsigned Integer quantization** | `uintx_weight_only` | `uint1wo`, `uint2wo`, `uint3wo`, `uint4wo`, `uint5wo`, `uint6wo`, `uint7wo` |\n","\n","Some quantization methods are aliases. For example, `int8wo` we used above is commonly used shorthand for `int8_weight_only`."],"metadata":{"id":"7yZL7E3K7I36"}},{"cell_type":"markdown","source":["## Serializing and deserializing quantized models"],"metadata":{"id":"MDnz1acb7io0"}},{"cell_type":"markdown","source":["To serialize a quantized model in a given dtype, we first load the model with the desired quantization dtype and then save it using the `save_pretrained()` method:"],"metadata":{"id":"NVIQ4TpP7nbE"}},{"cell_type":"code","source":["import torch\n","from diffusers import FluxTransformer2DModel, TorchAoConfig\n","\n","quant_config = TorchAoConfig('int8wo')\n","\n","transformer = FluxTransformer2DModel.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    subfolder='transformer',\n","    quantization_config=quant_config,\n","    torch_dtype=torch.bfloat16,\n",")\n","transformer.save_pretrained(\n","    '/path/to/flux_int8wo',\n","    safe_serialization=False\n",")"],"metadata":{"id":"lgcmSVqj56Wg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To load a serialized quantized model,"],"metadata":{"id":"1mIXG-OQ78NI"}},{"cell_type":"code","source":["import torch\n","from diffusers import FluxPipeline, FluxTransformer2DModel\n","\n","transformer = FluxTransformer2DModel.from_pretrained(\n","    '/path/to/flux_int8wo',\n","    torch_dtype=torch.bfloat16,\n","    use_safetensors=False,\n",")\n","\n","pipe = FluxPipeline.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    transformer=transformer,\n","    torch_dtype=torch.bfloat16,\n",")\n","pipe.to('cuda')"],"metadata":{"id":"8COWJTya7-Ne"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"A cat holding a sign that says hello world\"\n","image = pipe(\n","    prompt, \\\n","    num_inference_steps=30,\n","    guidance_scale=7.0\n",").images[0]\n","image"],"metadata":{"id":"kkSzwS4K8R78"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Some quantization methods, such as `uint4wo`, cannot be loaded directly and may result in an `UnpicklingError` when trying to load the models, but work as expected when saving them.\n","\n","In order to work around this, we can load the state dict manually into the model. However, this requires using `weights_only=False` in `torch.load`, so it should be run only if the weights were obtained from a trustable source."],"metadata":{"id":"ZHqdHxf78Whz"}},{"cell_type":"code","source":["import torch\n","from accelerate import init_empty_weights\n","from diffusers import FluxPipeline, FluxTransformer2DModel, TorchAoConfig\n","\n","quant_config = TorchAoConfig('uint4wo')\n","\n","# Serialize the model\n","transformer = FluxTransformer2DModel.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    subfolder='transformer',\n","    quantization_config=quant_config,\n","    torch_dtype=torch.bfloat16,\n",")\n","transformer.save_pretrained(\n","    '/path/to/flux_uint4wo',\n","    safe_serialization=False,\n","    max_shard_size=\"50GB\"\n",")"],"metadata":{"id":"mi-xyq848rWb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the model\n","state_dict = torch.load(\n","    'path/to/flux_uint4wo/diffusion_pytorch_model.bin',\n","    weights_only=False,\n","    map_location='cpu'\n",")\n","\n","with init_empty_weights():\n","    transformer = FluxTransformer2DModel.from_config(\n","        '/path/to/flux_uint4wo/config.json'\n","    )\n","transformer.load_state_dict(\n","    state_dict,\n","    strict=True,\n","    assign=True,\n",")\n","\n","pipe = FluxPipeline.from_pretrained(\n","    'black-forest-labs/FLUX.1-dev',\n","    transformer=transformer,\n","    torch_dtype=torch.bfloat16,\n",")\n","pipe.to('cuda')"],"metadata":{"id":"zQpIR8_p9C5R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"A cat holding a sign that says hello world\"\n","image = pipe(\n","    prompt, \\\n","    num_inference_steps=30,\n","    guidance_scale=7.0\n",").images[0]\n","image"],"metadata":{"id":"Ee1twkGD9Z7z"},"execution_count":null,"outputs":[]}]}