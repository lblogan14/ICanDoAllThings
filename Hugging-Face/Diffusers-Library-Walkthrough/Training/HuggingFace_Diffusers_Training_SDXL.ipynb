{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOesW22B7sKatF/J3WETlqe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Stable Diffusion XL"],"metadata":{"id":"GBRCV6-tuOCo"}},{"cell_type":"markdown","source":["SDXL is a larger and more powerful iteration of the Stable Diffusion Model, capapble of producing higher resolution images.\n","\n","SDXL's UNet is 3x larger and the model adds a second text encoder to the architecture.\n","\n","We will explore the [`train_text_to_image_sdxl.py`](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_sdxl.py) script to train SDXL.\n","\n","As always, make sure to install the `diffusers` library from source:\n","```bash\n","git clone https://github.com/huggingface/diffusers\n","cd diffusers\n","pip install .\n","```\n","\n","We need to navigate to the following folder and install the corresponding dependencies:\n","```bash\n","cd examples/text_to_image\n","pip install -r requirements.txt\n","```\n","HuggingFace Accelerate is also helpful to train on multiple GPUs with mixed-precision.\n","```bash\n","pip install accelerate\n","```\n","\n","Now we can initialize a HuggingFace Accelerate environment\n","```bash\n","accelerate config\n","```\n","To set up a default Acclerate environment without choosing any configurations:\n","```bash\n","accelerate config default\n","```\n","Or if our environment does not support an interactive shell like a notebook, we can use:\n","```python\n","from accelerate.utils import write_basic_config\n","write_basic_config()\n","```"],"metadata":{"id":"uq0g99YH1klo"}},{"cell_type":"markdown","source":["## Script parameters"],"metadata":{"id":"L_GxwM_s6hFE"}},{"cell_type":"markdown","source":["The training script provides many parameters to customize the training run. We can find all of the parameters and their descriptions in the `parse_args()` function.\n","\n","To speed up training with mixed precision using the `bp16` format, add the `--mixed_precision` parameter to the training command:\n","```bash\n","accelerate launch train_text_to_image.py --mixed_precision=\"bp16\"\n","```\n","\n","Most of the parameters are identical to the parameters in the **Text-to-image** training guide, so we will focus on parameters that are relevant to training SDXL:\n","* `--pretrained_vae_model_name_or_path`: path to a pretrained VAE; the SDXL VAE is known to suffer from numerical instability, so this parameter allows us to specify a better VAE\n","* `--proportion_empty_prompts`: the proportion of image prompts to replace with empty strings\n","* `--timestep_bias_strategy`: where (earlier vs. later) in the timestep to apply a bias, which can encourage the model to either learn low or high frequency details\n","* `--timestep_bias_multiplier`: the weight of the bias to apply to the timestep\n","* `--timestep_bias_begin`: the timestep to begin applying the bias\n","* `--timestep_bias_end`: the timestep to end applying the bias\n","* `--timestep_bias_portion`: the proportion of timesteps to apply the bias to"],"metadata":{"id":"BMyy1An69oTX"}},{"cell_type":"markdown","source":["## Min-SNR weighting"],"metadata":{"id":"hB-NBpfy-X21"}},{"cell_type":"markdown","source":["The `Min-SNR` weighting strategy can help with training by rebalancing the loss to achieve faster convergence. The training script supports predicting `epsilon` (noise) or `v_prediction`, but Min-SNR is compatible with both prediction types.\n","\n","We can add the `--snr_gamma` and set it to the recommended value of 5.0:\n","```bash\n","accelerate launch train_text_to_image.py --snr_gamma=5.0\n","```"],"metadata":{"id":"lrQ6QSxp-ct8"}},{"cell_type":"markdown","source":["## Training script"],"metadata":{"id":"u4XEjQdw-dw3"}},{"cell_type":"markdown","source":["The training script is similar to the Text-to-image training guide, but it has been modified to support SDXL training.\n","\n","The training script starts by creating an `encode_prompt` function to tokenize the prompts to calculate the prompt embeddings, and to compute the image embeddings with the VAE using the `compute_vae_encodings` function. Next we use the `generate_timestep_weights` to generate the timesteps weiights depending on the number of timesteps and the timestep bias strategy to apply.\n","\n","Within the `main()` function, in addition to loading a tokenizer, the script loads a second tokeinzer and text encoder because the SDXL architecture uses two of each:\n","```python\n","    # Load the tokenizers\n","    tokenizer_one = AutoTokenizer.from_pretrained(\n","        args.pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=args.revision, use_fast=False\n","    )\n","    tokenizer_two = AutoTokenizer.from_pretrained(\n","        args.pretrained_model_name_or_path, subfolder=\"tokenizer_2\", revision=args.revision, use_fast=False\n","    )\n","\n","    # import correct text encoder classes\n","    text_encoder_cls_one = import_model_class_from_model_name_or_path(\n","        args.pretrained_model_name_or_path, args.revision\n","    )\n","    text_encoder_cls_two = import_model_class_from_model_name_or_path(\n","        args.pretrained_model_name_or_path, args.revision, subfolder=\"text_encoder_2\"\n","    )\n","```\n","\n","The prompt and image embeddings are computed first and kept in memory. For larger datasets, this can lead to memory problems, then we should save the pre-computed embeddings to disk separately and load them into memory during the training process.\n","```python\n","    # Let's first compute all the embeddings so that we can free up the text encoders\n","    # from memory. We will pre-compute the VAE encodings too.\n","    text_encoders = [text_encoder_one, text_encoder_two]\n","    tokenizers = [tokenizer_one, tokenizer_two]\n","    compute_embeddings_fn = functools.partial(\n","        encode_prompt,\n","        text_encoders=text_encoders,\n","        tokenizers=tokenizers,\n","        proportion_empty_prompts=args.proportion_empty_prompts,\n","        caption_column=args.caption_column,\n","    )\n","    compute_vae_encodings_fn = functools.partial(compute_vae_encodings, vae=vae)\n","    with accelerator.main_process_first():\n","        from datasets.fingerprint import Hasher\n","\n","        # fingerprint used by the cache for the other processes to load the result\n","        # details: https://github.com/huggingface/diffusers/pull/4038#discussion_r1266078401\n","        new_fingerprint = Hasher.hash(args)\n","        new_fingerprint_for_vae = Hasher.hash(\"vae\")\n","        train_dataset = train_dataset.map(compute_embeddings_fn, batched=True, new_fingerprint=new_fingerprint)\n","        train_dataset = train_dataset.map(\n","            compute_vae_encodings_fn,\n","            batched=True,\n","            batch_size=args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps,\n","            new_fingerprint=new_fingerprint_for_vae,\n","        )\n","```\n","After calculating the embeddings, the text encoder, VAE, and tokenizer are deleted to free up some memory:\n","```python\n","    del text_encoders, tokenizers, vae\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","```\n","Finally, the training loop takes care of the rest. If we choose to apply a timestep bias strategy, we will see the timestep weights are calculated and added as noise:\n","```python\n","    # Sample a random timestep for each image, potentially biased by the timestep weights.\n","    # Biasing the timestep weights allows us to spend less time training irrelevant timesteps.\n","    weights = generate_timestep_weights(args, noise_scheduler.config.num_train_timesteps).to(\n","        model_input.device\n","    )\n","    timesteps = torch.multinomial(weights, bsz, replacement=True).long()\n","```"],"metadata":{"id":"VM9XaC0k-fgV"}},{"cell_type":"markdown","source":["## Launch the script"],"metadata":{"id":"A8spM5IXBMVe"}},{"cell_type":"markdown","source":["We will train our SDXL on the Naruto BLIP captions dataset to generate our own Naruto characters. We need to set the envrionment variables `MODEL_NAME` and `DATASET_NAME` to the model and the dataset. Also we need to specify a VAE other than the SDXL VAE with `VAE_NAME` to avoid numerical instabilities.\n","\n","To monitor training progress with **Weights & Biases**, add the `--report_to=\"wandb\"` parameter to the training command. We also need to add the `--validation_prompt` and `--validation_epochs` to the training command to keep track of results. This can be useful for debugging the model and viewing intermediate results.\n","\n","```bash\n","export MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\n","export VAE_NAME=\"madebyollin/sdxl-vae-fp16-fix\"\n","export DATASET_NAME=\"lambdalab/naruto-blip-captions\n","\n","accelerate launch train_text_to_image_sdxl.py \\\n","  --pretrained_model_name_or_path=$MODEL_NAME \\\n","  --pretrained_vae_model_or_path=$VAE_NAME \\\n","  --dataset_name=$DATASET_NAME \\\n","  --enable_xformers_memory_efficient_attention \\\n","  --resolution=512 \\\n","  --center_crop \\\n","  --random_flip \\\n","  --proportion_empty_prompts=0.2 \\\n","  --train_batch_size=1 \\\n","  --gradient_accumulation_steps=4 \\\n","  --gradient_checkpointing \\\n","  --max_train_steps=10000 \\\n","  --use_8bit_adam \\\n","  --leraning_rate=1e-06 \\\n","  --lr_scheduler=\"constant\" \\\n","  --lr_warmup_steps=0 \\\n","  --mixed_precision=\"fp16\" \\\n","  --report_to=\"wandb\" \\\n","  --validation_prompt=\"a cute Sundar Pichai creature\" \\\n","  --validation_epochs=5 \\\n","  --checkpointing_steps=5000 \\\n","  --output_dir=\"sdxl-naruto-model\"\n","  --push_to_hub\n","```"],"metadata":{"id":"JXY-cJ4PDlVp"}},{"cell_type":"markdown","source":["After training, we can use the customed SDXL model:"],"metadata":{"id":"uZdRwzyCFcyc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9-W0uHZRuLQ4"},"outputs":[],"source":["from diffusers import DiffusionPipeline\n","import torch\n","\n","pipeline = DiffusionPipeline.from_pretrained(\n","    'path/to/our/model',\n","    torch_dtype=torch.float16\n",").to('cuda')"]},{"cell_type":"code","source":["prompt = 'a naruto with green eyes and red legs'\n","image = pipeline(\n","    prompt,\n","    num_inference_steps=30,\n","    guidance_scale=7.5\n",").images[0]\n","image"],"metadata":{"id":"FhxvINO2Fp2U"},"execution_count":null,"outputs":[]}]}