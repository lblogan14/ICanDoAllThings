{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPlWRzNzwmXWpGsrM6xm93C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Custom Diffusion"],"metadata":{"id":"ovweariu1vl2"}},{"cell_type":"markdown","source":["**Custom Diffusion** is a training technique for personalizing image generation models. Like Textual Inversion, DreamBooth, and LoRA, Custom Diffusion only requires a few (~4-5) example images. This technique works by only training weights in the cross-attention layers, and it uses a special word to represent the newly learned concept. Custom Diffusion is unique because it can also learn multiple concepts at the same time.\n","\n","We will explore the [`train_custom_diffusion.py`](https://github.com/huggingface/diffusers/blob/main/examples/custom_diffusion/train_custom_diffusion.py)\n","\n","\n","As always, make sure to install the `diffusers` library from source:\n","```bash\n","git clone https://github.com/huggingface/diffusers\n","cd diffusers\n","pip install .\n","```\n","\n","We need to navigate to the following folder and install the corresponding dependencies:\n","```bash\n","cd examples/custom_diffusion\n","pip install -r requirements.txt\n","```\n","HuggingFace Accelerate is also helpful to train on multiple GPUs with mixed-precision.\n","```bash\n","pip install accelerate\n","```\n","\n","Now we can initialize a HuggingFace Accelerate environment\n","```bash\n","accelerate config\n","```\n","To set up a default Acclerate environment without choosing any configurations:\n","```bash\n","accelerate config default\n","```\n","Or if our environment does not support an interactive shell like a notebook, we can use:\n","```python\n","from accelerate.utils import write_basic_config\n","write_basic_config()\n","```"],"metadata":{"id":"XoTFTDaR1x8n"}},{"cell_type":"markdown","source":["## Script parameters"],"metadata":{"id":"xHoV6UAntemz"}},{"cell_type":"markdown","source":["The training script provides many parameters to customize the training run. We can find all of the parameters and their descriptions in the `parse_args()` function.\n","\n","Many parameters are described in the **DreamBooth** training guide, so we only focus on parameters relevant to Custom Diffusion:\n","* `--freeze_model`: freezes the key and value parameters in the cross-attention layer; the default it `crossattn_kv`, but we can set it to `crossattn` to train all the parameters in the cross-attention layer\n","* `--concepts_list`: learns multiple concepts, provide a path to a JSON file containing the concepts\n","* `--modifier_token`: a special word used to represent the learned concept\n","* `--initializer_token`: a special word used to initialize the embeddings of the `modifier_token`"],"metadata":{"id":"jVFLphH2tjPM"}},{"cell_type":"markdown","source":["### Prior preservation loss"],"metadata":{"id":"7ea5s2i9usEe"}},{"cell_type":"markdown","source":["Prior preservation loss is a method that uses a model's own generated samples to help it learn how to generate more diverse images. Because these generated sample images belong to the same class as the images we provided, they help the model retain what it has learned about the class and how it can use what it already knows about the class to make new compositions.\n","\n","Details about parameters for prior preservation loss can be found in the **DreamBooth** training guide."],"metadata":{"id":"1sh-qkpZvZNX"}},{"cell_type":"markdown","source":["### Regularization"],"metadata":{"id":"ZBnMQaJ2v1ST"}},{"cell_type":"markdown","source":["Custom Diffusion includes training the target images with a small set of real images to prevent overfitting. This can be easy to do when we are only training on a few images. We can download 200 real images with `clip_retrieval`. The `class_prompt` should be the same category as the target images. These images are stored in `class_data_dir`.\n","```bash\n","python retrieve.py \\\n","  --class_prompt cat \\\n","  --class_data_dir real_reg/samples_cat \\\n","  --num_class_images 200\n","```\n","\n","To enable regularization, we need to add the following parameters:\n","* `--with_prior_preservation`: whether to use prior preservation loss\n","* `--prior_loss_weight`: controls the influence of the prior preservation loss on the model\n","* `--real_prior`: whether to use a small set of real images to prevent overfitting\n","\n","Example:\n","```bash\n","accelerate launch train_custom_diffusion.py \\\n","  --with_prior_preservation \\\n","  --prior_loss_weight=1.0 \\\n","  --class_data_dir=\"./real_reg/samples_cat\" \\\n","  --class_prompt=\"cat\" \\\n","  --real_prior=True \\\n","```"],"metadata":{"id":"FbfV4MW-wTeP"}},{"cell_type":"markdown","source":["## Training script"],"metadata":{"id":"INqhkiCuAPaN"}},{"cell_type":"markdown","source":["The Custom Diffusion training script is similar to the **DreamBooth** script. This guide focuses on the code snippet that is relevant to Custom Diffusion.\n","\n","The Custom Diffusion training script has two dataset classes:\n","* `CustomDiffusionDataset`: preprocesses the image, class images, and prompts for training\n","* `PromptDataset`: prepares the prompts for generating class images\n","\n","In the `main()` function, the `modifier_token` is added to the tokenizer, converted to token ids, and the token embeddings are resized to account for the new `modifier_token`. Then the `modifier_token` embeddings are initialized with the embeddings of the `initializer_token`. All parameters in the text encoder are frozen, except for the token embeddings since this is what the model is trying to learn to associate with the concepts.\n","```python\n","        # Freeze all parameters except for the token embeddings in text encoder\n","        params_to_freeze = itertools.chain(\n","            text_encoder.text_model.encoder.parameters(),\n","            text_encoder.text_model.final_layer_norm.parameters(),\n","            text_encoder.text_model.embeddings.position_embedding.parameters(),\n","        )\n","        freeze_params(params_to_freeze)\n","```\n","\n","Then we need to add the Custom Diffusion weights to the attention layers. This is an important step for getting the shape and size of the attention weights correct, and for setting the appropriate number of attention processors in each UNet block:\n","```python\n","    st = unet.state_dict()\n","    for name, _ in unet.attn_processors.items():\n","        cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n","        if name.startswith(\"mid_block\"):\n","            hidden_size = unet.config.block_out_channels[-1]\n","        elif name.startswith(\"up_blocks\"):\n","            block_id = int(name[len(\"up_blocks.\")])\n","            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n","        elif name.startswith(\"down_blocks\"):\n","            block_id = int(name[len(\"down_blocks.\")])\n","            hidden_size = unet.config.block_out_channels[block_id]\n","        layer_name = name.split(\".processor\")[0]\n","        weights = {\n","            \"to_k_custom_diffusion.weight\": st[layer_name + \".to_k.weight\"],\n","            \"to_v_custom_diffusion.weight\": st[layer_name + \".to_v.weight\"],\n","        }\n","        if train_q_out:\n","            weights[\"to_q_custom_diffusion.weight\"] = st[layer_name + \".to_q.weight\"]\n","            weights[\"to_out_custom_diffusion.0.weight\"] = st[layer_name + \".to_out.0.weight\"]\n","            weights[\"to_out_custom_diffusion.0.bias\"] = st[layer_name + \".to_out.0.bias\"]\n","        if cross_attention_dim is not None:\n","            custom_diffusion_attn_procs[name] = attention_class(\n","                train_kv=train_kv,\n","                train_q_out=train_q_out,\n","                hidden_size=hidden_size,\n","                cross_attention_dim=cross_attention_dim,\n","            ).to(unet.device)\n","            custom_diffusion_attn_procs[name].load_state_dict(weights)\n","        else:\n","            custom_diffusion_attn_procs[name] = attention_class(\n","                train_kv=False,\n","                train_q_out=False,\n","                hidden_size=hidden_size,\n","                cross_attention_dim=cross_attention_dim,\n","            )\n","    del st\n","    unet.set_attn_processor(custom_diffusion_attn_procs)\n","    custom_diffusion_layers = AttnProcsLayers(unet.attn_processors)\n","```\n","\n","The optimizer is initialized to update the cross-attention layer parameters:\n","```python\n","    # Optimizer creation\n","    optimizer = optimizer_class(\n","        itertools.chain(text_encoder.get_input_embeddings().parameters(), custom_diffusion_layers.parameters())\n","        if args.modifier_token is not None\n","        else custom_diffusion_layers.parameters(),\n","        lr=args.learning_rate,\n","        betas=(args.adam_beta1, args.adam_beta2),\n","        weight_decay=args.adam_weight_decay,\n","        eps=args.adam_epsilon,\n","    )\n","```\n","\n","In the training loop, it is important to only update the embeddings for the concept we are trying to learn. This means setting the gradients of all the other token embeddings to zero."],"metadata":{"id":"28EUVJuJASzF"}},{"cell_type":"markdown","source":["## Launch the script"],"metadata":{"id":"0AjK6NjlCMal"}},{"cell_type":"markdown","source":["We will download and use the example cat images to train our Custom Diffusion.\n","\n","If we are training on human faces, the Custom Diffusion recommends:\n","* `--learning_rate=5e-6`\n","* `--max_train_steps` can be anywhere between 1000 and 2000\n","* `--freeze_model=\"crossattn\"`\n","* use at least 15-20 images to train with"],"metadata":{"id":"KudqrgxdCPsX"}},{"cell_type":"markdown","source":["##### single concept"],"metadata":{"id":"XJGAt2KpCquK"}},{"cell_type":"markdown","source":["```bash\n","export MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\n","export OUTPUT_DIR=\"path-to-save-model\"\n","export INSTANCE_DIR=\"./data/cat\"\n","\n","accelerate launch train_custom_diffusion.py \\\n","  --pretrained_model_name_or_path=$MODEL_NAME  \\\n","  --instance_data_dir=$INSTANCE_DIR \\\n","  --output_dir=$OUTPUT_DIR \\\n","  --class_data_dir=./real_reg/samples_cat/ \\\n","  --with_prior_preservation \\\n","  --real_prior \\\n","  --prior_loss_weight=1.0 \\\n","  --class_prompt=\"cat\" \\\n","  --num_class_images=200 \\\n","  --instance_prompt=\"photo of a <new1> cat\"  \\\n","  --resolution=512  \\\n","  --train_batch_size=2  \\\n","  --learning_rate=1e-5  \\\n","  --lr_warmup_steps=0 \\\n","  --max_train_steps=250 \\\n","  --scale_lr \\\n","  --hflip  \\\n","  --modifier_token \"<new1>\" \\\n","  --validation_prompt=\"<new1> cat sitting in a bucket\" \\\n","  --report_to=\"wandb\" \\\n","  --push_to_hub\n","```\n","\n","Once training is finished,"],"metadata":{"id":"itH-q-LrCtKD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"D6mwQY1g1sj4"},"outputs":[],"source":["import torch\n","from diffusers import DiffusionPipeline\n","\n","pipeline = DiffusionPipeline.from_pretrained(\n","    \"CompVis/stable-diffusion-v1-4\",\n","    torch_dtype=torch.float16,\n",").to(\"cuda\")\n","pipeline.unet.load_attn_procs(\n","    \"path-to-save-model\",\n","    weight_name=\"pytorch_custom_diffusion_weights.bin\"\n",")\n","pipeline.load_textual_inversion(\n","    \"path-to-save-model\",\n","    weight_name=\"<new1>.bin\"\n",")"]},{"cell_type":"code","source":["image = pipeline(\n","    \"<new1> cat sitting in a bucket\",\n","    num_inference_steps=100,\n","    guidance_scale=6.0,\n","    eta=1.0,\n",").images[0]\n","image"],"metadata":{"id":"cPlPqKNMC67d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### multiple concepts"],"metadata":{"id":"qcZZZdEyC86I"}},{"cell_type":"markdown","source":["We need to provide a JSON file with some details about each concept it should learn.\n","\n","Run clip-retrieval to collect some real images to use for regularization:\n","```bash\n","pip install clip-retrieval\n","python retrieve.py --class_prompt {} --class_data_dir {} --num_class_images 200\n","```\n","\n","Then we can launch the script:\n","```bash\n","export MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\n","export OUTPUT_DIR=\"path-to-save-model\"\n","\n","accelerate launch train_custom_diffusion.py \\\n","  --pretrained_model_name_or_path=$MODEL_NAME  \\\n","  --output_dir=$OUTPUT_DIR \\\n","  --concepts_list=./concept_list.json \\\n","  --with_prior_preservation \\\n","  --real_prior \\\n","  --prior_loss_weight=1.0 \\\n","  --resolution=512  \\\n","  --train_batch_size=2  \\\n","  --learning_rate=1e-5  \\\n","  --lr_warmup_steps=0 \\\n","  --max_train_steps=500 \\\n","  --num_class_images=200 \\\n","  --scale_lr \\\n","  --hflip  \\\n","  --modifier_token \"<new1>+<new2>\" \\\n","  --push_to_hub\n","```\n","\n","Once training is finished,"],"metadata":{"id":"R_wXG-PXDANT"}},{"cell_type":"code","source":["import torch\n","from huggingface_hub.repocard import RepoCard\n","from diffusers import DiffusionPipeline\n","\n","model_id = \"sayakpaul/custom-diffusion-cat-wooden-pot\"\n","pipeline = DiffusionPipeline.from_pretrained(\n","    model_id,\n","    torch_dtype=torch.float16\n",").to(\"cuda\")\n","pipeline.unet.load_attn_procs(\n","    model_id,\n","    weight_name=\"pytorch_custom_diffusion_weights.bin\"\n",")\n","pipeline.load_textual_inversion(\n","    model_id,\n","    weight_name=\"<new1>.bin\"\n",")\n","pipeline.load_textual_inversion(\n","    model_id,\n","    weight_name=\"<new2>.bin\"\n",")"],"metadata":{"id":"ewabjPsGC-PR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = pipeline(\n","    \"the <new1> cat sculpture in the style of a <new2> wooden pot\",\n","    num_inference_steps=100,\n","    guidance_scale=6.0,\n","    eta=1.0,\n",").images[0]\n","image"],"metadata":{"id":"AeudYRLTDpm6"},"execution_count":null,"outputs":[]}]}