{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17641,
     "status": "ok",
     "timestamp": 1731947993571,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "Gz0elX1J4yS0",
    "outputId": "43c9d776-5e83-44ab-84a3-1a38750543e7"
   },
   "outputs": [],
   "source": [
    "! pip install -qU langchain_community langchain-openai langchain-anthropic langchain langgraph bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jprQXsWh-hka"
   },
   "source": [
    "# Code Generation with RAG and Self-Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SK52GTNr-vlU"
   },
   "source": [
    "In this section, we will start with a set of documentation specified by a user. We then use a long context LLM to ingest it and perform RAG to answer a question based upon it. We will invoke a tool to produce a structured output. Finally, we will perform two unit tests (check imports and code execution) prior returning the solution to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1731947951820,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "z0CYc5kM-lY0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR_ANTHROPIC_API_KEY\"\n",
    "os.environ['OPENAI_API_KEY'] = 'YOUR_OPENAI_API_KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1x7KCDyJGKf"
   },
   "source": [
    "# Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpAUQRYEJH12"
   },
   "source": [
    "Load LangChain Expression Language (LCEL) docs as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1731954410855,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "1UzlWommJF0i"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "\n",
    "# LCEL docs\n",
    "url = 'https://python.langchain.com/docs/conecpts/lcel/'\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url,\n",
    "    max_depth=20,\n",
    "    extractor=lambda x: Soup(x, 'html.parser').text,\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Sort the list based on the URLs and get the text\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata['source'])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "\n",
    "concatenated_content = \"\\n\\n\\n ---\\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1TbhOCpJyCT"
   },
   "source": [
    "# LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbz7MApWJ2co"
   },
   "source": [
    "We will first try OpenAI and Claude3 with function calling. We will create a `code_gen_chain` with either OpenAI or Claude and test them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3125,
     "status": "ok",
     "timestamp": 1731954443898,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "B8ZkS5lAJxOZ",
    "outputId": "d1ca4f57-06c5-4b8b-cbf6-64fcce6d8cd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Code(prefix='To build a RAG chain in LCEL, you can define a series of nodes and edges to represent the chain. Each node will have a unique identifier and can contain data. Edges will connect the nodes in a specific order. Here is an example code snippet to create a simple RAG chain in LCEL.', imports=\"const { Graph } = require('langchain-js');\", code=\"const ragChain = new Graph();\\n\\n// Define nodes\\nragChain.addNode({ id: 'A', data: 'Node A' });\\nragChain.addNode({ id: 'B', data: 'Node B' });\\nragChain.addNode({ id: 'C', data: 'Node C' });\\n\\n// Define edges\\nragChain.addEdge('A', 'B');\\nragChain.addEdge('B', 'C');\\n\\n// Print the RAG chain\\nconsole.log(ragChain.toString());\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "### OpenAI\n",
    "\n",
    "# Grader prompt\n",
    "code_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            \"\"\"You are a coding assistant with expertise in LCEL, LangChain expression language. \\n\n",
    "            Here is a full set of LCEL documentation: \\n ------- \\n {context} \\n ------- \\n\n",
    "            Answer the user question based on the above provided documentation. Ensure any code\n",
    "            you provided can be executed with all required imports and variables defined.\n",
    "            Structure your answer with a description of the code solution. \\n\n",
    "            Then list the imports. And finally list the functioning code block.\n",
    "            Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\n",
    "            'placeholder', '{messages}',\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Data model\n",
    "class Code(BaseModel):\n",
    "    \"\"\"Schema for code solutions to questions about LCEL.\"\"\"\n",
    "    prefix: str = Field(description='Description of the problem and approach')\n",
    "    imports: str = Field(description='Code block import statements')\n",
    "    code: str = Field(description='Code block not including import statements')\n",
    "\n",
    "\n",
    "expt_llm = 'gpt-3.5-turbo'\n",
    "llm = ChatOpenAI(model=expt_llm, temperature=0)\n",
    "code_gen_chain_oai = code_gen_prompt | llm.with_structured_output(Code)\n",
    "\n",
    "question = \"How do I build a RAG chain in LCEL?\"\n",
    "solution = code_gen_chain_oai.invoke(\n",
    "    {\n",
    "        'context': concatenated_content,\n",
    "        'messages': [('user', question)]\n",
    "    }\n",
    ")\n",
    "\n",
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2565,
     "status": "ok",
     "timestamp": 1731949404686,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "dIHGWeG8Ljka",
    "outputId": "d6c38ffa-7db2-4e9c-dcbc-8c34418ef633"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Code(prefix='To build a RAG (Red-Amber-Green) chain in LCEL, we can use the following approach:', imports='from langchain.chains import RAGChain\\nfrom langchain.agents import Tool', code='# Define the tools for the RAG chain\\ntools = [\\n    Tool(name=\"tool1\", description=\"This is tool 1\", func=lambda x: \"green\"),\\n    Tool(name=\"tool2\", description=\"This is tool 2\", func=lambda x: \"amber\"),\\n    Tool(name=\"tool3\", description=\"This is tool 3\", func=lambda x: \"red\")\\n]\\n\\n# Create the RAG chain\\nrag_chain = RAGChain.from_tools(tools)\\n\\n# Run the RAG chain\\nresult = rag_chain.run(\"some input\")\\nprint(result)')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "### Anthropic\n",
    "\n",
    "# Prompt to enforce tool use\n",
    "code_gen_prompt_claude = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            \"\"\"<instructions> You are a coding assistant with expertise in LCEL, LangChain expression language. \\n\n",
    "    Here is the LCEL documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user  question based on the \\n\n",
    "    above provided documentation. Ensure any code you provide can be executed with all required imports and variables \\n\n",
    "    defined. Structure your answer: 1) a prefix describing the code solution, 2) the imports, 3) the functioning code block. \\n\n",
    "    Invoke the code tool to structure the output correctly. </instructions> \\n Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\n",
    "            'placeholder', '{messages}',\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# SAME Data model\n",
    "\n",
    "# LLM\n",
    "expt_llm = 'claude-3-haiku-20240307'\n",
    "llm = ChatAnthropic(model=expt_llm, default_headers={'anthropic-beta': 'tools-2024-04-04'})\n",
    "\n",
    "structured_llm_claude = llm.with_structured_output(Code, include_raw=True)\n",
    "\n",
    "\n",
    "# Optional: Check for errors in case tool use is flaky\n",
    "def check_claude_output(tool_output):\n",
    "    '''Check for parse error or failure to call the tools'''\n",
    "\n",
    "    # Error with parsing\n",
    "    if tool_output['parsing_error']:\n",
    "        # Report back output and parsing errors\n",
    "        print(\"Parsing error!\")\n",
    "        raw_output = str(tool_output['raw'].content)\n",
    "        error = tool_output['parsing_error']\n",
    "        raise ValueError(\n",
    "            f\"Error parsing your output! Be sure to invoke the tool. Output: {raw_output}. \\n Parse error: {error}\"\n",
    "        )\n",
    "\n",
    "    # Tool was not invoked\n",
    "    elif not tool_output['parsed']:\n",
    "        print(\"Failed to invoke tool!\")\n",
    "        raise ValueError(\n",
    "            \"You did not use the provided tool! Be sure to invoke the tool to structure the output.\"\n",
    "        )\n",
    "\n",
    "    return tool_output\n",
    "\n",
    "\n",
    "\n",
    "# Chain with output check\n",
    "code_chain_claude_raw = code_gen_prompt_claude | structured_llm_claude | check_claude_output\n",
    "\n",
    "\n",
    "def insert_errors(inputs):\n",
    "    \"\"\"Insert errors for tool parsing in the messages\"\"\"\n",
    "\n",
    "    # Get errors\n",
    "    error = inputs['error']\n",
    "    messages = inputs['messages']\n",
    "    messages += [\n",
    "        (\n",
    "            'assistant',\n",
    "            f\"Retry. You are required to fix the parsing errors: {error} \\n\\n You must invoke the provided tool.\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        'messages': messages,\n",
    "        'context': inputs['context'],\n",
    "    }\n",
    "\n",
    "\n",
    "# This will be run as a fallback chain\n",
    "fallback_chain = insert_errors | code_chain_claude_raw\n",
    "# Max re-tries\n",
    "N = 3\n",
    "code_gen_chain_retry = code_chain_claude_raw.with_fallbacks(\n",
    "    fallbacks=[fallback_chain] * N,\n",
    "    exception_key='error',\n",
    ")\n",
    "\n",
    "\n",
    "def parse_output(solution):\n",
    "    \"\"\"When we add `include_raw=True` to the structured output,\n",
    "    it will return a dict with `raw`, `parsed`, `parsing_error`.\"\"\"\n",
    "    return solution['parsed']\n",
    "\n",
    "\n",
    "\n",
    "# Optional: With retry to correct for failture to invoke tool\n",
    "code_gen_chain = code_gen_chain_retry | parse_output\n",
    "\n",
    "# No retry\n",
    "code_gen_chain = code_gen_prompt_claude | structured_llm_claude | parse_output\n",
    "\n",
    "\n",
    "# Test\n",
    "question = \"How do I build a RAG chain in LCEL?\"\n",
    "solution = code_gen_chain.invoke(\n",
    "    {'context': concatenated_content,\n",
    "     'messages': [('user', question)]}\n",
    ")\n",
    "\n",
    "solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgxCvZNOOxEE"
   },
   "source": [
    "# State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1731949540605,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "0HjmChg7Om4t"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        error: Binary flag for control flow to indicate whether test error was tripped\n",
    "        messages: With user question, error messages, reasoning\n",
    "        generation: Code solution\n",
    "        iterations: Number of tries\n",
    "    \"\"\"\n",
    "\n",
    "    error: str\n",
    "    messages: List\n",
    "    generation: str\n",
    "    iterations: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAt7amPNPL1E"
   },
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1731955529942,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "MsmhJ0-6PLPV"
   },
   "outputs": [],
   "source": [
    "### Parameters\n",
    "\n",
    "# Max trials\n",
    "max_iterations = 3\n",
    "\n",
    "# Reflect\n",
    "flag = 'do not reflect' # or change to 'reflect'\n",
    "\n",
    "\n",
    "### Nodes\n",
    "\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate a code solution\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "\n",
    "    print('---GENERATING CODE SOLUTION---')\n",
    "\n",
    "    # State\n",
    "    messages = state['messages']\n",
    "    iterations = state['iterations']\n",
    "    error = state['error']\n",
    "\n",
    "    # We have been routed back to generation with an error\n",
    "    if error == 'yes':\n",
    "        messages += [\n",
    "            (\n",
    "                'user',\n",
    "                \"Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:\",\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # Solution\n",
    "    code_solution = code_gen_chain.invoke(\n",
    "        {'context': concatenated_content, 'messages': messages}\n",
    "    )\n",
    "    messages += [\n",
    "        (\n",
    "            'assistant',\n",
    "            f\"{code_solution.prefix} \\n Imports: {code_solution.imports} \\n Code: {code_solution.code}\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Increment\n",
    "    iterations += 1\n",
    "\n",
    "    return {\n",
    "        'generation': code_solution,\n",
    "        'messages': messages,\n",
    "        'iterations': iterations,\n",
    "    }\n",
    "\n",
    "\n",
    "def code_check(state: GraphState):\n",
    "    \"\"\"\n",
    "    Check code\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, error\n",
    "    \"\"\"\n",
    "\n",
    "    print('---CHECKING CODE---')\n",
    "\n",
    "    # State\n",
    "    messages = state['messages']\n",
    "    code_solution = state['generation']\n",
    "    iterations = state['iterations']\n",
    "\n",
    "    # Get solution components\n",
    "    imports = code_solution.imports\n",
    "    code = code_solution.code\n",
    "\n",
    "    # Check imports\n",
    "    try:\n",
    "        exec(imports)\n",
    "    except Exception as e:\n",
    "        print('---CODE IMPORT CHECK: FAILED---')\n",
    "        error_message += [('user', f'Your solution failed the import test: {e}')]\n",
    "        messages += error_message\n",
    "\n",
    "        return {\n",
    "            'generation': code_solution,\n",
    "            'messages': messages,\n",
    "            'iterations': iterations,\n",
    "            'error': 'yes',\n",
    "        }\n",
    "\n",
    "    # Check execution\n",
    "    try:\n",
    "        exec(imports + '\\n' + code)\n",
    "    except Exception as e:\n",
    "        print('---CODE BLOCK CHECK: FAILED---')\n",
    "        error_message = [('user', f'Your solution failed the code execution test: {e}')]\n",
    "        messages += error_message\n",
    "\n",
    "        return {\n",
    "            'generation': code_solution,\n",
    "            'messages': messages,\n",
    "            'iterations': iterations,\n",
    "            'error': 'yes',\n",
    "        }\n",
    "\n",
    "    # No errors\n",
    "    print('---NO CODE TEST FAILURES---')\n",
    "    return {\n",
    "        'generation': code_solution,\n",
    "        'messages': messages,\n",
    "        'iterations': iterations,\n",
    "        'error': 'no',\n",
    "    }\n",
    "\n",
    "\n",
    "def reflect(state: GraphState):\n",
    "    \"\"\"\n",
    "    Reflect no errors\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "\n",
    "    print('---GENERATING CODE SOLUTION---')\n",
    "\n",
    "    # State\n",
    "    messages = state['messages']\n",
    "    iterations = state['iterations']\n",
    "    code_solution = state['generation']\n",
    "\n",
    "    ### Prompt reflection\n",
    "\n",
    "    # add reflection\n",
    "    reflections = code_gen_chain.invoke(\n",
    "        {'context': concatenated_content, 'messages': messages}\n",
    "    )\n",
    "    messages += [\n",
    "        (\n",
    "            'assistant',\n",
    "            f\"Here are reflections on the error: {reflections}\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        'generation': code_solution,\n",
    "        'messages': messages,\n",
    "        'iterations': iterations,\n",
    "    }\n",
    "\n",
    "\n",
    "## Edges\n",
    "def decide_to_finish(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determine whether to finish.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    error = state['error']\n",
    "    iterations = state['iterations']\n",
    "\n",
    "    if error == 'no' or iterations == max_iterations:\n",
    "        print('---DECISION: FINISH---')\n",
    "        return 'end'\n",
    "    else:\n",
    "        print('---DECISION: RE-TRY SOLUTION---')\n",
    "        if flag == 'reflect':\n",
    "            return 'reflect'\n",
    "        else:\n",
    "            return 'generate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1731955531610,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "NLXRc-S5kejd"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# define the nodes\n",
    "workflow.add_node('generate', generate) # generate solution\n",
    "workflow.add_node('check_code', code_check) # check code\n",
    "workflow.add_node('reflect', reflect) # reflect\n",
    "\n",
    "# define the edges\n",
    "workflow.add_edge(START, 'generate')\n",
    "workflow.add_edge('generate', 'check_code')\n",
    "workflow.add_conditional_edges(\n",
    "    'check_code',\n",
    "    decide_to_finish,\n",
    "    {\n",
    "        'end': END,\n",
    "        'reflect': 'reflect',\n",
    "        'generate': 'generate',\n",
    "    },\n",
    ")\n",
    "workflow.add_edge('reflect', 'generate')\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4910,
     "status": "ok",
     "timestamp": 1731955538018,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 360
    },
    "id": "6ValuIx1lkY2",
    "outputId": "4d0fb755-a0e4-4f96-baee-9d9aee1015de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "The given text is: This is the text I want to use.\n",
      "---NO CODE TEST FAILURES---\n",
      "---DECISION: FINISH---\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\n",
    "solution = app.invoke(\n",
    "    {\n",
    "        'messages': [('user', question)],\n",
    "        'iterations': 0,\n",
    "        'error': \"\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2RwVY5HmG-4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyObVrmchVfVECN4PmH0Hlz3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
