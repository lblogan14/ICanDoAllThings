{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# End-to-End MCP Application"
      ],
      "metadata": {
        "id": "TH2-Mvp-L0g3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Buliding the Gradio MCP Server"
      ],
      "metadata": {
        "id": "BCccJQkoL6_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we will create a sentiment analysis MCP server using Gradio. This server will expose a sentiment analysis tool that can be used by both human users through a web interface and AI models through the MCP protocol."
      ],
      "metadata": {
        "id": "z43Zjb24rYEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up\n",
        "\n",
        "```bash\n",
        "mkdir mcp-sentiment\n",
        "cd mcp-sentiment\n",
        "python -m venv venv\n",
        "source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n",
        "pip install \"gradio[mcp]\" textblob\n",
        "```"
      ],
      "metadata": {
        "id": "S1LAu7Y7rqSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the Server"
      ],
      "metadata": {
        "id": "yUajbqv2rwb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HuggingFace Spaces need an `app.py` file to build the space, so the name of the Python file has to be `app.py`."
      ],
      "metadata": {
        "id": "GD1LLTHLrzC2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCLUhThdLxz1"
      },
      "outputs": [],
      "source": [
        "# app.py\n",
        "\n",
        "import gradio as gr\n",
        "from textblob import TextBlob\n",
        "\n",
        "def sentiment_analysis(text: str) -> dict:\n",
        "    \"\"\"Analyze the sentiment of the given text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to analyze\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing polarity, subjectivity, and assessment\n",
        "    \"\"\"\n",
        "    blob = TextBlob(text)\n",
        "    sentiment = blob.sentiment\n",
        "\n",
        "    return {\n",
        "        'polarity': round(sentiment.polarity, 2), # -1 (negative) to 1 (positive)\n",
        "        'subjectivity': round(sentiment.subjectivity, 2), # 0 (objective) to 1 (subjective)\n",
        "        'assessment': 'positive' if sentiment.polarity > 0 else 'negative' if sentiment.polarity < 0 else 'neutral'\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "# Create the Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=sentiment_analysis,\n",
        "    inputs=gr.Textbox(placeholder=\"Enter text to analyze...\"),\n",
        "    outputs=gr.JSON(),\n",
        "    title=\"Text Sentiment Analysis\",\n",
        "    description=\"Analyze the sentiment of text using TextBlob\"\n",
        ")\n",
        "\n",
        "# Launch the interface and MCP server\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(mcp_server=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Function definition\n",
        "    - The `setiment_analysis` takes a text input and return a dictionary\n",
        "    - It uses `TextBlob` to analyze the sentiment\n",
        "    - The docstring is crucial as it helps Gradio generate the MCP tool schema\n",
        "    - Type hints (`str` and `dict`) help define the input/output schema\n",
        "\n",
        "- Gradio interface\n",
        "    - `gr.Interface` creates both the web UI and MCP server\n",
        "    - The function is exposed as an MCP tool automatically\n",
        "    - Input and output components define the tool's schema\n",
        "    - The JSON output component ensures proper serlaization\n",
        "\n",
        "- MCP server\n",
        "    - Setting `mcp_server=True` enables the MCP server\n",
        "    - The server will be available at `http://localhost:7860/gradio_api/mcp/sse`\n",
        "    - We can also enable it using the environment variable:\n",
        "    ```bash\n",
        "    export GRADIO_MCP_SERVER=True\n",
        "    ```\n",
        "\n",
        "\n",
        "\n",
        "To start the server, we just need to run\n",
        "```bash\n",
        "python app.py\n",
        "```"
      ],
      "metadata": {
        "id": "utPk3iE3twW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Troubleshooting"
      ],
      "metadata": {
        "id": "MRl-O1GAuvmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Type hints and docstrings\n",
        "    - Always provide type hints for our function parameters and return values\n",
        "    - Include a docstring with a \"Args: \" block for each parameter\n",
        "    - This helps Gradio generate accurate MCP tool schema\n",
        "- String inputs\n",
        "    - When in doubt, accept input arguments as `str`\n",
        "    - Convert them to the desired type inside the function\n",
        "    - This provides better compatibility with MCP clients\n",
        "- SSE support\n",
        "    - Some MCP clients do not support SSE-based MCP servers\n",
        "    - In those cases, use `mcp-remote`:\n",
        "    ```json\n",
        "    {\n",
        "        \"mcpServers\": {\n",
        "            \"gradio\": {\n",
        "            \"command\": \"npx\",\n",
        "            \"args\": [\n",
        "                \"mcp-remote\",\n",
        "                \"http://localhost:7860/gradio_api/mcp/sse\"\n",
        "            ]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    ```\n",
        "- Connection isues\n",
        "    - If we encounter connection issues, try restarting both the client and server\n",
        "    - Check that the server is running and accessible\n",
        "    - Verify that the MCP schema is available at the expected URL"
      ],
      "metadata": {
        "id": "ipS7VJcQu3PI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using MCP Clients with Our Application"
      ],
      "metadata": {
        "id": "IvrJuxxCvlV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we will create MCP clients that can interact with our MCP server using different programming languages.\n",
        "\n",
        "MCP hosts use configuration files to manage server connections. These files define which servers are available and how to connect them.\n",
        "\n",
        "The standard configuration file for MCP is named `mcp.json`. Here is a basic structure:\n",
        "```json\n",
        "{\n",
        "  \"servers\": [\n",
        "    {\n",
        "      \"name\": \"MCP Server\",\n",
        "      \"transport\": {\n",
        "        \"type\": \"sse\",\n",
        "        \"url\": \"http://localhost:7860/gradio_api/mcp/sse\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "Now we have a single server configured to use SSE transport, connecting to a local Gradio server running on port 7860. Here we connected to the Gradio app via SSE transport because we assume that the gradio app is running on a remote server. If want to connect to a local script, we need to switch to `stdio` transport.\n",
        "\n",
        "\n",
        "For example, for remote servers using HTTP+SSE transport, the configuration includes the server URL:\n",
        "```json\n",
        "{\n",
        "  \"servers\": [\n",
        "    {\n",
        "      \"name\": \"Remote MCP Server\",\n",
        "      \"transport\": {\n",
        "        \"type\": \"sse\",\n",
        "        \"url\": \"https://example.com/gradio_api/mcp/sse\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "When working with Gradio MCP servers, we can configure our UI client to connect to the server using the MCP protocol by creating a new file called `config.json`:\n",
        "```json\n",
        "{\n",
        "  \"mcpServers\": {\n",
        "    \"mcp\": {\n",
        "      \"url\": \"http://localhost:7860/gradio_api/mcp/sse\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "VZX2GOxTyhOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building an MCP Client with Gradio"
      ],
      "metadata": {
        "id": "J7_rn8qJzwyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we will use Gradio as an MCP Client to connect to an MCP Server."
      ],
      "metadata": {
        "id": "KVSkJ5o0z14I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"smolagents[mcp]\" \"gradio[mcp]\" mcp"
      ],
      "metadata": {
        "id": "Hf3mYP1mz9UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "from mcp.client.stdio import StdioServerParameters\n",
        "from smolagents import ToolCollection, CodeAgent, InferenceClientModel\n",
        "from smolagents.mcp_client import MCPClient"
      ],
      "metadata": {
        "id": "e3XBUuYpu2uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will connect to the MCP Server and get the tools that we can use to answer questions."
      ],
      "metadata": {
        "id": "8RBCbQUw0Mip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mcp_client = MCPClient(\n",
        "    {'url': \"http://localhost:7860/gradio_api/mcp/sse\"}\n",
        ")\n",
        "\n",
        "tools = mcp_client.get_tools()"
      ],
      "metadata": {
        "id": "bZVfIR_F0Qo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the tools, we can create a simple agent that uses them to answer questions."
      ],
      "metadata": {
        "id": "-Gcb8GSu0YQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = InferenceClientModel()\n",
        "agent = CodeAgent(model=model, tools=[*tools])"
      ],
      "metadata": {
        "id": "dyDKkoCS0c0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create a simple Gradio interface that uses the agent to answer questions."
      ],
      "metadata": {
        "id": "04o-jNPP0hmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.ChatInterface(\n",
        "    fn=lambda message, history: str(agent.run(message)),\n",
        "    type=\"messages\",\n",
        "    examples=['Prime factorization of 68'],\n",
        "    title='Agent with MCP Tools',\n",
        "    description=\"This is a simple agent that uses MCP tools to answer questions.\",\n",
        "    messages=[]\n",
        ")\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "xWSXve370k-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complete Snippet"
      ],
      "metadata": {
        "id": "R8g5X4uX05O3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "from mcp.client.stdio import StdioServerParameters\n",
        "from smolagents import ToolCollection, CodeAgent\n",
        "from smolagents import CodeAgent, InferenceClientModel\n",
        "from smolagents.mcp_client import MCPClient\n",
        "\n",
        "\n",
        "try:\n",
        "    mcp_client = MCPClient(\n",
        "        {\"url\": \"http://localhost:7860/gradio_api/mcp/sse\"}\n",
        "    )\n",
        "    tools = mcp_client.get_tools()\n",
        "\n",
        "    model = InferenceClientModel()\n",
        "    agent = CodeAgent(tools=[*tools], model=model)\n",
        "\n",
        "    demo = gr.ChatInterface(\n",
        "        fn=lambda message, history: str(agent.run(message)),\n",
        "        type=\"messages\",\n",
        "        examples=[\"Prime factorization of 68\"],\n",
        "        title=\"Agent with MCP Tools\",\n",
        "        description=\"This is a simple agent that uses MCP tools to answer questions.\",\n",
        "    )\n",
        "\n",
        "    demo.launch()\n",
        "finally:\n",
        "    mcp_client.close()"
      ],
      "metadata": {
        "id": "BTDvO-Ic07jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to have the `finally` block because the MCP Client is a long-lived object that needs to be closed when the program exits."
      ],
      "metadata": {
        "id": "Kz9jwA1J1Axs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Tiny Agent with TypeScript"
      ],
      "metadata": {
        "id": "gNu8P33Y1OVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we will implement a TypeScript (JS) MCP client that can coomunicate with any MCP server, including the Gradio-based sentiment analysis server we had in previous section.\n",
        "\n",
        "If we have NodeJS (with `pnpm` or `npm`), we can run\n",
        "```bash\n",
        "npx @huggingface/mcp-client\n",
        "```\n",
        "or if using `pnpm`:\n",
        "```bash\n",
        "pnpx @huggingface/mcp-client\n",
        "```\n",
        "This installs the package into a temporary folder then executes its command.\n",
        "\n",
        "We will see a simple agent connecting to multiple MCP servers (running locally), loading their tools, then prompting us for a conversation. By default, our example agent connects to two MCP servers:\n",
        "- the \"canonical\" [file system server](https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem) to get access to our Desktop, and\n",
        "- the [Playwright MCP server](https://github.com/microsoft/playwright-mcp) to use a sandboxed Chromium browser for us.\n"
      ],
      "metadata": {
        "id": "e5VpD7FJB5Ar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Default model and provider"
      ],
      "metadata": {
        "id": "_a2jHRNODObT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our example agent uses by default:\n",
        "- `Qwen/Qwen2.5-72B-Instruct`\n",
        "- running on [Nebius](https://huggingface.co/docs/inference-providers/providers/nebius)\n",
        "\n",
        "\n",
        "This is all configurable through `env` variable. We can add our Gradio MCP server:\n",
        "```typescript\n",
        "const agent = new Agent({\n",
        "\tprovider: process.env.PROVIDER ?? \"nebius\",\n",
        "\tmodel: process.env.MODEL_ID ?? \"Qwen/Qwen2.5-72B-Instruct\",\n",
        "\tapiKey: process.env.HF_TOKEN,\n",
        "\tservers: [\n",
        "\t\t// Default servers\n",
        "\t\t{\n",
        "\t\t\tcommand: \"npx\",\n",
        "\t\t\targs: [\"@modelcontextprotocol/servers\", \"filesystem\"]\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\tcommand: \"npx\",\n",
        "\t\t\targs: [\"playwright-mcp\"]\n",
        "\t\t},\n",
        "\t\t// Our Gradio sentiment analysis server\n",
        "\t\t{\n",
        "\t\t\tcommand: \"npx\",\n",
        "\t\t\targs: [\n",
        "\t\t\t\t\"mcp-remote\",\n",
        "\t\t\t\t\"http://localhost:7860/gradio_api/mcp/sse\"\n",
        "\t\t\t]\n",
        "\t\t}\n",
        "\t],\n",
        "});\n",
        "```\n",
        "We connect our Gradio-based MCP server via the [mcp-remote](https://www.npmjs.com/package/mcp-remote) pacakge."
      ],
      "metadata": {
        "id": "ESVJIUuXDQuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tool calling native support in LLMs"
      ],
      "metadata": {
        "id": "RJRgOhoTDsE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tool is defined by its name, a description, and a JSONSchema representation of its parameters - exactly how we defined our sentiment analysis function in the Gradio server. For example,\n",
        "```typescript\n",
        "const weatherTool = {\n",
        "\ttype: \"function\",\n",
        "\tfunction: {\n",
        "\t\tname: \"get_weather\",\n",
        "\t\tdescription: \"Get current temperature for a given location.\",\n",
        "\t\tparameters: {\n",
        "\t\t\ttype: \"object\",\n",
        "\t\t\tproperties: {\n",
        "\t\t\t\tlocation: {\n",
        "\t\t\t\t\ttype: \"string\",\n",
        "\t\t\t\t\tdescription: \"City and country e.g. BogotÃ¡, Colombia\",\n",
        "\t\t\t\t},\n",
        "\t\t\t},\n",
        "\t\t},\n",
        "\t},\n",
        "};\n",
        "```\n",
        "\n",
        "Inference engines let us pass a list of tools when calling the LLM, and the LLM is free to call zero, one, or more tools.\n",
        "\n",
        "In the backend (at the inference engine level), the tools are simply passed to the model in a specially-formatted `chat_template`, liek any other message, and then parsed out of the response (using model-specific special tokens) to expose them as tool calls."
      ],
      "metadata": {
        "id": "0VRug5tPDzeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing an MCP client on top of InferenceClient"
      ],
      "metadata": {
        "id": "0p-JWk9YE3iT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The complete `McpClient.ts` code file is [HERE](https://github.com/huggingface/huggingface.js/blob/main/packages/mcp-client/src/McpClient.ts).\n",
        "\n",
        "Our `McpClient` class has\n",
        "- an InferenceClient (works with any inference provider, and `huggingface/inference` supports both remote and local endpoints)\n",
        "- a set of MCP client sessions, one of each connected MCP server (this allows us to connect to multiple servers, including Gradio server)\n",
        "- a list of available tools that is going to be filled from the connected servers and slightly re-formatted\n",
        "\n",
        "```typescript\n",
        "export class McpClient {\n",
        "\tprotected client: InferenceClient;\n",
        "\tprotected provider: string;\n",
        "\tprotected model: string;\n",
        "\tprivate clients: Map<ToolName, Client> = new Map();\n",
        "\tpublic readonly availableTools: ChatCompletionInputTool[] = [];\n",
        "\n",
        "\tconstructor({ provider, model, apiKey }: { provider: InferenceProvider; model: string; apiKey: string }) {\n",
        "\t\tthis.client = new InferenceClient(apiKey);\n",
        "\t\tthis.provider = provider;\n",
        "\t\tthis.model = model;\n",
        "\t}\n",
        "\n",
        "\t// [...]\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "To connect to a MCP server (like our Gradio sentiment analysis server), the official TypeScript SDK provides a `Client` class with a `listTools()` method:\n",
        "```typescript\n",
        "async addMcpServer(server: StdioServerParameters): Promise<void> {\n",
        "\tconst transport = new StdioClientTransport({\n",
        "\t\t...server,\n",
        "\t\tenv: { ...server.env, PATH: process.env.PATH ?? \"\" },\n",
        "\t});\n",
        "\tconst mcp = new Client({ name: \"@huggingface/mcp-client\", version: packageVersion });\n",
        "\tawait mcp.connect(transport);\n",
        "\n",
        "\tconst toolsResult = await mcp.listTools();\n",
        "\tdebug(\n",
        "\t\t\"Connected to server with tools:\",\n",
        "\t\ttoolsResult.tools.map(({ name }) => name)\n",
        "\t);\n",
        "\n",
        "\tfor (const tool of toolsResult.tools) {\n",
        "\t\tthis.clients.set(tool.name, mcp);\n",
        "\t}\n",
        "\n",
        "\tthis.availableTools.push(\n",
        "\t\t...toolsResult.tools.map((tool) => {\n",
        "\t\t\treturn {\n",
        "\t\t\t\ttype: \"function\",\n",
        "\t\t\t\tfunction: {\n",
        "\t\t\t\t\tname: tool.name,\n",
        "\t\t\t\t\tdescription: tool.description,\n",
        "\t\t\t\t\tparameters: tool.inputSchema,\n",
        "\t\t\t\t},\n",
        "\t\t\t} satisfies ChatCompletionInputTool;\n",
        "\t\t})\n",
        "\t);\n",
        "}\n",
        "```\n",
        "\n",
        "The `StdioServerParameters` is an interface from the MCP SDK that will let us easily spawn a local process. For each MCP server we connect to (including our Gradio sentiment analysis server), we slightly re-format its list of tools and add them to `this.availableTools` variable.\n",
        "\n",
        "Then we pass `this.availableTools` to our LLM chat-completion, in addition to our usual array of messages:\n",
        "```typescript\n",
        "const stream = this.client.chatCompletionStream({\n",
        "\tprovider: this.provider,\n",
        "\tmodel: this.model,\n",
        "\tmessages,\n",
        "\ttools: this.availableTools,\n",
        "\ttool_choice: \"auto\",\n",
        "});\n",
        "```\n",
        "\n",
        "`tool_choice: \"auto\"` is the parameter we pass for the LLM to generate zero, one, or multiple tool calls.\n",
        "\n",
        "When parsing or streaming the output, the LLM will generate some tool calls (i.e., a function name, and some JSON-encoded arguments), which we need to compute. The MCP client SDK makes it easy by calling the `client.callTool()` method:\n",
        "```typescript\n",
        "const toolName = toolCall.function.name;\n",
        "const toolArgs = JSON.parse(toolCall.function.arguments);\n",
        "\n",
        "const toolMessage: ChatCompletionInputMessageTool = {\n",
        "\trole: \"tool\",\n",
        "\ttool_call_id: toolCall.id,\n",
        "\tcontent: \"\",\n",
        "\tname: toolName,\n",
        "};\n",
        "\n",
        "/// Get the appropriate session for this tool\n",
        "const client = this.clients.get(toolName);\n",
        "if (client) {\n",
        "\tconst result = await client.callTool({ name: toolName, arguments: toolArgs });\n",
        "\ttoolMessage.content = result.content[0].text;\n",
        "} else {\n",
        "\ttoolMessage.content = `Error: No session found for tool: ${toolName}`;\n",
        "}\n",
        "```\n",
        "\n",
        "If the LLM chooses to use our sentiment analysis tool, this code will automatically route the call to our Gradio server, execute the analysis, and return the result back to the LLM. Finally we will add the resulting tool message to our `messages` array and back into the LLM."
      ],
      "metadata": {
        "id": "5L7NZ2rtFApR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yiNRdmHT1RS-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}