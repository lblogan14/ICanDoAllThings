{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOfzT/NcHGiuhyIYCzLR/tg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MQCx8MlKct5F"},"outputs":[],"source":["!pip install -qU diffusers accelerate transformers huggingface_hub"]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","notebook_login()"],"metadata":{"id":"dSzmFcZPc4Fv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load pipelines"],"metadata":{"id":"m6uGwWUBd6gp"}},{"cell_type":"markdown","source":["## Load a pipeline"],"metadata":{"id":"aHwWo5fCeE6U"}},{"cell_type":"markdown","source":["### Generic pipeline"],"metadata":{"id":"pCyGYdnQhrIA"}},{"cell_type":"markdown","source":["The `DiffusionPipeline` class uses the `from_pretrained()` method to automatically detect the correct pipeline class for a task from the checkpoint, downloads and caches all the required configuration and weight files, and returns a pipeline ready for inference."],"metadata":{"id":"8-jTv7eFeNSl"}},{"cell_type":"code","source":["from diffusers import DiffusionPipeline\n","\n","pipeline = DiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    use_safetensors=True,\n",")"],"metadata":{"id":"XsuUHzBId8CP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The same checkpoint can also be used for an image-to-image task.\n","\n","The `DiffusionPipeline` class can handle any tasks as long as we provide the appropriate inputs."],"metadata":{"id":"K5eUCG9neo3I"}},{"cell_type":"code","source":["# for image-to-image task\n","# we keep the same pipeline instance\n","from diffusers.utils import load_image\n","\n","init_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\")\n","prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n","\n","image = pipeline(\n","    prompt,\n","    image=init_image,\n",").images[0]"],"metadata":{"id":"4dr-pXqse5SA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Specific pipeline"],"metadata":{"id":"G-Uv7lZkiepc"}},{"cell_type":"markdown","source":["Checkpoints can also be loaded by their specific pipeline class if we already know it."],"metadata":{"id":"tvK5gH81igqO"}},{"cell_type":"code","source":["from diffusers import StableDiffusionPipeline\n","\n","pipeline = StableDiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    use_safetensors=True,\n",")"],"metadata":{"id":"a05rcQJJifxO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The same checkpoint may also be used for another task like image-to-image. To differentiate what task we want to use the checkpoint for, we have to use the corresponding task-specific pipeline class. For example, to use the same checkpoint for image-to-image, we need to use the `StableDiffusionImg2ImgPipeline` class:"],"metadata":{"id":"uSQXrEOEivEe"}},{"cell_type":"code","source":["from diffusers import StableDiffusionImg2ImgPipeline\n","\n","pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    use_safetensors=True,\n",")"],"metadata":{"id":"kmHc12kri-Q6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Local pipeline"],"metadata":{"id":"bCVpTn-HjEPf"}},{"cell_type":"markdown","source":["We can also manually download a checkpoint to our local disk and load a pipeline locally:"],"metadata":{"id":"2k5iTScokh9_"}},{"cell_type":"code","source":["from diffusers import DiffusionPipeline\n","\n","stable_diffusion = DiffusionPipeline.from_pretrained(\n","    './stable-diffusion-v1-5',\n","    use_safetensors=True,\n",")"],"metadata":{"id":"P2Qk5GXSjFoA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This assumes that the checkpoint file is the folder `./stable-diffusion-v1-5`. The `from_pretrained()` method will not download files from the Hub when it detects a local path."],"metadata":{"id":"kPvASWawkxRp"}},{"cell_type":"markdown","source":["## Customize a pipeline"],"metadata":{"id":"VUOJW4DRlCRQ"}},{"cell_type":"markdown","source":["We can customize a pipeline by loading different components into it. Then we can\n","* change to a scheduler with faster generation speed or higher generation quality depending on our needs\n","* change a default pipeline component to a newer and better performing one\n","\n","For example, we can customize the default `stabilityai/stable-diffusion-xl-base1.0` checkpoint with:\n","* The `HeunDiscreteScheduler` to generate higher quality images at the expense of slower generation speed. We must pass the `subfolder=\"scheduler\"` parameter in `from_pretrained()` to load the scheduler configuration into the correct subfolder of the pipeline repository.\n","* A more stable VAE that runs in `fp16`."],"metadata":{"id":"TALDsCPclEQt"}},{"cell_type":"code","source":["from diffusers import StableDiffusionXLPipeline, HeunDiscreteScheduler, AutoencoderKL\n","import torch\n","\n","scheduler = HeunDiscreteScheduler.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    subfolder='scheduler',\n",")\n","\n","vae = AutoencoderKL.from_pretrained(\n","    'madebyollin/sdxl-vae-fp16-fix',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True,\n",")"],"metadata":{"id":"hTOKGI5clBun"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can pass the new scheduler and VAE to the `StableDiffusionXLPipeline`:"],"metadata":{"id":"35pdzHlRmH-3"}},{"cell_type":"code","source":["pipeline = StableDiffusionXLPipeline.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    scheduler=scheduler,\n","    vae=vae,\n","    torch_dtype=torch.float16,\n","    variant='fp16',\n","    use_safetensors=True,\n",").to('cuda')"],"metadata":{"id":"U6DJ6HLVmLyT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Reuse a pipeline"],"metadata":{"id":"PwmcYeXcnlQm"}},{"cell_type":"markdown","source":["When we load multiple pipelines that share the same model checkpoints, it makes sense to reuse the shared components instead of reloading everything into memory again, espeically if our hardware is memory-constrained.\n","\n","With the `DiffusionPipeline.from_pipe()`, we can switch between multiple pipelines to take advantage of their different features without increasing memory-usage. It is similar to turning on and off a feature in our pipeline."],"metadata":{"id":"D-B5k1jXnnBG"}},{"cell_type":"markdown","source":["We will start with a `StableDiffusionPipeline` and then reuse the loaded model components to create a `StableDiffusionSAGPipeline` to increase generation quality. We will use the `StableDiffusionPipeline` with an `IP-Adapter` to generate a bear eating pizza."],"metadata":{"id":"GfAfVGbJC8x4"}},{"cell_type":"code","source":["from diffusers import DiffusionPipeline, StableDiffusionSAGPipeline\n","import torch\n","import gc\n","from diffusers.utils import load_image\n","from accelerate.utils import compute_module_sizes"],"metadata":{"id":"TGaoYjVdnmdk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/load_neg_embed.png\")\n","\n","pipe_sd = DiffusionPipeline.from_pretrained(\n","    'SG161222/Realistic_Vision_V6.0_B1_noVAE',\n","    torch_dtype=torch.float16,\n",")\n","# load IP-Adapter\n","pipe_sd.load_ip_adapter(\n","    'h94/IP-Adapter',\n","    subfolder='models',\n","    weight_name='ip-adapter_sd15.bin',\n",")\n","pipe_sd.set_ip_adapter_scale(0.6)\n","pipe_sd.to('cuda')\n","\n","generator = torch.Generator(device='cpu').manual_seed(111)"],"metadata":{"id":"ogVTd0zTDnCz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out_sd = pipe_sd(\n","    prompt='bear eating pizza',\n","    negative_prompt=\"wrong white balance, dark, sketches, worst quality, low quality\",\n","    ip_adapter_image=image,\n","    num_inference_steps=50,\n","    generator=generator,\n",").images[0]\n","out_sd"],"metadata":{"id":"MHwgZeHmEGi7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can check how much memory this process consumed:"],"metadata":{"id":"5AbSShU3EWKR"}},{"cell_type":"code","source":["def bytes_to_giga_bytes(bytes):\n","    return bytes / 1024 / 1024 / 1024\n","\n","print(f\"Max memory allocated: {bytes_to_giga_bytes(torch.cuda.max_memory_allocated())} GB.\")"],"metadata":{"id":"aq95M4YPEYVs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we wil reuse the same pipeline components from `StableDiffusionPipeline` in the `StableDiffusionSAGPipeline` with the `from_pipe()` method:"],"metadata":{"id":"dcyy4xn0ElrY"}},{"cell_type":"code","source":["pipe_sag = StableDiffusionSAGPipeline.from_pipe(\n","    pipe_sd,\n",")\n","\n","generator = torch.Generator(device='cpu').manual_seed(111)"],"metadata":{"id":"B7429zUBEvXT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out_sag = pipe_sag(\n","    prompt='bear eating pizza',\n","    negative_prompt='wrong white balance, dark, sketches, worst quality, low quality',\n","    ip_adapter_image=image,\n","    num_inference_steps=50,\n","    generator=generator,\n","    guidance_scale=1.0,\n","    sag_scale=0.75,\n",").images[0]\n","out_sag"],"metadata":{"id":"-d1MUJt6E4qv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Max memory allocated: {bytes_to_giga_bytes(torch.cuda.max_memory_allocated())} GB.\")"],"metadata":{"id":"bvQnszmYFKrR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see that the memory usage remains the same as before because the `StableDiffusionPipeline` and the `StableDiffusionSAGPipeline` are sharing the same pipeline components."],"metadata":{"id":"JSKX85iAFL7c"}},{"cell_type":"markdown","source":["We can animate the image with the `AnimateDiffPipeline` and also add a `MotionAdapter` module to the pipeline. For the `AnimateDiffPipeline`, we need to unload the IP-Adapter first and reload it *after* we have created our new pipeline (this only applies to the `AnimateDiffPipeline`)."],"metadata":{"id":"zV4FCTNNRXor"}},{"cell_type":"code","source":["from diffusers import AnimateDiffPipeline, MotionAdapter, DDIMScheduler\n","from diffusers.utils import export_to_gif\n","\n","pipe_sag.unload_ip_adapter()\n","adapter = MotionAdapter.from_pretrained(\n","    'guoyww/animatediff-motion-adapter-v1-5-2',\n","    torch_dtype=torch.float16,\n",")"],"metadata":{"id":"Rw5TTaqhRXPD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipe_animate = AnimateDiffPipeline.from_pipe(\n","    pipe_sd,\n","    motion_adapter=adapter,\n",")\n","pipe_animate.scheduler = DDIMScheduler.from_config(\n","    pipe_animate.scheduler.config,\n","    beta_schedule='linear'\n",")"],"metadata":{"id":"-UGLbfmqT0_A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load IP-Adapter and LoRA weights again\n","pipe_animate.load_ip_adapter(\n","    'h94/IP-Adapter',\n","    subfolder='models',\n","    weight_name='ip-adapter_sd15.bin',\n",")\n","pipe_animate.load_lora_weights(\n","    'guoyww/animatediff-motion-lora-zoom-out',\n","    adapter_name='zoom-out',\n",")\n","pipe_animate.to('cuda')"],"metadata":{"id":"nvto1PZqUCE-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generator = torch.Generator(device='cpu').manual_seed(111)\n","\n","pipe_animate.set_adapters('zoom-out', adapter_weights=0.75)\n","out = pipe_animate(\n","    prompt='bear eating pizza',\n","    num_frames=16,\n","    num_inference_steps=50,\n","    ip_adapter_image=image,\n","    generator=generator,\n",").frames[0]\n","\n","export_to_gif(out, 'out_animate.gif')"],"metadata":{"id":"R5HsnRXNUV2J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Max memory allocated: {bytes_to_giga_bytes(torch.cuda.max_memory_allocated())} GB.\")"],"metadata":{"id":"hakhAWINUpeZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Modify `from_pipe` components"],"metadata":{"id":"Zxu_pr1wUsuA"}},{"cell_type":"markdown","source":["Pipelines loaded with `from_pipe()` can be customized with different model components or methods, but it affects all other pipelines that share the same components whenever we modify the **state** of the model components.\n","\n","For example, if we call `unload_ip_adapter()` on the `StableDiffusionSAGPipeline`, we will not be able to use IP-Adapter with the `StableDiffusionPipeline` because the IP-Adapter has been removed from their shared components."],"metadata":{"id":"nIKSFn9dUv0i"}},{"cell_type":"code","source":["pipe.sag_unloaded_ip_adapter()"],"metadata":{"id":"RrLqMKpSUvdI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generator = torch.Generator(device='cpu').manual_seed(111)\n","\n","out_sd = pipe_sd(\n","    prompt='bear eating pizza',\n","    negative_prompt='wrong white balance, dark, sketches, worst quality, low quality',\n","    ip_adapter_image=image,\n","    num_inference_steps=50,\n","    generator=generator,\n",").images[0]"],"metadata":{"id":"my9Cn7kWTCD2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Safety checker"],"metadata":{"id":"2FWbqImHTY-G"}},{"cell_type":"markdown","source":["The safety checker screens the generated output against the not-safe-for-work (NSFW) content. To disable the safety checker, pass `safety_checker=None`:"],"metadata":{"id":"EAFoJt47TbWi"}},{"cell_type":"code","source":["from diffusers import DiffusionPipeline\n","\n","pipeline = DiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    safety_checker=None,\n","    use_safetensors=True,\n",")"],"metadata":{"id":"SdCgruv_TaXZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Checkpoint variants"],"metadata":{"id":"ggA1hqkoT3c6"}},{"cell_type":"markdown","source":["A **checkpoint variant** is a checkpoint whose weights are\n","* stored in a different floating point type, such as `torch.float16`, because it only requries half the bandwidth and storage to download. We cannot use this variant if we are continuing training or using a CPU.\n","* non-exponential mean averaged (EMA) weights which should not be used for inference. We should use this variant to continue finetuning a model.\n","\n","When the checkpoints have identical model structures, but they were trained on different datasets and with a different training setup, they should be stored in separate repositories.\n","\n","NOTE:\n","* `torch_dtype` specifies the floating point precision of the loaded checkpoint. If we want to save bandwidth by loading a fp16 variant, we should set `variant=\"fp16\"` and `torch_dtype=torch.float16` to *convert the weights to fp16*. Otherwise, the `fp16` weights are converted to the default fp32 precision. If we only set `torch_dtype=torch.float16`, the default fpt32 are downloaded first and then converted to fp16.\n","* `variant` specifies which files should be loaded from the repository. If we want to load a non-EMA variant of a UNet from `stable-diffusion-v1-5/stable-diffusion-v1-5`, set `variant=\"non_ema\"` to download the `non_ema` file."],"metadata":{"id":"2q-By1yLXH4c"}},{"cell_type":"code","source":["from diffusers import DiffusionPipeline\n","import torch\n","\n","# load a fp16 model\n","pipeline = DiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    variant='fp16',\n","    torch_dtype=torch.float16,\n","    use_safetensors=True,\n",")"],"metadata":{"id":"GZnw_ZhxbO30"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load a non-ema model\n","pipeline = DiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    variant='non_ema',\n","    use_safetensors=True,\n",")"],"metadata":{"id":"CrAwNZARkr53"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Use the `variant` parameter in the `DiffusionPipeline.save_pretrained()` method to save a checkpoint as a different floating point type or as a non-EMA variant.\n","\n","We should save a variant to the same folder as the original checkpoint, so we have the option of loading both from the same folder."],"metadata":{"id":"N-RIlWREk2qG"}},{"cell_type":"code","source":["from diffusers import DiffusionPipeline\n","\n","# save a fp16 model\n","pipeline.save_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    variant='fp16',\n",")"],"metadata":{"id":"mNIA9DLcT49j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save a non-ema model\n","pipeline.save_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    variant='non_ema',\n",")"],"metadata":{"id":"J2zwLB79lM5Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DiffusionPipeline explained"],"metadata":{"id":"hBXlX8NblbVt"}},{"cell_type":"markdown","source":["The `DiffusionPipeline.from_pretrained()` will\n","* download the latest version of the folder structure required for inference and cache it. If the latest folder structure is available in the local cache, `DiffusionPipeline.from_pretrained()` reuses the cache and will not redownload the files.\n","* load the cached weights into the correct pipelin class - retrieved from the `model_index.json` file - and return an instance of it.\n","\n","The pipelines' underlying folder structure corresponds directly with their class instance."],"metadata":{"id":"OFpGIgnolhTt"}},{"cell_type":"code","source":["from diffusers import DiffusionPipeline\n","\n","repo_id = 'stable-diffusion-v1-5/stable-diffusion-v1-5'\n","pipeline = DiffusionPipeline.from_pretrained(repo_id, use_safetensors=True)"],"metadata":{"id":"bkuw-N22ldWB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(pipeline)"],"metadata":{"id":"Qa4r0KTKmiuA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `StableDiffusionPipeline` instance consists of\n","* `\"feature_extractor\"`: a `CLIPImageProcessor` from HuggingFace Transformers,\n","* `\"safety_checker\"`: a component for screening against harmful content,\n","* `\"scheduler\"`: an instance of `PNDMScheduler`,\n","* `\"text_encoder\"`: a `CLIPTextModel` from HuggingFace Transformers,\n","* `\"tokenizer\"`: a `CLIPTokenizer` from HuggingFace Transformers,\n","* `\"unet\"`: an instance of `UNet2DConditionModel`,\n","* `\"vae\"`: an instance of `AutoencoderKL`."],"metadata":{"id":"qSWOy5q_mmOI"}},{"cell_type":"markdown","source":["We can access each of the components of the pipeline as an attribute to view its configuration:"],"metadata":{"id":"3bExaNxZnxVs"}},{"cell_type":"code","source":["pipeline.tokenizer"],"metadata":{"id":"d3Vk454vnwx5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Every pipeline expects a `model_index.json` file that tells the `DiffusionPipeline`:\n","* which pipeline class to load from `_class_name`,\n","* which verison of Diffusers was used to create the model in `_diffusers_version`\n","* what components from which library are stored in the subfolders (`name` corresponds to the component and subfolder name, `library` corresponds to the name of the library to load the class from, and `class` corresponds to the class name)"],"metadata":{"id":"JFWs0bosn4JW"}}]}