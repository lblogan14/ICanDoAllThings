{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMGd7BWkgvHBz6KiHSZdxCV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dTL95JkrHouh"},"outputs":[],"source":["!pip install -qU diffusers accelerate transformers huggingface_hub"]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","notebook_login()"],"metadata":{"id":"1kG51xv-Hs3J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Text or image-to-video"],"metadata":{"id":"y16Dv6vBHuSc"}},{"cell_type":"markdown","source":["## Popular models"],"metadata":{"id":"tdHd_S6SJG8w"}},{"cell_type":"markdown","source":["### CogVideoX"],"metadata":{"id":"VjfRFmPdJPlx"}},{"cell_type":"markdown","source":["CogVideoX uses a 3D Variational Autoencoder (VAE) to compress videos along the spatial and temporal dimensions."],"metadata":{"id":"Fzt7RtZTJTcM"}},{"cell_type":"code","source":["import torch\n","from diffusers import CogVideoXImageToVideoPipeline\n","from diffusers.utils import export_to_video, load_image\n","\n","pipe = CogVideoXImageToVideoPipeline.from_pretrained(\n","    'THUDM/CogVideoX-5b-I2V',\n","    torch_dtype=torch.float16,\n",")\n","\n","pipe.vae.enable_tiling()\n","pipe.vae.enable_slicing()"],"metadata":{"id":"zpfSEd-KHwJQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"A vast, shimmering ocean flows gracefully under a twilight sky, its waves undulating in a mesmerizing dance of blues and greens. The surface glints with the last rays of the setting sun, casting golden highlights that ripple across the water. Seagulls soar above, their cries blending with the gentle roar of the waves. The horizon stretches infinitely, where the ocean meets the sky in a seamless blend of hues. Close-ups reveal the intricate patterns of the waves, capturing the fluidity and dynamic beauty of the sea in motion.\"\n","image = load_image(image=\"cogvideox_rocket.png\")\n","\n","video = pipe(\n","    prompt,\n","    image=image,\n","    num_videos_per_prompt=1,\n","    num_inference_steps=50,\n","    num_frames=49,\n","    guidance_scale=6,\n","    generator=torch.Generator('cuda').manual_seed(111),\n",").frames[0]\n","\n","export_to_video(video, 'output.mp4', fps=8)"],"metadata":{"id":"7fHKWqVKJ4_9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Stable Video Diffusion"],"metadata":{"id":"2LK9ciBrKM9z"}},{"cell_type":"markdown","source":["SVD is based on the Stable Diffusion 2.1 model and it is trained on images, then low-resolution videos, and finally a smaller dataset of high-resolution videos. This model genrates a short 2-4 second video from an initial image."],"metadata":{"id":"AKo2jLtLKPzr"}},{"cell_type":"code","source":["import torch\n","from diffusers import StableVideoDiffusionPipeline\n","from diffusers.utils import load_image, export_to_video\n","\n","pipeline = StableVideoDiffusionPipeline.from_pretrained(\n","    'stabilityai/stable-video-diffusion-img2vid-xt',\n","    torch_dtype=torch.float16,\n","    variant='fp16',\n",")\n","pipeline.enable_model_cpu_offload()"],"metadata":{"id":"OKPjFc99KPCx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png\")\n","image = image.resize((1024, 576))\n","\n","generator = torch.manual_seed(111)\n","\n","frames = pipeline(\n","    image,\n","    decode_chunk_size=8,\n","    generator=generator,\n",").frames[0]\n","export_to_video(frames, 'generated.mp4', fps=7)"],"metadata":{"id":"0ccl9GAMMSav"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### I2VGen-XL"],"metadata":{"id":"fxxeIirAMi5n"}},{"cell_type":"markdown","source":["I2VGen-XL is a diffusion model that can generate higher resolution videos than SVD and it is also capable of accepting text prompts in addition to images. The model is trained with two hierarchical encoders (detail encoder and global encoder) to better capture low and high-level details in images. These learned details are used to train a video diffusion model which refines the video resolution and details in the generatred video."],"metadata":{"id":"Pm56TxWDMluT"}},{"cell_type":"code","source":["import torch\n","from diffusers import I2VGenXLPipeline\n","from diffusers.utils import export_to_gif, load_image\n","\n","pipeline = I2VGenXLPipeline.from_pretrained(\n","    'ali-vilab/i2vgen-xl',\n","    torch_dtype=torch.float16,\n","    variant='fp16',\n",")\n","pipeline.enable_model_cpu_offload()"],"metadata":{"id":"f9DhuMmnMlSQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_url = \"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/i2vgen_xl_images/img_0009.png\"\n","image = load_image(image_url).convert('RGB')\n","\n","prompt = 'Papers were floating in the air on a table in the library'\n","negative_prompt = \"Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms\"\n","generator = torch.manual_seed(111)\n","\n","frames = pipeline(\n","    prompt,\n","    negative_prompt=negative_prompt,\n","    image=image,\n","    num_inference_steps=50,\n","    guidance_scale=9.0,\n","    generator=generator,\n",").frames[0]\n","export_to_gif(frames, 'i2v.gif')"],"metadata":{"id":"zahjZp0-NQIC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### AnimateDiff"],"metadata":{"id":"xbH6XxahNq9B"}},{"cell_type":"markdown","source":["AnimateDiff is an adapter model that inserts a motion module into a pretrained diffusion model to animate an image. The adapter is trained on video clips to learn motion which is used to condition the generation process to create a video. It is faster and easier to only train the adapter and it can be loaded into most diffusion models, effectively turning them into \"video models\"."],"metadata":{"id":"QyIv6voJNtJ-"}},{"cell_type":"code","source":["# load a `MotionAdapter` first\n","import torch\n","from diffusers import AnimateDiffPipeline, DDIMScheduler, MotionAdapter\n","from diffusers.utils import export_to_gif\n","\n","adapter = MotionAdapter.from_pretrained(\n","    'guoyww/animatediff-motion-adapter-v1-5-2',\n","    torch_dtype=torch.float16,\n",")"],"metadata":{"id":"u3HEIsmSNszn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load a finetuned SD with AnimateDiffPipeline\n","pipeline = AnimateDiffPipeline.from_pretrained(\n","    'emilianJR/epiCRealism',\n","    motion_adapter=adapter,\n","    torch_dtype=torch.float16,\n",")\n","\n","scheduler = DDIMScheduler.from_pretrained(\n","    'emilianJR/epiCRealism',\n","    subfolder='scheduler',\n","    clip_sample=False,\n","    timestep_spacing='linspace',\n","    beta_schedule='linear',\n","    steps_offset=1,\n",")\n","\n","pipeline.scheduler = scheduler\n","pipeline.enable_vae_slicing()\n","pipeline.enable_model_cpu_offload()"],"metadata":{"id":"iETI6pu6OQrd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = pipeline(\n","    prompt=\"A space rocket with trails of smoke behind it launching into space from the desert, 4k, high resolution\",\n","    negative_prompt=\"bad quality, worse quality, low resolution\",\n","    num_frames=16,\n","    guidance_scale=7.5,\n","    num_inference_steps=50,\n","    generator=torch.Generator('cpu').manual_seed(111),\n",")\n","frames = output.frames[0]\n","export_to_gif(frames, 'animation.gif')"],"metadata":{"id":"tRZ7c2_yOtlx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ModelscopeT2V"],"metadata":{"id":"5glDMg7_PBSt"}},{"cell_type":"markdown","source":["ModelscopeT2V adds spatial and temporal convolutions and attention to a UNet, and it is trained on image-text and video-text datasets to enhance what it learns during training. The model takes a prompt, encodes it and creates text embeddings which are denoised by the UNet, and then decoded by a VQGAN into a video."],"metadata":{"id":"7eYXCy-QPDt1"}},{"cell_type":"code","source":["import torch\n","from diffusers import DiffusionPipeline\n","from diffusers.utils import export_to_video\n","\n","pipeline = DiffusionPipeline.from_pretrained(\n","    'damo-vilab/text-to-video-ms-1.7b',\n","    torch_dtype=torch.float16,\n","    variant='fp16',\n",")\n","pipeline.enable_model_cpu_offload()\n","pipeline.enable_vae_slicing()"],"metadata":{"id":"zNwwP7qbPDVs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = 'Confident teddy bear surfer rides the wave in the tropics'\n","video_frames = pipeline(prompt).frames[0]\n","export_to_video(video_frames, 'modelscopet2v.mp4', fps=10)"],"metadata":{"id":"qC6zU_gfQtdE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Configure model parameters"],"metadata":{"id":"D0It0uisQ5n1"}},{"cell_type":"markdown","source":["### Number of frames"],"metadata":{"id":"ozlIV-izQ8Dd"}},{"cell_type":"markdown","source":["`num_frames` determines how many video frames are generated per second. A frame is an image that is played in a sequence of other frames to create motion or a video. This affects video length because the pipeline generates a certain number of frames per second. To increase the video duration, we need to increase the `num_frames` parameter."],"metadata":{"id":"xQumV3R6Q9p0"}},{"cell_type":"code","source":["import torch\n","from diffusers import StableVideoDiffusionPipeline\n","from diffusers.utils import export_to_video, load_image\n","\n","pipeline = StableVideoDiffusionPipeline.from_pretrained(\n","    'stabilityai/stable-video-diffusion-img2vid',\n","    torch_dtype=torch.float16,\n","    variant='fp16',\n",")\n","pipeline.enable_model_cpu_offload()"],"metadata":{"id":"32C5J15OQ7qP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png\")\n","image = image.resize((1024, 576))\n","generator = torch.manual_seed(111)\n","\n","frames = pipeline(\n","    image,\n","    decode_chunk_size=8,\n","    generator=generator,\n","    num_frames=25,\n",").frames[0]\n","export_to_video(frames, 'generated.mp4', fps=7)"],"metadata":{"id":"eU-MSVy_RZXx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Guidance scale"],"metadata":{"id":"OBVE2LEZRoFh"}},{"cell_type":"markdown","source":["`guidance_scale` controls how closely aligned the generated video and text prompt or initial image is. A higher `guidance_scale` value means our generated video is more aligned with the text prompt or initial image, while a lower `guidance_scale` value means our generated video is less aligned which could give the model more \"creativity\" to interpret the conditioning input."],"metadata":{"id":"k6Hx21iMRt-5"}},{"cell_type":"code","source":["import torch\n","from diffusers import I2VGenXLPipeline\n","from diffusers.utils import export_to_gif, load_image\n","\n","pipeline = I2VGenXLPipeline.from_pretrained(\n","    'ali-vilab/i2vgen-xl',\n","    torch_dtype=torch.float16,\n","    variant='fp16',\n",")\n","pipeline.enable_model_cpu_offload()"],"metadata":{"id":"lFvIKxvHRqV5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_url = \"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/i2vgen_xl_images/img_0009.png\"\n","image = load_image(image_url).convert(\"RGB\")\n","\n","prompt = \"Papers were floating in the air on a table in the library\"\n","negative_prompt = \"Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms\"\n","generator = torch.manual_seed(0)\n","\n","frames = pipeline(\n","    prompt=prompt,\n","    image=image,\n","    num_inference_steps=50,\n","    negative_prompt=negative_prompt,\n","    guidance_scale=1.0,\n","    generator=generator\n",").frames[0]\n","export_to_gif(frames, \"i2v.gif\")"],"metadata":{"id":"ayPMjLqrTB2z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Negative prompt"],"metadata":{"id":"qpdhoen1TExg"}},{"cell_type":"markdown","source":["A negative prompt deters the model from generating things we do not want it to."],"metadata":{"id":"tnLb2MiUTG84"}},{"cell_type":"code","source":["import torch\n","from diffusers import AnimateDiffPipeline, DDIMScheduler, MotionAdapter\n","from diffusers.utils import export_to_gif\n","\n","adapter = MotionAdapter.from_pretrained(\n","    \"guoyww/animatediff-motion-adapter-v1-5-2\",\n","    torch_dtype=torch.float16\n",")\n","\n","pipeline = AnimateDiffPipeline.from_pretrained(\n","    \"emilianJR/epiCRealism\",\n","    motion_adapter=adapter,\n","    torch_dtype=torch.float16\n",")\n","scheduler = DDIMScheduler.from_pretrained(\n","    \"emilianJR/epiCRealism\",\n","    subfolder=\"scheduler\",\n","    clip_sample=False,\n","    timestep_spacing=\"linspace\",\n","    beta_schedule=\"linear\",\n","    steps_offset=1,\n",")\n","pipeline.scheduler = scheduler\n","pipeline.enable_vae_slicing()\n","pipeline.enable_model_cpu_offload()"],"metadata":{"id":"raJ3yjCsTGgi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = pipeline(\n","    prompt=\"360 camera shot of a sushi roll in a restaurant\",\n","    negative_prompt=\"Distorted, discontinuous, ugly, blurry, low resolution, motionless, static\",\n","    num_frames=16,\n","    guidance_scale=7.5,\n","    num_inference_steps=50,\n","    generator=torch.Generator(\"cpu\").manual_seed(0),\n",")\n","frames = output.frames[0]\n","export_to_gif(frames, \"animation.gif\")"],"metadata":{"id":"d34neeAdTYqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Control video generation"],"metadata":{"id":"LNtRT1U_TcUM"}},{"cell_type":"markdown","source":["### Text2Video-Zero"],"metadata":{"id":"bvmgU8u2TfNI"}},{"cell_type":"markdown","source":["Text2Video-Zero video generation can be conditioned on pose and edge images for even greater control over a subject's motion in the generated video or to preserve the identity of a subject/object in the video."],"metadata":{"id":"BueinY5rTvGn"}},{"cell_type":"markdown","source":["##### Pose control"],"metadata":{"id":"IbZhm1SJT-qB"}},{"cell_type":"code","source":["from huggingface_hub import hf_hub_download\n","from PIL import Image\n","import imageio\n","\n","filename = \"__assets__/poses_skeleton_gifs/dance1_corr.mp4\"\n","repo_id = \"PAIR/Text2Video-Zero\"\n","video_path = hf_hub_download(repo_type=\"space\", repo_id=repo_id, filename=filename)\n","\n","reader = imageio.get_reader(video_path, \"ffmpeg\")\n","frame_count = 8\n","pose_images = [Image.fromarray(reader.get_data(i)) for i in range(frame_count)]"],"metadata":{"id":"D0JvgrzyTegx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load a ControlNetModel for pose estimation and a checkpoint into the `StableDiffusionControlNetPipeline`. Then we will use the `CrossFrameAttnProcessor` for the UNet and ControlNet."],"metadata":{"id":"kkHrXJv5UJDf"}},{"cell_type":"code","source":["import torch\n","from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n","from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n","\n","model_id = 'stable-diffusion-v1-5/stable-diffusion-v1-5'\n","controlnet = ControlNetModel.from_pretrained(\n","    'llyasviel/sd-controlnet-openpose',\n","    torch_dtype=torch.float16\n",")\n","pipeline = StableDiffusionControlNetPipeline.from_pretrained(\n","    model_id,\n","    controlnet=controlnet,\n","    torch_dtype=torch.float16\n",").to('cuda')\n","\n","pipeline.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n","pipeline.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))"],"metadata":{"id":"QFfPvYFIUSuZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fix the latents for all frames, and then pass our prompt and extracted pose images to the model to generate a video."],"metadata":{"id":"iHVm5TowU945"}},{"cell_type":"code","source":["latents = torch.randn((1, 4, 64, 64), device='cuda', dtype=torch.float16).repeat(len(pose_images), 1, 1, 1)\n","\n","prompt = 'Darth Vader dancing in a dessert'\n","result = pipeline(\n","    prompt=[prompt]*len(pose_images),\n","    image=pose_images,\n","    latents=latents\n",").images\n","imageio.mimsave('video.mp4', result, fps=4)"],"metadata":{"id":"vutzMLExVMcZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Edge control"],"metadata":{"id":"2IHlQpZ7VotH"}},{"cell_type":"code","source":["from huggingface_hub import hf_hub_download\n","from PIL import Image\n","import imageio\n","\n","filename = \"__assets__/poses_skeleton_gifs/dance1_corr.mp4\"\n","repo_id = \"PAIR/Text2Video-Zero\"\n","video_path = hf_hub_download(repo_type=\"space\", repo_id=repo_id, filename=filename)\n","\n","reader = imageio.get_reader(video_path, \"ffmpeg\")\n","frame_count = 8\n","pose_images = [Image.fromarray(reader.get_data(i)) for i in range(frame_count)]"],"metadata":{"id":"c-CXmW0cVqc-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n","from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n","\n","model_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n","controlnet = ControlNetModel.from_pretrained(\n","    \"lllyasviel/sd-controlnet-canny\",\n","    torch_dtype=torch.float16\n",")\n","pipeline = StableDiffusionControlNetPipeline.from_pretrained(\n","    model_id,\n","    controlnet=controlnet,\n","    torch_dtype=torch.float16\n",").to(\"cuda\")\n","\n","pipeline.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n","pipeline.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))"],"metadata":{"id":"J1cdD79nV1Sw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["latents = torch.randn((1, 4, 64, 64), device=\"cuda\", dtype=torch.float16).repeat(len(pose_images), 1, 1, 1)\n","\n","prompt = \"Darth Vader dancing in a desert\"\n","result = pipeline(\n","    prompt=[prompt] * len(pose_images),\n","    image=pose_images,\n","    latents=latents\n",").images\n","imageio.mimsave(\"video.mp4\", result, fps=4)"],"metadata":{"id":"PxPVNnxDV-pm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### InstructPix2Pix"],"metadata":{"id":"W66nnCe_WDtc"}},{"cell_type":"markdown","source":["InstructPix2Pix allows us to use text to describe the changes we want to make to the video."],"metadata":{"id":"aeqlaVKoWF2s"}},{"cell_type":"code","source":["from huggingface_hub import hf_hub_download\n","from PIL import Image\n","import imageio\n","\n","filename = \"__assets__/pix2pix video/camel.mp4\"\n","repo_id = \"PAIR/Text2Video-Zero\"\n","video_path = hf_hub_download(repo_type=\"space\", repo_id=repo_id, filename=filename)\n","\n","reader = imageio.get_reader(video_path, 'ffmpeg')\n","frame_count = 8\n","video = [Image.fromarray(reader.get_data(i)) for i in range(frame_count)]"],"metadata":{"id":"TDEMrAxAWFNA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from diffusers import StableDiffusionInstructPix2PixPipeline\n","from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n","\n","pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n","    \"timbrooks/instruct-pix2pix\",\n","    torch_dtype=torch.float16\n",").to(\"cuda\")\n","pipeline.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=3))"],"metadata":{"id":"cP6pmK8tWbhK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"make it Van Gogh Starry Night style\"\n","result = pipeline(prompt=[prompt] * len(video), image=video).images\n","imageio.mimsave(\"edited_video.mp4\", result, fps=4)"],"metadata":{"id":"HdxTBckmWfNO"},"execution_count":null,"outputs":[]}]}