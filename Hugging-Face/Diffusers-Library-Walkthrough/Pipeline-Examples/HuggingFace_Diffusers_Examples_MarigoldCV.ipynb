{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMr98PR5okKW9TBs+4NapzZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Marigold Pipelines for Computer Vision Tasks"],"metadata":{"id":"-YcM9UDOr1U_"}},{"cell_type":"markdown","source":["**Marigold** is a diffusion-based dense prediction approach, and a set of pipelines for various comptuer vision tasks, such as monocular depth estimation.\n","\n","Each pipeline supports one CV task, which takes an input RGB image as input and produces a *prediction* of the modality of interest.\n","\n","| Pipeline | Predicted Modality |\n","| -------- | ------------------ |\n","| MarigoldDepthPipeline | Depth, Disparity |\n","| MarigoldNormalsPipeline | Surface normals |\n","\n","The official checkpoints is under the [PRES-ETH](https://huggingface.co/prs-eth/) and can be used in the official [codebase](https://github.com/prs-eth/marigold)."],"metadata":{"id":"0qPY2cxWr44i"}},{"cell_type":"markdown","source":["## Depth Prediction Quick Start"],"metadata":{"id":"stJHl2ocwryW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eE7JluOErvfQ"},"outputs":[],"source":["import torch\n","from diffusers import MarigoldDepthPipeline\n","from diffusers.utils import load_image, make_image_grid\n","\n","pipe = MarigoldDepthPipeline.from_pretrained(\n","    'prs-eth/marigold-depth-lcm-v1-0', # we use LCM ckpt here for speed\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",").to('cuda')"]},{"cell_type":"code","source":["image = load_image(\"https://marigoldmonodepth.github.io/images/einstein.jpg\")\n","depth = pipe(image)\n","\n","vis = pipe.image_processor.visualize_depth(depth.prediction)\n","depth_16bit = pipe.image_processor.export_depth_to_16bit_png(depth.prediction)\n","make_image_grid([image, vis, depth_16bit], rows=1, cols=3)"],"metadata":{"id":"zEuKn64lxDkV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `visualize_depth` function applies `matplotlib.Colormaps` to map the predicted pixel values from a single-channel `[0, 1]` depth range into an RGB image."],"metadata":{"id":"97cSQ97vxixg"}},{"cell_type":"markdown","source":["## Surface Normals Prediction Quick Start"],"metadata":{"id":"_wrr1gwtxyOK"}},{"cell_type":"code","source":["import torch\n","from diffusers import MarigoldNormalsPipeline\n","from diffusers.utils import load_image, make_image_grid\n","\n","pipe = MarigoldNormalsPipeline.from_pretrained(\n","    'prs-eth/marigold-normals-lcm-v1-0', # we use LCM ckpt here for speed\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",").to('cuda')"],"metadata":{"id":"uUEoDPNMyKMF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = diffusers.utils.load_image(\"https://marigoldmonodepth.github.io/images/einstein.jpg\")\n","normals = pipe(image)\n","\n","vis = pipe.image_processor.visualize_normals(normals.prediction)\n","make_image_grid([image, vis], rows=1, cols=2)"],"metadata":{"id":"Azw27MDwyV61"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `visualize_normals` function maps the three-dimensional prediction with pixel values in the range `[-1, 1]` into an RGB image. Conceptually, each pixel is painted according to the surface normal vector in the frame of reference, where `X` axis points right, `Y` axis points up, and `Z` axis points at the viewer."],"metadata":{"id":"mXRtgeTcyhkg"}},{"cell_type":"markdown","source":["## Speeding up inference"],"metadata":{"id":"E3IJIhpszBfq"}},{"cell_type":"markdown","source":["We have already optimized for speed by\n","* loading a LCM checkpoint,\n","* use `fp16` variant of weights and computation,\n","* perform just one denosing diffusion step.\n","\n","Internally,\n","1. the VAE encoder encodes the input image,\n","2. the UNet performs one denoising step,\n","3. the VAE decoder decodes the prediction latent into pixel space.\n","\n","Since Marigold's latent space is compatible with the base SD, it is possible to speed up the pipeline by using a lightweight replacement of the SD VAE"],"metadata":{"id":"HoGf8M5YzFoi"}},{"cell_type":"code","source":["import torch\n","from diffusers import MarigoldNormalsPipeline, AutoencoderTiny\n","from diffusers.utils import load_image, make_image_grid\n","\n","pipe = MarigoldNormalsPipeline.from_pretrained(\n","    'prs-eth/marigold-depth-lcm-v1-0', # we use LCM ckpt here for speed\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",").to('cuda')\n","\n","pipe.vae = AutoencoderTiny.from_pretrained(\n","    'madebyollin/taesd',\n","    torch_dtype=torch.float16\n",").cuda()"],"metadata":{"id":"JmYCwFDzzDa8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = load_image(\"https://marigoldmonodepth.github.io/images/einstein.jpg\")\n","depth = pipe(image)\n","\n","vis = pipe.image_processor.visualize_depth(depth.prediction)\n","make_image_grid([image, vis], rows=1, cols=2)"],"metadata":{"id":"-xrdG2T30Z2F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As suggested in the Optimization notebook, we can add `torch.compile` to squeeze extra performance:"],"metadata":{"id":"Hbi_rlnV0et4"}},{"cell_type":"code","source":["import torch\n","from diffusers import MarigoldNormalsPipeline, AutoencoderTiny\n","from diffusers.utils import load_image, make_image_grid\n","\n","pipe = MarigoldNormalsPipeline.from_pretrained(\n","    'prs-eth/marigold-depth-lcm-v1-0', # we use LCM ckpt here for speed\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",").to('cuda')\n","\n","pipe.vae = AutoencoderTiny.from_pretrained(\n","    'madebyollin/taesd',\n","    torch_dtype=torch.float16\n",").cuda()\n","\n","pipe.unet = torch.compile(\n","    pipe.unet,\n","    mode='reduce-overhead',\n","    fullgraph=True\n",")"],"metadata":{"id":"vytKtbK70ni2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = load_image(\"https://marigoldmonodepth.github.io/images/einstein.jpg\")\n","depth = pipe(image)\n","\n","vis = pipe.image_processor.visualize_depth(depth.prediction)\n","make_image_grid([image, vis], rows=1, cols=2)"],"metadata":{"id":"X0bgLLOf0tvs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Qualitative Comparison with Depth Anything"],"metadata":{"id":"p9mOM9jE0uvK"}},{"cell_type":"code","source":["# Marigold Depth\n","import torch\n","from diffusers import MarigoldDepthPipeline\n","from diffusers.utils import load_image, make_image_grid\n","\n","pipe = MarigoldDepthPipeline.from_pretrained(\n","    'prs-eth/marigold-depth-lcm-v1-0', # we use LCM ckpt here for speed\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",").to('cuda')"],"metadata":{"id":"2dm0A-xBpemC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = load_image(\"https://marigoldmonodepth.github.io/images/einstein.jpg\")\n","depth_marigold = pipe(image)\n","\n","vis = pipe.image_processor.visualize_depth(depth.prediction)"],"metadata":{"id":"sNeoMLV2peE6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Depth Anything\n","from transformers import pipeline\n","\n","pipe = pipeline(\n","    task='depth-estimation',\n","    model='LiheYoung/depth-anything-large-hf'\n",").to('cuda')"],"metadata":{"id":"yiwZRmWyp2L1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["depth_anything = pipe(image)['depth']"],"metadata":{"id":"d5-9Ih2NqLjS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["make_image_grid([image, vis, depth_anything], rows=1, cols=3)"],"metadata":{"id":"ApUp-E93qXo3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Maximizing Prediction and Ensembling"],"metadata":{"id":"1a7NFjybqbOD"}},{"cell_type":"markdown","source":["Marigold pipelines have a built-in ensembling mechanism combining multiple predictions from different random latents. This is a brute-force way of improving the precision of predictions, capitalizing on the generative nature of diffusion."],"metadata":{"id":"m44owu7DqeOu"}},{"cell_type":"code","source":["from diffusers import MarigoldNormalsPipeline\n","from diffusers.schedulers import DDIMScheduler, LCMScheduler\n","from diffusers.utils import load_image, make_image_grid\n","\n","model_path = 'prs-eth/marigold-normals-v1-0'\n","\n","model_paper_kwargs = {\n","    DDIMScheduler: {\n","        'num_inference_steps': 10,\n","        'ensemble_size': 10,\n","    },\n","    LCMScheduler: {\n","        'num_inference_steps': 4,\n","        'ensemble_size': 5\n","    }\n","}\n","\n","pipe = MarigoldNormalsPipeline.from_pretrained(model_path).to('cuda')\n","pipe_kwargs = model_paper_kwargs[type(pipe.scheduler)]"],"metadata":{"id":"aYqOxmILqdaY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = load_image(\"https://marigoldmonodepth.github.io/images/einstein.jpg\")\n","\n","normals = pipe(image, **pipe_kwargs)\n","\n","vis = pipe.image_processor.visualize_normals(normals.prediction)\n","make_image_grid([image, vis], rows=1, cols=2)"],"metadata":{"id":"S4fmzfJEryn7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Quantitative Evaluation"],"metadata":{"id":"4t6EPG-0r_v7"}},{"cell_type":"markdown","source":["To evaluate Marigold quantitatively in standard leaderboards and benchmarks (such as NYU, KITTI, and other datasets), follow the evaluation protocol:\n","* load the full precision `fp32` model and use appropriate values for `num_inference_steps` and `ensemble_size`.\n","* set up random seed for reproducibility."],"metadata":{"id":"P7Qywz8h4T17"}},{"cell_type":"code","source":["from diffusers import MarigoldDepthPipeline\n","from diffusers.schedulers import DDIMScheduler, LCMScheduler\n","from diffusers.utils import load_image, make_image_grid\n","import torch\n","\n","device = 'cuda'\n","seed = 111\n","model_path = 'prs-eth/marigold-v1-0'\n","\n","model_paper_kwargs = {\n","\tdiffusers.schedulers.DDIMScheduler: {\n","\t\t\"num_inference_steps\": 50,\n","\t\t\"ensemble_size\": 10,\n","\t},\n","\tdiffusers.schedulers.LCMScheduler: {\n","\t\t\"num_inference_steps\": 4,\n","\t\t\"ensemble_size\": 10,\n","\t},\n","}\n","\n","generator = torch.Generator(device).manual_seed(seed)\n","\n","pipe = MarigoldDepthPipeline.from_pretrained(model_path).to(device)\n","pipe_kwargs = model_paper_kwargs[type(pipe.scheduler)]"],"metadata":{"id":"4i6P9F75sBhv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = diffusers.utils.load_image(\"https://marigoldmonodepth.github.io/images/einstein.jpg\")\n","depth = pipe(image, generator=generator, **pipe_kwargs)"],"metadata":{"id":"50sd5kft5Bci"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Using Predictive Uncertainty"],"metadata":{"id":"Ke9dsdhj5FfG"}},{"cell_type":"markdown","source":["The ensembling mechanism built into Marigold pipelines combines multiple predictions obtained from different random latents. As a side effect, it can be used to quantify epistemic (model) uncertainty."],"metadata":{"id":"nMm1DMB65Zhm"}},{"cell_type":"code","source":["from diffusers import MarigoldDepthPipeline\n","from diffusers.utils import load_image, make_image_grid\n","import torch\n","\n","pipe = MarigoldDepthPipeline.from_pretrained(\n","    'prs-eth/marigold-depth-lcm-v1-0',\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",").to('cuda')"],"metadata":{"id":"n_AcEiy-5HkW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = diffusers.utils.load_image(\"https://marigoldmonodepth.github.io/images/einstein.jpg\")\n","\n","depth = pipe(\n","    image,\n","    ensemble_size=10, # any number greater than 1; higher values yield higher precision\n","    output_uncertainty=True\n",")\n","\n","uncertainty = pipe.image_processor.visualize_uncertainty(depth.uncertainty)\n","vis = pipe.image_processor.visualize_depth(depth.prediction)\n","make_image_grid([image, vis, uncertainty], rows=1, cols=3)"],"metadata":{"id":"Hbx91-5b5nBA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Higher values (white) correspod to pixels, where the model struggles to make consistent predictions."],"metadata":{"id":"L-qLbHKI523Q"}},{"cell_type":"markdown","source":["## Frame-by-frame Video Processing with Temporal Consistency"],"metadata":{"id":"237pBsdp5_33"}},{"cell_type":"markdown","source":["Due to Marigold's generative nature, each prediction is unique and defined by the random noise sampled for the latent initialization. This becomes an obvious drawback compared to traditional end-to-end dense regression networks.\n","\n","To address this issue, it is possible to pass `latents` to the pipeline, which defines the starting point of diffusion."],"metadata":{"id":"El_0uWq36JMH"}},{"cell_type":"code","source":["from diffusers import MarigoldDepthPipeline, AutoencoderTiny\n","from diffusers.utils import load_image, export_to_gif\n","import imageio\n","from PIL import Image\n","from tqdm import tqdm\n","import torch\n","\n","device = 'cuda'\n","path_in = 'obama.mp4'\n","path_out = 'obama_depth.gif'\n","\n","pipe = MarigoldDepthPipeline.from_pretrained(\n","    'prs-eth/marigold-depth-lcm-v1-0',\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",").to(device)\n","\n","pipe.vae = AutoencoderTiny.from_pretrained(\n","    'madebyollin/taesd',\n","    torch_dtype=torch.float16\n",").to(device)\n","pipe.set_progress_bar_config(disable=True)"],"metadata":{"id":"jRxc8eH-5_al"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with imageio.get_reader(path_in) as reader:\n","    size = reader.get_meta_data()['size']\n","    last_frame_latent = None\n","\n","    latent_common = torch.rand(\n","        (1, 4, 768 * size[1] // (8 * max(size)), 768 * size[0] // (8 * max(size)))\n","    ).to(device=device, dtype=torch.float16)\n","\n","    out = []\n","    for frame_id, frame in tqdm(enumerate(reader), desc=\"Processing Video\"):\n","        frame = Image.fromarray(frame)\n","        latents = latent_common\n","        if last_frame_latent is not None:\n","            latents = 0.9 * latents + 0.1 * last_frame_latent\n","\n","        depth = pipe(\n","            frame,\n","            match_input_resolution=False,\n","            latents=latents,\n","            output_latent=True\n","        )\n","        last_frame_latent = depth.latent\n","\n","        out.append(pipe.image_processor.visualize_depth(depth.prediction)[0])\n","\n","    export_to_gif(out, path_out, fps=reader.get_meta_data()['fps'])"],"metadata":{"id":"zk2mo9af67kO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The diffusion process starts from the given computed latent. The pipeline sets `output_latent=True` to access `out.latent` and computes its contribution to the next frame's latent initialization."],"metadata":{"id":"oa5ovtfE7_lx"}},{"cell_type":"markdown","source":["## Marigold for ControlNet"],"metadata":{"id":"6VB84AOC8ZSF"}},{"cell_type":"code","source":["from diffusers import MarigoldDepthPipeline, ControlNetModel, StableDiffusionXLControlNetPipeline, DPMSolverMultistepScheduler\n","from diffusers.utils import load_image, make_image_grid\n","import torch\n","\n","device = 'cuda'\n","\n","pipe = MarigoldDepthPipeline.from_pretrained(\n","    'prs-eth/marigold-depth-lcm-v1-0',\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",").to(device)\n","\n","controlnet = ControlNetModel.from_pretrained(\n","    'diffusers/controlnet-depth-sdxl-1.0',\n","    torch_dtype=torch.float16,\n","    variant='fp16',\n",").to(device)\n","\n","pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n","    'SD161222/RealVisXL_V4.0',\n","    controlnet=controlnet,\n","    torch_dtype=torch.float16,\n","    variant='fp16'\n",").to(device)\n","pipe.scheduler = DPMSolverMultistepScheduler.from_config(\n","    pipe.scheduler.config,\n","    use_karras_sigmas=True\n",")"],"metadata":{"id":"nCpTRplx8ar9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = diffusers.utils.load_image(\n","    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_depth_source.png\"\n",")\n","generator = torch.Generator(device).manual_seed(111)\n","\n","depth_image = pipe(image, generator=generator).prediction\n","depth_image = pipe.image_processor.visualize_depth(depth_image, color_map='binary')\n","\n","controlnet_out = pipe(\n","    prompt='high quality photo of a sports bike, city',\n","    negative_prompt=\"\",\n","    guidance_scale=6.5,\n","    num_inference_steps=25,\n","    image=depth_image,\n","    generator=generator,\n","    controlnet_conditioning_scale=0.7,\n","    control_guidance_end=0.7,\n",").images[0]\n","make_image_grid([image, depth_image, controlnet_out], rows=1, cols=3)"],"metadata":{"id":"oikKEimfDs27"},"execution_count":null,"outputs":[]}]}