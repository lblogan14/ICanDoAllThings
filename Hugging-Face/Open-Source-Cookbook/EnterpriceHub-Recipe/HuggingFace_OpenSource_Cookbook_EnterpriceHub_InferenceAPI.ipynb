{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMjtcsQgqJNZHaKgwpQqKeZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Serverless Inference API"],"metadata":{"id":"gbpQ5mQEbIxx"}},{"cell_type":"markdown","source":["HuggingFace provides a [Serverless Inference API](https://huggingface.co/docs/inference-providers/index) to quickly test and evaluate ML models with simple API calls."],"metadata":{"id":"GeZODmCql7wY"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"lEgkSU28mGxl"}},{"cell_type":"markdown","source":["In this example, we need a fine-grained token with\n","- `Inference > Make calls to the serverless Inference API` user permissions,\n","- read access to `meta-llama/Meta-Llama-3-8B-Instruct` and `HuggingFaceM4/idefics2-8b-chatty` repos"],"metadata":{"id":"foWutV19mKQQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"a51rANTe7qMZ"},"outputs":[],"source":["!pip install -qU huggingface_hub transformers"]},{"cell_type":"markdown","source":["## Querying the Serverless Inference API"],"metadata":{"id":"YC_F1hcwm1l0"}},{"cell_type":"markdown","source":["The Serverless Inference API exposes models on the Hub with a simple API:\n","```\n","https://api-inference.huggingface.co/models/<MODEL_ID>\n","```\n","where `<MODEL_ID>` corresponds to the name of the model repo on the Hub. For example, `codellama/CodeLlama-7b-hf` becomes `https://api-inference.huggingface.co/models/codellama/CodeLlama-7b-hf`."],"metadata":{"id":"jh1ce908m-nH"}},{"cell_type":"markdown","source":["### With an HTTP request"],"metadata":{"id":"6wDw7amQnbsz"}},{"cell_type":"markdown","source":["We can call this API with a simple `POST` request."],"metadata":{"id":"m96uR-GyqB66"}},{"cell_type":"code","source":["import requests\n","\n","API_URL = \"https://api-inference.huggingface.co/models/codellama/CodeLlama-7b-hf\"\n","HEADERS = {\"Authorization\": f\"Bearer {get_token()}\"}\n","\n","\n","def query(payload):\n","    response = requests.post(\n","        API_URL,\n","        headers=HEADERS,\n","        json=payload\n","    )\n","    return response.json()"],"metadata":{"id":"FHM4CvdCm674"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(query(\n","    payload={\n","        'inputs': \"A HTTP POST request is used to\",\n","        'parameters': {'temperature': 0.8, 'max_new_tokens': 50, 'seed': 111}\n","    }\n","))"],"metadata":{"id":"XrnfIuSxqaFm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The inference API will dynamically load the requested model onto shared compute infrastructure to serve predictions. When the model is loaded, the Serverless Inference API will use the specified `pipeline_tag` from the Model Card to determine the appropriate inference task."],"metadata":{"id":"gPb2uU0erHQK"}},{"cell_type":"markdown","source":["### With the `huggingface_hub` library"],"metadata":{"id":"XY26mIBlrQKQ"}},{"cell_type":"code","source":["from huggingface_hub import InferenceClient\n","\n","client = InferenceClient()"],"metadata":{"id":"iP8NXlR5rTC7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response = client.text_generation(\n","    prompt=\"A HTTP POST request is used to\",\n","    model=\"codellama/CodeLlama-7b-hf\",\n","    temperature=0.8,\n","    max_new_tokens=50,\n","    seed=111,\n","    return_full_text=True\n",")\n","\n","print(response)"],"metadata":{"id":"8ggpleLvrZpT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Applications"],"metadata":{"id":"vrtLT8xGriBH"}},{"cell_type":"markdown","source":["### Generating text with open LLMs"],"metadata":{"id":"Y9_06Ca22WzL"}},{"cell_type":"markdown","source":["- **Base models** - refer to plan, pre-trained language models. These models are good at continuing generation from a provided prompt. However, they have not been fine-tuned for conversational use like answering questions.\n","- **Instruction-tuned models** - trained in a multi-task manner to follow a broad range of instructions. Instruction-tuned models will produce better responses to instructions than base models. Often, these models are also fine-tuned for multi-turn chat dialogs, making them great for conversational use cases."],"metadata":{"id":"tLQUM5z62dHs"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","# define the system and user messages\n","system_input = \"You're an expert prompt engineer with artistic flair.\"\n","user_input = \"Write a concise prompt for a fun image containing a llama and a cookbook. Only return the prompt.\"\n","messages = [\n","    {'role', 'system', 'content': system_input},\n","    {'role', 'user', 'content': user_input}\n","]\n","\n","# load the tokenizer\n","model_id = 'meta-llama/Meta-Llama-3-8B-Instruct'\n","tokenizer = AutoTokenizer.from_pretrained(model_id)"],"metadata":{"id":"47DixORTri_S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# apply the chat template to the messages\n","prompt = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize=False,\n","    add_generation_prompt=True\n",")\n","print(f\"\\nPROMPT:\\n-----\\n\\n{prompt}\")"],"metadata":{"id":"YafpaqvW3Tpw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm_response = client.text_generation(\n","    prompt,\n","    model=model_id,\n","    max_new_tokens=250,\n","    seed=111\n",")"],"metadata":{"id":"oDlg16SZ3egm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Querying an LLM without adhering to the model's prompt template will not produce any outright errors but it will result in poor quality outputs."],"metadata":{"id":"RDrjJJsF3mBt"}},{"cell_type":"code","source":["out = client.text_generation(\n","    system_input + \" \" + user_input,\n","    model=model_id,\n","    max_new_tokens=250,\n","    seed=111\n",")\n","print(out)"],"metadata":{"id":"4R4omfmf3tBh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To simplify the prompting process and ensure the proper chat template is being used, the `InferenceClient` also offers a `chat_completion` method that abstracts away the `chat_template` details:"],"metadata":{"id":"JHPbbXm_32O-"}},{"cell_type":"code","source":["for token in client.chat_completion(\n","    messages,\n","    model=model_id,\n","    max_tokens=250,\n","    stream=True,\n","    seed=111,\n","):\n","    print(token.choices[0].delta.content)"],"metadata":{"id":"bL8jzEjT4Kpm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `stream=True` enables streaming text from the endpoint."],"metadata":{"id":"jnRY7Ujf4UQK"}},{"cell_type":"markdown","source":["### Creating images with Stable Diffusion"],"metadata":{"id":"f-60WwNX4YmC"}},{"cell_type":"code","source":["image = client.text_to_image(\n","    prompt=llm_response,\n","    model='stabilityai/stable-diffusion-xl-base-1.0',\n","    guidance_scale=8,\n","    seed=111\n",")"],"metadata":{"id":"Jp5H6ZK44bIt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display(image.resize((image.width // 2, image.height // 2)))\n","print(\"PROMPT: \", llm_response)"],"metadata":{"id":"3HZq_s-W5tlV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `InferenceClient` will cache API responses by default. That means if we query the API with the same payload multiple times, we will see the result returned by the API is exactly the same:"],"metadata":{"id":"OtaSvrxj5xAM"}},{"cell_type":"code","source":["image = client.text_to_image(\n","    prompt=llm_response,\n","    model='stabilityai/stable-diffusion-xl-base-1.0',\n","    guidance_scale=8,\n","    seed=111\n",")\n","\n","display(image.resize((image.width // 2, image.height // 2)))\n","print(\"PROMPT: \", llm_response)"],"metadata":{"id":"v5cl2A1G56kY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To force a different response each time, we can use a HTTP header to have the client ignore the cache and run a new generation: `x-use-cache: 0`."],"metadata":{"id":"-yhZRrSk5-__"}},{"cell_type":"code","source":["# turn caching off\n","client.headers['x-use-cache'] = \"0\"\n","\n","image = client.text_to_image(\n","    prompt=llm_response,\n","    model='stabilityai/stable-diffusion-xl-base-1.0',\n","    guidance_scale=8,\n","    seed=111\n",")\n","\n","display(image.resize((image.width // 2, image.height // 2)))\n","print(\"PROMPT: \", llm_response)"],"metadata":{"id":"d8YYRKE06GPK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Reasoning over images with Idefics2"],"metadata":{"id":"iuI7wEYe6M9X"}},{"cell_type":"markdown","source":["Vision Language Models (VLMs) can take both text and images as input simultaneously and produce text as output. This allows them to tackle many tasks from visual question answering to image captioning.\n","\n","For images, we first need to convert our PIL image to a `base64` encoded string so that we can send it to the model over the network."],"metadata":{"id":"cxqmI4p66PjI"}},{"cell_type":"code","source":["import base64\n","from io import BytesIO\n","\n","def pil_image_to_base64(image):\n","    buffered = BytesIO()\n","    image.save(buffered, format='JPEG')\n","    img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')\n","    return img_str\n","\n","\n","img_b64 = pil_image_to_base64(image)"],"metadata":{"id":"pCqGNz8E6PMj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then we need to properly format our text and image prompt using a chat template."],"metadata":{"id":"5rH4oDqH7Mzj"}},{"cell_type":"code","source":["from transfomrers import AutoProcessor\n","\n","# load the processor\n","vlm_model_id = 'HuggingFaceM4/idefics2-8b-chatty'\n","processor = AutoProcessor.from_pretrained(vlm_model_id)\n","\n","# define the user message\n","messages = [\n","    {\n","        'role': 'user',\n","        'content': [\n","            {'type': 'image'},\n","            {'type': 'text', 'text': \"Write a short limerick about this image.\"}\n","        ]\n","    }\n","]\n","\n","# apply the chat template to the messages\n","prompt = processor.apply_chat_template(\n","    messages,\n","    add_generation_prompt=True\n",")\n","\n","# add the base64 encdoed image to the prompt\n","image_input = f\"data:image/jpeg;base64,{imgb64}\"\n","image_input = f\"![]({image_input})\"\n","prompt = prompt.replace(\"<image>\", image_input)"],"metadata":{"id":"6oSSIZob7QVB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["limerick = client.text_generation(\n","    prompt,\n","    model=vlm_model_id,\n","    max_new_tokens=200,\n","    seed=111\n",")\n","print(limerick)"],"metadata":{"id":"ODFvaveS79W6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generating speech from text"],"metadata":{"id":"ZNrFqzxu8BbZ"}},{"cell_type":"code","source":["tts_model_id = 'suno/bark'\n","speech_out = client.text_to_speech(\n","    text=limerick,\n","    model=tts_model_id\n",")"],"metadata":{"id":"8Wt6c7hr8DQn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import Audio\n","\n","display(Audio(speech_out, rate=24000))\n","print(limerick)"],"metadata":{"id":"N57lB78K8K43"},"execution_count":null,"outputs":[]}]}