{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMHeed30+lRjB8mZ5adcp9O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Quantization"],"metadata":{"id":"7RyLP1telA5N"}},{"cell_type":"markdown","source":["**Quantization** techniques focus on representing data with less information while also trying to not lose too much accuracy. This often means converting a data type to represent the same information with fewer bits."],"metadata":{"id":"8Txsban7lDCy"}},{"cell_type":"markdown","source":["# bitsandbytes"],"metadata":{"id":"eenVMOZ4lQ0G"}},{"cell_type":"markdown","source":["**bitsandbytes** is the easiest option for quantizing a model to 8- and 4ibit.\n","\n","**8-bit quantization** multiplies outliers in fp16 with non-outliers in int8, converts the non-outlier values back to fp16, and then adds them together to return the weights in fp16."],"metadata":{"id":"vBaJa84HlTMC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3yHjGlVck9Jm"},"outputs":[],"source":["!pip install -qU transformers accelerate bitsandbytes"]},{"cell_type":"markdown","source":["Quantizing a model in 8-bit havles the memory-usage, and for large models, set `device_map='auto'` to efficiently use the GPUs available:"],"metadata":{"id":"JBr0c5oOI3tk"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n","\n","quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n","\n","model_8bit = AutoModelForCausalLM.from_pretrained(\n","    'bigscience/bloom-1b7',\n","    quantization_config=quantization_config,\n","    device_map='auto'\n",")\n","tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")"],"metadata":{"id":"Rj1yezdhJCCZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["By default, all the other modules such as `torch.nn.LayerNorm` are converted to `torch.float16`. We can change the data type of these modules with the `torch_dtype` parameter if we want. Setting `torch_dtype='auto'` loads the model in the data type defined in a model's `config.json` file."],"metadata":{"id":"Xew_m2U2JT4b"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n","\n","quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n","\n","model_8bit = AutoModelForCausalLM.from_pretrained(\n","    'facebook/opt-350m',\n","    quantization_config=quantization_config,\n","    device_map='auto',\n","    torch_dtype='auto'\n",")"],"metadata":{"id":"n8dxGpLmJkeg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_8bit.model.decoder.layers[-1].final_layer_norm.weight.dtype"],"metadata":{"id":"EWHnmXPYJxgE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can push the quantized model to the Hub:"],"metadata":{"id":"xZxe-yQjJ7qO"}},{"cell_type":"code","source":["model_8bit.push_to_hub('bloom-350m-8bit')"],"metadata":{"id":"KzWXLd7LKGcu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can check our memory footprint:"],"metadata":{"id":"aDMQbEvRKMC6"}},{"cell_type":"code","source":["model_8bit.get_memory_footprint()"],"metadata":{"id":"B_h1B4CgKNtG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Quantized models can be loaded from the `from_pretrained()` method without needing to specify the `load_in_8bit` or `load_in_4bit` parameters:"],"metadata":{"id":"ql7vnYaQKSiO"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","model = AutoModelForCausalLM.from_pretrained('bloom-350m-8bit')\n","tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")"],"metadata":{"id":"h8H2f0RIKYtv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Quantizing a model in 4-bit follows the same fashion and reduces our memory usage by 4x."],"metadata":{"id":"bRdTOEc1Kej9"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n","\n","quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n","\n","model_4bit = AutoModelForCausalLM.from_pretrained(\n","    'bigscience/bloom-1b7',\n","    quantization_config=quantization_config,\n","    device_map='auto'\n",")\n","tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")"],"metadata":{"id":"-vtBQYKuKkz7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n","\n","quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n","\n","model_4bit = AutoModelForCausalLM.from_pretrained(\n","    'facebook/opt-350m',\n","    quantization_config=quantization_config,\n","    device_map='auto',\n","    torch_dtype='auto'\n",")"],"metadata":{"id":"THUu4R6rKomp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_4bit.model.decoder.layers[-1].final_layer_norm.weight.dtype"],"metadata":{"id":"DnvrCUR_KuNJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_4bit.get_memory_footprint()"],"metadata":{"id":"Xp0wCcUcK7Vr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 8-bit - LLM.int8() algorithm"],"metadata":{"id":"b8YLLoc-K-DS"}},{"cell_type":"markdown","source":["### Offloading"],"metadata":{"id":"ZQv7IJrmLHA9"}},{"cell_type":"markdown","source":["8-bit models can offload weihts between the CPU and GPU to support fitting very large models into memory. The weights dispatched to the CPU are actually stored in **float32**, and are not converted to 8-bit.\n","\n","For example, to enable offloading for the `bigscience/bloom-1b7` model, we start by creating a `BitsAndBytesConfig`:"],"metadata":{"id":"vQ3JskN9LIeW"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n","\n","quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)"],"metadata":{"id":"hsf3ef9ZLB2t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we need to design a custom device map to fit everything on our GPU except for the `lm_head`, which we will dispatch to the CPU:"],"metadata":{"id":"mqM7VCeiLf-r"}},{"cell_type":"code","source":["device_map = {\n","    'transformer.word_embeddings': 0,\n","    'transformer.word_embeddings_layernorm': 0,\n","    'lm_head': 'cpu',\n","    'transformer.h': 0,\n","    'transformer.ln_f': 0,\n","}"],"metadata":{"id":"HIeTbh8_Lllu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we load our model with the custom `device_map` and `quantization_config`:"],"metadata":{"id":"RpNxi9TkLvYl"}},{"cell_type":"code","source":["model_8bit = AutoModelForCausalLM.from_pretrained(\n","    'bigscience/bloom-1b7',\n","    torch_dtype='auto',\n","    device_map=device_map,\n","    quantization_config=quantization_config\n",")"],"metadata":{"id":"v47rDJ1sLyqf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Outlier threshold"],"metadata":{"id":"Jxxrh7ulL5dG"}},{"cell_type":"markdown","source":["An **\"outlier\"** is a hidden state value greater than a certain threshold, and these values are computed in fp16. While the values are usually normally distributed ([-3.5, 3.5]), this distribution can be very different for large models ([-60, 6] or [6, 60]). 8-bit quantization works well for values ~5, but beyond that, there is a significant performance penalty. A good threshold value is 6, but a lower threshold may be needed for more unstable models (small models or finetuning)\n","\n","We can experiment with `llm_int8_threshold` to find the best threshold for our model:"],"metadata":{"id":"PH91UwWPo9YU"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n","\n","model_id = 'bigscience/bloom-1b7'\n","\n","quantization_config = BitsAndBytesConfig(\n","    llm_int8_threshold=10.0,\n","    llm_int8_enable_fp32_cpu_offload=True\n",")\n","\n","model_8bit = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    torch_dtype='auto',\n","    device_map=device_map,\n","    quantization_config=quantization_config\n",")"],"metadata":{"id":"EfvOW_fjL7Ra"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Skip module conversion"],"metadata":{"id":"xzSabytFp4vR"}},{"cell_type":"markdown","source":["For some models, like `Jukebox`, we do not need to quantize every module to 8-bit which can cause instability. With `Jukebox`, there are several `lm_head` modules that should be skipped using the `llm_int8_skip_modules`:"],"metadata":{"id":"yCrwwvH3p74W"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTOkenizer, BitsAndBytesConfig\n","\n","model_id = 'bigscience/bloom-1b7'\n","\n","quantization_config = BitsAndBytesConfig(\n","    llm_int8_skip_modules=['lm_head']\n",")\n","\n","model_8bit = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    torch_dtype='auto',\n","    device_map=device_map,\n","    quantization_config=quantization_config\n",")"],"metadata":{"id":"RS7IVG1lp6kp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4-bit (QLoRA algorithm)"],"metadata":{"id":"0hmF5xfNqYWD"}},{"cell_type":"markdown","source":["### Compute data type"],"metadata":{"id":"DgaH9oiuqeWr"}},{"cell_type":"markdown","source":["To speed up computation, we can change the data type from float32 (the default value) to bf16 using the `bnb_4bit_compute_dtype`:"],"metadata":{"id":"qjJZ8UBjqgxi"}},{"cell_type":"code","source":["import torch\n","from transformers import BitsAndBytesConfig\n","\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")"],"metadata":{"id":"w4qYET-uqc1v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Normal Float 4 (NF4)"],"metadata":{"id":"z514KBLFqztQ"}},{"cell_type":"markdown","source":["NF4 is a 4-bit data type from **QLoRA** paper, adapted for weights initialized from a normal distribution. We should use NF4 for training 4-bit base models."],"metadata":{"id":"cn9TtsXVq3D8"}},{"cell_type":"code","source":["from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n","\n","model_id = 'bigscience/bloom-1b7'\n","\n","nf4_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type='nf4'\n",")\n","\n","model_nf4 = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    torch_dtype='auto',\n","    quantization_config=nf4_config\n",")"],"metadata":{"id":"0UJRA-70q2pQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For inference, the `bnb_4bit_quant_type` does not have a huge impact on performance. However, to remain consistent with the model weights, we should use the `bnb_4bit_compute_dtype` and `torch_dtype` values."],"metadata":{"id":"vbWExEOMrWYA"}},{"cell_type":"markdown","source":["### Nested quantization"],"metadata":{"id":"QSx4-2zFrho7"}},{"cell_type":"markdown","source":["Nested quantization is a technique that can save additional memory at no additional performance cost. This feature performs a second quantization of the already quantized weights to save an additional 0.4 bits/parameter.\n","\n","With nested quantization, we can finetune a `llama-13b` model on a 16GB T4 GPU with a sequence length of 1024, a batch size of 1, and enabling gradient accumulation with 4 steps."],"metadata":{"id":"BsfpATQSrkg9"}},{"cell_type":"code","source":["from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n","\n","model_id = 'meta-llama/Llama-2-13b-chat-hf'\n","\n","double_quant_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True\n",")\n","\n","model_double_quant = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    torch_dtype='auto',\n","    quantization_config=double_quant_config\n",")"],"metadata":{"id":"5gqGAA8arhBL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dequantizing bitsandbytes models"],"metadata":{"id":"i0bCU1bosJ9B"}},{"cell_type":"markdown","source":["Once quantized, we can dequantize the model to the original precision but this may result in a small quality loss of the model."],"metadata":{"id":"oOCdPdHusM4H"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","\n","model_id = 'facebook/opt-125m'\n","\n","quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    quantization_config=quantization_config\n",").to('cuda:0')\n","\n","model.dequantize()"],"metadata":{"id":"M1t_Zvx6sMc_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = tokenizer(\"Hello, my name is\", return_tensors='pt').to('cuda:0')\n","\n","out = model.generate(**text)\n","tokenizer.decode(out[0])"],"metadata":{"id":"xRqssuBRsqdu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# GPTQ"],"metadata":{"id":"OibZAh4Ltc8O"}},{"cell_type":"markdown","source":["Both **GPTQModel** and **AutoGPTQ** libraries implement the GPTQ algorithm, *a post-training quantization technique where each row of the weight matrix is quantized indepedently to find a version of the weights that minimizes error*. These weights are quantized to in4, stored as int32 (int4 x 8) and dequantized (restored) to fp16 on the fly during inference.\n","\n","This can save memory by almost 4x because the int4 weights are often dequantized in a fused kernel.\n","\n"],"metadata":{"id":"gtYCp6TttjbR"}},{"cell_type":"code","source":["!pip install -qU accelerate optimum transformers gptqmodel auto-gptq"],"metadata":{"id":"vhtdJo_2teBT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To quantize a model, we need to create a `GPTQConfig` class and set the number of bits to quantize to, a dataset to calibrate the weights for quantization, and a tokenizer to prepare the dataset."],"metadata":{"id":"CR_PFigpuZ9N"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n","\n","model_id = 'facebook/opt-125m'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","gptq_config = GPTQConfig(\n","    bits=4,\n","    dataset='c4',\n","    tokenizer=tokenizer\n",")"],"metadata":{"id":"BgVoAvzhwfhM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We could also pass our own dataset as a list of strings, but it is highly recomended to use the same dataset from the GPTQ paper."],"metadata":{"id":"cG1Um1qkwslD"}},{"cell_type":"code","source":["dataset = [\"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"]\n","gptq_config = GPTQConfig(\n","    bits=4,\n","    dataset=dataset,\n","    tokenizer=tokenizer\n",")"],"metadata":{"id":"jK6Ztjg9w05x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can load a model to quantize and pass the `gptq_config` to the `from_pretrained()` method."],"metadata":{"id":"J1-pssU3w5fh"}},{"cell_type":"code","source":["quantized_model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    device_map='auto',\n","    quantization_config=gptq_config\n",")"],"metadata":{"id":"o7xnKq9e0U68"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If we run out of memory because a dataset is too large, disk offloading is not supported. We can try passing the `max_memory` to allocate the amount of memory to use on our device:"],"metadata":{"id":"nXpzV95R0awN"}},{"cell_type":"code","source":["quantized_model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    device_map='auto',\n","    quantization_config=gptq_config,\n","    max_memory={\n","        0: \"12GiB\",\n","        1: \"16GiB\",\n","        'cpu': \"32GiB\",\n","        # assume we have 2 gpus and a cpu\n","    }\n",")"],"metadata":{"id":"OIUZFApO0k4K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once our model is quantized, we can push the model and tokenizer to the Hub"],"metadata":{"id":"pfXbPFh302R-"}},{"cell_type":"code","source":["quantized_model.push_to_hub('opt-125m-gptq')\n","tokenizer.push_to_hub('opt-125m-gptq')"],"metadata":{"id":"2QMgu93x06qF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also save our quantized model locally:"],"metadata":{"id":"ZMm22YEi08et"}},{"cell_type":"code","source":["quantized_model.save_pretrained('opt-125m-gptq')\n","tokenizer.save_pretrained('opt-125m-gptq')\n","\n","# if quantized with device_map set\n","quantized_model.to('cpu')\n","quantized_model.save_pretrained('opt-125m-gptq')\n","tokenizer.save_pretrained('opt-125m-gptq')"],"metadata":{"id":"UGbxOCvo1AsT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can reload a quantized model and set `device_map=\"auto\"` to automatically distribute the model on all available GPUs to load the model faster without using more memory than needed:"],"metadata":{"id":"x_UC5xkf1Itg"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    'opt-125m-gptq',\n","    device_map='auto'\n",")"],"metadata":{"id":"2yXPqFom1U1n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Marlin"],"metadata":{"id":"xb1SNtc11eXj"}},{"cell_type":"markdown","source":["**Marlin** is a 4-bit only CUDA GPTQ kernel, highly optimized for the NVIDIA A100 GPU (Ampere) architecture.\n","\n","Marlin is only available for quantized inference and does not support model quantization."],"metadata":{"id":"5xUTGVQ93TCL"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, GPTQConfig\n","\n","gptq_config = GPTQConfig(\n","    bits=4,\n","    backend='marlin'\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    'opt-125m-gptq',\n","    device_map='auto',\n","    quantization_config=gptq_config\n",")"],"metadata":{"id":"qSWfLV491fNm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ExLlama"],"metadata":{"id":"X5AEkT_o3xBk"}},{"cell_type":"markdown","source":["**ExLlama** is a CUDA implmentation of the Llama model that is designed for faster inference with 4-bit GPTQ weights.\n","\n","To boost inference speed even further, we can use the **ExLlamaV2** kernel by configuring the `exllama_config`:"],"metadata":{"id":"dF87xAn53zIt"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForCausalLM, GPTQConfig\n","\n","gptq_config = GPTQConfig(\n","    bits=4,\n","    exllama_config={'version': 2}\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    'opt-125m-gptq',\n","    device_map='auto',\n","    quantization_config=gptq_config\n",")"],"metadata":{"id":"57vvofxF3yqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Only 4-bit models are supported. If we are finetuning a quantized model with PEFT, we should deactivate the ExLlama kernels.\n","\n","The ExLlama kernels are only supported when the entire model is on the GPU. If we are doing inference on a CPU with AutoGPTQ or GPTQModel, then we will need to disable the ExLlama kernel."],"metadata":{"id":"26700I-S4dCU"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForCausalLM, GPTQConfig\n","\n","gptq_config = GPTQConfig(\n","    bits=4,\n","    use_exllama=False\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    'opt-125m-gptq',\n","    device_map='auto',\n","    quantization_config=gptq_config\n",")"],"metadata":{"id":"inoK9FCA4u57"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# AWQ"],"metadata":{"id":"cOEU6jBe45NM"}},{"cell_type":"markdown","source":["**Activation-aware Weight Quantization (AWQ)** does not quantize all the weights in a model, and instead, it preserves a small percentage of weights that are important for LLM performance. This significantly reduces quantization loss such that we can run models in 4-bit precision without experiencing any performance degradation.\n","\n","There are several libraries for quantizing models wih the AWQ algorithm, such as `llm-awq`, `autoawq`, or `optimum-intel`.\n"],"metadata":{"id":"q7lY3zwN46zf"}},{"cell_type":"code","source":["!pip install -qU autoawq transformers"],"metadata":{"id":"eHHH9DGk46XU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["AWQ-quantized models can be identified by checking the `quantization_config` attribute in the model's `config.json`:\n","```yaml\n","{\n","  \"_name_or_path\": \"/workspace/process/huggingfaceh4_zephyr-7b-alpha/source\",\n","  \"architectures\": [\n","    \"MistralForCausalLM\"\n","  ],\n","  ...\n","  ...\n","  ...\n","  \"quantization_config\": {\n","    \"quant_method\": \"awq\",\n","    \"zero_point\": true,\n","    \"group_size\": 128,\n","    \"bits\": 4,\n","    \"version\": \"gemm\"\n","  }\n","}\n","```\n","\n","A quantized model is loaded with the `from_pretrained()` method. If we load our model on the CPU, make sure to move it to a GPU device first."],"metadata":{"id":"FWXZYmax83A3"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","model_id = 'TheBloke/zephyr-7B-alpha-AWQ'\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    device_map='auto'\n",")"],"metadata":{"id":"MSCVso5e9vZb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Loading an AWQ-quantized model automatically sets other weights to fp16 by default for performance reasons. If we want to load these other weights in a different format, we need to use the `torch_dtype`:"],"metadata":{"id":"I9OF-ohR-I2f"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","\n","model_id = 'TheBloke/zephyr-7B-alpha-AWQ'\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    torch_dtype=torch.float32\n",")"],"metadata":{"id":"dG-R7TFIBLhY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["AWQ quantization can also be combined with **FlashAttention-2** to further accelerate inference:"],"metadata":{"id":"BOEussOA2Hh0"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    'TheBloke/zephyr-7B-alpha-AWQ',\n","    attn_implementation='flash_attention_2',\n","    device_map='cuda:0'\n",")"],"metadata":{"id":"QzwbiyJ22Qkq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fused modules"],"metadata":{"id":"l3KUiOJE2cqf"}},{"cell_type":"markdown","source":["**Fused modules** offer improved accuracy and performance and it is supported out-of-the-box for AWQ modules for Llama and Mistral architectures, and we can also fuse AWQ modules for unsupported architectures.\n","\n","**Fused modules cannot be combined with other optimization techniques such as FlashAttention-2**.\n","\n","To enable fused modules for supported architectures, we need to create an `AwqConfig` and set `fuse_max_seq_len` and `do_fuse=True`. The `fuse_max_seq_len` is the total sequence length and it should include the context length and the expected generation length.\n","\n","For example, to fuse the AWQ modules of the `TheBloke/Mistral-7B-OpenOrca-AWQ`:"],"metadata":{"id":"THbe3Qpt2eVI"}},{"cell_type":"code","source":["import torch\n","from transformers import AwqConfig, AutoModelForCausalLM\n","\n","model_id = 'TheBloke/Mistral-7B-OpenOrca-AWQ'\n","\n","quantization_config = AwqConfig(\n","    bits=4,\n","    fuse_max_seq_len=512,\n","    du_fuse=True\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    quantization_config=quantization_config\n",").to('cuda:0')"],"metadata":{"id":"YqCRQBmC2d39"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ExLlama-v2 support"],"metadata":{"id":"1jg7FCPj3fJo"}},{"cell_type":"markdown","source":["Newer versions of `autoawq` supports ExLlama-v2 kernels for faster prefill and decoding."],"metadata":{"id":"TEWjL48V3ujT"}},{"cell_type":"code","source":["!pip install git+https://github.com/casper-hansen/AutoAWQ.git"],"metadata":{"id":"XISU3Eus3hYX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\n","\n","quantization_config = AwqConfig(version='exllama')\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    'TheBloke/Mistral-7B-Instruct-v0.1-AWQ',\n","    quantization_config=quantization_config,\n","    device_map='auto'\n",")\n","tokenizer = AutoTokenizer.from_pretrained('TheBloke/Mistral-7B-Instruct-v0.1-AWQ')"],"metadata":{"id":"E2Hwiiqf3ncI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids = tokenizer.encode(\n","    \"How to make a cake\",\n","    return_tensors=\"pt\"\n",").to(model.device)\n","output = model.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=50,\n","    pad_token_id=50256\n",")\n","\n","tokenizer.decode(output[0], skip_special_tokens=True)"],"metadata":{"id":"EYMtm-U939Ss"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Intel CPU/GPU support"],"metadata":{"id":"9SHHIL7X4NGZ"}},{"cell_type":"markdown","source":["Newer version of `autoawq` supports Intel CPU/GPU with IPEX op optimizations."],"metadata":{"id":"ubEXHPSM4Q_3"}},{"cell_type":"code","source":["pip install intel-extension-for-pytorch # for IPEX-GPU refer to https://intel.github.io/intel-extension-for-pytorch/xpu/2.5.10+xpu/\n","pip install git+https://github.com/casper-hansen/AutoAWQ.git"],"metadata":{"id":"2cBg2TK74OgE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\n","\n","device = \"cpu\" # set to \"xpu\" for Intel GPU\n","quantization_config = AwqConfig(version=\"ipex\")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"TheBloke/TinyLlama-1.1B-Chat-v0.3-AWQ\",\n","    quantization_config=quantization_config,\n","    device_map=device,\n",")\n","tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/TinyLlama-1.1B-Chat-v0.3-AWQ\")"],"metadata":{"id":"4ELz7zZK4Wlp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids = tokenizer.encode(\n","    \"How to make a cake\",\n","    return_tensors=\"pt\"\n",").to(device)\n","pad_token_id = tokenizer.eos_token_id\n","output = model.generate(input_ids, do_sample=True, max_length=50, pad_token_id=pad_token_id)\n","tokenizer.decode(output[0], skip_special_tokens=True)"],"metadata":{"id":"r0O_l8Wm4agf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# AQLM"],"metadata":{"id":"kmadosqK4f8k"}},{"cell_type":"markdown","source":["**Additive Quantization of Language Models (AQLM)** is a Large Language Models compression method. It quantizes multiple weights together and takes advantage of interdependecies between them. AQLM represents groups of 8-16 weights as a sum of multiple vector codes."],"metadata":{"id":"ro_KyZZS4pDB"}},{"cell_type":"code","source":["!pip install -qU aqlm[gpu,cpu]"],"metadata":{"id":"49V2zPfu4l4O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This library provides efficient kernels for both GPU and CPU inference and training.\n","\n","To run AQLM models:"],"metadata":{"id":"1mK_20Ls48xm"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","model_id = 'ISTA-DASLab/Mixtral-8x7b-AQLM-2Bit-1x16-hf'\n","\n","quantized_model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    torch_dtype='auto',\n","    device_map='auto'\n",")\n","tokenizer = AutoTokenizer.from_pretrained(model_id)"],"metadata":{"id":"cZkOuq_N5G17"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can check the detailed instruction on how to quantize models on their official GitHub repository."],"metadata":{"id":"cnM-O7gt5aQk"}},{"cell_type":"markdown","source":["# VPTQ"],"metadata":{"id":"2UXGjDFa5hRI"}},{"cell_type":"markdown","source":["**Vector Post-Training Quantization (VPTQ)** is a novel Post-Training Quantization method that leverages Vector Quantization to high accuracy on LLMs at an extremely low bit-width (<2-bit).\n","\n","VPTQ can compress 70B, even the 405B model, to 1-2bits without retraining and maintain high accuracy:\n","* Better accuracy on 1-2bits\n","* Lightweight quantization algorithm: only cost ~17 hours to quantize 405B Llama-3.1\n","* Agile quantization inference: low decode overhead, best throughput, and TTFT (Time to First Token; TTFT measures the speed from the time when a user sends a query to when the user gets the first response.)"],"metadata":{"id":"7SAB77ZQ5jDu"}},{"cell_type":"code","source":["!pip install -qU vptq"],"metadata":{"id":"08Wt30xh5iID"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To run VPTQ models,"],"metadata":{"id":"13Ygeu-M6wq2"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","model_id = 'VPTQ-community/Meta-Llama-3.1-70B-Instruct-v16-k65536-65536-woft'\n","\n","quantized_model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    torch_dtype='auto',\n","    device_map='auto'\n",")\n","tokenizer = AutoTokenizer.from_pretrained(model_id)"],"metadata":{"id":"OZc9yLNO6x-0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids = tokenizer(\"hello, how are you\", return_tensors=\"pt\").to(\"cuda\")\n","out = model.generate(**input_ids, max_new_tokens=32, do_sample=False)\n","tokenizer.decode(out[0], skip_special_tokens=True)"],"metadata":{"id":"s70PVu487FTl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# SpQR"],"metadata":{"id":"z3pvfwO57RSH"}},{"cell_type":"markdown","source":["**Sparse-Quantized Representation (SpQR)** involves a 16x16 tiled bi-level group 3-bit quantization structure, with sparse outlier. The details are in the paper *SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression*.\n","\n","To run a SpQR-quantized model,"],"metadata":{"id":"RRoMS5hF7W-B"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","model_id = 'elvircrn/Llama-2-7b-SPQR-3Bit-16x16-red_pajama-hf'\n","\n","quantized_model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    torch_dtype=torch.half,\n","    device_map='auto'\n",")\n","tokenizer = AutoTokenizer.from_pretrained(model_id)"],"metadata":{"id":"ML2l8DkX7S4o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids = tokenizer(\"hello, how are you\", return_tensors=\"pt\").to(\"cuda\")\n","out = model.generate(**input_ids, do_sample=False)\n","tokenizer.decode(out[0], skip_special_tokens=True)"],"metadata":{"id":"X0SWyOn-79sW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Optimum-quanto"],"metadata":{"id":"hjht7mu18IJZ"}},{"cell_type":"markdown","source":["**HuggingFace optimum-quanto** Library."],"metadata":{"id":"heJffZj-8K3n"}},{"cell_type":"code","source":["!pip install -qU optimum-quanto accelerate transformers"],"metadata":{"id":"1VXHMxRo8KC4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can quantize a model by passing `QuantoConfig` object in the `from_pretrained()` method. This works for any model in any modality, as long as it contains `torch.nn.Linear` layers.\n","\n","The `optimum-quanto` library does not only integrate the weights quantization (already in `transformers`), but also support more complex use case such as activation quantization, calibration and quantization aware training.\n","\n","By default, the weights are loaded in full precision (`torch.float32`) regardless of the actual data type. We can set the `torch_dtype=\"auto\"` to load the weights in the data type defined in a model's `config.json` file to automatically load the most memory-optimal data type."],"metadata":{"id":"OWwFbG7kS1Wa"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig\n","\n","model_id = 'facebook/opt-125m'\n","\n","quantization_config = QuantoConfig(weights='int8')\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","quantized_model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    torch_dtype='auto',\n","    device_map='cuda:0',\n","    quantization_config=quantization_config,\n",")"],"metadata":{"id":"haSdH_kNUBmc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# EETQ"],"metadata":{"id":"TDkS3E5FUbjv"}},{"cell_type":"markdown","source":["The [**EETQ**](https://github.com/NetEase-FuXi/EETQ) supports int8 per-channel weight-only quantization for NVIDIA GPUs. The high-performance GEMM and GEMV kernels are from FasterTransformer and TensorRT-LLM. It requires no calibration dataset and does not need to pre-quantize our model."],"metadata":{"id":"hwFW6AT_UdV1"}},{"cell_type":"code","source":["!pip install --no-cache-dir https://github.com/NetEase-FuXi/EETQ/releases/download/v1.0.0/EETQ-1.0.0+cu121+torch2.1.2-cp310-cp310-linux_x86_64.whl"],"metadata":{"id":"XI1Sb5wuUcgD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["An unquantized model can be quantized via `from_pretrained()` method:"],"metadata":{"id":"tS5AZ6IvY8n-"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, EetqConfig\n","\n","model_id = 'facebook/opt-125m'\n","\n","quantization_config = EetqConfig('int8')\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    device_map='auto',\n","    quantization_config=quantization_config\n",")"],"metadata":{"id":"8L-GPU84ZAcf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# HIGGS"],"metadata":{"id":"Ubds0tqeZReI"}},{"cell_type":"markdown","source":["**HIGGS** is a zero-shot quantization algorithm that combines Hadamard preprocessing with MSE-Optimal quantization grids to achieve lower quantization error and SOTA performance."],"metadata":{"id":"rPme_0gaZTCf"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer, HiggsConfig\n","\n","model_id = 'google/gemma-2-9b-it'\n","\n","quantization_config = HiggsConfig(bits=4)\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    quantization_config=quantization_config,\n","    device_map='auto'\n",")"],"metadata":{"id":"qwc_m2LmZSVd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.decode(model.generate(\n","    **tokenizer(\"Hi,\", return_tensors=\"pt\").to(model.device),\n","    temperature=0.5,\n","    top_p=0.80,\n",")[0])"],"metadata":{"id":"R5aw0F5qZr1z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# HQQ"],"metadata":{"id":"VMbH4twvZu2e"}},{"cell_type":"markdown","source":["**Half-Quadratic Quantization (HQQ)** implements on-the-fly quantization via fast robust optimization. It does not require calibration data and can be used to quantize any model."],"metadata":{"id":"ga75_Ee4ZwR8"}},{"cell_type":"code","source":["!pip install -qU hqq"],"metadata":{"id":"xeZVVAgzZvwz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To quantize a model, we need to create a `HqqConfig`."],"metadata":{"id":"-A9WL8KzZ_Es"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer, HqqConfig\n","\n","# method 1: all linear layers will use the same quantization config\n","quantization_config = HqqConfig(nbits=8, group_size=64)"],"metadata":{"id":"9A2Bf9SUarpm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# method 2: each linear layer with the same tag will use a dedicated quantization config\n","q4_config = {'nbits': 4, 'group_size': 64}\n","q4_config = {'nbits': 3, 'group_size': 32}\n","\n","quantization_config = HqqConfig(dynamic_config={\n","    'self_attn.q_proj': q4_config,\n","    'self_attn.k_proj': q4_config,\n","    'self_attn.v_proj': q4_config,\n","    'self_attn.o_proj': q4_config,\n","\n","    'mlp.gate_proj': q3_config,\n","    'mlp.up_proj': q3_config,\n","    'mlp.down_proj': q3_config,\n","})"],"metadata":{"id":"g8nzVPtia3Pe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The second apparoch (method 2) is useful for quantizing Mixture-of-Experts (MoEs) because the experts are less affected by lower quantization settings."],"metadata":{"id":"VlTSOvzobSIb"}},{"cell_type":"code","source":["model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    torch_dtype=torch.float16,\n","    device_map='auto',\n","    quantization_config=quantization_config\n",")"],"metadata":{"id":"Ub_KWjWabcFk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# FBGEMM FP8"],"metadata":{"id":"hHhIuLsgbiMr"}},{"cell_type":"markdown","source":["With FBGEMM FP8 quantization method, we can quantize our model in FP8 (W8A8):\n","* the weights will be quantized in 8bit (FP8) per channel\n","* the activation will be quantized in 8bit (FP8) per token"],"metadata":{"id":"mGXyQpQtblCb"}},{"cell_type":"code","source":["!pip install -qU accelerate fbgemm-gpu torch"],"metadata":{"id":"xn3-rzOQbkUj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["By default, the weights are loaded in full precision regardless of the actual data type."],"metadata":{"id":"0gGEv5xpb1lL"}},{"cell_type":"code","source":["from transformers import FbgemmFp8Config, AutoModelForCausalLM, AutoTokenizer\n","\n","model_name = 'meta-llama/Meta-Llama-3-8B'\n","\n","quantization_config = FbgemmFp8Config()\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","quantized_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    torch_dtype='auto',\n","    device_map='auto',\n","    quantization_config=quantization_config\n",")"],"metadata":{"id":"86kpq_PLcGzc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_text = \"What are we having for dinner?\"\n","input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n","\n","output = quantized_model.generate(**input_ids, max_new_tokens=10)\n","tokenizer.decode(output[0], skip_special_tokens=True)"],"metadata":{"id":"1vLHyX1ccWIo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TorchAO"],"metadata":{"id":"_aelIFwkcam-"}},{"cell_type":"markdown","source":["**TorchAO** is an architecture optimization library for PyTorch. It provides high performance dtypes, optimization techniques and kernels for inference and training, featuring composability with native PyTorch features."],"metadata":{"id":"4Zp2NpB2ccim"}},{"cell_type":"code","source":["!pip install -qU torch torchao transformers"],"metadata":{"id":"FtDOS5IQcb0E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n","\n","model_name = 'meta-llama/Meta-Llama-3-8B'\n","\n","quantization_config = TorchAoConfig('int4_weight_only', group_size=128)\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","quantized_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    torch_dtype='auto',\n","    device_map='auto',\n","    quantization_config=quantization_config\n",")"],"metadata":{"id":"Kga2WEqGcqh3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_text = \"What are we having for dinner?\"\n","input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n","\n","output = quantized_model.generate(\n","    **input_ids,\n","    max_new_tokens=10,\n","    cache_implementation='static'\n",")\n","tokenizer.decode(output[0], skip_special_tokens=True)"],"metadata":{"id":"PLiRZ1BZdscE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# benchmark performance\n","import torch.utils.benchmark as benchmark\n","\n","def benchmark_fn(f, *args, **kwargs):\n","    # Manual warmup\n","    for _ in range(5):\n","        f(*args, **kwargs)\n","\n","    t0 = benchmark.Timer(\n","        stmt='f(*args, **kwargs)',\n","        globals={'args': args, 'kwargs': kwargs, 'f': f},\n","        num_threads=torch.get_num_threads()\n","    )\n","\n","    return f\"{(t0.blocked_autorange().mean):.3f}\"\n","\n","\n","MAX_NEW_TOKENS = 1000\n","\n","print(\"int4wo-128 model:\", benchmark_fn(quantized_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\"))\n","\n","bf16_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda\", torch_dtype=torch.bfloat16)\n","output = bf16_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\") # auto-compile\n","print(\"bf16 model:\", benchmark_fn(bf16_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\"))"],"metadata":{"id":"wGTpXHrbd5Do"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Serialization and Deserialization"],"metadata":{"id":"hfsbhzA0eYQP"}},{"cell_type":"markdown","source":["torchao quantization is implemented with tensor subclasses, it only work with huggingface non-safetensor serialization and deserialization."],"metadata":{"id":"Nx3zoZL6gR0g"}},{"cell_type":"code","source":["# save quantized model locally\n","output_dir = \"llama3-8b-int4wo-128\"\n","quantized_model.save_pretrained(output_dir, safe_serialization=False)\n","\n","# load quantized model\n","ckpt_id = \"llama3-8b-int4wo-128\"  # or huggingface hub model id\n","loaded_quantized_model = AutoModelForCausalLM.from_pretrained(ckpt_id, device_map=\"cuda\")\n","\n","# confirm the speedup\n","loaded_quantized_model = torch.compile(loaded_quantized_model, mode=\"max-autotune\")\n","print(\"loaded int4wo-128 model:\", benchmark_fn(loaded_quantized_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS))"],"metadata":{"id":"nrEGsN27ebJw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Compressed-tensors"],"metadata":{"id":"o_xy9qQhgjWV"}},{"cell_type":"markdown","source":["The `compressed-tensors` library provides a versatile and efficient way to store and manage compressed model checkpoints. The library supports various quantization and sparsity schemes, making it a unified format for handling different model optimization like GPTQ, AWQ, SmoothQuant, INT8, FP8, SparseGPT and more."],"metadata":{"id":"obxxJmeHgmeD"}},{"cell_type":"code","source":["!pip install -qU compressed-tensors"],"metadata":{"id":"l-gBM4ZygkuR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM\n","\n","ct_model = AutoModelForCausalLM.from_pretrained(\n","    'nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf'\n",")\n","\n","# measure memory usage\n","mem_params = sum(\n","    [param.nelement() * param.element_size() for param in ct_model.parameters()]\n",")\n","print(f\"{mem_params/2**30:.4f} GB\")"],"metadata":{"id":"b7yQ_A3XhLYh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","model_name = 'nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","quantized_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    device_map='auto'\n",")"],"metadata":{"id":"ZPidpf1PhnnH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = [\n","    \"Hello, my name is\",\n","    \"The capital of France is\",\n","    \"The future of AI is\"\n","]\n","\n","inputs = tokenizer(prompt, return_tensors='pt')\n","generated_ids = quantized_model.generate(\n","    **inputs,\n","    max_length=50,\n","    do_sample=False\n",")\n","outputs = tokenizer.batch_decode(generated_ids)\n","print(outputs)"],"metadata":{"id":"cyMMvzcZh4Fb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fine-grained FP8"],"metadata":{"id":"iPzSIRPXiHdq"}},{"cell_type":"markdown","source":["With FP8 quantization model, we can quantize our model in FP8:\n","* the weights will be quantized in 8bit (FP8) per 2D block (e.g. weight_block_size=(128, 128)) which is inspired from the deepseek implementation\n","* the activations are quantized to 8 bits (FP8) per group per token, with the group value matching that of the weights in the input channels (128 by default)"],"metadata":{"id":"MMiRgzu3iK76"}},{"cell_type":"code","source":["!pip install -qU accelerate torch"],"metadata":{"id":"vieAxsRxiJWV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import FineGrainedFP8Config, AutoModelForCausalLM, AutoTokenizer\n","\n","model_name = 'meta-llama/Meta-Llama-3-8B'\n","\n","quantization_config = FineGrainedFP8Config()\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","quantized_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    torch_dtype='auto',\n","    device_map='auto',\n","    quantization_config=quantization_config\n",")"],"metadata":{"id":"ah_t-QJbiWue"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_text = \"What are we having for dinner?\"\n","input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n","\n","output = quantized_model.generate(**input_ids, max_new_tokens=10)\n","tokenizer.decode(output[0], skip_special_tokens=True)"],"metadata":{"id":"JjQHNsshinvq"},"execution_count":null,"outputs":[]}]}