{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMRk/N4lksrMoOXBv6Wt2Cu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Textual Inversion"],"metadata":{"id":"kcv5qMRG_DYe"}},{"cell_type":"markdown","source":["**Textual Inversion** is a training technique for personalizing image generation models with just a few example images of what we want it to learn. This technique works by learning and updating the text embeddings (the new embeddings are tied to a special word we must use in the prompt) to match the example images we provide.\n","\n","We will explore the [`textual_inversion.py`](https://github.com/huggingface/diffusers/blob/main/examples/textual_inversion/textual_inversion.py) to train our custom textual inversion.\n","\n","As always, make sure to install the `diffusers` library from source:\n","```bash\n","git clone https://github.com/huggingface/diffusers\n","cd diffusers\n","pip install .\n","```\n","\n","We need to navigate to the following folder and install the corresponding dependencies:\n","```bash\n","cd examples/textual_inversion\n","pip install -r requirements.txt\n","```\n","HuggingFace Accelerate is also helpful to train on multiple GPUs with mixed-precision.\n","```bash\n","pip install accelerate\n","```\n","\n","Now we can initialize a HuggingFace Accelerate environment\n","```bash\n","accelerate config\n","```\n","To set up a default Acclerate environment without choosing any configurations:\n","```bash\n","accelerate config default\n","```\n","Or if our environment does not support an interactive shell like a notebook, we can use:\n","```python\n","from accelerate.utils import write_basic_config\n","write_basic_config()\n","```"],"metadata":{"id":"_5DdLYdoXPpV"}},{"cell_type":"markdown","source":["## Script parameters"],"metadata":{"id":"QgiUmaAsaNt7"}},{"cell_type":"markdown","source":["The training script provides many parameters to customize the training run. We can find all of the parameters and their descriptions in the `parse_args()` function.\n","\n","Some basic and important parameters to specify:\n","* `--pretrained_model_name_or_path`: the name of the model on the Hub or a local path to the pretrained model\n","* `--train_data_dir`: path to a folder containing the training dataset (example images)\n","* `--output_dir`: where to save the trained model\n","* `--push_to_hub`: whether to push the trained model to the Hub\n","* `--checkpointing_steps`: frequency of saving a checkpoint as the model trains; this is useful if for some reason training is interrupted, we can continue training from that checkpoint by adding `--resume_from_checkpoint` to our trainining command\n","* `--num_vectors`: the number of vectors to learn the embeddings with; increasing this parameter helps the model learn better but it comes with increased training costs\n","* `--placeholder_token`: the special word to tie the learned embeddings to (we must use the word in our prompt for inference)\n","* `--initializer_token`: a single-word that roughly describes the object or style we are trying to train on\n","* `--learnable_property`: whether we are training the model to learn a new \"style\" (for example, Van Gogh's painting style) or \"object\" (for example, new discovery)"],"metadata":{"id":"ltCHrknVaP2q"}},{"cell_type":"markdown","source":["## Training script"],"metadata":{"id":"YxfBT2WMbkJ2"}},{"cell_type":"markdown","source":["The `textual_inversion.py` has a custom dataset class, `TextualInversionDataset` for creating a dataset. We can customize the image size, placeholder token, interpolation method, whether to crop the image, and more. If we need to change how the dataset is created, we can modify `TextualInversionDataset`.\n","\n","Next, in the `main()` function, we will find the dataset preprocessing code. The script starts by loading the `tokenizer`, `scheduler` and `model`:\n","```python\n","    # Load tokenizer\n","    if args.tokenizer_name:\n","        tokenizer = CLIPTokenizer.from_pretrained(args.tokenizer_name)\n","    elif args.pretrained_model_name_or_path:\n","        tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n","\n","    # Load scheduler and models\n","    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n","    text_encoder = CLIPTextModel.from_pretrained(\n","        args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\n","    )\n","    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision)\n","    unet = UNet2DConditionModel.from_pretrained(\n","        args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision\n","    )\n","```\n","The special `placeholder_token` is added next to the tokenizer, and the embedding is readjusted to account for the new token:\n","```python\n","    # Add the placeholder token in tokenizer\n","    placeholder_tokens = [args.placeholder_token]\n","\n","    if args.num_vectors < 1:\n","        raise ValueError(f\"--num_vectors has to be larger or equal to 1, but is {args.num_vectors}\")\n","\n","    # add dummy tokens for multi-vector\n","    additional_tokens = []\n","    for i in range(1, args.num_vectors):\n","        additional_tokens.append(f\"{args.placeholder_token}_{i}\")\n","    placeholder_tokens += additional_tokens\n","\n","    num_added_tokens = tokenizer.add_tokens(placeholder_tokens)\n","    if num_added_tokens != args.num_vectors:\n","        raise ValueError(\n","            f\"The tokenizer already contains the token {args.placeholder_token}. Please pass a different\"\n","            \" `placeholder_token` that is not already in the tokenizer.\"\n","        )\n","\n","    # Convert the initializer_token, placeholder_token to ids\n","    token_ids = tokenizer.encode(args.initializer_token, add_special_tokens=False)\n","    # Check if initializer_token is a single token or a sequence of tokens\n","    if len(token_ids) > 1:\n","        raise ValueError(\"The initializer token must be a single token.\")\n","\n","    initializer_token_id = token_ids[0]\n","    placeholder_token_ids = tokenizer.convert_tokens_to_ids(placeholder_tokens)\n","\n","    # Resize the token embeddings as we are adding new special tokens to the tokenizer\n","    text_encoder.resize_token_embeddings(len(tokenizer))\n","\n","    # Initialise the newly added placeholder token with the embeddings of the initializer token\n","    token_embeds = text_encoder.get_input_embeddings().weight.data\n","    with torch.no_grad():\n","        for token_id in placeholder_token_ids:\n","            token_embeds[token_id] = token_embeds[initializer_token_id].clone()\n","```\n","Then, the script creates a dataset from the `TextualInversionDataset`:\n","```python\n","    # Dataset and DataLoaders creation:\n","    train_dataset = TextualInversionDataset(\n","        data_root=args.train_data_dir,\n","        tokenizer=tokenizer,\n","        size=args.resolution,\n","        placeholder_token=(\" \".join(tokenizer.convert_ids_to_tokens(placeholder_token_ids))),\n","        repeats=args.repeats,\n","        learnable_property=args.learnable_property,\n","        center_crop=args.center_crop,\n","        set=\"train\",\n","    )\n","    train_dataloader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=args.train_batch_size, shuffle=True, num_workers=args.dataloader_num_workers\n","    )\n","```\n","Finally, the training loop handles everything else from predicting the noisy residual to updating the embedding weights of the special placeholder token."],"metadata":{"id":"lfowLOfZbmt7"}},{"cell_type":"markdown","source":["## Launch the script"],"metadata":{"id":"n7w_tOFqdYXR"}},{"cell_type":"markdown","source":["We will download images in the `cat_toy` dataset and store them in a directory in this guide.\n","```python\n","from huggingface_hub import snapshot_download\n","local_dir = './cat\n","snapshot_download(\n","    'diffusers/cat_toy_example',\n","    local_dir=local_dir,\n","    repo_type='dataset',\n","    ignore_patterns='.gitattributes'\n",")\n","```\n","\n","Once we launch the script, it will create and save the following files to our repository:\n","* `learned_embeds.bin`: the learned embedding vectors corresponding to our example images\n","* `token_identifier.txt`: the special placeholder token\n","* `type_of_ceoncept.txt`: the type of concept we are training on (either \"object\" or \"style\")\n","\n","If we are interetsted in following along with the training process, we can preiodically save generated images as training progresses. Add the following parameters to the training command:\n","```bash\n"," --validation_prompt=\"A <cat-toy> train\"\n"," --num_validation_images=4\n"," --validation_steps=100\n","```\n","\n","Run the script:\n","```bash\n","export MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n","export DATA_DIR=\"./cat\"\n","\n","accelerate launch textual_inversion.py \\\n","  --pretrained_model_name_or_path=$MODEL_NAME \\\n","  --train_data_dir=$DATA_DIR \\\n","  --learnable_property=\"object\" \\\n","  --placeholder_token=\"<cat-toy>\" \\\n","  --initializer_token=\"toy\" \\\n","  --resolution=512 \\\n","  --train_batch_size=1 \\\n","  --gradient_accumulation_steps=4 \\\n","  --max_train_steps=3000 \\\n","  --learning_rate=5.0e-04 \\\n","  --scale_lr \\\n","  --lr_scheduler=\"constant\" \\\n","  --lr_warmup_steps=0 \\\n","  --output_dir=\"textual_inversion_cat\" \\\n","  --push_to_hub\n","```\n","\n","After training is complete, we can use our newly trained model for inference:"],"metadata":{"id":"7a7zfW_-dbXg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LL3NJ4X6--3F"},"outputs":[],"source":["from diffusers import StableDiffusionPipeline\n","import torch\n","\n","pipeline = StableDiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    torch_dtype=torch.float16\n",").to('cuda')\n","pipeline.load_textual_inversion('sd-concepts-library/cat-toy')"]},{"cell_type":"code","source":["image = pipeline(\n","    \"A <cat-toy> train\",\n","    num_inference_steps=50\n",").images[0]\n","image"],"metadata":{"id":"HjwHkZyyiU_V"},"execution_count":null,"outputs":[]}]}