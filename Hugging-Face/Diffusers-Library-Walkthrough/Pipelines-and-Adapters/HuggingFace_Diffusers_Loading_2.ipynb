{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPkZr6iO21a41E3cZuzIZbo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pIUUm4NPoWQ6"},"outputs":[],"source":["!pip install -qU diffusers accelerate transformers huggingface_hub"]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","notebook_login()"],"metadata":{"id":"Op5kU0Jaoazj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load community pipelines and components"],"metadata":{"id":"ZdwUO4pJobV2"}},{"cell_type":"markdown","source":["## Community pipelines"],"metadata":{"id":"6sUJf5oCqRCX"}},{"cell_type":"markdown","source":["Community pipelines are any `DiffusionPipeline` class that are different from the original paper implementation.\n","\n","There are two types of community pipelines, those stored on the HuggingFace Hub and those stored on Diffusers GitHub repository.\n","\n","Hub pipelines are completely customizable (schedulers, models, pipeline code, etc.) while Diffusers GitHub pipelines are only limited to custom pipeline code."],"metadata":{"id":"CPFkjOu6qUfm"}},{"cell_type":"markdown","source":["### Load from a local file"],"metadata":{"id":"s2c3Fkiqkgld"}},{"cell_type":"code","source":["from transformers import CLIPImageProcessor, CLIPModel\n","\n","clip_model_id = 'laion/CLIP-ViT-B-32-laion2B-s34B-b79K'\n","clip_model = CLIPModel.from_pretrained(clip_model_id)\n","feature_extractor = CLIPImageProcessor.from_pretrained(clip_model_id)"],"metadata":{"id":"S2ewOK85lTiM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from diffusers import DiffusionPipeline\n","\n","pipeline = DiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    custom_pipeline=\"<path_to_pipeline_directory>\", # change path\n","    clip_model=clip_model,\n","    feature_extractor=feature_extractor,\n","    use_safetensors=True,\n",")"],"metadata":{"id":"TXC1jGcRodqo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load from a specific version"],"metadata":{"id":"6mwUpYmvk3P0"}},{"cell_type":"markdown","source":["By default, community pipelines are loaded from the latest stable version of Diffusers.\n","\n","Load from the main branch:"],"metadata":{"id":"wWFIMvBmk7Yd"}},{"cell_type":"code","source":["pipeline = DiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    custom_pipeline=\"clip_guided_stable_diffusion\",\n","    custom_revision='main',\n","    clip_model=clip_model,\n","    feature_extractor=feature_extractor,\n","    use_safetensors=True,\n",")"],"metadata":{"id":"h24UVtImk5KY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load from a previous version of Diffusers:"],"metadata":{"id":"XhW-o-pJlqhL"}},{"cell_type":"code","source":["pipeline = DiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    custom_pipeline=\"clip_guided_stable_diffusion\",\n","    custom_revision='v0.25.0',\n","    clip_model=clip_model,\n","    feature_extractor=feature_extractor,\n","    use_safetensors=True,\n",")"],"metadata":{"id":"O5jJHW_EltI-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load with `from_pipe`"],"metadata":{"id":"3CWSQssel0HB"}},{"cell_type":"markdown","source":["Using `from_pipe()` method to load and reuse pipelines without any additional memory overhead.\n","\n","For example, we can load a community pipeline that supports long prompts with weighting from a Stable Diffusion pipeline."],"metadata":{"id":"FZ4jdOQWpjjY"}},{"cell_type":"code","source":["from diffusers import StableDiffusionPipeline\n","import torch\n","\n","pipe_sd = DiffusionPipeline.from_pretrained(\n","    'emilianJR/CyberRealistic_V3',\n","    torch_dtype=torch.float16,\n",")\n","pipe_sd.to('cuda')"],"metadata":{"id":"KPPuwMVXl2gq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# long prompt weighting pipeline\n","pipe_lpw = DiffusionPipeline.from_pipe(\n","    pipe_sd,\n","    custom_pipeline='lpw_stable_diffusion',\n",").to('cuda')"],"metadata":{"id":"uPYhIHGvu1iL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"cat, hiding in the leaves, ((rain)), zazie rainyday, beautiful eyes, macro shot, colorful details, natural lighting, amazing composition, subsurface scattering, amazing textures, filmic, soft light, ultra-detailed eyes, intricate details, detailed texture, light source contrast, dramatic shadows, cinematic light, depth of field, film grain, noise, dark background, hyperrealistic dslr film still, dim volumetric cinematic lighting\"\n","neg_prompt = \"(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, mutated hands and fingers:1.4), (deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, disconnected limbs, mutation, mutated, ugly, disgusting, amputation\"\n","generator = torch.Generator(device='cpu').manual_seed(111)\n","\n","out_lpw = pipe_lpw(\n","    prompt,\n","    neg_prompt,\n","    width=512,\n","    height=512,\n","    max_embeddings_multiples=3,\n","    num_inference_steps=50,\n","    generator=generator,\n",").images[0]\n","out_lpw"],"metadata":{"id":"S_tZDEkhu9w-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Example community pipelines"],"metadata":{"id":"4lkQmTO_vqrs"}},{"cell_type":"markdown","source":["We can find all community pipelines in the `diffusers/examples/community` under the official GitHub repository."],"metadata":{"id":"g5lq70WrhMCD"}},{"cell_type":"markdown","source":["### Marigold"],"metadata":{"id":"CWeF7jDyh-59"}},{"cell_type":"markdown","source":["`Marigold` is a depth estimation diffusion pipeline that uses the rich existing and inherent visual knowledge in diffusion models. It takes an input image and denoises it into a depth map."],"metadata":{"id":"E9XRy8igiBRc"}},{"cell_type":"code","source":["import torch\n","from PIL import Image\n","from diffusers import DiffusionPipeline\n","from diffusers.utils import load_image\n","\n","pipeline = DiffusionPipeline.from_pretrained(\n","    'prs-eth/marigold-lcm-v1-0',\n","    custom_pipeline='marigold_depth_estimation',\n","    torch_dtype=torch.float16,\n","    variant='fp16',\n",")\n","pipeline.to('cuda')"],"metadata":{"id":"xo4OWcpPvsId"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/community-marigold.png\")\n","\n","output = pipeline(\n","    image,\n","    denoising_steps=4,\n","    ensemble_size=5,\n","    processing_size=768,\n","    match_input_res=True,\n","    batch_size=0,\n","    seed=111,\n","    color_map='Spectral',\n","    show_progress_bar=True,\n",")\n","\n","depth_colored: Image.Image = output.depth_colored"],"metadata":{"id":"tg0iS4C8i3O-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### HD-Painter"],"metadata":{"id":"h5qhcH2DjdmG"}},{"cell_type":"markdown","source":["`HD-Painter` is a high-resolution inpainting pipeline. It introduces a *Prompt-Aware Introverted Attention (PAIntA)* layer to better align a prompt with the area to be painted, and *Reweighting Attention Score Guidance (RASG)* to keep the latents more prompt-aligned and within their trained domain to generate realistic images."],"metadata":{"id":"kQ6AZgeejfAY"}},{"cell_type":"code","source":["import torch\n","from diffusers import DiffusionPipeline, DDIMScheduler\n","from diffusers.utils import load_image\n","\n","pipeline = DiffusionPipeline.from_pretrained(\n","    'stable-diffusion-v1-5/stable-diffusion-v1-5-inpainting',\n","    custom_pipeline='hd_painter',\n",")\n","pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)"],"metadata":{"id":"Wt1QJNn9jeoX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["init_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/hd-painter.jpg\")\n","mask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/hd-painter-mask.png\")\n","prompt = \"football\"\n","\n","image = pipeline(\n","    prompt,\n","    init_image,\n","    mask_image,\n","    use_rasg=True,\n","    use_painta=True,\n","    generator=torch.Generator(device='cpu').manual_seed(111),\n",").images[0]\n","image"],"metadata":{"id":"oJw0EdaikK0t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Community components"],"metadata":{"id":"D6mLwphSka45"}},{"cell_type":"markdown","source":["Community components allow us to build pipelines that may have customized components that are not a part of Diffusers."],"metadata":{"id":"KONqT5ZXkdom"}},{"cell_type":"markdown","source":["In this section, we will build a customized pipeline using the `showlab/show-1-base`"],"metadata":{"id":"T3Vx6GboBRvM"}},{"cell_type":"markdown","source":["1. Import and load the text encoder from Transformers:"],"metadata":{"id":"dtmhyYHTBZpH"}},{"cell_type":"code","source":["from transformers import T5Tokenizer, T5EncoderModel\n","\n","pipe_id = 'showlab/show-1-base'\n","tokenizer = T5Tokenizer.from_pretrained(pipe_id, subfolder='tokenizer')\n","text_encoder = T5EncoderModel.from_pretrained(pipe_id, subfolder='text_encoder')"],"metadata":{"id":"k8lxZ3TNkdN4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Load a scheduler:"],"metadata":{"id":"-eNaGtBEBqFb"}},{"cell_type":"code","source":["from diffusers import DPMSolverMultistepScheduler\n","\n","scheduler = DPMSolverMultistepScheduler.from_pretrained(pipe_id, subfolder='scheduler')"],"metadata":{"id":"tKD-QpXVBrL4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Load an image processor:"],"metadata":{"id":"kg0mirQiBxqA"}},{"cell_type":"code","source":["from transformers import CLIPImageProcessor\n","\n","feature_extractor = CLIPImageProcessor.from_pretrained(pipe_id, subfolder='feature_extractor')"],"metadata":{"id":"bd9wPIXuB0D7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. Load a custom UNet. The implemented class can be found under [showone_unet_3d_condition.py](https://huggingface.co/sayakpaul/show-1-base-with-code/tree/main). Make sure to import properly."],"metadata":{"id":"YMo6cZlfB7e2"}},{"cell_type":"code","source":["from showone_unet_3d_condition import ShowOneUNet3DConditionModel\n","\n","unet = ShowOneUNet3DConditionModel.from_pretrained(pipe_id, subfolder='unet')"],"metadata":{"id":"6B73iluYC3m1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5. Load a custom pipeline code. The implemented class can be found under [pipeline_t2v_base_pixel.py](https://huggingface.co/sayakpaul/show-1-base-with-code/blob/main/pipeline_t2v_base_pixel.py). Make sure to import properly."],"metadata":{"id":"AxotwG6KDEaF"}},{"cell_type":"code","source":["from pipeline_t2v_base_pixel import Text2VideoIFPipeline\n","import torch\n","\n","pipeline = Text2VideoIFPipeline(\n","    unet=unet,\n","    text_encoder=text_encoder,\n","    tokenizer=tokenizer,\n","    scheduler=scheduler,\n","    feature_extractor=feature_extractor,\n",")\n","pipeline.to('cuda')\n","pipeline.torch_dtype = torch.float16"],"metadata":{"id":"oTCltWfgDaeH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"a cat sitting on a chair\"\n","\n","# text embeds\n","prompt_embeds, negative_embeds = pipeline.encode_prompt(prompt)\n","\n","# Keyframes generation (8x64x40, 2fps)\n","video_frames = pipeline(\n","    prompt_embeds=prompt_embeds,\n","    negative_prompt_embeds=negative_embeds,\n","    num_frames=8,\n","    height=40,\n","    width=64,\n","    num_inference_steps=2,\n","    guidance_scale=9.0,\n","    output_type='pt',\n",").frames"],"metadata":{"id":"DKtU14wdDtYb"},"execution_count":null,"outputs":[]}]}