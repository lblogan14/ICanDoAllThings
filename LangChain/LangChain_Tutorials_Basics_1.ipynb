{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25595,
     "status": "ok",
     "timestamp": 1730121464594,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "V2j4h8wE0ktf",
    "outputId": "f3490025-e121-47cb-ae4a-79a855249840"
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain langchain-openai langchain-community langchain-experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJV46Int0Irs"
   },
   "source": [
    "# Build a Simple LLM Application with LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ppQ8wo30Lnr"
   },
   "source": [
    "In this session, we will build a simple LLM application with LangChain. This application will translate text from English into another language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrLDeiO-1Xab"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMzZ9aAW2dWh"
   },
   "source": [
    "Many of the applications we build with LangChain will contain multiple steps with multiple invocations of LLM calls. It becomes crucial to inspect what exactly is going on inside our chain or agent as these applications gets more and more complex. It is better to do this with LangSmith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1730121464594,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "2KfZMwa80DKR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "langchain_api_key = 'your_langchain_api_key_here'  # Replace with your actual LangChain API key\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_API_KEY'] = langchain_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2zL3PU82cBM"
   },
   "source": [
    "## Using Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKwamhudH2Yq"
   },
   "source": [
    "Use OpenAI as a n example. (Other models can be found in the official tutorial pages.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1730121464595,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "5OGAPNuz2vzY"
   },
   "outputs": [],
   "source": [
    "openai_api_key = 'your_openai_api_key_here'  # Replace with your actual OpenAI API key\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4703,
     "status": "ok",
     "timestamp": 1730121469291,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "e3Qgc2NG24h6"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1V-Hj__b3Z01"
   },
   "source": [
    "`ChatModel`s are instances of LangChain \"Runnables\", which means they expose a standard interface for interacting with them. To call the model, we can pass in a list of messages to the `.invoke()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 746,
     "status": "ok",
     "timestamp": 1730121470034,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "pSN1WER13mTX",
    "outputId": "ab7e2b85-13fd-4178-81fb-4cf1194d2fcb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 23, 'total_tokens': 31, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2bfa78c3-34f3-4e57-ab7d-3ea32058d480-0', usage_metadata={'input_tokens': 23, 'output_tokens': 8, 'total_tokens': 31, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into French.\"),\n",
    "    HumanMessage(content=\"I love programming.\")\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-_hPp2YIqfG"
   },
   "source": [
    "## OutputParsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiQC-a3SItPD"
   },
   "source": [
    "The `AIMessage` object contains a string response along with other metadata about the response. We can parase out this response by using a simple output parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730121470962,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "JyXuomzUIRx1"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730121470962,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "p7ikvV5wI8GJ",
    "outputId": "77f6ff42-26f3-41da-c792-bde1a7d70388"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"J'adore la programmation.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyZLm_CgJGXn"
   },
   "source": [
    "We can \"chain\" the model with this output parser. This chain takes on the input type of the language model (string or list of message) and returns the output type of the output parser (string).\n",
    "\n",
    "We can create the chain using `|` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1730121470962,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "mciNL_tOJErr"
   },
   "outputs": [],
   "source": [
    "chain = model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 672,
     "status": "ok",
     "timestamp": 1730121471631,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "DsApVjQWJWKh",
    "outputId": "53eb3bf5-9f29-4239-9288-a7c24286fc48"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"J'adore programmer.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GxK_eSrJoWx"
   },
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMzYyu8aRm7v"
   },
   "source": [
    "We passed a list of messages (`HumanMessage` and `SystemMessag`) into the language model. It is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model.\n",
    "\n",
    "`PromptTemplates` are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1730121840161,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "nUgeaM27JX-o"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1730121921796,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "8yPTOaj4TBZj"
   },
   "outputs": [],
   "source": [
    "system_template = 'Translate the following into {language}'\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', system_template),\n",
    "        ('user', '{text}')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kycVlTe4TX-9"
   },
   "source": [
    "This PromptTemplate is a combination of the `system_template` as well as a simpler tempalte for where to put the text to be translated.\n",
    "\n",
    "The input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1730122074464,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "CDpQs4VXTVE-",
    "outputId": "762069cc-9713-4072-b55e-85165fcdc5c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following into french', additional_kwargs={}, response_metadata={}), HumanMessage(content='I love programming.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = prompt_template.invoke(\n",
    "    {'language': 'french',\n",
    "     'text': 'I love programming.'}\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ts-_rAgZUBjT"
   },
   "source": [
    "If we want to access the messages directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1730122108430,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "7DkE2rLuT7Ds",
    "outputId": "f25f0c9f-b51f-48ed-9366-c57dd662d9ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following into french', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I love programming.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klR6OgJhUF8I"
   },
   "source": [
    "## Chaining together components with LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vF9_6LW3VHCi"
   },
   "source": [
    "We can combine the PromptTemplate with the model and the output parser using the pipe operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1730122420640,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "Lixr9jgBUEun"
   },
   "outputs": [],
   "source": [
    "chain = prompt_template | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 1158,
     "status": "ok",
     "timestamp": 1730122445234,
     "user": {
      "displayName": "Bin Liu",
      "userId": "03585165976699804089"
     },
     "user_tz": 300
    },
    "id": "FcbUnq7PVQ9I",
    "outputId": "608b435f-60fa-4cb8-c248-eb491cbd2b05"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"J'adore programmer.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {'language': 'french',\n",
    "     'text': 'I love programming.'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wXIzvPsVYpU"
   },
   "source": [
    "This is a simple example of using **LangChain Expression Language (LCEL)** to chain together LangChain modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CI7BVeHOViz_"
   },
   "source": [
    "## Serving with LangServe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YG5ZFrXbVpld"
   },
   "source": [
    "LangServe helps developers deploy LangChain chains as a REST API. To install,\n",
    "```bash\n",
    "pip install \"langserve[all]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkkCU7-xV36h"
   },
   "source": [
    "### Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFIKj_S8V5mh"
   },
   "source": [
    "To create a server for our application, we need to make a `serve.py` file, which contains our logic for serving our application. It consists of three things:\n",
    "1. the definition of our chain\n",
    "2. our FastAPI app\n",
    "3. a definition of a route from which to serve the chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLRXpLJsWsJe"
   },
   "source": [
    "```python\n",
    "# This is what serve.py should have.\n",
    "\n",
    "#!/usr/bin/env python\n",
    "from fastapi import FastAPI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langserve import add_routes\n",
    "\n",
    "# 1. Create prompt template\n",
    "system_template = \"Translate the following into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_template),\n",
    "    ('user', '{text}'),\n",
    "])\n",
    "\n",
    "# 2. Create model\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# 3. Create parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 4. Create chain\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "# 5. App definition\n",
    "app = FastAPI(\n",
    "    title='LangChain Server',\n",
    "    version='1.0',\n",
    "    description=\"A simple API server using LangChain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "# 6. Add chain route\n",
    "add_routes(\n",
    "    app,\n",
    "    chain,\n",
    "    path='/chain',\n",
    ")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host='localhost', port=8080)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwPdu8kdYcZX"
   },
   "source": [
    "If we execute this file:\n",
    "```bash\n",
    "python serve.py\n",
    "```\n",
    "we should see our chain being served at http://localhost:8080."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t01cxwiVYl2m"
   },
   "source": [
    "### Playground\n",
    "\n",
    "Every LangServe service comes with a simple built-in UI for configuring and invoking the application with streaming output and visibility into intermediate steps.\n",
    "\n",
    "If we visit http://localhost:8080/chain/playground and pass `{'language': 'french', 'text': 'I love programming.'}`, we should get the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A95Zty7JZI4H"
   },
   "source": [
    "### Client\n",
    "\n",
    "Now that the service is online, we can set up a client for programmatically interacting with our service using `langserve.RemoteRunnable`. Using this, we can interact with the served chain as if it were running client-side:\n",
    "```python\n",
    "from langserve import RemoteRunnable\n",
    "\n",
    "remote_chain = RemoteRunnable('http://localhost:8080/chain/')\n",
    "remote_chain.invoke(\n",
    "    {'language': 'french',\n",
    "     'text': 'I love programming.'}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTWjndLrVWwF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM0jVHMIcgVUCMSNLDpgc5t",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
