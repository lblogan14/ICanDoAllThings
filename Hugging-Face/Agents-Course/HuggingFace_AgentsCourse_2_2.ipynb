{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The LlamaIndex Framework"
      ],
      "metadata": {
        "id": "eBs9Ds2K8QOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to LlamaIndex"
      ],
      "metadata": {
        "id": "o21XqL-L8Y_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LlamaIndex is a complete toolkit for creating LLM-powered agents over your data using indexes and workflows.\n",
        "\n",
        "LlamaIndex has some key benefits over smolagents:\n",
        "- **Clear Workflow System**: Workflows help break down how agents should make decisions step by step using an event-driven and async-first syntax. This helps us clearly compose and organize our logic.\n",
        "- **Advanced Document Parsing with LlamaParse**\n",
        "- **Many Ready-to-Use Components**: LlamaIndex has been around for a while, so it works with lots of other frameworks.\n",
        "- **LlamaHub**: is a registry of hundreds of these components, agents, and tools that we can use within LlamaIndex."
      ],
      "metadata": {
        "id": "_zUwrhn38c-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to LlamaHub"
      ],
      "metadata": {
        "id": "gGkRypod9D18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LlamaHub is a registry of hundreds of integrations, agents, and tools that we can use within LlamaIndex."
      ],
      "metadata": {
        "id": "H3MNvfsDoEOd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2vd8v_28O_U"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index-llms-huggingface-api llama-index-embeddings-huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once installed, we can use the HuggingFace Inference API."
      ],
      "metadata": {
        "id": "UlAxi_34oOml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "llm = HuggingFaceInferenceAPI(\n",
        "    model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=100,\n",
        "    token=hf_token\n",
        ")"
      ],
      "metadata": {
        "id": "5U528Rs2oWsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.complete(\"Hello, how are you?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "dAhvmetoonYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Components in LlamaIndex"
      ],
      "metadata": {
        "id": "FXoB4arhop7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For alll components in LlamaIndex, we will focuus on the `QueryEngine` component which can be used as a RAG tool for an agent.."
      ],
      "metadata": {
        "id": "5HaLUg023UXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU llama-index datasets llama-index-callbacks-arize-phoenix arize-phoenix llama-index-vector-stores-chroma llama-index-llms-huggingface-api llama-index-embeddings-huggingface"
      ],
      "metadata": {
        "id": "mCcM3B6woswe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a RAG Pipeline using components"
      ],
      "metadata": {
        "id": "6-ZZBi_K4AAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are five key stages within RAG:\n",
        "-  **Loading**: this refers to getting our data from where it lives - whether it's text files, PDFs, another website, a database, or an API - into our workflow.\n",
        "- **Indexing**: this means creating a data structure that allows for querying the data. For LLMs, this nearly always means creating vector embeddings. Indexing can also refer to numerous other metadata strategies to make it easy to accurately find contextually rellevant data based on properties.\n",
        "- **Storing** once our data is indexed we will want to store our index, as well as other metadata, to avoid having to re-index it.\n",
        "- **Querying**: for any given indexing strategy, there are many ways we can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies.\n",
        "- **Evaluation**: a critical step in any fllow is checking how effective it is relative to other strategies, or when we make changes."
      ],
      "metadata": {
        "id": "jo8PsM6r4HVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting up the persona database"
      ],
      "metadata": {
        "id": "B0unzCrK5Ikt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using personas from the [dvilasuero/finepersonas-v0.1-tiny](https://huggingface.co/datasets/dvilasuero/finepersonas-v0.1-tiny) dataset. This dataset contains 5K personas that will be attending the party!"
      ],
      "metadata": {
        "id": "cH9nq2Sz5MBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from pathlib import Path\n",
        "\n",
        "dataset = load_dataset(\"dvilasuero/finepersonas-v0.1-tiny\", split=\"train\")\n",
        "\n",
        "Path('data').mkdir(parents=True, exist_ok=True)\n",
        "for i, persona in enumerate(dataset)::\n",
        "    with open(Path('data') / f\"persona_{i}.txt\", 'w') as f:\n",
        "        f.write(persona['persona'])"
      ],
      "metadata": {
        "id": "Gg7l9YJp4GBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a local directory with all the personas to use."
      ],
      "metadata": {
        "id": "kLsx-8xq5l5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading and embedding documents"
      ],
      "metadata": {
        "id": "m6Kwk5_i5tXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three ways to load data into LlamaIndex\n",
        "- `SimpleDirectoryReader`: A built-in loader for various file types from a local directory.\n",
        "- `LlamaParse`: LlamaParse, LlamaIndex's official tool for PDF parsing, available as a managed API.\n",
        "- `LlamaHub`: A registry of hundreds of data-loading libraries to ingest data from any source.\n",
        "\n",
        "The simplest way to load data is with `SimpleDirectoryReader`."
      ],
      "metadata": {
        "id": "KCklxKuZ8ze4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "reader = SimpleDirectoryReader('data')\n",
        "documents = reader.load_data()\n",
        "len(documents)"
      ],
      "metadata": {
        "id": "GEnRzkDf5sds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a list of `Document` objects. After loading the documents, we need to break them into smaller pieces called `Node` objects. A `Node` is a chunk of text from the original document that is easier for LLMs to work with, while it still has references to the original `Document` object.\n",
        "\n",
        "Then, we can use the `IngestionPipeline` to create nodes from the documents and prepare them for the `QueryEngine`. We will use the `SentenceSplitter` to split the documents into smaller chunks and the `HuggingFaceEmbedding` to embed the chunks."
      ],
      "metadata": {
        "id": "_rYhCoda9Kbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "\n",
        "# Create the pipeline with transformations\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        SentenceSplitter(),\n",
        "        HuggingFaceEmbedding(model_name='BAAI/bge-small-en-v1.5')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Run the pipeline sync or async\n",
        "nodes = await pipeline.arun(documents=documents[:10])\n",
        "len(nodes)"
      ],
      "metadata": {
        "id": "VUpZ2zPB9ybm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have created a list of `Node` objects."
      ],
      "metadata": {
        "id": "0FSf86X1-TOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Storing and indexing documents"
      ],
      "metadata": {
        "id": "3oLlb5g7-WwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After creating our `Node` objects we need to index them to make them searchable, but before that, we need a place to store our data. Since we are using an ingestion pipeline, we can directly attach a vector store to the pipeline to populate it. In this case, we will use `Chroma` to store our documents.\n",
        "\n",
        "We will run the pipeline again with the vector store attached. The `IngestionPipeline` caches the operations so this should be fast."
      ],
      "metadata": {
        "id": "9AlPu-Xd-ZVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "# Create a vector store\n",
        "db = chromadb.PersistentClient(path='./chroma_db')\n",
        "chroma_collection = db.get_or_create_collection('agent')\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "# Attach the vector store to the pipeline\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        SentenceSplitter(),\n",
        "        HuggingFaceEmbedding(model_name='BAAI/bge-small-en-v1.5')\n",
        "    ],\n",
        "    vector_store=vector_store\n",
        ")\n",
        "\n",
        "nodes = await pipeline.arun(documents=documents[:10])\n",
        "len(nodes)"
      ],
      "metadata": {
        "id": "Vq4XpePq-V-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can create a `VectorStoreIndex` from the vector store and use it to query the documents by passing the vector store and embedding model to the `from_vector_store()` method. Make sure we must use the same embedding model during ingestion to ensure consistency."
      ],
      "metadata": {
        "id": "u-9aPYrZ_eS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name='BAAI/bge-small-en-v1.5')\n",
        "index = VectorStoreIndex.from_vector_store(\n",
        "    vector_store=vector_store,\n",
        "    embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "id": "bnMxKTvE_uuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All information is automatically persisted within the `ChromaVectorStore` object and the passed directory path."
      ],
      "metadata": {
        "id": "yY9ku1OFAEPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Querying a VectorStoreIndex with prompts and LLMs"
      ],
      "metadata": {
        "id": "i7HKLrJUALEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can query our index, we need to convert it to a query interface. The most common conversion options are\n",
        "- `as_retriever`: For basic document retrieval, returning a list of `NodeWithScore` objects with similarity scores\n",
        "- `as_query_engine`: For single question-answer interactions, returning a written response\n",
        "- `as_chat_engine`: For conversational interactions that maintain memory across multiple messages, returning a written response using chat history and indexed context\n",
        "\n",
        "\n",
        "We will focus on the query engine since it is more common for anget-like interactions. We also pass in an LLM to the query engine to use for the response."
      ],
      "metadata": {
        "id": "wR7zfSbyAO20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "import nest_asyncio\n",
        "\n",
        "# required to run the query engine\n",
        "nest_asyncio.apply()\n",
        "\n",
        "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
        "query_engine = index.as_query_engine(\n",
        "    llm=llm,\n",
        "    response_mode='tree_summarize'\n",
        ")"
      ],
      "metadata": {
        "id": "e18E3ffJAOPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"Respond using a persona that describes author and travel experiences?\"\n",
        ")\n",
        "response"
      ],
      "metadata": {
        "id": "YHdFMgXrBH9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Response processing\n",
        "\n",
        "Under the hood, the query engine doesn't only use the LLM to answer the question but also uses a `ResponseSynthesizer` as a strategy to process the response. This is customizable and there are three main options for the `response_mode`:\n",
        "- `refine` - create and refine an answer by sequentially going through each retrieved text chunk. This makes a separate LLM call per Node/retrieved chunk.\n",
        "- `compact` - (default) similar to refining but concatenating the chunks beforehand, resulting in fewer LLM calls.\n",
        "- `tree_summarize` - create a detailed answer by going through each retrieved text chunk and creating a tree structure of the answer."
      ],
      "metadata": {
        "id": "b11OGuB7BPJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation and observability"
      ],
      "metadata": {
        "id": "EZN1VoGFCT7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LlamaIndex provides **built-in evaluation tools to access response quality**. These evaluators leverage LLMs to analyze responses across different dimensions:\n",
        "- `FaithfulnessEvaluator` - evaluates the faithfulness of the answer by checking if the answer is supported by the context.\n",
        "- `AnswerRelevancyEvaluator` - evaluates the relevance of the answer by checking if the answer is relevant to the question.\n",
        "- `CorrectnessEvaluator` - evaluates the correctness of the answer by checking if the answer is correct."
      ],
      "metadata": {
        "id": "mbbOf63nCW8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
        "\n",
        "# query index\n",
        "evaluator = FaithfulnessEvaluator(llm=llm)\n",
        "eval_result = evaluator.evaluate_response(response=response)\n",
        "eval_result.passing"
      ],
      "metadata": {
        "id": "uQAVNPa8BQzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even without direct evaluation, we can gain insights into how our system is performing through observability. This is useful when we build more complex workflows and want to understand how each component is performing.\n",
        "\n",
        "If one of these LLM based evaluators does not give enough context, we can check the response using the Arize Phoenix tool, after creating an account at [LlamaTrace](https://llamatrace.com/login) and generating an API key:"
      ],
      "metadata": {
        "id": "-0OSLu1_DE-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import llama_index\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OTEL_EXPORTER_OTLP_HEADERS'] = f\"api_key={userdata.get(PHOENIX_API_KEY)}\"\n",
        "\n",
        "llama_index.set_global_handler(\n",
        "    'arize_phoenix',\n",
        "    endpoint='https://llamatrace.com/v1/traces'\n",
        ")"
      ],
      "metadata": {
        "id": "sgqjyvTcDlAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can query the index and see the response in the Arize Phoenix tool:"
      ],
      "metadata": {
        "id": "GrHWoSQ2D8Pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"What is the name of the someone that is interested in AI and techhnology?\"\n",
        ")\n",
        "response"
      ],
      "metadata": {
        "id": "h5L74FNXD_0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Tools in LlamaIndex"
      ],
      "metadata": {
        "id": "o0i7NE6FEDlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clear tool interfaces are easier for LLMs to use.\n",
        "\n",
        "There are four main types of tools in LlamaIndex:\n",
        "- `FunctionTool` - convert any Python function into a tool that an agent can use.\n",
        "- `QueryEngineTool` - let agents use query engines. Since agents are built on query engines, they can also use other agents as tools.\n",
        "- `Toolspecs` - sets of tools created by the community, which often include tools for specific services like Gmail.\n",
        "- `UtilityTools` - special tools that help handle large amount of data from other tools."
      ],
      "metadata": {
        "id": "HuQxCY0UEqb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU llama-index llama-index-vector-stores-chroma llama-index-llms-huggingface-api llama-index-embeddings-huggingface llama-index-tools-google"
      ],
      "metadata": {
        "id": "Pc3_X_UTFO6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a FunctionTool"
      ],
      "metadata": {
        "id": "8G6JXc8UFQ04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A `FunctionTool` provides a simple way to wrap any Python function and make it available to an agent. We can pass either a synchronous or asynchronous function to the tool, along with optional `name` and `description` parameters. The `name` and `description` are particularly important as they help the agent understand when and how to use the tool effectively"
      ],
      "metadata": {
        "id": "zXiej3F6FVZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "def get_weather(location: str) -> str:\n",
        "    \"\"\"Useful for getting the weather for a given location.\"\"\"\n",
        "    print(f\"Getting weather for {location}\")\n",
        "    return f\"The weather in {location} is sunny.\"\n",
        "\n",
        "\n",
        "tool = FunctionTool.from_defaults(\n",
        "    get_weather,\n",
        "    name=\"my_weather_tool\",\n",
        "    description=\"Useful for getting the weather for a given location.\"\n",
        ")\n",
        "\n",
        "tool.call(\"Houston\")"
      ],
      "metadata": {
        "id": "bdPI8hyyEHvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a QueryEngineTool"
      ],
      "metadata": {
        "id": "jeLol36gF_gP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `QueryEngine` we defined in the previous section can be easily transformed into a tool using the `QueryEngineTool` class."
      ],
      "metadata": {
        "id": "-j3PM6MgGCdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "\n",
        "# Load the chroma db created in previous section\n",
        "db = chromadb.PersistentClient(path='./chroma_db')\n",
        "chroma_collection = db.get_or_create_collection('agent')\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name='BAAI/bge-small-en-v1.5')\n",
        "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
        "\n",
        "index = VectorStoreIndex.from_vector_store(\n",
        "    vector_store=vector_store,\n",
        "    embed_model=embed_model\n",
        ")\n",
        "\n",
        "query_engine = index.as_query_engine(llm=llm)\n",
        "\n",
        "tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=query_engine,\n",
        "    name='personas',\n",
        "    description=\"Descriptions for various types of personas\",\n",
        ")"
      ],
      "metadata": {
        "id": "wLcrinbVGB9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await tool.acall(\n",
        "    \"Responds about research on the impact of AI on the future of work and society?\"\n",
        ")"
      ],
      "metadata": {
        "id": "wFYZBjGlbjmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Toolspecs"
      ],
      "metadata": {
        "id": "_6o7ibJ2bkqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ToolSepcs` is a collection of tools that work together. A `ToolSpec` combines related tools for specific purposes.\n",
        "\n",
        "For example, we can load the `ToolSpec` from Google:"
      ],
      "metadata": {
        "id": "nF2Ft1gYbnaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.tools.google import GmailToolSpec\n",
        "\n",
        "tool_spec = GmailToolSpec()\n",
        "tool_spec_list = tool_spec.to_tool_list()\n",
        "tool_spec_list"
      ],
      "metadata": {
        "id": "cdowk7XubmIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get a more detailed view of the tools, we can take a look at the `metadata` of each tool:"
      ],
      "metadata": {
        "id": "h_ZMKdUlcEmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[(tool.metadata.name, tool.metadata.description) for tool in tool_spec_list]"
      ],
      "metadata": {
        "id": "1hP715KncJbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Context Protocol (MCP) in LlamaIndex"
      ],
      "metadata": {
        "id": "t-fu5ooRcP0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LlamaIndex also allows using MCP tools through a `ToolSpec` on the LlamaHub"
      ],
      "metadata": {
        "id": "LVvn4gdLcUGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.tools.mcp import BasicMCPClient, McpToolSpec\n",
        "\n",
        "# Assume there is a mcp server running on 127.0.0.1:8000\n",
        "mcp_client = BasicMCPClient(\"http://127.0.0.1:8000/sse\")\n",
        "mcp_tool = McpToolSpec(client=mcp_client)\n",
        "\n",
        "# async\n",
        "tools = await mcp_tool_spec.to_tool_list_async()"
      ],
      "metadata": {
        "id": "7Rq8SHPzcS_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent.workflow import FunctionAgent\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "agent = FunctionAgent(\n",
        "    name=\"Agent\",\n",
        "    description=\"Some description\",\n",
        "    llm=OpenAI(model=\"gpt-4o\"),\n",
        "    tools=tools,\n",
        "    system_prompt=\"You are a helpful assistant.\",\n",
        ")\n",
        "\n",
        "resp = await agent.run(\"What is the weather in Tokyo?\")"
      ],
      "metadata": {
        "id": "jZIjzW3Uc3Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Agents in LlamaIndex"
      ],
      "metadata": {
        "id": "NFSMmX5dc8Cd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LlamaIndex supports three main types of reasoning agents:\n",
        "- **Functio Calling Agents** - these work with AI models that can call specific functions.\n",
        "- **ReAct Agents** - these can work with any AI that does chat or text endpoint and deal with complex reasoning tasks.\n",
        "- **Advanced Custom Agents** - these use more complex methods to deal with more complex tasks and workflows."
      ],
      "metadata": {
        "id": "TLq9bE_GdAm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index llama-index-vector-stores-chroma llama-index-llms-huggingface-api llama-index-embeddings-huggingface"
      ],
      "metadata": {
        "id": "aLI2GqJSdYf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing Agents"
      ],
      "metadata": {
        "id": "_9jYJXePdaLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create an agent, we start by providing it with a set of functions/tools that define its capabilities.\n",
        "\n",
        "ReAct agents are also good at complex reasoning tasks and can work with any LLM that has chat or text completion capabilities. They are more verbose, and show the reasoning behind certain actions that they take.\n",
        "\n",
        "We start by initializing an agent and using the basic `AgentWorkflow` class to create an agent."
      ],
      "metadata": {
        "id": "e1680tYxeUTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "from llama_index.core.agent.workflow import AgentWorkflow, ToolCallResult, AgentStream\n",
        "\n",
        "# define sample tools\n",
        "# type annotations, function names, and docstrings, are all included in parsed schemas\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two numbers\"\"\"\n",
        "    return a + b\n",
        "\n",
        "def subtract(a: int, b: int) -> int:\n",
        "    \"\"\"Subtract two numbers\"\"\"\n",
        "    return a - b\n",
        "\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two numbers\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "def divide(a: int, b: int) -> int:\n",
        "    \"\"\"Divide two numbers\"\"\"\n",
        "    return a / b\n",
        "\n",
        "# initialize llm\n",
        "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
        "\n",
        "# initialize agent\n",
        "agent = AgentWorkflow.from_tools_or_functions(\n",
        "    llm=llm,\n",
        "    tools_or_functions=[add, subtract, multiply, divide],\n",
        "    system_prompt=\"You are a math agent that can add, subtract, multiply, and divide numbers using provided tools.\",\n",
        ")"
      ],
      "metadata": {
        "id": "eUogRFl-c-Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we can run the agent and get the response and reasoning behind the tool calls."
      ],
      "metadata": {
        "id": "9kuXI2gLfSnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "handler = agent.run(\"What is (2 + 2) * 2?\")\n",
        "\n",
        "async for ev in handler.stream_events():\n",
        "    if isinstance(ev, ToolCallResult)::\n",
        "        print()\n",
        "        print(\"Called tool: \", ev.tool_name, ev.tool_kwargs, \"=>\", ev.tool_output)\n",
        "    elif isinstance(ev, AgentStream):\n",
        "        # Showing the thought process\n",
        "        print(ev.delta, end=\"\", flush=True)\n",
        "\n",
        "resp = await handler\n",
        "resp"
      ],
      "metadata": {
        "id": "rGrKNFhVAqwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agents are stateless by default**, add remembering past interactions is opt-in using a `Context` object.\n",
        "\n",
        "This may be useful if we want to use an agent that needs to remember previous interactions, like a chatbot that maintains context across multiple messages or a task manager that needs to track progress over time."
      ],
      "metadata": {
        "id": "F9CqoXAtBKhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remembering state\n",
        "from llama_index.core.workflow import Context\n",
        "\n",
        "ctx = Context(agent)\n",
        "\n",
        "response = await agent.run(\"My name is Bin.\", ctx=ctx)\n",
        "response = await agent.run(\"What is my name?\", ctx=ctx)\n",
        "response"
      ],
      "metadata": {
        "id": "P6YzhTRgfWJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating RAG Agents with QueryEngineTools"
      ],
      "metadata": {
        "id": "undU2EM1Bso4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agentic RAG is a powerful way to use agents to answer questions about our data**. We can pass various tools to help answer questions. However, instead of answering question on top of documents automatically, the agent needs to decode to use any other tools to answer the question.\n",
        "\n",
        "We will wrap `QueryEngine` as a tool for an agent. When doing so, we need to define a name a description. The LLM will use this information to correctly use the tool."
      ],
      "metadata": {
        "id": "VjxpF_zqBxYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "# Create a vector store\n",
        "db = chromadb.PersistentClient(path='./chroma_db')\n",
        "chroma_collection = db.get_or_create_collection('agent')\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name='BAAI/bge-small-en-v1.5')\n",
        "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
        "index = VectorStoreIndex.from_vector_store(\n",
        "    vector_store=vector_store,\n",
        "    embed_model=embed_model\n",
        ")\n",
        "\n",
        "# Create a query engine tool\n",
        "query_engine = index.as_query_engine(llm=llm)\n",
        "query_engine_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=query_engine,\n",
        "    name='personas',\n",
        "    description=\"Descriptions for various types of personas\",\n",
        "    return_direct=False\n",
        ")\n",
        "\n",
        "\n",
        "# Create a RAG agent\n",
        "query_engine_agent = AgentWorkflow.from_tools_or_functions(\n",
        "    llm=llm,\n",
        "    tools_or_functions=[query_engine_tool],\n",
        "    system_prompt=\"You are a helpful assistant that has access to a database containing persona descriptions.\"\n",
        ")"
      ],
      "metadata": {
        "id": "2KVPbWAzBwKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can get the response and reasoning behind the tool calls."
      ],
      "metadata": {
        "id": "mIxihs95DGqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "handler = query_engine_agent.run(\n",
        "    \"Search the database for 'science fiction' and return some persona descriptions.\"\n",
        ")\n",
        "\n",
        "async for ev in handler.stream_events():\n",
        "    if isinstance(ev, ToolCallResult)::\n",
        "        print()\n",
        "        print(\"Called tool: \", ev.tool_name, ev.tool_kwargs, \"=>\", ev.tool_output)\n",
        "    elif isinstance(ev, AgentStream):\n",
        "        # Showing the thought process\n",
        "        print(ev.delta, end=\"\", flush=True)\n",
        "\n",
        "resp = await handler\n",
        "resp"
      ],
      "metadata": {
        "id": "sYuKaGZkDJSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Multi-Agent Systems"
      ],
      "metadata": {
        "id": "IZZNXT_NDR0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `AgentWorkflow` class also directly supports multi-agent systems. By giving each agent a name and description, the system maintains a single active speaker, with each agent having the ability to hand off to another agent.\n",
        "\n",
        "By narrowing the scope of each agent, we can help increase their general accuracy when responding to user messages.\n",
        "\n",
        "**Agents in LlamaIndex can also directly be used as tools** for other agents, for more complex and custom scenarios."
      ],
      "metadata": {
        "id": "HqDAw0pKDYgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent.workflow import AgentWorkflow, ReActAgent\n",
        "\n",
        "# Define some tools\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two numbers.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "def subtract(a: int, b: int) -> int:\n",
        "    \"\"\"Subtract two numbers.\"\"\"\n",
        "    return a - b"
      ],
      "metadata": {
        "id": "cGvQfGqJDV9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create agent configs\n",
        "# NOTE: we can use FunctionAgent or ReActAgent here.\n",
        "# FunctionAgent works for LLMs with a function calling API.\n",
        "# ReActAgent works for any LLM.\n",
        "calculator_agent = ReActAgent(\n",
        "    name=\"calculator\",\n",
        "    description=\"Performs basic arithmetic operations\",\n",
        "    system_prompt=\"You are a calculator assistant. Use your tools for any math operation.\",\n",
        "    tools=[add, subtract],\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "query_agent = ReActAgent(\n",
        "    name=\"info_lookup\",\n",
        "    description=\"Looks up information about XYZ\",\n",
        "    system_prompt=\"Use your tool to query a RAG system to answer information about XYZ\",\n",
        "    tools=[query_engine_tool],\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "# Create and run the workflow\n",
        "agent = AgentWorkflow(\n",
        "    agents=[calculator_agent, query_agent],\n",
        "    root_agent='calculator'\n",
        ")"
      ],
      "metadata": {
        "id": "-8AH1qURDt4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = await agent.run(user_msg=\"Can you add 5 and 3?\")"
      ],
      "metadata": {
        "id": "kXPGzjfvD_YG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async for ev in handler.stream_events():\n",
        "    if isinstance(ev, ToolCallResult):\n",
        "        print(\"\")\n",
        "        print(\"Called tool: \", ev.tool_name, ev.tool_kwargs, \"=>\", ev.tool_output)\n",
        "    elif isinstance(ev, AgentStream):  # showing the thought process\n",
        "        print(ev.delta, end=\"\", flush=True)\n",
        "\n",
        "resp = await handler\n",
        "resp"
      ],
      "metadata": {
        "id": "KsBOUL0KEE12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Agentic Workflows in LlamaIndex"
      ],
      "metadata": {
        "id": "VmFnAgPWEIN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A workflow in LlamaIndex provides a structured way to organize our code into sequential and manageable steps.\n",
        "\n",
        "Such a workflow is created by defining `Steps` which are triggered by `Events`, and themselves emit `Events` to trigger further steps.\n",
        "\n",
        "Benefits of workflows:\n",
        "- clear organization of code into discrete steps\n",
        "- event-driven architecture for flexible control flow\n",
        "- type-safe communication between steps\n",
        "- built-in state management\n",
        "- support for both simple and complex agent interactions"
      ],
      "metadata": {
        "id": "lrLd0hsY7jQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU llama-index llama-index-vector-stores-chroma llama-index-utils-workflow llama-index-llms-huggingface-api pyvis"
      ],
      "metadata": {
        "id": "M8Bm2GZZEMFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Workflow Creation"
      ],
      "metadata": {
        "id": "8sBtmYYq8Kse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create a single-step workflow by defining a class that inherits from `Workflow` and decorating our functions with `@step`. We will also need to add `StartEvent` and `StopEvent`, which are special events that are used to indicate the start and end of the workflow."
      ],
      "metadata": {
        "id": "PtjTspwH8NmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import StartEvent, StopEvent, Workflow, step\n",
        "\n",
        "class MyWorkflow(Workflow):\n",
        "    @step\n",
        "    async def my_step(self, ev: StartEvent) -> StopEvent:\n",
        "        # do something here\n",
        "        return StopEvent(result=\"Hello, world!\")\n",
        "\n",
        "\n",
        "w = MyWorkflow(timeout=10, verbose=False)\n",
        "result = await w.run()\n",
        "result"
      ],
      "metadata": {
        "id": "EgQMbwZ88MXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connecting Multiple Steps"
      ],
      "metadata": {
        "id": "OmNeQSXM8vgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To connect multiple steps, we **create custom events that carry data between steps**. To do so, we need to add an `Event` that is passed between the steps and transfers the output of the first step to the second step."
      ],
      "metadata": {
        "id": "_9v4KYx88yc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import Event\n",
        "\n",
        "class ProcessingEvent(Event):\n",
        "    intermediate_result: str\n",
        "\n",
        "\n",
        "class MultiStepWorkflow(Workflow):\n",
        "    @step\n",
        "    async def step_one(self, ev: StartEvent) -> ProcessingEvent:\n",
        "        # Process initial data\n",
        "        return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
        "\n",
        "    @step\n",
        "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
        "        # Use the intermediate result\n",
        "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
        "        return StopEvent(result=final_result)\n",
        "\n",
        "\n",
        "w = MultiStepWorkflow(timeout=10, verbose=False)\n",
        "result = await w.run()\n",
        "result"
      ],
      "metadata": {
        "id": "zJb4rzvZ8xt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The type hinting is important here, as it ensures that the workflow is executed correctly."
      ],
      "metadata": {
        "id": "46JWM2BJ9k6_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loops and Branches"
      ],
      "metadata": {
        "id": "o6aUhqDP9m4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The type hinting allows us to create branches, loops, and joins to facilitate more complex workflows. We can create a loop by using the union operator `|`."
      ],
      "metadata": {
        "id": "FH0St_L19rJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import Event\n",
        "import random\n",
        "\n",
        "\n",
        "class ProcessingEvent(Event):\n",
        "    intermediate_result: str\n",
        "\n",
        "class LoopEvent(Event):\n",
        "    loop_output: str\n",
        "\n",
        "\n",
        "class MultiStepWorkflow(Workflow):\n",
        "    @step\n",
        "    async def step_one(self, ev: StartEvent | LoopEvent) -> ProcessingEvent | LoopEvent:\n",
        "        if random.randint(0, 1) == 0:\n",
        "            print(\"Bad thing happened\")\n",
        "            return LoopEvent(loop_output=\"Back to step one.\")\n",
        "        else:\n",
        "            print(\"Good thing happened\")\n",
        "            return ProcessingEvent(intermediate_result=\"First step complete.\")\n",
        "\n",
        "    @step\n",
        "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
        "        # Use the intermediate result\n",
        "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
        "        return StopEvent(result=final_result)\n",
        "\n",
        "\n",
        "w = MultiStepWorkflow(verbose=False)\n",
        "result = await w.run()\n",
        "result"
      ],
      "metadata": {
        "id": "amsDIlg69mhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drawing Workflows"
      ],
      "metadata": {
        "id": "WBzF0msc-fpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.utils.workflow import draw_all_possible_flows\n",
        "\n",
        "draw_all_possible_flows(w)"
      ],
      "metadata": {
        "id": "jokz9fdG-hK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### State Management"
      ],
      "metadata": {
        "id": "CZhCsvuj-qoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "State management is useful when we want to keep track of the state of the workflow, so that every step has access to the same state. Instead of passing the event information between steps, we can use the `Context` type hint to pass information between steps. This may be useful for long running workflows, where we want to store information between steps."
      ],
      "metadata": {
        "id": "zJVfI110-t-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import Context, StartEvent, StopEvent\n",
        "\n",
        "class ProcessingEvent(Event):\n",
        "    intermediate_result: str\n",
        "\n",
        "\n",
        "class MultiStepWorkflow(Workflow):\n",
        "    @step\n",
        "    async def step_one(self, ev: StartEvent, ctx: Context) -> ProcessingEvent:\n",
        "        # Process initial data\n",
        "        await ctx.set(\"query\", \"What is the capital of France?\")\n",
        "        return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
        "\n",
        "    @step\n",
        "    async def step_two(self, ev: ProcessingEvent, ctx: Context) -> StopEvent:\n",
        "        # Use the intermediate result\n",
        "        query = await ctx.get(\"query\")\n",
        "        print(f\"Query: {query}\")\n",
        "\n",
        "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
        "        return StopEvent(result=final_result)\n",
        "\n",
        "\n",
        "w = MultiStepWorkflow(timeout=10, verbose=False)\n",
        "result = await w.run()\n",
        "result"
      ],
      "metadata": {
        "id": "EJSmMb3R-sjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automating Workflows with Multi-Agent Workflows"
      ],
      "metadata": {
        "id": "jBSUtzEx_vXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of manual workflow creation, we can use the `AgentWorkflow` class to create **a multi-agent workflow**.\n",
        "\n",
        "The `AgentWorkflow` usess Workflow Agents to allow us to create a system of one or more agents that can collaborate and hand off tasks to each other based on their specialized capabilities. This enables building complex agent systems where different agents handle different aspects of a task."
      ],
      "metadata": {
        "id": "W4bGnoK8_zb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent.workflow import AgentWorkflow, ReActAgent\n",
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "\n",
        "# Define some tools\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two numbers.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two numbers.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")"
      ],
      "metadata": {
        "id": "HvgVVk0r_ytO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can pass functions directly without FunctionTool -- the fn/docstring are parsed for the name/description\n",
        "multiply_agent = ReActAgent(\n",
        "    name='multiply_agent',\n",
        "    description='Is able to multiply two intergers',\n",
        "    system_prompt='A helpful assistant that can use a tool to multiply numbers.',\n",
        "    tools=[multiply],\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "addition_agent = ReActAgent(\n",
        "    name='addition_agent',\n",
        "    description='Is able to add two intergers',\n",
        "    system_prompt='A helpful assistant that can use a tool to add numbers.',\n",
        "    tools=[add],\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "# Create the workflow\n",
        "workflow = AgentWorkflow(\n",
        "    agents=[multiply_agent, addition_agent],\n",
        "    root_agent='multiply_agent',\n",
        ")"
      ],
      "metadata": {
        "id": "mrw6XBG7AciW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = await workflow.run(user_msg=\"Can you add 5 and 3?\")\n",
        "response"
      ],
      "metadata": {
        "id": "5GQfzThvA4a-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}