{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM0vmj04XL4vKkp2F9hM0wq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Analyzing Artistic Styles with Multimodal Embeddings"],"metadata":{"id":"fyu-7t5EokWS"}},{"cell_type":"markdown","source":["Visual data is information-rich, but the unstructured nature of that data make it difficult to analyze.\n","\n","In this example, we will explore multimodal embeddings and computed attributes to analyze artistic styles in images.\n","\n","We will use the [`wikiart`](https://huggingface.co/datasets/huggan/wikiart) dataset and the [FiftyOne](https://docs.voxel51.com/index.html) for data analysis and visualization."],"metadata":{"id":"8cXuSZGEorHa"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"ai9PTozspK88"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YlmCwMDKobqN"},"outputs":[],"source":["!pip install -qU transformers huggingface_hub fiftyone umap-learn hf-transfer"]},{"cell_type":"code","source":["# HF Transfer makes downloads fast\n","import os\n","os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1"],"metadata":{"id":"OS6VJWN-pQTy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import fiftyone as fo # base library and app\n","import fiftyone.zoo as foz # zoo datasets and models\n","import fiftyone.brain as fob # ML routines\n","from fiftyone import ViewField as F # for defining custom views\n","import fiftyone.utils.huggingface as fouh # for loading datasets from HuggingFace"],"metadata":{"id":"h2NjAm1WpX5e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset"],"metadata":{"id":"C9e0j9oiq4Bl"}},{"cell_type":"markdown","source":["We start by loading the WikiArt dataset from HuggingFace Hub into FiftyOne. This dataset can also be loaded through HuggingFace's `datasets` library, but here we use FiftyOne's HF Hub integration to get the data directly from the Datasets server."],"metadata":{"id":"mA70OGN6q5GF"}},{"cell_type":"code","source":["dataset = fouh.load_from_hub(\n","    'huggan/wikiart', # repo_id\n","    format='parquet',\n","    classification_fields=['artist', 'style', 'genre'], # columns to store as classification fields\n","    max_samples=1000, # number of samples to load\n","    name='wikiart', # name of the dataset in FiftyOne\n",")"],"metadata":{"id":"vdWiDRpgq4o9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset"],"metadata":{"id":"0uRtRiVVtGJI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can visualize the dataset in the FiftyOne App:"],"metadata":{"id":"slnqRC49tixT"}},{"cell_type":"code","source":["session = fo.launch_app(dataset)"],"metadata":{"id":"L2Wh9ZAPtk3x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can list the names of the artists whose styles we will be analyzing:"],"metadata":{"id":"y8rcOxHju2Jc"}},{"cell_type":"code","source":["artists = dataset.distinct('artist.label')\n","artists"],"metadata":{"id":"SdSQt3Liu68A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Find similar artwork"],"metadata":{"id":"8JueFUuJu_JC"}},{"cell_type":"markdown","source":["By using multimodal embeddings, we will unlock the ability to find paintings that closely resemble a given text query, which could be a description of painting or even a poem.\n","\n","To generate multimodal embeddings for the images, we will use a pretrained CLIP Vision Transformer (ViT) model from HuggingFace Transformers. We will run `compute_similarity` from the [FiftyOne Brain](https://docs.voxel51.com/brain.html) to compute these embeddings and use them to generate a similarity index on the dataset."],"metadata":{"id":"ijBjPl-fx8BX"}},{"cell_type":"code","source":["# fiftyone brain\n","fob.compute_similarity(\n","    dataset,\n","    model='zero-shot-classification-transformer-torch' # model to load from model zoo\n","    name_or_path='openai/clip-vit-base-patch32', # repo_id of checkpoint\n","    embeddings='clip_embeddings', # name of the field to store embedings\n","    brain_key='clip_sim', # key to store similarity index info\n","    batch_size=32, # batch_size for inference\n",")"],"metadata":{"id":"vA158ZTKvBtz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If we want to load the model directly from HuggingFace Transformers library,"],"metadata":{"id":"XrfecgpJzXAy"}},{"cell_type":"code","source":["from transformers import CLIPModel\n","\n","model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n","\n","fob.compute_similarity(\n","    dataset,\n","    model=model,\n","    embeddings='clip_embeddings',\n","    brain_key='clip_sim'\n",")"],"metadata":{"id":"2a37JzaAzfAL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once the embeddings are generated, we can refresh the FiftyOne App, select the checkbox for an image in the sample grid, and click the photo icon to see the most similar images in the dataset.\n","\n","Clicking this button triggers a query to the similarity index to find the most similar images to the selected image, based on the pre-computed embeddings, and displays them in the App.\n","\n","This is useful for finding similar art pieces (to recommend to users or add to a collection) or getting inspiration for a new piece.\n","\n","Because CLIP is multimodal, we can also use it to perform semantic searches, which means that we can search for images based on text quueries. For example, we can search for \"pastel trees\" and see all the images in the dataset that are similar to that query.\n","\n","Behind the scenes, the text is tokenized, embedded with CLIPâ€™s text encoder, and then used to query the similarity index to find the most similar images in the dataset."],"metadata":{"id":"Da-5DjGzzphz"}},{"cell_type":"markdown","source":["## Uncover artistic motifs with clustering and visualization"],"metadata":{"id":"o-eO8yD70c44"}},{"cell_type":"markdown","source":["By performing similarity and semantic searches, we can begin to interact with the data more effectively.\n","\n","We can also add some unsupervised learning here to help us identify artistic patterns in the WikiArt dataset.\n","1. **Dimensionality reduction**: We will use UMAP to reduce the dimensionality of the embeddings to 2D and visualize the data in a scatter plot. This will allow us to see how the images cluster based on their style, genre, and artist.\n","2. **Clustering**: We will use K-Means clustering to cluster the images based on their embeddings and see what groups emerge."],"metadata":{"id":"QJ1ElC2ej0S1"}},{"cell_type":"code","source":["fob.computer_visualization(\n","    dataset,\n","    embeddings='clip_embeddings',\n","    method='umap',\n","    brain_key='clip_vis'\n",")"],"metadata":{"id":"nKyM-x1e0f9M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here We pass in the previously computed embeddings `'clip_embeddings'` and specify `method='umap'` to use UMAP for dimensionality reduction.\n","\n","After that we can open a panel in the FiftyOne App, where we will see one 2D point for each image in the dataset.\n","\n","We can also run clustering on the embeddings to group similar images together. To cluster our data, we will need to download the FiftyOne Clustering Plugin:"],"metadata":{"id":"uLdRJM60klBt"}},{"cell_type":"code","source":["!fiftyone plugins download https://github.com/jacobmarks/clustering-plugin"],"metadata":{"id":"EsVK-ukClM3I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once installed, we need to refresh the app again, and then we can access the clustering functionality via an operator in the app."],"metadata":{"id":"OpzWRoh9lNtD"}},{"cell_type":"markdown","source":["## Identify the most unique works of art"],"metadata":{"id":"of55Y8FwlVaq"}},{"cell_type":"markdown","source":["Our image embeddings allow us to quantitatively assign each sample a uniqueness score based on how similar it is to other samples in the dataset. Explicitly, the FiftyOne Brain's `compute_uniqueness()` function looks at the distance between each sample's embedding and its nearest neighbors, and computes a score between 0 and 1 based on this distance. A score of 0 means the sample is nondescript or very similar to others."],"metadata":{"id":"IzsEd-BSlYo1"}},{"cell_type":"code","source":["fob.compute_uniqueness(\n","    dataset,\n","    embeddings='clip_embeddings'\n",")"],"metadata":{"id":"GCIYO2AilXtd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can then color by this in the embeddings panel, filter by uniqueness score, or even sort it to see the most unique images in the dataset."],"metadata":{"id":"EYDW7LPWl9Cj"}},{"cell_type":"code","source":["most_unique_view = dataset.sort_by('uniqueness', reverse=True)\n","seesion.view = most_unique_view.view()"],"metadata":{"id":"2woGnfZCmEdq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["least_unique_view = dataset.sort_by(\"uniqueness\", reverse=False)\n","session.view = least_unique_view.view()  # Least unique images"],"metadata":{"id":"lPS_886kmJxY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also answer the question of which artist tends to produce the most unique works. We can compute the average uniqueness score for each artist across all of their works of art:"],"metadata":{"id":"RYkBXVStmMDG"}},{"cell_type":"code","source":["artist_unique_scores = {\n","    artist: dataset.match(F('artist.label') == artist).mean('uniqueness')\n","    for artist in artists\n","}\n","\n","sorted_artists = sorted(\n","    artist_unique_scores,\n","    key=artist_unique_scores.get,\n","    reverse=True\n",")\n","\n","for artist in sorted_artists:\n","    print(f\"{artist}: {artist_unique_scores[artist]}\")"],"metadata":{"id":"p3KZPBMQmTnn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["kustodiev_view = dataset.match(F(\"artist.label\") == \"boris-kustodiev\")\n","session.view = kustodiev_view.view()"],"metadata":{"id":"9Pe_-EGdmsCv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Characterize art with visual qualities"],"metadata":{"id":"YKz6x9gPmtVW"}},{"cell_type":"markdown","source":["We will compute standard metrics like brightness, contrast, and saturation for each image and see how these metrics correlate with the artistic style and genre of the art pieces.\n","\n","We will need to download the FiftyOne Image Quality Plugin:"],"metadata":{"id":"NvBHbblVmxrH"}},{"cell_type":"code","source":["!fiftyone plugins download https://github.com/jacobmarks/image-quality-issues/"],"metadata":{"id":"w_IkTHwKmwrW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Refresh the app and open the operators list again. This time type compute and select one of the image quality operators."],"metadata":{"id":"8S1oZvQOnC6V"}},{"cell_type":"code","source":[],"metadata":{"id":"akxa-BIInAtd"},"execution_count":null,"outputs":[]}]}