{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNFd8fu6nbYuR7jLgleYx16"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Implementing Semantic Cache to Improve a RAG System with FAISS"],"metadata":{"id":"I_nko0FunhQ9"}},{"cell_type":"markdown","source":["In this example, we will explore a typical RAG solution where we will utilize an open-source model and the vector database Chroma DataBase. We will integrate a **semantic cache system** that will store various user queries and decide whether to generate the prompt enriched with information from the vector database or the cache.\n","\n","A **semantic caching system** aims to identify similar or identical user requests. When a matching request is found, the system retrieves the corresponding information from the **cache**, reducing the need to fetch it from the *original source*.\n","\n","As the comparison takes into account the semantic meansing of the requests, they do not have to be identical for the system to recognize them as the same question. They can be formulated differently or contain inaccuracies, be typographical or in the sentence structure, and we can still identify that the user is actually requesting the same information.\n","\n","For example, if a user has three queries, like \"What is the capital of France?\", \"Tell me the name of the capital of France?\", and \"What the capital of France is?\", all convey the same intent and should be identified as the same question.\n","\n","While the model's response may differ based on the request for a concise answer, the information retrieved from the vector database should be the same. Therefore, the cache system is inserted between the user and the vector database, instead of between the user and the LLM.\n","```\n","Documents --> ChromaDB <--> SemanticCache <-- UserQuery\n","                                |--> AugmentedPrompt --> LLM\n","```\n","\n","To enhance the performance of RAG system in production, we may need one or multiple semantic caches. This cache retains the results of previous requests, and before resolving a new request, it checks if a similar one has been received before. If so, instead of re-executing the process, it retrieves the information from the cache.\n","\n","In a RAG system, there are two points that are time consuming:\n","* Retrieving the information used to construct the enriched prompt\n","* Calling the LLM to obtain the response\n","\n","In both points, a semantic cache system can be implemented, and we could even have two caches, one for each point. Note that placing the cache at the model's response point may lead to a loss of influence over the obtained response. For example, our cache system may consider \"Explain the French Revolution in 10 words\" and \"Explain the French Resolution in 100 words\" as the same query. If our cache system stores model responses, the resopnses may not follow the user instructions accurately.\n","\n","In this example, we will place the semantic cache system between the user's request and the information retrieval from the vector database."],"metadata":{"id":"4d5S6HaGwNa0"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"kEYyYusAz5gD"}},{"cell_type":"markdown","source":["In this example, we will need\n","* `sentence_transformers` - transform the sentences into fixed-length vectors, AKA, embeddings\n","* `xformers` - provide libraries to facilitate the work with `transformers` models.\n","* `chromadb` - vector database\n","* `accelerate` - run model in a GPU"],"metadata":{"id":"vDvT-ybZz6rb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rhdo7gp5nbyg"},"outputs":[],"source":["!pip install -qU transformers accelerate sentence-transformers xformers chromadb datasets faiss-cpu torch"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd"],"metadata":{"id":"3Kma1Zdw0cNU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load the Dataset"],"metadata":{"id":"w2cxbK7-0clp"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","data = load_dataset(\n","    'keivalya/MedQuad-MedicalQnADataset',\n","    split='train'\n",")"],"metadata":{"id":"MBV1lYNn0gGX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ChromaDB requires that the data has a unique identifier."],"metadata":{"id":"AZCZw9SV0qKj"}},{"cell_type":"code","source":["data = data.to_pandas()\n","data['id'] = data.index\n","data.head(5)"],"metadata":{"id":"bvVfNWnU0tu7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# some constants\n","MAX_ROWS = 15000\n","document = 'Answer'\n","TOPIC = 'qtype'\n","\n","subset_data = data.head(MAX_ROWS)"],"metadata":{"id":"_uL5ub-T00MN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import and configure the vector database"],"metadata":{"id":"hl6LaC0M08sV"}},{"cell_type":"code","source":["import chromadb\n","\n","# set the path where the vector database will be stored\n","chroma_client = chromadb.PersistentClient(path='./chroma_db')"],"metadata":{"id":"rpzd3CeN0-nb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fill and query the chromaDB database"],"metadata":{"id":"HHNDRM1P1MWt"}},{"cell_type":"markdown","source":["The data in ChromaDB is stored in collections. If the collection exists, we need to delete it."],"metadata":{"id":"0zcjOEs61RC1"}},{"cell_type":"code","source":["collection_name = 'news_collection'\n","\n","if len(chroma_client.list_collections()) > 0 and collection_name in [chroma_client.list_collections()[0].name]:\n","    chroma_client.delete_collection(name=collection_name)\n","\n","collection = chroma_client.create_collection(name=collection_name)"],"metadata":{"id":"oQNXap2a1Qjn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we will add the data to the collection with the following information:\n","* `documents` to store the content of the `Answer` column in the dataset\n","* `metadatas` to store a list of topics. Here is the `qtype` column\n","* `id` to store unique identifiers for each row."],"metadata":{"id":"_vv0PkT71nUR"}},{"cell_type":"code","source":["collection.add(\n","    documents=subset_data[DOCUMENT].tolist(),\n","    metadatas=[{TOPIC: topic} for topic in subset_data[TOPIC].tolist()],\n","    ids=[f\"id{x} for x in range(MAX_ROWS)\"]\n",")"],"metadata":{"id":"503VzX0l19W4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once we have the inforamtion in the database, we can query it and ask for data that matches our needs. The search is done inside the content of the document. The result will be based on the similarity between the search terms and the content of documents.\n","\n","Metadata is not directly involved in the initial search process, but it can be used to filter or refine the results after retrieval, enabling further customization and precision."],"metadata":{"id":"7ypahKdG2IJX"}},{"cell_type":"code","source":["def query_database(query_text, n_results=10):\n","    results = collection.query(\n","        query_texts=query_text,\n","        n_results=n_results\n","    )\n","    return results"],"metadata":{"id":"rz2_zfdn2iFi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create the semantic cache system"],"metadata":{"id":"O9jLsno42qtL"}},{"cell_type":"markdown","source":["To implement the cache system, we will use FAISS, a library that allows storing embeddings in memory. It is similar to what Chroma does, but without its persistence. Here we will create a class called `SemanticCache` that works with its own encoder and provide the necessary functions for the user to perform queries.\n","\n","In this class, we first query the cache implemented with FAISS that contains the previous petitions, and if the returned results are above a specified threshold, it will return the content of the cache. Otherwise, it will fetch the result from the Chroma database. The cache is stored in a JSON file."],"metadata":{"id":"SDLYmS3OiuAO"}},{"cell_type":"code","source":["import faiss\n","from sentence_transformers import SentenceTransformer\n","import time\n","import json"],"metadata":{"id":"X-ip-0Il2s-c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will implement a `init_cache()` function, which employs the `FlatLS` index. We choose this index because it aligns well with the example. It can be used with vectors of high dimensions, consumes minimal memory, and performs well with small datasets.\n","\n","There are other indexing options available with FAISS:\n","* `FlatL2` or `FlatIP` - well-suited for small datasets, it may not be the fastest, but its memory consumption is not excessive\n","* `LSH` - works effectively with small datasets and is recommended for use with vectors of up to 128 dimensions\n","* `HNSW` - very fast but demands a substantial amount of RAM\n","* `IVF` - works well with large datasets without consuming much memory or compromising performance"],"metadata":{"id":"TQBXpmwYjaNT"}},{"cell_type":"code","source":["def init_cache():\n","    index = faiss.IndexFlatL2(768)\n","    if index.is_trained:\n","        print('Index trained')\n","\n","    # Initialize sentence transformer model\n","    encoder = SentenceTransformer('all-mpnet-base-v2')\n","\n","    return index, encoder"],"metadata":{"id":"pPXKkCsLkMn3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We also need a `retrieve_cache` function to retrieve a JSON file from disk in case there is a need to reuse the cache across sessions."],"metadata":{"id":"eI6p8cFRkY0B"}},{"cell_type":"code","source":["def retrieve_cache(json_file):\n","    try:\n","        with open(json_file, 'r') as file:\n","            cache = json.load(file)\n","    except FileNotFoundError:\n","        cache = {'question': [], 'embeddings': [], 'answers': [], 'response_text':[]}\n","\n","    return cache"],"metadata":{"id":"LrwVW5q4lLIA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `store_cache` function saves the file containing the cache data to disk."],"metadata":{"id":"jB7mDF7Xlt8P"}},{"cell_type":"code","source":["def store_cache(json_file, cache):\n","    with open(json_file, 'w') as file:\n","        json.dump(cache, file)"],"metadata":{"id":"uZs2y09Xlx7y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SemanticCache:\n","    def __init__(self, json_file='cache_file.json', threshold=0.35, max_repsonse=100, eviction_policy=None):\n","        \"\"\"Initialize the semantic cache\n","\n","        Parameters\n","        ----------\n","        json_file: str\n","            The name of the JSON file where the cache is stored\n","        threshold: float\n","            The threshold for the Euclidean distance to determine if a question is similar\n","        max_response: int\n","            The maximum number of responses the cache can store\n","        eviction_policy: str\n","            The policy for evicting items from the cache.\n","            This can be any policy, but 'FIFO' (First In First Out) has been implemented for now.\n","            If None, no eviction policy will be applied\n","        \"\"\"\n","        # Initialize FAISS index with Eucliean distance\n","        self.index, self.encoder = init_cache()\n","\n","        # Set Eucliean distance threshold\n","        # A distance of 0 means identical sentences\n","        # We only return from cache sentences under this threshold\n","        self.euclidean_threshold = threshold\n","\n","        self.json_file = json_file\n","        self.cache = retrieve_cache(self.json_file)\n","        self.max_response = max_response\n","        self.eviction_policy = eviction_policy\n","\n","    def evict(self):\n","        \"\"\"Evict an item from the cache based on the eviction policy\"\"\"\n","        if self.eviction_policy and len(self.cache['questions']) > self.max_size:\n","            for _ in range((len(self.cache['questions']) - self.max_response)):\n","                if self.eviction_policy = 'FIFO':\n","                    self.cache['questions'].pop(0)\n","                    self.cache['embeddings'].pop(0)\n","                    self.cache['answers'].pop(0)\n","                    self.cache['response_text'].pop(0)\n","\n","    def ask(self, question: str) -> str:\n","        # Method to retrieve an answer from the cache or generate a new one\n","        state_time = time.time()\n","        try:\n","            # First we obtain the embeddings corresponding to the user question\n","            embedding = self.encoder.encode([question])\n","\n","            # Search for the nearest neighbor in the index\n","            self.index.nprobe = 8\n","            D, I = self.index.search(embedding, 1)\n","\n","            if D[0] >= 0:\n","                if I[0][0] >= 0 and D[0][0] <= self.euclidean_threshold:\n","                    row_id = int(I[0][0])\n","\n","                    print('Answer recovered from Cache.')\n","                    print(f\"{D[0][0]:.3f} smaller than {self.euclidean_threshold}\")\n","                    print(f\"Found cache in row: {row_id} with score {D[0][0]:.3f}\")\n","                    print(f\"response_text: {self.cache['response_text'][row_id]}\")\n","\n","                    end_time = time.time()\n","                    elapsed_time = end_time - start_time\n","                    print(f\"TIme taken: {elapsed_time:.3f} seconds\")\n","\n","                    return self.cache['response_test'][row_id]\n","\n","            # Handle the case when there are not enough results\n","            # or Euclidean distance is not met, asking to chromaDB\n","            answer = query_database([question], 1)\n","            response_text = answer['documents'][0][0]\n","\n","            self.cache['questions'].append(question)\n","            self.cache['embeddings'].append(embedding[0].tolist())\n","            self.cache['answers'].append(answer)\n","            self.cache['response_text'].append(response_text)\n","\n","            print('Answer recovered from ChromaDB.')\n","            print(f\"response_text: {response_text}\")\n","\n","            self.index.add(embedding)\n","            self.evict()\n","\n","            store_cache(self.json_file, self.cache)\n","            end_time = time.time()\n","            elapsed_time = end_time - start_time\n","            print(f\"TIme taken: {elapsed_time:.3f} seconds\")\n","\n","            return response_text\n","\n","        except Exception as e:\n","            raise RuntimeError(f\"Error during 'ask' method: {e}\")"],"metadata":{"id":"wy8v7Lf9mlBa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test the `SemanticCache` class"],"metadata":{"id":"CCYnnr9uqdbM"}},{"cell_type":"code","source":["cache = SemanticCache('4cache.json')"],"metadata":{"id":"zRCzwq3qqgb0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = cache.ask('How do vaccines work?')"],"metadata":{"id":"Pa9S107Eqks7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If we send a second question that is quite different, the response should also be retreived from ChromaDB. This is because the question stored previously is so dissimilar that it would surpass the specified threshold in terms of Euclidean distance."],"metadata":{"id":"nFaycrtaqpGW"}},{"cell_type":"code","source":["results = cache.ask('Explain briefly what is a Sydenham chorea')"],"metadata":{"id":"Z6ROT_l2q1px"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now if we test it with a question very similar to the one we just asked. The response should come directly from the cache without the need to access the ChromaDB database."],"metadata":{"id":"kqdm1QdTrAEl"}},{"cell_type":"code","source":["results = cache.ask('Briefly explain me what is a Sydenham chorea')"],"metadata":{"id":"XJioU6fsrKdP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The previous two questions are so similar that their Euclidean distance is truly minimal, almost as if they were identical.\n","\n","If we ask a more distinct question,"],"metadata":{"id":"XCVeF399rP6Q"}},{"cell_type":"code","source":["question_ref = 'Write in 20 words what is a Sydenham chorea'\n","results = cache.ask(question_ref)"],"metadata":{"id":"JBSDTQDer-Lq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We see that the Euclidean distance has increased, but it still remains within the specified threshold."],"metadata":{"id":"91xq_GhLsEQk"}},{"cell_type":"markdown","source":["## Load the model and create the prompt"],"metadata":{"id":"mW7J1HZCsMa8"}},{"cell_type":"code","source":["from torch import cuda, torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","device = f\"cuda:{cuda.current_device()}\" if cuda.is_avilable() else 'cpu'\n","\n","model_id = 'google/gemma-2b-it'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    device_map='cuda',\n","    torch_dtype=torch.bfloat16\n",")"],"metadata":{"id":"e_JDNmnlsUsB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create the extended prompt"],"metadata":{"id":"3iixaDNIsuqy"}},{"cell_type":"markdown","source":["To create the prompt, we use the result from query the `SemanticCache` class and the question introduced by the user.\n","\n","The prompt have two parts, the **relevant context** that is the information recovered from the database and the **user's question**. We only need to put the two parts together to create the prompt then send it to the model."],"metadata":{"id":"3IhYVjVVsyBg"}},{"cell_type":"code","source":["prompt_template = f\"Relevant context: {results}\\n\\nThe user's question: {question_ref}\"\n","prompt_template"],"metadata":{"id":"kLuwgAa2swuG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids = tokenizer(\n","    prompt_template,\n","    return_tensors='pt'\n",").to('cuda')\n","\n","outputs = model.generate(\n","    **input_ids,\n","    max_new_tokens=256\n",")\n","print(tokenizer.decode(outputs[0]))"],"metadata":{"id":"6t_UtujUtSIl"},"execution_count":null,"outputs":[]}]}