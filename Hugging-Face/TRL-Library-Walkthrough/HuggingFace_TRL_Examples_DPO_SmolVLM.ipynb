{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP6H8U+eAQ+1e9pPJlTs6KN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Fine-Tuning SmolVLM Using Direct Preference Optimization (DPO) with TRL on a Consumer GPU"],"metadata":{"id":"ZvQHyJNRD-TZ"}},{"cell_type":"markdown","source":["In this example, we will fine-tune [`SmolVLM`](https://huggingface.co/blog/smolvlm) using a preference dataset o help the model align with desired outputs. SmolVLM is a highly performant and memory-efficient model, making it ideal choice for this task.\n","\n","We will use the [`HuggingFaceH4/rlaif-v_formatted`](https://huggingface.co/datasets/HuggingFaceH4/rlaif-v_formatted) dataset, which contains pairs of `prompt + image` along with a `chosen` and `rejected` answer for each pair.\n","\n","The goal of this fine-tuning process is to make the model consistently prefer the **chosen answers** from the dataset, reducing hallucinations."],"metadata":{"id":"4Kbr9HTrEIkI"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"mhRMu6CCExEp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1_aA4igyD0VZ"},"outputs":[],"source":["!pip install  -U -q transformers trl datasets bitsandbytes peft accelerate\n","# Tested with transformers==4.46.3, trl==0.12.2, datasets==3.2.0, bitsandbytes==0.45.0, peft==0.14.0, accelerate==1.2.0"]},{"cell_type":"code","source":["!pip install -q flash-attn --no-build-isolation"],"metadata":{"id":"8-yCmb7xEzDh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","notebook_login()"],"metadata":{"id":"0C1eSTguE0i0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load dataset"],"metadata":{"id":"JtDw9EQdHXrR"}},{"cell_type":"markdown","source":["Training with DPO requires the dataset to be formatted with a `chosen` and `rejected` answers for each pair.\n","\n","In this example, we only use a subset of the dataset to demonstrate the process."],"metadata":{"id":"ViFht5DzHbPS"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset_id = 'HuggingFaceH4/rlaif-v_formatted'\n","\n","train_dataset, test_dataset = load_dataset(\n","    dataset_id,\n","    split=['train[:6%]', 'test[:1%]']\n",")"],"metadata":{"id":"gFCuJObNHYuy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We also need to ensure that all the images are in RGB format"],"metadata":{"id":"Q4zlSedJH28G"}},{"cell_type":"code","source":["from PIL import Image\n","\n","def ensure_rgb(example):\n","    # convert to RGB if it is not\n","    image = example['images'][0]\n","    if isinstance(image, Image.Image):\n","        if image.mode != 'RGB':\n","            image = image.convert('RGB')\n","        example['images'] = [image]\n","\n","    return example\n","\n","# apply this transformations\n","train_dataset = train_dataset.map(ensure_rgb, num_proc=32)\n","test_dataset = test_dataset.map(ensure_rgb, num_proc=32)"],"metadata":{"id":"9KL2-45AH56V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset[0]"],"metadata":{"id":"DWsS6dFdITjn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset[0]['images'][0]"],"metadata":{"id":"nt1YM3qlIVEN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fine-tune the model using `trl`"],"metadata":{"id":"hKjEzH1dIYcJ"}},{"cell_type":"markdown","source":["### Load the quantized model for training"],"metadata":{"id":"DAMxwRI7Iayh"}},{"cell_type":"code","source":["import torch\n","from transformers import Idefics3ForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n","\n","model_id = 'HuggingFaceTB/SmolVLM-Instruct'\n","\n","# bnb int-4 config\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type='nf4',\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","# load model\n","processor = AutoProcessor.from_pretrained(model_id)\n","model = Idefics3ForConditionalGeneration.from_pretrained(\n","    model_id,\n","    device_map='auto',\n","    torch_dtype=torch.bfloat16,\n","    quantization_config=bnb_config,\n","    _attn_implementation='flash_attention_2'\n",")"],"metadata":{"id":"O6OPujm1IaQa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Set up Q-LoRA and `DPOConfig`"],"metadata":{"id":"BXciVQydI2-e"}},{"cell_type":"code","source":["from peft import LoraConfig, get_peft_model\n","\n","# configure lora\n","peft_config = LoraConfig(\n","    r=8,\n","    lora_alpha=8,\n","    lora_dropout=0.1,\n","    target_modules=['down_proj', 'o_proj', 'k_proj', 'q_proj', 'v_proj', 'gate_proj', 'up_proj'],\n","    use_dora=True,\n","    init_lora_weights='gaussian'\n",")\n","\n","# apply peft\n","peft_model = get_peft_model(model, peft_config)\n","\n","peft_model.print_trainable_parameters()"],"metadata":{"id":"Xfc9Xp7kI6Zn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from trl import DPOConfig\n","\n","training_args = DPOConfig(\n","    output_dir='smolvlm-instruct-trl-dpo-rlaif-v',\n","    bf16=True,\n","    gradient_checkpointing=True,\n","    gradient_accumulation_steps=32,\n","    per_device_train_batch_size=1,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=5,\n","    dataset_num_proc=8, # tokenization will use 8 processes\n","    dataloader_num_workers=8, # data loading will use 8 workers\n","    logging_steps=10,\n","    report_to='tensorboard',\n","    push_to_hub=False,\n","    save_strategy='steps',\n","    save_steps=10,\n","    save_total_limit=1,\n","    eval_steps=10,\n","    eval_strategy='steps',\n",")"],"metadata":{"id":"B03Y9-D2JRPr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**DPO** uses labeled preference data to guide the model toward generating responses that align with preferences. `trl`'s `DPOTrainer` will tokenize the dataset before training and save it to disk. This process can consume significant disk space, depending on the amount of data used for training."],"metadata":{"id":"9NOJER3EJ39S"}},{"cell_type":"code","source":["from trl import DPOTrainer\n","\n","trainer = DPOTrainer(\n","    model=model,\n","    ref_model=None,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n","    peft_config=peft_config,\n","    tokenizer=processor\n",")"],"metadata":{"id":"D2zf4A1JKHBq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()\n","trainer.save_model(training_args.output_dir)"],"metadata":{"id":"gmdesEQdKP_9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test the fine-tuned model"],"metadata":{"id":"YQaVlcI2KVwq"}},{"cell_type":"code","source":["import gc\n","import time\n","\n","def clear_memory():\n","    # Delete variables if they exist in the current global scope\n","    if 'inputs' in globals(): del globals()['inputs']\n","    if 'model' in globals(): del globals()['model']\n","    if 'processor' in globals(): del globals()['processor']\n","    if 'trainer' in globals(): del globals()['trainer']\n","    if 'peft_model' in globals(): del globals()['peft_model']\n","    if 'bnb_config' in globals(): del globals()['bnb_config']\n","    time.sleep(2)\n","\n","    # Garbage collection and clearing CUDA memory\n","    gc.collect()\n","    time.sleep(2)\n","    torch.cuda.empty_cache()\n","    torch.cuda.synchronize()\n","    time.sleep(2)\n","    gc.collect()\n","    time.sleep(2)\n","\n","    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n","    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n","\n","clear_memory()"],"metadata":{"id":"C3OBbKQOKXzn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can reload the base model using the same pipeline:"],"metadata":{"id":"0eob2lmmKdE-"}},{"cell_type":"code","source":["processor = AutoProcessor.from_pretrained(model_id)\n","model = Idefics3ForConditionalGeneration.from_pretrained(\n","    model_id,\n","    device_map='auto',\n","    torch_dtype=torch.bfloat16,\n","    _attn_implementation='flash_attention_2'\n",")"],"metadata":{"id":"i9QJY_PJKfkC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will attach the trained adapter to the pretrained model. This adapter contains the fine-tuning adjustments made during training, enabling the base model to leverage the new knowledge while keeping its core parameters intact. By integrating the adapter, we enhance the model's capabilities without altering its original structure."],"metadata":{"id":"29CadQlpKoCL"}},{"cell_type":"code","source":["adapter_path = \"sergiopaniego/smolvlm-instruct-trl-dpo-rlaif-v\"\n","model.load_adapter(adapter_path)"],"metadata":{"id":"nn4xtSD4LCxO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example_test = test_dataset[0]\n","example_test"],"metadata":{"id":"mRz2-uWNLGi6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example_test['images'][0]"],"metadata":{"id":"95p0szA5LJxP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We need a function to evaluate the model's performance on multiple examples efficiently without needing to rewrite code for each one."],"metadata":{"id":"pm0ukPt_LMYo"}},{"cell_type":"code","source":["def generate_text_from_sample(model, processor, sample, max_new_tokens=1024, device='cuda'):\n","    # Prepare the text input by applying the chat template\n","    text_input = processor.apply_chat_template(\n","        sample['prompt'],\n","        add_generation_prompt=True\n","    )\n","\n","    image_inputs = []\n","    image = sample['images'][0]\n","    if image.mode != 'RGB':\n","        image = image.convert('RGB')\n","    image_inputs.append([image])\n","\n","    # Prepare the inputs for the model\n","    model_inputs = processor(\n","        text=text_input,\n","        images=image_inputs,\n","        return_tensors=\"pt\",\n","    ).to(device)  # Move inputs to the specified device\n","\n","    # Generate text with the model\n","    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n","\n","    # Trim the generated ids to remove the input ids\n","    trimmed_generated_ids = [\n","        out_ids[len(in_ids):] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)\n","    ]\n","\n","    # Decode the output text\n","    output_text = processor.batch_decode(\n","        trimmed_generated_ids,\n","        skip_special_tokens=True,\n","        clean_up_tokenization_spaces=False\n","    )\n","\n","    return output_text[0]  # Return the first decoded output text"],"metadata":{"id":"JYhEPN2qLXjF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = generate_text_from_sample(model, processor, example_test)\n","output"],"metadata":{"id":"bNTU5qAULtYX"},"execution_count":null,"outputs":[]}]}