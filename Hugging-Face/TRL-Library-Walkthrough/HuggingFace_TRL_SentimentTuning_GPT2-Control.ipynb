{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOd9mlTieWz5JhLnOsnk1lB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Fine-Tune GPT2 to Generate Controlled Sentiment Reviews"],"metadata":{"id":"xun4MNKAM1SW"}},{"cell_type":"markdown","source":["In this example, we will optimize GPT2 to produce IMDB movie reviews with controlled setiment using a BERT sentiment classifier for rewards.\n","\n","This example is similar to the previous fine-tuned GPT2 to generate positive sentiments. However, we will fine-tune a GPT2 (small) to generate **controlled** moview reviews based on the IMDB dataset.\n","\n","The model gets the target sentiment and 5 tokens from a real review and is tasked to produce continuations with the targeted sentiment.\n","\n","The reward for the continuation is calculated with the logits of a BERT sentiment classifier, and then is used for PPO training."],"metadata":{"id":"J3fVFxmdNW67"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"9vX-y54nODu-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uqf8mFrFMpb3"},"outputs":[],"source":["import random\n","import torch\n","import wandb\n","import time\n","import os\n","from tqdm import tqdm\n","import numpy as np\n","import pandas as pd\n","from rnadom import choices\n","import matplotlib.pyplot as plt\n","\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, pipeline\n","from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n","\n","tqdm.pandas()"]},{"cell_type":"code","source":["sentiment_pipe_kwargs = {\n","    'top_k': None,\n","    'function_to_apply': 'none',\n","}\n","\n","config = PPOConfig(\n","    model_name='lvwerra/gpt2-imdb',\n","    steps=51200,\n","    learning_rate=1.41e-5,\n","    remove_unused_columns=False,\n","    log_with='wandb'\n",")\n","\n","text_in_len = 5\n","text_out_len = 20\n","seed = 1\n","\n","np.random.seed(seed)"],"metadata":{"id":"yGiufpbEP3Gg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will load a GPT2 model called `gpt2_imdb`, which was additionally fine-tuned on the IMDB dataset for 1 epoch. Other parameters are mostly taken from the original paper [*Fine-Tuning Language Models from Human Preferences*](https://huggingface.co/papers/1909.08593)."],"metadata":{"id":"0pqresH4R_sN"}},{"cell_type":"markdown","source":["## Load data and models"],"metadata":{"id":"GncafFZ0SXRZ"}},{"cell_type":"markdown","source":["### Load pretrained GPT2 language models"],"metadata":{"id":"jwnGAliGSYst"}},{"cell_type":"markdown","source":["We will load the GPT2 model with a value head and the tokenizer.\n","\n","Here, we need to load the model twice: the first model will be optimized while the second model serves as a reference to calcualte the KL-divergence from the starting point. This serves as an additional reward signal in the PPO training to make sure the optimized model does not deviate too much from the original language model."],"metadata":{"id":"5oG1jFGnUiVE"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n","ref_model = create_reference_model(model)"],"metadata":{"id":"7SDSg_MVSWc2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load IMDB dataset"],"metadata":{"id":"G1zg9EQIVAse"}},{"cell_type":"markdown","source":["We will load the IMDB dataset into a DataFrame and filter comments that are at least 500 characters long and take the first 1000 characters of each comment. The first filter is to avoid comments that are less than 500 characters long and the second to avoid tokenizing way more text than we actually need."],"metadata":{"id":"Z8DTDplsVEI1"}},{"cell_type":"code","source":["dataset = load_dataset('stanfordnlp/imdb', split='train')\n","dataset = dataset.rename_columns({'text': 'review', 'label': 'sentiment'})\n","\n","dataset = dataset.filter(lambda x: len(x['review']) > 500, batched=False)\n","dataset = dataset.map(lambda x: {'review': x['review'][:1000]}, batched=False)\n","\n","dataset"],"metadata":{"id":"fPqc8ufLVCY4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Tokenize IMDB reviews"],"metadata":{"id":"xTErYix4WNKs"}},{"cell_type":"markdown","source":["We need to tokenize all IMDB in advance to avoid tokenizing twice. In the first step we encode the queries and slice the first `text_in_len` tokens. In the second step we decode these tokens back to text for later display."],"metadata":{"id":"uQJKkep7WR2i"}},{"cell_type":"code","source":["dataset = dataset.map(\n","    lambda x: {\n","        'input_ids': tokenizer.encode(\" \" + x['review'], return_tensors='pt')[0, :text_in_len]\n","    },\n","    batched=False\n",")\n","\n","dataset = dataset.map(\n","    lambda x: {\n","        'query': tokenizer.decode(x['input_ids'])\n","    },\n","    batched=False\n",")\n","dataset = dataset[:20480]\n","\n","from datasets import Dataset\n","dataset = Dataset.from_dict(dataset)\n","dataset.set_format('pytorch')"],"metadata":{"id":"DimpStdVWPOt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset[0]['input_ids']"],"metadata":{"id":"oPLFm9CYXKJ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collator(data):\n","    return dict(\n","        (key, [d[key] for d in data])\n","        for key in data[0]\n","    )"],"metadata":{"id":"xE3DXywpXL65"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = PPOTrainer(\n","    config,\n","    model,\n","    ref_model=ref_model,\n","    tokenizer=tokenizer,\n","    train_dataset=dataset,\n","    data_collator=collator\n",")"],"metadata":{"id":"gkoEaSpgXSTY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load BERT classifier"],"metadata":{"id":"bUuwkzYlXZyN"}},{"cell_type":"markdown","source":["We will load a BERT classifier that is fine-tuned on the IMDB dataset"],"metadata":{"id":"_vXMQNmXXfnB"}},{"cell_type":"code","source":["if trainer.accelerator.num_processes == 1:\n","    device = 0 if torch.cuda.is_available() else 'cpu'\n","else:\n","    device = trainer.accelerator.device\n","\n","sentiment_pipeline = pipeline(\n","    'sentiment-analysis',\n","    'lvwerra/distilbert-imdb',\n","    device=device\n",")"],"metadata":{"id":"juWOyrkbXbB4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model outputs are the logits for the negative and positive classes. We will use the logits for positive class as a reward signal for the language model."],"metadata":{"id":"yu_navJGXymI"}},{"cell_type":"code","source":["text = \"this movie was really bad!!\"\n","output = sentiment_pipe(text, **sentiment_pipe_kwargs)\n","output"],"metadata":{"id":"sFtR10GmX53N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"this movie was really good!!\"\n","output = sentiment_pipe(text, **sentiment_pipe_kwargs)\n","output"],"metadata":{"id":"3LLOoO8LYHNp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"this movie was a documentary\"\n","output = sentiment_pipe(text, **sentiment_pipe_kwargs)\n","output"],"metadata":{"id":"b04ebwNeYJT3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The resulting reward signal:"],"metadata":{"id":"idWerRjbYPql"}},{"cell_type":"code","source":["def extract_pipe_output(outputs):\n","    positive_logits = []\n","    for out in outputs:\n","        for element in out:\n","            if element['label'] == 'POSITIVE':\n","                positive_logits.append(torch.tensor(element['score']))\n","\n","    return positive_logits"],"metadata":{"id":"tznQcWUNYREp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Control token dict"],"metadata":{"id":"LHUyuyKBYiMV"}},{"cell_type":"markdown","source":["We will append the control token at the beginning of each query to signal the model what the target sentiment is. Each control sequence consists of three tokens:"],"metadata":{"id":"6j0WuI_rYmpz"}},{"cell_type":"code","source":["ctrl_str = [\"[negative]\", \"[neutral]\", \"[positive]\"]\n","device = torch.device(\n","    'cuda' if torch.cuda.is_available() else 'cpu'\n",")\n","\n","ctrl_tokens = dict(\n","    (s, tokenizer.encode(s, return_tensors='pt').squeeze().to(device))\n","    for s in ctrl_str\n",")"],"metadata":{"id":"_wNO3SqyYmPU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this is why each control sequence has three tokens:\n","ctrl_tokens"],"metadata":{"id":"0vepgWcja3Cl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Reward function"],"metadata":{"id":"-KkZGMc3a9Cy"}},{"cell_type":"code","source":["def pos_logit_to_reward(logit, task):\n","    \"\"\"Take the positive sentiment logit and scale it for the task.\n","        task [negative]: reward = -logit\n","        task [neutral]: reward = -2 * abs(logit) + 4\n","        task [postive]: reward = logit\n","    \"\"\"\n","    for i in range(len(logit)):\n","        if task[i] == \"[negative]\":\n","            logit[i] = -logit[i]\n","        elif task[i] == '[neutral]':\n","            logit[i] = -2 * torch.abs(logit[i]) + 4\n","        elif task[i] == '[positive]':\n","            pass\n","        else:\n","            raise ValueError('task has to be in [0, 1, 2]!')\n","\n","    return logit"],"metadata":{"id":"1TLode2za-Vp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the examples below, we show the rewards for the cases where the classifier logit is 4, -4, and 0 for the three targets `'[negative]'`, `'[neutral]'`, and `'[positive]'`.\n","\n","Ideally, we want to use the logit output for each class individually, but since there is no dedicated class for neutral, we will use this as a workaround."],"metadata":{"id":"SATl7Wqlb_IJ"}},{"cell_type":"code","source":["ctrl_str"],"metadata":{"id":"fDxnfaO0cc9p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# logit is 4\n","pos_logits_to_reward(torch.Tensor([4, 4, 4]), ctrl_str)"],"metadata":{"id":"JWlArxwAceZP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# logit is -4\n","pos_logits_to_reward(torch.Tensor([-4, -4, -4]), ctrl_str)"],"metadata":{"id":"hCMijsoUck5I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# logit is 0\n","pos_logits_to_reward(torch.Tensor([0, 0, 0]), ctrl_str)"],"metadata":{"id":"NTSu_NK-cneN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generation settings"],"metadata":{"id":"JiaRZ30Bcq16"}},{"cell_type":"code","source":["generation_kwargs = {\n","    'min_length': -1,\n","    'top_k': 0.,\n","    'top_p': 1.,\n","    'do_smaple': True,\n","    'pad_token_id': tokenizer.eos_token_id,\n","    'max_new_tokens': text_out_len,\n","    'eos_token_id': -1\n","}"],"metadata":{"id":"tfdrJa3DcsMz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Optimize model"],"metadata":{"id":"DvUqqRWhc2jx"}},{"cell_type":"markdown","source":["The training loop consts of\n","1. get a batch of queries and create random controls\n","2. get the query responses from the policy\n","3. join query and responses and tokenize for BERT analysis\n","4. get sentiments for query/responses from BERT\n","5. optimize policy with PPO using the (query, response, reward) triplet\n","6. log all the training statistics"],"metadata":{"id":"5g8DEivpc35y"}},{"cell_type":"code","source":["for epoch in range(2):\n","    for batch in tqdm(trainer.dataloader):\n","        logs, game_data = dict(), dict()\n","\n","        # prepend a random control token\n","        task_list = cohices(ctrl_str, k=config.batch_size)\n","        game_data['query'] = [t + q for t,q in zip(task_list, batch['query'])]\n","        query_tensors = [\n","            torch.cat((ctrl_tokens[t], input_ids))\n","            for t, input_ids in zip(task_list, batch['input_ids'])\n","        ]\n","\n","        # get response from model\n","        response_tensors = []\n","        for query in query_tensors:\n","            response = trainer.generate(query, **generation_kwargs)\n","            response_tensors.append(response.squeeze()[-text_out_len:])\n","        game_data['response'] = [\n","            tokenizer.decode(r.squeeze()) for r in response_tensors\n","        ]\n","\n","        # sentiment analysis\n","        texts = [q + r for q,r in zip(batch['query'], game_data['response'])]\n","        logits = extract_pipe_output(sentiment_pipe(text, **sentiment_pipe_kwargs))\n","        rewards = pos_logit_to_reward(logits, task_list)\n","\n","        # run PPO training\n","        t = time.time()\n","        stats = trainer.step(query_tensors, response_tensors, rewards)\n","\n","        for cs in ctrl_str:\n","            key = 'env/reward_' + cs.strip('[]')\n","            stats[key] = np.mean(\n","                [r.cpu().numpy() for r, t in zip(rewards, task_list) if t == cs]\n","            )\n","\n","        trainer.log_stats(stats, game_data, rewards)"],"metadata":{"id":"ulPwoaUec3jw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model inspection"],"metadata":{"id":"6VncL8HAecj1"}},{"cell_type":"markdown","source":["We can have a look at the rewaqrd distribution. Both the negative and positive rewards are clearly shifted to high rewards. The neutral rewards, however, are still centered around zero."],"metadata":{"id":"oprulRQVeej_"}},{"cell_type":"code","source":["for ctrl_s in ctrl_str:\n","    plt.hist(\n","        [r for r, t in zip(logs['env/reward_disk'], task_list) if t == ctrl_s],\n","        density=True,\n","        alpha=0.5,\n","        label=ctrl_s\n","    )\n","plt.legend(loc='best')\n","plt.title('Reward distribution')\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"w4uJNH_hedqC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.save_pretrained('gpt2-imdb-controlled-sentiment')\n","tokenizer.save_pretrained('gpt2-imdb-controlled-sentiment')"],"metadata":{"id":"f_Kv61Oge9ZI"},"execution_count":null,"outputs":[]}]}