{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNwn3Ft04XztbEf09K03MHQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Pertrubed-Attention Guidance"],"metadata":{"id":"DXbBaXMfryYJ"}},{"cell_type":"markdown","source":["**Perturbed-Attention Guidance (PAG)** is a diffusion sampling guidance that improves sample quality across both unconditional and conditional settings, achieving this without requiring further training or the integration of external modules.\n","\n","PAG is designed to progressively enhance the structure of synthesized samples throughout the denoising process by considering the self-attention mechanisms' ability to capture structural information. It involves generating intermediate samples with degraded structure by substituting self-attention maps in diffusion UNet with an identity matrix, and guiding the denoising process away from these degraded samples."],"metadata":{"id":"Mjx3cVfRrxhe"}},{"cell_type":"markdown","source":["## General tasks"],"metadata":{"id":"fIsI599ZsZ8G"}},{"cell_type":"markdown","source":["To enable PAG, we can load the pipeline using the `AutoPipelin` API with the `enable_pag=True` and the `pag_applied_layers` argument."],"metadata":{"id":"wOUO4YKqsdqn"}},{"cell_type":"markdown","source":["##### Text-to-image"],"metadata":{"id":"iEkc9jRMstj5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JvODOjQ4rttW"},"outputs":[],"source":["import torch\n","from diffusers import AutoPipelineForText2Image\n","from diffusers.utils import load_image, make_image_grid\n","\n","pipeline = AutoPipelineForText2Image.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    enable_pag=True,\n","    pag_applied_layers=['mid'],\n","    torch_dtype=torch.float16,\n",")\n","pipeline.enable_model_cpu_offload()"]},{"cell_type":"markdown","source":["If we have a pipeline created and loaded, we can enable PAG on it using the `from_pipe` API with the `enable_pag` flag."],"metadata":{"id":"Q9dLUxWIubw1"}},{"cell_type":"code","source":["pipeline_sdxl = AutoPipelineForText2Image.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    torch_dtype=torch.float16,\n",")\n","\n","pipeline = AutoPipelineForText2Image.from_pipe(\n","    pipeline_sdxl,\n","    enable_pag=True,\n",")"],"metadata":{"id":"OadkMVV5urvS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To generate an image, we need to pass a `pag_scale`.\n","* When `pag_scale` increases, images gain more semantically coherent structures and exhibit fewer artifacts.\n","* Overly large guidance scale can lead to smoother textures and slight saturation in the images, similarly to CFG.\n","* PAG is disabled when `pag_scale=0`.\n","\n","`pag_scale=3.0` is used in the official demo and works well in most of the use cases."],"metadata":{"id":"WNYJMOjlu1Fp"}},{"cell_type":"code","source":["prompt = 'an insect robot preparing a delicious meal, anime style'\n","generator = torch.Generator('cpu').manual_seed(111)\n","\n","images = []\n","for pag_scale in [0, 1, 2, 3, 5, 10]:\n","    image = pipeline(\n","        prompt,\n","        num_inference_steps=25,\n","        guidance_scale=7.,\n","        generator=generator,\n","        pag_scale=pag_scale,\n","    ).images[0]\n","    images.append(image)\n","\n","make_image_grid(images, rows=1, cols=(len(images)))"],"metadata":{"id":"MRxjbXvXvKvr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Image-to-image"],"metadata":{"id":"E3WfzO2Lvyk4"}},{"cell_type":"code","source":["from diffusers import AutoPipelineForImage2Image\n","from diffusers.utils import load_image, make_image_grid\n","import torch\n","\n","pipeline = AutoPipelineForImage2Image.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    enable_pag=True,\n","    pag_applied_layers=['mid'],\n","    torch_dtype=torch.float16,\n",")\n","pipeline.enable_model_cpu_offload()"],"metadata":{"id":"HgJro1Xvv0EP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If we already have a image-to-image pipeline and would like to enable PAG,"],"metadata":{"id":"EPJACaODOSDs"}},{"cell_type":"code","source":["pipeline_sdxl = AutoPipelineForImage2Image.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    torch_dtype=torch.float16,\n",")\n","\n","pipeline = AutoPipelineForImage2Image.from_pipe(\n","    pipeline_sdxl,\n","    enable_pag=True,\n",")"],"metadata":{"id":"dP9hu0K6OW4g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To directly switch from a text-to-image pipeline to a PAG-enabled image-to-image pipeline"],"metadata":{"id":"-IEucITZOrSM"}},{"cell_type":"code","source":["from diffusers import AutoPipelineForText2Image\n","\n","pipeline_t2i = AutoPipelineForText2Image.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    torch_dtype=torch.float16,\n",")\n","\n","pipeline = AutoPipelineForImage2Image.from_pipe(\n","    pipeline_t2i,\n","    enable_pag=True,\n",")"],"metadata":{"id":"LNWxLD9cOwxL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If we have a PAG-enabled text-to-image pipeline, we can directly switch to an image-to-image pipeline with PAG still enabled:"],"metadata":{"id":"DGlGcN77PH2i"}},{"cell_type":"code","source":["pipeline_pag = AutoPipelineForText2Image.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    enable_pag=True,\n","    torch_dtype=torch.float16,\n",")\n","\n","pipeline = AutoPipelineForImage2Image.from_pipe(pipeline_pag)"],"metadata":{"id":"3iek_NAOPPVp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pag_scales = 4.0\n","guidance_scales = 7.0\n","\n","url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-text2img.png\"\n","init_image = load_image(url)\n","prompt = \"a dog catching a frisbee in the jungle\"\n","generator = torch.Generator('cpu').manual_seed(111)\n","\n","image = pipeline(\n","    prompt,\n","    image=init_image,\n","    strength=0.8,\n","    guidance_scale=guidance_scale,\n","    pag_scale=pag_scale,\n","    generator=generator,\n",").images[0]\n","make_image_grid([init_image, image], rows=1, cols=2)"],"metadata":{"id":"S1KlaHosPaoa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Inpainting"],"metadata":{"id":"v8QVwlfpQWsE"}},{"cell_type":"code","source":["from diffusers import AutoPipelineForInpainting\n","from diffusers.utils import load_image, make_image_grid\n","import torch\n","\n","pipeline = AutoPipelineForInpainting.from_pretrained(\n","    'stabilityai/stable_diffusion-xl-base-1.0',\n","    enable_pag=True,\n","    pag_applied_layers=['mid'],\n","    torch_dtype=torch.float16,\n",")\n","pipeline.enable_model_cpu_offload()"],"metadata":{"id":"D1h2WP0rQXwX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["On an existing inpainting pipeline,"],"metadata":{"id":"x2y6JzJpTsS5"}},{"cell_type":"code","source":["pipeline_sdxl = AutoPipelineForInpaint.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    torch_dtype=torch.float16,\n",")\n","\n","pipeline = AutoPipelineForInpainting.from_pipe(\n","    pipeline_sdxl,\n","    enable_pag=True,\n",")"],"metadata":{"id":"o-wx1jvETu1L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Switching from another pipeline task:"],"metadata":{"id":"hZzzq5vKT6i-"}},{"cell_type":"code","source":["from diffusers import AutoPipelineForText2Image\n","\n","pipeline_t2i = AutoPipelineForText2Image.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    torch_dtype=torch.float16,\n",")\n","\n","pipeline = AutoPipelineForInpainting.from_pipe(\n","    pipeline_t2i,\n","    enable_pag=True\n",")"],"metadata":{"id":"cMd94z4lT9hb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n","mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n","init_image = load_image(img_url).convert(\"RGB\")\n","mask_image = load_image(mask_url).convert(\"RGB\")\n","\n","prompt = \"A majestic tiger sitting on a bench\"\n","\n","pag_scales = 3.0\n","guidance_scale = 7.5\n","generator = torch.Generator('cpu').manual_seed(111)\n","\n","image = pipeline(\n","    prompt,\n","    image=init_image,\n","    mask_image=mask_image,\n","    strength=0.8,\n","    num_inference_steps=50,\n","    guidance_scale=guidance_scale,\n","    generator=generator,\n","    pag_scale=pag_scale,\n",").images[0]\n","make_image_grid([init_image, mask_image, image], rows=1, cols=3)"],"metadata":{"id":"R8m162m4UQay"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PAG with ControlNet"],"metadata":{"id":"HQfYTzL-UyWZ"}},{"cell_type":"code","source":["from diffusers import AutoPipelineForText2Image, ControlNetModel\n","import torch\n","\n","controlnet = ControlNetModel.from_pretrained(\n","    'diffusers/controlnet-canny-sdxl-1.0',\n","    torch_dtype=torch.float16,\n",")\n","\n","pipeline = AutoPipelineForText2Image.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    controlnet=controlnet,\n","    enable_pag=True,\n","    pag_applied_layers=['mid'],\n","    torch_dtype=torch.float16\n",")\n","pipeline.enable_model_cpu_offload()"],"metadata":{"id":"5GhBKiARU0xl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If we already have a controlnet pipeline and want to enable PAG:"],"metadata":{"id":"CbcoCDlQVNEl"}},{"cell_type":"code","source":["pipeline_controlnet = AutoPipelineForText2Image.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    controlnet=controlnet,\n","    torch_dtype=torch.float16,\n",")\n","\n","pipeline = AutoPipelineForText2Image.from_pipe(\n","    pipeline_controlnet,\n","    enable_pag=True,\n",")"],"metadata":{"id":"jSdf0j75VQlw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from diffusers.utils import load_image, make_image_grid\n","\n","canny_image = load_image(\n","    \"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/pag_control_input.png\"\n",")\n","generator = torch.Generator('cpu').manual_seed(111)\n","\n","images = []\n","images.append(canny_image)\n","for pag_scale in [0., 3.0]:\n","    image = pipeline(\n","        prompt=\"\",\n","        controlnet_conditioning_scale=0.8,\n","        image=canny_image,\n","        num_inference_steps=50,\n","        guidance_scale=0,\n","        generator=generator,\n","        pag_scale=pag_scale,\n","    ).images[0]\n","    images.append(image)\n","\n","make_image_grid(images, rows=1, cols=(len(images)))"],"metadata":{"id":"_AQdf8dFV4gd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PAG with IP-Adapter"],"metadata":{"id":"HEnVktIwWyzm"}},{"cell_type":"code","source":["from diffusers import AutoPipelineForText2Image\n","from diffusers.utils import load_image, make_image_grid\n","from transformers import CLIPVisionModelWithProjection\n","import torch\n","\n","image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n","    'h94/IP-Adapter',\n","    subfolder='models/image_encoder',\n","    torch_dtype=torch.float16\n",")\n","\n","pipeline = AutoPipelineForText2Image.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    image_encoder=image_encoder,\n","    enable_pag=True,\n","    torch_dtype=torch.float16,\n",").to('cuda')\n","\n","pipeline.load_ip_adapter(\n","    'h94/IP-Adapter',\n","    subfolder='sdxl_models',\n","    weight_name='ip-adapter-plus_sdxl_vit-h.bin'\n",")"],"metadata":{"id":"K6G_ToLwW0dC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pag_scales = 5.0\n","ip_adapter_scale = 0.8\n","\n","ip_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_diner.png\")\n","propmt = 'a polar bear sitting in a chair drinking a milkshake'\n","negative_prompt=\"deformed, ugly, wrong proportion, low res, bad anatomy, worst quality, low quality\"\n","generator = torch.Generator('cpu').manual_seed(111)\n","\n","pipeline.set_ip_adapter_scale(ip_adapter_scale)\n","image = pipeline(\n","    prompt,\n","    negative_prompt=negative_prompt,\n","    ip_adapter_image=ip_image,\n","    num_inference_steps=25,\n","    guidance_scale=3.0,\n","    generator=generator,\n","    pag_scale=pag_scale,\n",").images[0]\n","make_image_grid([ip_image, image], rows=1, cols=2)"],"metadata":{"id":"PHv0XnjAXfeq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Configure parameters"],"metadata":{"id":"RVkx6-DNYBQ0"}},{"cell_type":"markdown","source":["The `pag_applied_layers` argument allows us to specify which layers PAG is applied to. By default, it applies only to the mid blocks.\n","\n","We can use the `set_pag_applied_layers` to adjust the PAG layers after the pipeline is created."],"metadata":{"id":"WrT5sSOIYsb-"}},{"cell_type":"code","source":["from diffusers import AutoPipelineText2Image\n","import torch\n","\n","pipeline = AutoPipelineForText2Image.from_pretrained(\n","    'stabilityai/stable-diffusion-xl-base-1.0',\n","    torch_dtype=torch.float16,\n","    enable_pag=True,\n",")\n","pipeline.enable_model_cpu_offload()"],"metadata":{"id":"Zoq3YutQYCa_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"an insect robot preparing a delicious meal, anime style\"\n","generator = torch.Generator(device=\"cpu\").manual_seed(0)\n","pag_layers = [\n","    ['mid'],\n","    ['down.block_1'],\n","    ['down.block_2', 'up.block_1.attentions_0'],\n","]\n","\n","images = []\n","for pag_applied_layers in pag_layers:\n","    pipeline.set_pag_applied_layers(pag_applied_layers)\n","    image = pipeline(\n","        prompt,\n","        num_inference_steps=25,\n","        guidance_scale=5.,\n","        generator=generator,\n","        pag_scale=3\n","    ).images[0]\n","    images.append(image)\n","\n","make_image_grid(images, rows=1, cols=(len(images)))"],"metadata":{"id":"BIzlanw5ZL25"},"execution_count":null,"outputs":[]}]}