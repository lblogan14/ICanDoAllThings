{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6b5a87",
   "metadata": {},
   "source": [
    "# Project: Deep Research with LangGraph - Module 3: Research Agent with MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80090d9",
   "metadata": {},
   "source": [
    "# Research Agent with MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99f534e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e36fd7d",
   "metadata": {},
   "source": [
    "In previous module, we built a research agent that will apply a custom search tool to gather information about a research topic. Additionally, we can also use the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/specification/2025-06-18/architecture) as another way to access tools.\n",
    "\n",
    "MCP servers provide a standard protocol for accessing tools.\n",
    "\n",
    "[LangChain MCP Adapters](https://github.com/langchain-ai/langchain-mcp-adapters) provides a seamless bridge between the MCP and LangChain/LangGraph ecosystems.\n",
    "\n",
    "In this module, we will build a research agent that uses the [Filesystem MCP Server](https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem) to provide secure, controlled access to local file systems with granular permission management.\n",
    "\n",
    "\n",
    "The **Filesystem MCP Server** will provide:\n",
    "- **File operations** with strict access control\n",
    "- **Directory management** with dynamic permissions\n",
    "- **Search capabilities** across allowed directories\n",
    "- **Metadata access** for files and directories\n",
    "\n",
    "It has the following tools:\n",
    "- **File operations**: `read_file`, `write_file`, `edit_file`, `read_multiple_files`\n",
    "- **Directory management**: `create_directory`, `list_directory`, `move_file`\n",
    "- **Search & discovery**: `search_files`, `get_file_info`, `list_allowed_directories`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c33ab8",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746865ba",
   "metadata": {},
   "source": [
    "We first will define a prompt that instructs our agent to use available search tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260d5ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭───────────────────────────────────── </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Research Agent Instructions with MCP</span><span style=\"color: #000080; text-decoration-color: #000080\"> ──────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  You are a research assistant conducting research on the user's input topic using local files. For context,     <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  today's date is {date}.                                                                                        <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;Task&gt;</span>                                                                                                         <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  Your job is to use file system tools to gather information from local research files.                          <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  You can use any of the tools provided to you to find and read files that help answer the research question.    <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  You can call these tools in series or in parallel, your research is conducted in a tool-calling loop.          <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;/Task&gt;</span>                                                                                                        <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;Available Tools&gt;</span>                                                                                              <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  You have access to file system tools and thinking tools:                                                       <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - **list_allowed_directories**: See what directories you can access                                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - **list_directory**: List files in directories                                                                <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - **read_file**: Read individual files                                                                         <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - **read_multiple_files**: Read multiple files at once                                                         <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - **search_files**: Find files containing specific content                                                     <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - **think_tool**: For reflection and strategic planning during research                                        <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  **CRITICAL: Use think_tool after reading files to reflect on findings and plan next steps**                    <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;/Available Tools&gt;</span>                                                                                             <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;Instructions&gt;</span>                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  Think like a human researcher with access to a document library. Follow these steps:                           <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  1. **Read the question carefully** - What specific information does the user need?                             <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  2. **Explore available files** - Use list_allowed_directories and list_directory to understand what's          <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  available                                                                                                      <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  3. **Identify relevant files** - Use search_files if needed to find documents matching the topic               <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  4. **Read strategically** - Start with most relevant files, use read_multiple_files for efficiency             <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  5. **After reading, pause and assess** - Do I have enough to answer? What's still missing?                     <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  6. **Stop when you can answer confidently** - Don't keep reading for perfection                                <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;/Instructions&gt;</span>                                                                                                <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;Hard Limits&gt;</span>                                                                                                  <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  **File Operation Budgets** (Prevent excessive file reading):                                                   <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - **Simple queries**: Use 3-4 file operations maximum                                                          <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - **Complex queries**: Use up to 6 file operations maximum                                                     <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - **Always stop**: After 6 file operations if you cannot find the right information                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  **Stop Immediately When**:                                                                                     <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - You can answer the user's question comprehensively from the files                                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - You have comprehensive information from 3+ relevant files                                                    <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - Your last 2 file reads contained similar information                                                         <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;/Hard Limits&gt;</span>                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;Show Your Thinking&gt;</span>                                                                                           <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  After reading files, use think_tool to analyze what you found:                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - What key information did I find?                                                                             <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - What's missing?                                                                                              <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - Do I have enough to answer the question comprehensively?                                                     <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - Should I read more files or provide my answer?                                                               <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - Always cite which files you used for your information                                                        <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;/Show Your Thinking&gt;</span>                                                                                          <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭─\u001b[0m\u001b[34m────────────────────────────────────\u001b[0m\u001b[34m \u001b[0m\u001b[1;32mResearch Agent Instructions with MCP\u001b[0m\u001b[34m \u001b[0m\u001b[34m─────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  You are a research assistant conducting research on the user's input topic using local files. For context,     \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  today's date is {date}.                                                                                        \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;34m<Task>\u001b[0m                                                                                                         \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  Your job is to use file system tools to gather information from local research files.                          \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  You can use any of the tools provided to you to find and read files that help answer the research question.    \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  You can call these tools in series or in parallel, your research is conducted in a tool-calling loop.          \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;34m</Task>\u001b[0m                                                                                                        \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;34m<Available Tools>\u001b[0m                                                                                              \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  You have access to file system tools and thinking tools:                                                       \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - **list_allowed_directories**: See what directories you can access                                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - **list_directory**: List files in directories                                                                \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - **read_file**: Read individual files                                                                         \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - **read_multiple_files**: Read multiple files at once                                                         \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - **search_files**: Find files containing specific content                                                     \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - **think_tool**: For reflection and strategic planning during research                                        \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  **CRITICAL: Use think_tool after reading files to reflect on findings and plan next steps**                    \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;34m</Available Tools>\u001b[0m                                                                                             \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;34m<Instructions>\u001b[0m                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  Think like a human researcher with access to a document library. Follow these steps:                           \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  1. **Read the question carefully** - What specific information does the user need?                             \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  2. **Explore available files** - Use list_allowed_directories and list_directory to understand what's          \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  available                                                                                                      \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  3. **Identify relevant files** - Use search_files if needed to find documents matching the topic               \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  4. **Read strategically** - Start with most relevant files, use read_multiple_files for efficiency             \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  5. **After reading, pause and assess** - Do I have enough to answer? What's still missing?                     \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  6. **Stop when you can answer confidently** - Don't keep reading for perfection                                \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;34m</Instructions>\u001b[0m                                                                                                \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;34m<Hard Limits>\u001b[0m                                                                                                  \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  **File Operation Budgets** (Prevent excessive file reading):                                                   \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - **Simple queries**: Use 3-4 file operations maximum                                                          \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - **Complex queries**: Use up to 6 file operations maximum                                                     \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - **Always stop**: After 6 file operations if you cannot find the right information                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  **Stop Immediately When**:                                                                                     \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - You can answer the user's question comprehensively from the files                                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - You have comprehensive information from 3+ relevant files                                                    \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - Your last 2 file reads contained similar information                                                         \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;34m</Hard Limits>\u001b[0m                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;34m<Show Your Thinking>\u001b[0m                                                                                           \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  After reading files, use think_tool to analyze what you found:                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - What key information did I find?                                                                             \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - What's missing?                                                                                              \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - Do I have enough to answer the question comprehensively?                                                     \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - Should I read more files or provide my answer?                                                               \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - Always cite which files you used for your information                                                        \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;34m</Show Your Thinking>\u001b[0m                                                                                          \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import show_prompt\n",
    "from deep_research_from_scratch.prompts import research_agent_prompt_with_mcp\n",
    "\n",
    "show_prompt(research_agent_prompt_with_mcp, \"Research Agent Instructions with MCP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb80431",
   "metadata": {},
   "source": [
    "# Research Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5d2c4",
   "metadata": {},
   "source": [
    "Next, we will use the *Filesystem MCP Server* to access research tools.\n",
    "\n",
    "When using MCP tools with LangChain, we must use async methods because the MCP protocol is inherently asynchronous:\n",
    "- Server communication uses async JSON-RPC over stdio/http\n",
    "- Tool invocations involve network/IPC calls that can be slow\n",
    "- Async enables non-blocking, concurrent operations\n",
    "\n",
    "Hence, the **LangChain MCP Adapters** are designed for async-only operations:\n",
    "- Tools are wrapped as async StructuredTools\n",
    "- Sync invocation is intentionally not implemented\n",
    "- This ensures consistent async behavior across all MCP tools\n",
    "\n",
    "Async enables\n",
    "- Concurrent tool execution\n",
    "- Non-blocking server communication\n",
    "- Better resource utilization\n",
    "- Responsive applications\n",
    "\n",
    "MCP servers run as subprocesses:\n",
    "- Communication happens over pipes (stdin/stdout)\n",
    "- These I/O operations are naturally async\n",
    "- Blocking sync calls would freeze the entire process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcaec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy MCP Example\n",
    "import os\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.table import Table\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# Get the absolute path to our sample research docs\n",
    "sample_docs_path = os.path.abspath('./files/')\n",
    "console.print(f\"[bold blue] Sample docs path:[/bold blue] {sample_docs_path}\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(sample_docs_path):\n",
    "    console.print(f\"[green] Directory exists with files:[/green] {os.listdir(sample_docs_path)}\")\n",
    "else:\n",
    "    console.print(f\"[red] Directory does not exist![/red]\")\n",
    "\n",
    "\n",
    "# MCP Client configuration - filesystem server for local document access\n",
    "mcp_config = {\n",
    "    'filesystem': {\n",
    "        'command': 'npx',\n",
    "        'args': [\n",
    "            '-y',  # Auto-install if needed\n",
    "            '@modelcontextprotocol/server-filesystem',\n",
    "            sample_docs_path,\n",
    "        ],\n",
    "        'transport': 'stdio',\n",
    "    }\n",
    "}\n",
    "\n",
    "console.print(Panel(\"[bold yellow]Creating MCP Client...[/bold yellow]\", expand=False))\n",
    "client = MultiServerMCPClient(mcp_config)\n",
    "console.print(\"[green] MCP Client created successfully![/green]\")\n",
    "\n",
    "\n",
    "# Test getting tools\n",
    "console.print(Panel(\"[bold yellow] Getting tools...[/bold yellow]\", expand=False))\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# Create a rich table for tool display\n",
    "table = Table(title=\"Available MCP Tools\", show_header=True, header_style='bold magenta')\n",
    "table.add_column('Tool Name', style='cyan', width=25)\n",
    "table.add_column('Description', style='while', width=80)\n",
    "\n",
    "for tool in tools:\n",
    "    # Truncate long descriptions for better display\n",
    "    description = tool.description[:80] + '...' if len(tool.description) > 80 else tool.description\n",
    "    table.add_row(tool.name, description)\n",
    "\n",
    "console.print(table)\n",
    "console.print(f\"[bold green] Successfully retrieved {len(tools)} tools from MCP server![/bold green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b96f47",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f54d533",
   "metadata": {},
   "source": [
    "Now, we will define our research agent that uses the MCP tools.\n",
    "\n",
    "MCP servers run as separate subprocesses that communicate via\n",
    "- **stdin/stdout pipes** for local servers\n",
    "- **HTTP/WebSocket connections** for remote servers\n",
    "\n",
    "Tool operations often involve\n",
    "- File system operations (reading, writing, searching)\n",
    "- Network requests (for remote MCP servers)\n",
    "- Database queries (for data-backed servers)\n",
    "- External API calls\n",
    "\n",
    "These operations have variable latency and benefit from non-blocking async calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b8a6abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/deep_research_from_scratch/research_agent_mcp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/deep_research_from_scratch/research_agent_mcp.py\n",
    "\"\"\"Research Agent with MCP Integration.\n",
    "\n",
    "Description: This module implements a research agent that integrates with Model Context Protocol (MCP)\n",
    "servers to access tools and resources. The agent demonstrates how to use MCP filesystem\n",
    "server for local document research and analysis.\n",
    "\n",
    "Key features:\n",
    "- MCP server integration for tool access\n",
    "- Async operations for concurrent tool execution (required by MCP protocol)\n",
    "- Filesystem operations for local document research\n",
    "- Secure directory access with permission checking\n",
    "- Research compression for efficient processing\n",
    "- Lazy MCP client initialization for LangGraph Platform compatibility\n",
    "\"\"\"\n",
    "import os\n",
    "from typing_extensions import Literal\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage, filter_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "from deep_research_from_scratch.prompts import research_agent_prompt_with_mcp, compress_research_system_prompt, compress_research_human_message\n",
    "from deep_research_from_scratch.state_research import ResearcherState, ResearcherOutputState\n",
    "from deep_research_from_scratch.utils import get_today_str, think_tool, get_current_dir\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "\n",
    "# MCP server configuration - filesystem server for local document access\n",
    "mcp_config = {\n",
    "    'filesystem': {\n",
    "        'command': 'npx',\n",
    "        'args': [\n",
    "            '-y',  # Auto-install if needed\n",
    "            '@modelcontextprotocol/server-filesystem',\n",
    "            str(get_current_dir() / 'files') # Path to research documents\n",
    "        ],\n",
    "        'transport': 'stdio',  # Communication via stdin/stdout\n",
    "    }\n",
    "}\n",
    "\n",
    "# Global client variable\n",
    "_client = None\n",
    "\n",
    "def get_mcp_client():\n",
    "    \"\"\"Get or initialize MCP client to avoid issues with LangGraph Platform.\"\"\"\n",
    "    global _client\n",
    "    if _client is None:\n",
    "        _client = MultiServerMCPClient(mcp_config)\n",
    "    return _client\n",
    "\n",
    "\n",
    "# Initialize chat model\n",
    "compress_model = init_chat_model(model=\"openai:gpt-4.1\", max_tokens=32000)\n",
    "model = init_chat_model(model=\"anthropic:claude-sonnet-4-20250514\")\n",
    "\n",
    "\n",
    "# ===== AGENT NODES =====\n",
    "\n",
    "async def llm_call(state: ResearcherState):\n",
    "    \"\"\"Analyze current state and decide on tool usage with MCP integration.\n",
    "    \n",
    "    This node:\n",
    "    1. Retrieve available tools from MCP server.\n",
    "    2. Bind tools to the language model.\n",
    "    3. Process user input and decide on tool usage.\n",
    "    \n",
    "    Return updated state with model response.\n",
    "    \"\"\"\n",
    "    # Get available tools from MCP server\n",
    "    client = get_mcp_client()\n",
    "    mcp_tools = await client.get_tools()\n",
    "\n",
    "    # Use MCP tools for local document access\n",
    "    tools = [think_tool] + mcp_tools\n",
    "\n",
    "    # Initialize model with tool binding\n",
    "    model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "    # Process user input with system prompt\n",
    "    response = model_with_tools.invoke(\n",
    "        [SystemMessage(content=research_agent_prompt_with_mcp.format(date=get_today_str()))] + state[\"researcher_messages\"]\n",
    "    )\n",
    "\n",
    "    return {'researcher_messages': [response]}\n",
    "\n",
    "\n",
    "async def tool_node(state: ResearcherState):\n",
    "    \"\"\"Execute tool calls using MCP tools.\n",
    "    \n",
    "    This node:\n",
    "    1. Retrieves current tool calls from the last message\n",
    "    2. Executes all tool calls using async operations (required for MCP)\n",
    "    3. Returns formatted tool results\n",
    "\n",
    "    Note: MCP requires async operations due to inter-process communication\n",
    "    with the MCP server subprocess. This is unavoidable.\n",
    "    \"\"\"\n",
    "    tool_calls = state['researcher_messages'][-1].tool_calls\n",
    "\n",
    "    async def execute_tools():\n",
    "        \"\"\"Execute all tool calls. MCP tools require async execution.\"\"\"\n",
    "        # Get fresh tool references from MCP server\n",
    "        client = get_mcp_client()\n",
    "        mcp_tools = await client.get_tools()\n",
    "        tools = [think_tool] + mcp_tools\n",
    "        tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "        # Execute tool calls (sequentially for reliability)\n",
    "        observations = []\n",
    "        for tool_call in tool_calls:\n",
    "            tool = tools_by_name[tool_call['name']]\n",
    "            if tool_call['name'] == 'think_tool':\n",
    "                # think_tool is sync, use regular invoke\n",
    "                observation = tool.invoke(tool_call['args'])\n",
    "            else:\n",
    "                # MCP tools are async, use ainvoke\n",
    "                observation = await tool.ainvoke(tool_call['args'])\n",
    "            observations.append(observation)\n",
    "\n",
    "        # Format results as tool messages\n",
    "        tool_outputs = [\n",
    "            ToolMessage(\n",
    "                content=observation,\n",
    "                name=tool_call['name'],\n",
    "                tool_call_id=tool_call['id']\n",
    "            )\n",
    "            for observation, tool_call in zip(observations, tool_calls)\n",
    "        ]\n",
    "\n",
    "        return tool_outputs\n",
    "\n",
    "    messages = await execute_tools()\n",
    "\n",
    "    return {'researcher_messages': messages}\n",
    "\n",
    "\n",
    "def compress_research(state: ResearcherState) -> dict:\n",
    "    \"\"\"Compress research findings into a concise summary.\n",
    "\n",
    "    Takes all the research messages and tool outputs and creates\n",
    "    a compressed summary suitable for further processing or reporting.\n",
    "\n",
    "    This function filters out think_tool calls and focuses on substantive\n",
    "    file-based research content from MCP tools.\n",
    "    \"\"\"\n",
    "    system_message = compress_research_system_prompt.format(date=get_today_str())\n",
    "    messages = [SystemMessage(content=system_message)] + state.get(\"researcher_messages\", []) + [HumanMessage(content=compress_research_human_message)]\n",
    "\n",
    "    response = compress_model.invoke(messages)\n",
    "\n",
    "    # Extract raw notes from tool and AI messages\n",
    "    raw_notes = [\n",
    "        str(m.content) for m in filter_messages(\n",
    "            state['researcher_messages'],\n",
    "            include_types=['tool', 'ai']\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        'compressed_research': str(response.content),\n",
    "        'raw_notes': ['\\n'.join(raw_notes)]\n",
    "    }\n",
    "\n",
    "\n",
    "# ===== ROUTING LOGIC =====\n",
    "\n",
    "def should_continue(state: ResearcherState) -> Literal['tool_node', 'compress_research']:\n",
    "    \"\"\"Determine whether to continue with tool execution or compress research.\n",
    "\n",
    "    Determines whether to continue with tool execution or compress research\n",
    "    based on whether the LLM made tool calls.\n",
    "    \"\"\"\n",
    "    messages = state['researcher_messages']\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # Continue to tool execution if tools were called\n",
    "    if last_message.tool_calls:\n",
    "        return 'tool_node'\n",
    "    else:\n",
    "        return 'compress_research'\n",
    "\n",
    "\n",
    "# ===== GRAPH CONSTRUCTION =====\n",
    "\n",
    "# Build agent workflow\n",
    "agent_builder_mcp = StateGraph(\n",
    "    ResearcherState,\n",
    "    output_schema=ResearcherOutputState\n",
    ")\n",
    "\n",
    "# Add nodes\n",
    "agent_builder_mcp.add_node('llm_call', llm_call)\n",
    "agent_builder_mcp.add_node('tool_node', tool_node)\n",
    "agent_builder_mcp.add_node('compress_research', compress_research)\n",
    "\n",
    "# Add edges\n",
    "agent_builder_mcp.add_edge(START, 'llm_call')\n",
    "agent_builder_mcp.add_conditional_edges(\n",
    "    'llm_call',\n",
    "    should_continue,\n",
    "    {\n",
    "        'tool_node': 'tool_node',   # Continue to tool execution\n",
    "        'compress_research': 'compress_research'  # Or compress research\n",
    "    }\n",
    ")\n",
    "agent_builder_mcp.add_edge('tool_node', 'llm_call')  # Loop back to LLM after tool execution\n",
    "agent_builder_mcp.add_edge('compress_research', END)  # End after compression\n",
    "\n",
    "# Compile\n",
    "agent_mcp = agent_builder_mcp.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a8319b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from deep_research_from_scratch.research_agent_mcp import agent_mcp\n",
    "\n",
    "display(Image(agent_mcp.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent\n",
    "from utils import format_messages\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "research_brief = \"\"\"I want to identify and evaluate the coffee shops in San Francisco that are considered the best based specifically  \n",
    "on coffee quality. My research should focus on analyzing and comparing coffee shops within the San Francisco area, \n",
    "using coffee quality as the primary criterion. I am open regarding methods of assessing coffee quality (e.g.,      \n",
    "expert reviews, customer ratings, specialty coffee certifications), and there are no constraints on ambiance,      \n",
    "location, wifi, or food options unless they directly impact perceived coffee quality. Please prioritize primary    \n",
    "sources such as the official websites of coffee shops, reputable third-party coffee review organizations (like     \n",
    "Coffee Review or Specialty Coffee Association), and prominent review aggregators like Google or Yelp where direct  \n",
    "customer feedback about coffee quality can be found. The study should result in a well-supported list or ranking of\n",
    "the top coffee shops in San Francisco, emphasizing their coffee quality according to the latest available data as  \n",
    "of July 2025.\"\"\"\n",
    "\n",
    "result = await agent_mcp.ainvoke({\"researcher_messages\": [HumanMessage(content=f\"{research_brief}.\")]})\n",
    "format_messages(result['researcher_messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096183fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.markdown import Markdown\n",
    "Markdown(result['compressed_research'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f810b132",
   "metadata": {},
   "source": [
    "# Understanding MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a0ab8",
   "metadata": {},
   "source": [
    "## The Client-Server Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad02bff",
   "metadata": {},
   "source": [
    "### Basic Client-Server Model\n",
    "\n",
    "- **Client**: `MultiServerMCPClient` acts as the client that requests services\n",
    "- **Server**: The MCP server provides tools and executes operations\n",
    "- **Communication**: They communicate using a standardized protocol\n",
    "\n",
    "```python\n",
    "    client = MultiServerMCPClient(mcp_config)\n",
    "```\n",
    "\n",
    "\n",
    "In the LangChain MCP Adapters, the `MultiServerMCPClient` can\n",
    "- start MCP servers based on configuration\n",
    "- manage communication with one or more MCP servers\n",
    "- convert MCP protocols to LangChain-compatible formats\n",
    "\n",
    "\n",
    "### `mcp_config`\n",
    "\n",
    "The `mcp_config` configuration tells the MCP client how to start and connect to MCP servers. For example,\n",
    "```python\n",
    "mcp_config = {\n",
    "    \"filesystem\": {  # Server name (arbitrary label)\n",
    "        \"command\": \"npx\",  # Command to run\n",
    "        \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/docs\"],\n",
    "        \"transport\": \"stdio\"  # Communication method\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Configuration components\n",
    "- **Server Name** (`\"filesystem\"`): An arbitrary label for this server instance\n",
    "- **Command** (`\"npx\"`): The command to start the MCP server\n",
    "- **Arguments** (`[\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/docs\"]`): Arguments to pass to the command\n",
    "- **Transport** (`\"stdio\"`): The communication method (e.g., stdio, http)\n",
    "\n",
    "\n",
    "#### Two transport methods\n",
    "\n",
    "1. **stdio transport** (local servers):\n",
    "    - Uses standard input/output for communication\n",
    "    - Server runs as a subprocess on our local machine\n",
    "    - Communication via pipes (stdin/stdout)\n",
    "\n",
    "2. **http transport** (remote servers):\n",
    "    - Uses HTTP requests for communication\n",
    "    - Server runs on a remote machine or cloud service\n",
    "    - Communication via HTTP/WebSocket endpoints\n",
    "\n",
    "\n",
    "#### Remote MCP Servers\n",
    "- Typeical URL format: `https://mcp.[company-name].com/sse`\n",
    "- Requires authentication credentials and trust verification\n",
    "\n",
    "Example remote config:\n",
    "```python\n",
    "mcp_config = {\n",
    "    \"remote_service\": {\n",
    "        \"url\": \"https://mcp.example.com/sse\",\n",
    "        \"transport\": \"http\",\n",
    "        \"headers\": {\n",
    "            \"Authorization\": \"Bearer your-token-here\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff1f725",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
