{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPH4ogv+KChQMGirQ1O3FnJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Embedding Multimodal Data for Similarity Search Using HuggingFace Transformers, Datasets, and FAISS"],"metadata":{"id":"v4ksdsZ7nN-i"}},{"cell_type":"markdown","source":["Embeddings are semantically meaningful compressions of informations. They can be used to do similarity search, zero-shot classification or simply train a new model.\n","\n","Use cases for similarity search:\n","- searching for similar products in e-commerce,\n","- content search in social media\n","- etc..."],"metadata":{"id":"pEYK3YMZnXir"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"v1hoNfF2npgi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"D09b0w0NnIFR"},"outputs":[],"source":["!pip install -q datasets faiss-gpu transformers sentencepiece"]},{"cell_type":"markdown","source":["In this example, we will use the [`clip`](https://huggingface.co/openai/clip-vit-base-patch16) model to extract the features.\n","\n","**CLIP** introduced joint training of a text encoder and an image encoder to connect two modalities."],"metadata":{"id":"1UZD4FDjnsCo"}},{"cell_type":"code","source":["import torch\n","from PIL import Image\n","from transformers import AutoImageProcessor, AutoModel, AutoTokenizer\n","import faiss\n","import numpy as np\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# text encoder\n","tokenizer = AutoTokenizer.from_pretrained('openai/clip-vit-base-patch16')\n","# image encoder\n","processor = AutoImageProcessor.from_pretrained('openai/clip-vit-base-patch16')\n","# model\n","model = AutoModel.from_pretrained('openai/clip-vit-base-patch16').to(device)"],"metadata":{"id":"d-zhgCZ3n786"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will use a small captioning dataset, [`jmhessel/newyorker_caption_contest`](https://huggingface.co/datasets/jmhessel/newyorker_caption_contest)"],"metadata":{"id":"lqWtrT7OirRD"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","ds = load_dataset('jmhessel/newyorker_caption_contest', 'explanation')"],"metadata":{"id":"kb-Ls9NYizLX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ds"],"metadata":{"id":"0b_F2eVxi5zh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ds['train']"],"metadata":{"id":"15lxjkKSi6GR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ds['train'][0]['image']"],"metadata":{"id":"Md2ouFZSi7FH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We do not have to write any function to embed examples or create an index. The HuggingFace datasets library's FAISS integraion abstracts these processes. We can use `map` method to create a new column with the embeddings for each example:"],"metadata":{"id":"-XB-EvWLj9W-"}},{"cell_type":"code","source":["dataset = ds['train']\n","ds_with_embeddings = dataset.map(\n","    lambda example: {\n","        'embeddings': model.get_text_features(\n","            **tokenizer(\n","                [example['image_description']],\n","                truncation=True,\n","                return_tensors='pt'\n","            ).to('cuda')\n","        )[0].detach().cpu().numpy()\n","    }\n",")"],"metadata":{"id":"RaLP69smkJ2i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can do the same to the image embeddings."],"metadata":{"id":"L5NJZJAakcx_"}},{"cell_type":"code","source":["ds_with_embeddings = ds_with_embeddings.map(\n","    lambda example:{\n","        'image_embeddings': model.get_image_features(\n","            **processor(\n","                [example['image']],\n","                return_tensors='pt'\n","            ).to('cuda')\n","        )[0].detach().cpu().numpy()\n","    }\n",")"],"metadata":{"id":"OIADslTBkeZ8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can create an index for each column:"],"metadata":{"id":"HpIsE8fZkwqh"}},{"cell_type":"code","source":["ds_with_embeddings.add_faiss_index(column='embeddings')\n","ds_with_embeddings.add_faiss_index(column='image_embeddings')"],"metadata":{"id":"nVeABF3Ek967"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Querying the data with text prompts"],"metadata":{"id":"vlxLCJ9FlFel"}},{"cell_type":"code","source":["prompt = 'a snowy day'\n","prompt_embedding = model.get_text_features(\n","    **tokenizer(\n","        [prompt],\n","        truncation=True,\n","        return_tensors='pt'\n","    ).to('cuda')\n",")[0].detach().cpu().numpy()\n","\n","scores, retrieved_examples = ds_with_embeddings.get_nearest_examples(\n","    'embeddings',\n","    prompt_embedding,\n","    k=1\n",")"],"metadata":{"id":"rZWrOARYlHuu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def downscale_images(image):\n","    width = 200\n","    ratio = width / float(image.size[0])\n","    height = int((float(image.size[1]) * float(ratio)))\n","    img = image.resize((width, height), Image.Resampling.LANCZOS)\n","\n","    return img"],"metadata":{"id":"TdO3Bq5moIP0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images = [downscale_images(image) for image in retrieved_examples['image']]\n","# see the closest text and image\n","print(retrieved_examples['image_description'])\n","display(images[0])"],"metadata":{"id":"AhZDwLH9oaOV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Querying the data with image prompts"],"metadata":{"id":"F5qby_vPolil"}},{"cell_type":"code","source":["import requests\n","\n","url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/beaver.png\"\n","image = Image.open(requests.get(url, stream=True).raw)\n","display(downscale_images(image))"],"metadata":{"id":"mX2IBEcqonbr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_embedding = model.get_image_features(\n","    **processor(\n","        [image],\n","        truncation=True,\n","        return_tensors='pt'\n","    ).to('cuda')\n",")[0].detach().cpu().numpy()"],"metadata":{"id":"cpnvjc-GosHD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images = [downscale_images(image) for image in retrieved_examples['image']]\n","# see the cloest text and image\n","print(retrieved_examples['image_description'])\n","display(image[0])"],"metadata":{"id":"EKPk7l4Fo1jI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Saving, pushing, and loading the embeddings"],"metadata":{"id":"Gajp3Kbro_9q"}},{"cell_type":"markdown","source":["Save the embeddings locally:"],"metadata":{"id":"Rnq1Zd5cpTHt"}},{"cell_type":"code","source":["ds_with_embeddings.save_faiss_index(\n","    'embeddings',\n","    'embeddings/embeddings.faiss'\n",")\n","ds_with_embeddings.save_faiss_index(\n","    'image_embeddings',\n","    'embeddings/image_embeddings.faiss'\n",")"],"metadata":{"id":"cl0qDepfpDXa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Push the embeddings to the Hub"],"metadata":{"id":"Z1ffa5zBpPqa"}},{"cell_type":"code","source":["from huggingface_hub import HfApi, snapshot_download\n","\n","api = HfApi()\n","api.create_repo('<username>/faiss_embeddings', repo_type='dataset')\n","api_upload_folder(\n","    folder_path='embeddings',\n","    repo_id='<username>/faiss_embeddings',\n","    repo_type='dataset'\n",")"],"metadata":{"id":"KgTfGVZIpYk8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["snapshot_download(\n","    repo_id=\"<username>/faiss_embeddings\",\n","    repo_type=\"dataset\",\n","    local_dir=\"downloaded_embeddings\"\n",")"],"metadata":{"id":"DACeEJxvpmDk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also load the embeddings to the dataset with no embeddings using `load_faiss_index`."],"metadata":{"id":"o3PQQTvLpsxq"}},{"cell_type":"code","source":["ds = ds['train']\n","ds.load_faiss_index(\n","    'embeddings',\n","    './downloaded_embeddings/embeddings.faiss'\n",")"],"metadata":{"id":"4CgAxhlcpwmM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test inference\n","prompt = 'people under the rain'\n","prompt_embedding = model.get_text_features(\n","    **tokenizer(\n","        [prompt],\n","        truncation=True,\n","        return_tensors='pt'\n","    ).to('cuda')\n",")[0].detach().cpu().numpy()\n","\n","scores, retrieved_examples = ds.get_nearest_examples(\n","    'embeddings',\n","    prompt_embedding,\n","    k=1\n",")"],"metadata":{"id":"-jWQ4AS6p3El"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display(retrieved_examples['image'][0])"],"metadata":{"id":"j8R9y9Dcp8h8"},"execution_count":null,"outputs":[]}]}