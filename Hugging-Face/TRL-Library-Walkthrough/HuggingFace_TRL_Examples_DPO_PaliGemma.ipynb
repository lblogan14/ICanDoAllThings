{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNHUxcMmjcmSJRrNYWUx+0Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Using Direct Preference Optimization to Fine-Tune PaliGemma"],"metadata":{"id":"w9BXomcFRWgD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJ7AjgxBRLc7"},"outputs":[],"source":["!pip install -qU transformers trl peft accelerate bitsandbytes"]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","notebook_login()"],"metadata":{"id":"tkKjrtTnRlZD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the model and the processor:"],"metadata":{"id":"zHQZS9uGqpLg"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoProcessor, AutoModelForVision2Seq\n","\n","model_id = 'google/paligemma-3b-pt-448'\n","\n","processor = AutoProcessor.from_pretrained(model_id)\n","tokenizer = processor.tokenizer\n","\n","# reference base model\n","model_ref = AutoModelForVision2Seq.from_pretrained(model_id)"],"metadata":{"id":"9_EY3si-qq_X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will use Q-LoRA for fine-tuning:"],"metadata":{"id":"09AaF9-WBk3w"}},{"cell_type":"code","source":["from trl import ModelConfig, get_peft_config, get_quantization_config, get_kbit_device_map\n","from transformers import BitsAndBytesConfig\n","from peft import LoraConfig\n","\n","model_config = ModelConfig()\n","\n","quantization_config = get_quantization_config(model_config)\n","device_map = get_kbit_device_map(model_config)\n","peft_config = get_peft_config(model_config)\n","\n","model = AutoModelForVision2Seq.from_pretrained(\n","    model_id,\n","    torch_dtype=torch.bfloat16,\n","    device_map=device_map\n",")"],"metadata":{"id":"Qyu-Afu8Bp40"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We need a dataset for fine-tuning"],"metadata":{"id":"iNMdG1B1CeO3"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset('HuggingFaceH4/rlaif-v_formatted')"],"metadata":{"id":"gwrbAqEkCgsL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["PaliGemma does not have a chat template by default, so we need to create a template:"],"metadata":{"id":"PJzcx9k5Cn0H"}},{"cell_type":"code","source":["import logging\n","import os\n","import torch\n","\n","processor.chat_template = \"\"\"\n","{% if not add_generation_prompt is defined %}\n","    {% set add_generation_prompt = false %}\n","{% endif %}\n","A chat between a curious user and an artificial intelligence assistant.\n","The assistant gives helpful, detailed, and polite answers to the user's questions.\n","{% for message in messages %}\n","    {% if message['role'] == 'user' %}\n","        USER:\n","    {% else %}\n","        ASSISTANT:\n","    {% endif %}\n","\n","    {% for item in message['content'] %}\n","        {% if item['type'] == 'text' %}\n","            {{ item['text'] }}\n","        {% elif item['type'] == 'image' %}\n","            <image>\n","        {% endif %}\n","    {% endfor %}\n","\n","    {% if message['role'] == 'user' %}\n","    {% else %}\n","        {{eos_token}}\n","    {% endif %}\n","{% endfor %}\n","\n","{% if add_generation_prompt %}\n","    ASSISTANT:\n","{% endif %}\n","\"\"\"\n","\n","def process(row):\n","  row[\"prompt\"] = processor.apply_chat_template(row[\"prompt\"], tokenize=False)\n","  row[\"chosen\"] = processor.apply_chat_template(row[\"chosen\"], tokenize=False)\n","  row[\"rejected\"] = processor.apply_chat_template(row[\"rejected\"], tokenize=False)\n","  row[\"images\"][0] = row[\"images\"][0].convert(\"RGB\")\n","  return row"],"metadata":{"id":"4rWOm0DfCt2y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = dataset[\"train\"].train_test_split(test_size=0.5)[\"train\"]\n","eval_dataset = dataset[\"test\"]"],"metadata":{"id":"W-T3bS1oDjXF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can prepare the trainer:"],"metadata":{"id":"Zjlw7QKvDpSH"}},{"cell_type":"code","source":["from trl import DPOConfig, DPOTrainer, ModelConfig\n","\n","training_args = DPOConfig(output_dir=\"/content/outs\",\n","                          per_device_train_batch_size=8,\n","                          num_train_epochs=1)\n","\n","trainer = DPOTrainer(\n","    model,\n","    model_ref,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    tokenizer=processor,\n","    dataset_num_proc=32,\n","    # uncomment for peft\n","    # peft_config=get_peft_config(model_config)\n",")\n","trainer.train()\n","\n","trainer.save_model(training_args.output_dir)"],"metadata":{"id":"z15L4L9lDrlp"},"execution_count":null,"outputs":[]}]}