{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNMgNHmODKfIoSi2ooBTpIP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Generation with LLMs"],"metadata":{"id":"ClTZ8iWlg5jK"}},{"cell_type":"markdown","source":["LLMs are the key component behind text generation.\n","\n","Autoregressive generation is the inference-time procedure of iteratively calling a model with its own generated outputs, given a few initial inputs. In Transformers library, this is handled by the `generate()` method, which is available to all models with generative capabilities."],"metadata":{"id":"IJ8PhR-WhTL7"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"se19-Qxzg3df","executionInfo":{"status":"ok","timestamp":1729775293965,"user_tz":300,"elapsed":21134,"user":{"displayName":"Bin Liu","userId":"03585165976699804089"}}},"outputs":[],"source":["!pip install transformers bitsandbytes>=0.39.0 -q"]},{"cell_type":"markdown","source":["## Generate text"],"metadata":{"id":"VeHEEBSDhs40"}},{"cell_type":"markdown","source":["A language model trained for **causal language modeling** takes a sequence of text tokens as input and returns the probability distribution for the next token.\n","\n","A critical aspect of autoregressive generation with LLMs is how to select the next token from this probability distribution. Anything goes in this step as long as we end up with a token for the next iteration. This means it can be as simple as selecting the most likely token from the probability distribution or as complex as applying a dozen transformations before sampling from the resulting distribution.\n","\n","Ideally, the stopping condition is dictated by the model, which should learn when to output an end-of-sequence (`EOS`) token. If this is not the case, generation stops when some predefined maximum length is reached.\n","\n","Properly setting up the token selection step and the stopping condition is essential to make our model behave as we would expect on our task. This is why we have a `GenerationConfig` file associated with each model, which contains a good default generative parameterization and is loaded alongside our model."],"metadata":{"id":"42VzEZkohve5"}},{"cell_type":"markdown","source":["First, we need to load the model."],"metadata":{"id":"RNFJnrdSlSst"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    'mistralai/Mistral-7B-v0.1',\n","    device_map='auto',\n","    load_in_4bit=True,\n",")"],"metadata":{"id":"f99cxF_GhqVY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the `from_pretrained()` call:\n","* `device_map` ensures the model is move to GPU(s)\n","* `load_in_4bit` applies 4-bit dynamic quantization to massively reduce the resource requirements"],"metadata":{"id":"Q0fA6DUFl4ZN"}},{"cell_type":"markdown","source":["Next, we need to preprocess our text input with a tokenizer:"],"metadata":{"id":"pjD1cWVJmTD2"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-v0.1', padding_side='left')"],"metadata":{"id":"RfueGaF3mVYF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_inputs = tokenizer(['A list of colors: red, blue, green'],\n","                         return_tensors='pt').to('cuda')"],"metadata":{"id":"o3iuvfFnmeQn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `model_inputs` variable holds the tokenized text input, as well as the attention mask. While `generate()` does its best effort to infer the attention mask when it is not passed, we still recommend passing it whenever possible for optimal results.\n","\n","After tokenizing the inputs, we can call the `generate()` method to return the generated results. The generated tokens should be converted to text before printing:"],"metadata":{"id":"R5Z6HrtbntwL"}},{"cell_type":"code","source":["generated_ids = model.generate(**model_inputs)\n","\n","tokenizer.batch_decode(generated_ids, skip_speical_tokens=True)[0]"],"metadata":{"id":"1z4drdyqoHEu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we don't need to do it one sequence at a time. We can batch our inputs, which will greatly improve the throughput at a small lantency and memory cost. All we need to do is to make sure we pad our inputs properly:"],"metadata":{"id":"nCMt6_ROoPin"}},{"cell_type":"code","source":["tokenizer.pad_token = tokenizer.eos_token # most LLMs don't have a pad token by default\n","\n","model_inputs = tokenizer(\n","    ['A list of colors: red, blue, green', 'Paris is'],\n","    return_tensors='pt',\n","    padding=True,\n",").to('cuda')\n","\n","generated_ids = model.generate(**model_inputs)\n","\n","tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"],"metadata":{"id":"xESyVRu9ocVC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Common pitfalls"],"metadata":{"id":"Z2HnQzbzrgxd"}},{"cell_type":"markdown","source":["There are many generation strategies, and sometimes the default values may not be appropriate for our use case. If our outputs are not aligned with what we are expecting, we have created a list of the most common pitfalls and how to avoid them."],"metadata":{"id":"eVNOLTgLrkSZ"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","model_checkpoint = 'mistralai/Mistral-7B-v0.1'\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","tokenizer.pad_token = tokenizer.eos_token # most LLMs don't have a pad token by default\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_checkpoint,\n","    device_map='auto',\n","    load_in_4bit=True,\n",")"],"metadata":{"id":"FNAhz3Jnribx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generated output is too short/long"],"metadata":{"id":"gaf6PYl8sYUy"}},{"cell_type":"markdown","source":["If not specified in he `GenerationConfig` file, `generate` returns up to 20 tokens by default.\n","\n","LLMs (more precisely, decoder-only models) also return the input prompt as part of the output."],"metadata":{"id":"BBQCcsULscVu"}},{"cell_type":"code","source":["model_inputs = tokenizer(\n","    ['A sequence of numbers: 1, 2'],\n","    return_tensors = 'pt',\n",").to('cuda')"],"metadata":{"id":"RkavVJ3XsZ4B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# by default, the output will contain up to 20 tokens\n","generated_ids = model.generate(**model_inputs)\n","tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"],"metadata":{"id":"xcs4Ey4JtCN0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# setting `max_new_tokens` allows us to control the maximum length\n","generated_ids = model.generate(**model_inputs, max_new_tokens=50)\n","tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"],"metadata":{"id":"GEMZM3KUtCsu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Incorrect generation mode"],"metadata":{"id":"hgcjh0KntLpp"}},{"cell_type":"markdown","source":["By default, `generate` selects the most likely token at each iteration (greedy decoding). Depending on our task, this may be undesirable; creative tasks like chatbots or writing an essay benefit from sampling.\n","\n","Enable sampling with `do_sampling=True`:"],"metadata":{"id":"hMWDLIcetQKU"}},{"cell_type":"code","source":["# set seed for reproducibility\n","from transformers import set_seed\n","set_seed(101)\n","\n","model_inputs = tokenizer(\n","    ['I am a cat.'],\n","    return_tensors = 'pt',\n",").to('cuda')"],"metadata":{"id":"QbY4COLntOLX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LLM + greedy decoding\n","generated_ids = model.generate(**model_inputs)\n","tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"],"metadata":{"id":"bl-ysuNdtvCU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# With sampling, the output becomes more creative\n","generated_ids = model.generate(**model_inputs, do_sample=True)\n","tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"],"metadata":{"id":"EfuoEVREt2wK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Wrong padding side"],"metadata":{"id":"-W8mg1xqt9CR"}},{"cell_type":"markdown","source":["LLMs are decoder-only architectures, meaning they continue to iterate on our input prompt. Since LLMs are not trained to continue from pad tokens, our input needs to be left-padded. Make sure we also do not forget to pass the attention mask to generate."],"metadata":{"id":"FK5XtVV_t_1c"}},{"cell_type":"code","source":["# The tokenizer initialized above has right-padding active by default\n","model_inputs = tokenizer(\n","    ['1, 2, 3', 'A, B, C, D, E'],\n","    padding=True,\n","    return_tensors='pt',\n",").to('cuda')\n","\n","generated_ids = model.generate(**model_inputs)\n","tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"],"metadata":{"id":"WXQa0iL9uSji"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# With left-padding, it works as expected\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, padding_side='left')\n","tokenizer.pad_token = tokenizer.eos_token # most LLMs don't have a pad token by default\n","model_inputs = tokenizer(\n","    ['1, 2, 3', 'A, B, C, D, E'],\n","    padding=True,\n","    return_tensors='pt',\n",").to('cuda')\n","\n","generated_ids = model.generate(**model_inputs)\n","tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"],"metadata":{"id":"0JyKhf3xuizR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Wrong prompt"],"metadata":{"id":"6CC87FAsu5q7"}},{"cell_type":"markdown","source":["Some models and tasks expect a certain input prompt format to work properly."],"metadata":{"id":"hz-OsZ_9u7z_"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","model_checkpoint = 'HuggingFaceH4/zephyr-7b-alpha'\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_checkpoint,\n","    device_map='auto',\n","    load_in_4bit=True,\n",")"],"metadata":{"id":"py7CAwu5u6-a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["set_seed(101)\n","prompt = \"\"\"How many helicopters can a human eat in one sitting? Reply as a thug.\"\"\"\n","model_inputs = tokenizer([prompt], return_tensors='pt').to('cuda')\n","input_length = model_inputs.input_ids.shape[1]\n","generated_ids = model.generate(**model_inputs, max_new_tokens=20)\n","print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])"],"metadata":{"id":"1QwXWaSevyxa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["set_seed(101)\n","messages = [\n","    {\n","        'role': 'system',\n","        'content': 'You are a friendly chatbot who always responds in the style of a thug',\n","    },\n","    {\n","        'role': 'user',\n","        'content': 'How many helicopters can a human eat in one sitting?',\n","\n","    },\n","]\n","model_inputs = tokenizer.apply_chat_template(\n","    messages,\n","    add_generation_prompt=True,\n","    return_tensors='pt',\n",").to('cuda')\n","input_length = model_inputs.input_ids.shape[1]\n","generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20)\n","print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])"],"metadata":{"id":"KW6PYs7bv5sT"},"execution_count":null,"outputs":[]}]}