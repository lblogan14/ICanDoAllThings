{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO7OzqYQABLrWCAPALT059n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Reproduce DeepSeek R1 \"Aha moment\""],"metadata":{"id":"DfmHpaXk9JIz"}},{"cell_type":"markdown","source":["**DeepSeek-R1** is an open model that rivals OpenAI's o1 in complex reasoning tasks, introduced using **Group Relative Policy Optimization (GRPO)** and RL-forcused multi-stage training approach.\n","\n","In this session, we will train an open model using RL trying to teach it self-verification and search abilities on its own to solve the Countdown Game, which is a number puzzle where players use a set of randomly drawn numbers and basic arithmetic operations (+, -, x, /) to reach or get as close as possible to a target number:\n","```text\n","Target Number: 952\n","Available Numbers: 25, 50, 75, 100, 3, 6\n","\n","(100 x (3 x 3)) + (50 + 6 / 3) = 952\n","```"],"metadata":{"id":"PnEcuSQWHr-K"}},{"cell_type":"markdown","source":["## Group Relative Policy Optimization (GRPO)"],"metadata":{"id":"pAnrKvyzIpJD"}},{"cell_type":"markdown","source":["**Group Relative Policy Optimization (GRPO)** is a reinforcement learning algorithm to improve the reasoning capabilities of LLMs, which is introduced in the [DeepSeekMath](https://arxiv.org/abs/2402.03300) paper.\n","\n","GRPO modifies the traditional **Proximal Policy Optimization (PPO)** by eliminating the need of a value function model. Instead, it estimates baselines from group scores, reducing memory usage and computational overhead.\n","\n","GRPO, now also used by the Qwen team, can be used with rule/binary-based Rewards Models as well as General Rewards Models to improve models on helpfulness.\n","\n","Steps of GRPO:\n","1. **Sampling** - Generating multiple outputs for each prompt using the current policy\n","2. **Reward Scoring** - Each generation is scored using a reward function(, could be rule-based or outcome-based)\n","3. **Advantage Calculation** - The average reward of the generated outputs is used as a baseline. The advantage of each solution within the group is them computed relative to this baseline. The reward is normalized within a group.\n","4. **Policy Optimization** - The policy tries to maximize the GRPO objective, which includes the calculated advantages and a KL divergence term. This is different from how PPO implements the KL term within the reward."],"metadata":{"id":"iaTrvjH-Itur"}},{"cell_type":"markdown","source":["## Set up development environment"],"metadata":{"id":"jHiyWtj0LVPN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pieXai9E9AmV"},"outputs":[],"source":["!pip install \"torch==2.5.1\" tensorboard \"setuptools<71.0.0\"  --index-url https://download.pytorch.org/whl/cu121"]},{"cell_type":"code","source":["!pip install -qU flash-attn transformers datasets accelerate hf-transfer deepspeed trl vllm peft bitsandbytes"],"metadata":{"id":"_6KiMiJiLngx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generate training samples with reasoning prefix from the Coundown Game"],"metadata":{"id":"EHWvnWMELyMD"}},{"cell_type":"markdown","source":["Dataset: [`Jiayi-Pan/Countdown-Tasks-3to4`](https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4)\n","\n","Model: [`Qwen/Qwen2.5-3B-Instruct`](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)"],"metadata":{"id":"DSUFmT_LMQXz"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","from datasets import load_dataset\n","\n","# Load dataset from HuggingFace Hub\n","dataset_id = 'Jiayi-Pan/Countdown-Tasks-3to4'\n","dataset = load_dataset(dataset_id, split='train')\n","# select a random subset of 50k samples\n","dataset = dataset.shuffle(seed=111).select(range(50_000))"],"metadata":{"id":"Im0JzE2EL14v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load a tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-3B-Instruct')"],"metadata":{"id":"Ynle4RzAdBBd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate R1 prompt with a prefix for the model to already start with the thinking process\n","def generate_r1_prompt(numbers, target):\n","    r1_prefix = [\n","        {\n","            'role': 'system',\n","            'content': 'You are a helpful assistant. You first think about the reasoning process in the mind and then provides the user with the answer.'\n","        },\n","        {\n","            'role': 'user',\n","            'content': f\"\"\"Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) and each number can\n","             only be used once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example,\n","             <answer> (1 + 2) / 3 = 1 </answer>.\n","            \"\"\"\n","        },\n","        {\n","            'role': 'assistant',\n","            'content': 'Let me solve this step by step.\\n<think>'\n","        }\n","    ]\n","\n","    return {\n","        'prompt': tokenizer.apply_chat_template(r1_prefix, tokenize=False, continue_final_message=True),\n","        'target': target\n","    }"],"metadata":{"id":"SfKWLwKZdXRQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert dataset to the R1 prompt\n","dataset = dataset.map(lambda x: generate_r1_prompt(x['nums'], x['target']))\n","\n","# Split the dataset\n","train_test_split = dataset.train_test_split(test_size=0.1)\n","train_dataset = train_test_split['train']\n","test_dataset = train_test_split['test']"],"metadata":{"id":"pTTXSPqXesON"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train the model using GRPO"],"metadata":{"id":"5vErAz6_e8B3"}},{"cell_type":"markdown","source":["TRL supports Group Relative Policy Optimization (GRPO) through a dedicated `GRPOTrainer` for aligning LLMs from preference data. The `GRPOTrainer` is a subclass of the `Trainer` from the `transformers` library and supports all the same features, including logging, checkpointing, distributed training, and parameter efficient fine-tuning (PEFT).\n","\n","The `GRPOTrainer` supports generic **Outcome Reward Models (ORM)** and custom reward functions, that can be used to implement **Rule-Based Reward Models**.\n","\n","In the Deepseek R1 paper, they implemented Rule-Based Reward Models to verify the correctness of the generated solutions. In this example, we will create two reward functions that:\n","1. *Format Reward* checks if the generated format is correct `<think> [thinking] </think><answer> [answer] </answer>`\n","2. *Accuracy Reward* extracts the equation from the `<answer>` tag and evaluates it against the target and if every number is used once.\n","\n","NOTE: Correct `<answer>` in our example includes the equation, for example, `<answer> 55 + 36 - 7 - 19 </answer>`"],"metadata":{"id":"9zrPS7X1g_UZ"}},{"cell_type":"code","source":["import re\n","\n","def format_reward_func(completions, target, **kwargs):\n","    \"\"\"\n","    Format: <think>...</think><answer>...</answer>\n","    Args:\n","        completions (list[str]): Generated outputs\n","        target (list[str]): Expected answers\n","\n","    Returns:\n","        list[float]: Reward scores\n","    \"\"\"\n","    rewards = []\n","\n","    for completion, gt in zip(completions, target):\n","        try:\n","            # add synthetic <think> as it is already part of the prompt and prefilled for the assistant to more easily match the regex\n","            completion = \"<think>\" + completion\n","            # check if the format is correct\n","            regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n","\n","            _match = re.search(regex, completion, re.DOTALL)\n","            # if the format is not correct, reward is 0\n","            if _match is None or len(_match.groups()) != 2:\n","                rewards.append(0.)\n","            else:\n","                rewards.append(1.)\n","\n","        except Exception:\n","            rewards.append(0.)\n","\n","    return rewards\n","\n","\n","def equation_reward_func(completions, target, nums, **kwargs):\n","    \"\"\"\n","    Evaluates completions based on:\n","    Mathematical correctness of the answer\n","\n","    Args:\n","        completions (list[str]): Generated outputs\n","        target (list[str]): Expected answers\n","        nums (list[str]): Numbers used in the equation\n","\n","    Returns:\n","        list[float]: Reward scores\n","    \"\"\"\n","    rewards = []\n","\n","    for completion, gt, numbers in zip(completions, target, nums):\n","        try:\n","            # add synthetic <think> as it is already part of the prompt and prefilled for the assistant to more easily match the regex\n","            completion = \"<think>\" + completion\n","            # check if the format is correct\n","            _match = re.search(r\"<answer>(.*?)<\\/answer>\", completion)\n","            if _match is None:\n","                rewards.append(0.)\n","                continue\n","\n","            # Extract the answer part from the completion\n","            equation = _match.group(1).strip()\n","            # Extract all numbers from the equation\n","            used_numbers = [int(n) for n in re.findall(r'\\d+', equation)]\n","\n","            # Check if all numbers are used exactly once\n","            if sorted(used_numbers) != sorted(numbers):\n","                rewards.append(0.)\n","                continue\n","\n","            # Define a regex pattern that only allows numbers, operators, parentheses, and whitespace\n","            allowed_pattern = r'^[\\d+\\-*/().\\s]+$'\n","            if not re.match(allowed_pattern, equation):\n","                rewards.append(0.)\n","                continue\n","\n","            # Evaluate the equation with restricted globals and locals\n","            result = eval(equation, {\"__builti'ns__\": None}, {})\n","            # Check if the equation is correct and matches the ground truth\n","            if abs(float(result) - float(gt)) < 1e-5:\n","                rewards.append(1.)\n","            else:\n","                rewards.append(0.)\n","\n","        except Exception:\n","            # If evaluation fails, reward is 0\n","            rewards.append(0.)\n","\n","    return rewards"],"metadata":{"id":"zTt3zMJke_Xe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can test our reward functions. Note that none of the examples starts with `<think>` tag as we added it synthetically to the prompt."],"metadata":{"id":"dVpooBoQy5eX"}},{"cell_type":"code","source":["correct_sample_1 = \"\"\"We need to find an equation using the numbers 19, 36, 55, and 7\n","exactly once, with basic arithmetic operations, that equals 65. One possible\n","combination is 55 + 36 - 19 + 7... </think>\n","<answer> 55 + 36 - 7 - 19 </answer>\"\"\"\n","\n","correct_sample_2 = \"\"\" ... </think>\n","<answer> 55 + 36 - 7 - 19 </answer>\"\"\"\n","\n","wrong_format = \"\"\"User: Using the numbers [19, 36, 55, 7], create an equation that equals 65.\"\"\"\n","\n","wrong_format_2 = \"\"\"To find the equation that equals 79 using the numbers 95, 78, 6, 88, I'll start by adding 88 and 95:\n","95 + 88 = 183\n","Now, let's subtract 104 from 183 to get 79:\n","183 - 104 = 79\n","<think> 183 - 104 = 79 </think><think> 183 - 104 = 79 </think><answer> 183 - 104 = 79 </answer>\"\"\"\n","\n","wrong_result = \"\"\" ... </think>\n","<answer> 55 + 36 - 7 - 18 </answer>\"\"\""],"metadata":{"id":"kP4mxHhUi-1S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_rewards = format_reward_func(\n","    completions=[correct_sample_1, correct_sample_2, wrong_format, wrong_format_2, wrong_result],\n","    target=[\"65\", \"65\", \"65\", \"65\", \"65\"],\n","    nums=[[19, 36, 55, 7] * 5]\n",")\n","assert test_rewards == [1., 1., 0., 0., 1.], \"Reward function is not working\""],"metadata":{"id":"LRl9Lu_1zOCP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_rewards = equation_reward_func(\n","    completions=[correct_sample_1, correct_sample_2, wrong_format, wrong_format_2, wrong_result],\n","    target=[\"65\", \"65\", \"65\", \"65\", \"65\"],\n","    nums=[[19, 36, 55, 7]] * 5\n",")\n","assert test_rewards == [1.0, 1.0, 0.0, 0.0, 0.0], \"Reward function is not working\""],"metadata":{"id":"yIm7n-8Pzfu4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can define our remaining training parameters, create a trainer and start training."],"metadata":{"id":"SIvXLU85zkHK"}},{"cell_type":"code","source":["from trl import GRPOConfig, GRPOTrainer, get_peft_config, ModelConfig\n","\n","# The model we will use as policy\n","model_config = ModelConfig(\n","    model_name_or_path='Qwen/Qwen2.5-3B-Instruct',\n","    torch_dtype='bfloat16',\n","    attn_implementation='flash_attention_2',\n","    use_peft=True,\n","    load_in_4bit=True,\n",")\n","\n","# Hyperparameters in the Configuration\n","training_args = GRPOConfig(\n","    output_dir='qwen-r1-aha-moment',\n","    learning_rate=5e-7,\n","    lr_scheduler_type='cosine',\n","    logging_steps=10,\n","    max_steps=100,\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=1,\n","    gradient_checkpointing=True,\n","    gradient_checkpointing_kwargs={'use_reentrant': False},\n","    bf16=True,\n","    # GRPO specific parameters\n","    max_prompt_length=256,\n","    max_completion_length=1024, # max length of the generated output for our solution\n","    num_generations=2,\n","    beta=0.001,\n",")\n","\n","# Trainer setup\n","trainer = GRPOTrainer(\n","    model=model_config.model_name_or_path,\n","    reward_funcs=[format_reward_func, equation_reward_func],\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n","    peft_config=get_peft_config(model_config)\n",")"],"metadata":{"id":"74E3Kfa3zoTq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can start our training:"],"metadata":{"id":"OPH0Ilpq0oeH"}},{"cell_type":"code","source":["trainer.train()\n","# save model\n","trainer.save_model(training_args.output_dir)"],"metadata":{"id":"3LhQJ6u00rIz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Reinforcement Training** is very slow and compute intensive. Running a single step on 1xL4 with Q-LoRA, batch size of 1 only 2 generations per samples takes >20 minutes."],"metadata":{"id":"jJi7kQ-Z0ua5"}},{"cell_type":"markdown","source":["## Distributed training example for GRPO using DeepSpeed and vLLM"],"metadata":{"id":"KbHCeBAo09Mm"}},{"cell_type":"markdown","source":["TRL added support for distributed training with DeepSpeed and using vLLM for faster generation. We can use the [run_r1_grpo.py](https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/scripts/run_r1_grpo.py) script and the correspodning [receipe config](https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/receipes/grpo-qwen-2.5-3b-deepseek-r1-countdown.yaml) to run the distributed training.\n","\n","To run with `accelerate`:\n","```bash\n","accelerate launch --num_processes 3 --config_file configs/accelerate_configs/deepspeed_zero3.yaml scripts/run_r1_grpo.py --config receipes/grpo-qwen-2.5-3b-deepseek-r1-countdown.yaml\n","\n","```\n","\n","The `num_processes` is set to the number of GPUs we have. Make sure to leave the last one for vLLM generation. If we have a Node with 8 A100 GPUs, we need to change the `vllm_device` in the config file to the last index GPU, e.g., we need to set `vllm_device=7` and we set `num_processes` to 7."],"metadata":{"id":"H31VtfR41Il1"}},{"cell_type":"markdown","source":["## Results"],"metadata":{"id":"VhDlg4LB29Jy"}},{"cell_type":"markdown","source":["The script saves random completions to the `completion_samples` folder, which you can use to inspect the model's progress. It includes `completion_samples.txt` and `success_completion_samples.txt`. The `completion_samples.txt` includes all completions, while the `success_completion_samples.txt` which correctly solves the equation."],"metadata":{"id":"xn6WEr9A2_RL"}},{"cell_type":"markdown","source":["Training Observations:\n","* At ~50 steps the model has learned the correct format `<think>...</think>\\n<answer>...</answer>`.\n","* At 100 steps the success rate for solving the equation is around 25%. The model starts to \"reason\" with words see examples below.\n","* At 200 steps the performance seems to converge much slower and we are at ~40% success rate. The model starts to learn a new \"format\" where it solves the equation similar to how you would do it programmatically, by trying different combinations and reviewing the results, see \"Successfull Reasoning Samples between step 200 and 450\".\n","* At 450 steps we have 50% success rate for solving the equation. The performance still improves slowly and the model kept its new format form from step 200."],"metadata":{"id":"WWxihAUA3RO-"}},{"cell_type":"code","source":[],"metadata":{"id":"vDZ370mj05JU"},"execution_count":null,"outputs":[]}]}