{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOx+oZDO5QFgnZFeOESvy/1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Finetuning Granite Vision 3.1 2B with TRL"],"metadata":{"id":"y2-Y8jfdkxp9"}},{"cell_type":"markdown","source":["In this example, we will finetune the IBM's [`Granite Vision 3.1 2B`](https://huggingface.co/ibm-granite/granite-vision-3.1-2b-preview) model. It is lightweight and trained by finetuning a [`Granite Language`] model with both images and text modalities.\n","\n","We will finetune and evaluate the [`Granite Vision`](https://huggingface.co/ibm-granite/granite-vision-3.1-2b-preview) model using the [`Geometric Perception`](https://huggingface.co/datasets/euclid-multimodal/Geoperception) dataset, containing tasks that the model was not initially trained for. The **Geometric Perception** dataset provides images of various geometric diagrams, compiled from high-school textbooks, paired with question-answer pairs."],"metadata":{"id":"U_bCj46ik1Wh"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"jrgiE2Bez8hx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tw5w1e-7ksWK"},"outputs":[],"source":["!pip install -q git+https://github.com/huggingface/transformers.git\n","!pip install  -U -q trl datasets bitsandbytes peft accelerate\n","# Tested with transformers==4.49.0.dev0, trl==0.14.0, datasets==3.2.0, bitsandbytes==0.45.2, peft==0.14.0, accelerate==1.3.0"]},{"cell_type":"code","source":["!pip install -q flash-attn --no-build-isolation\n","\n","try:\n","    import flash_attn\n","    print(\"FlashAttention is installed\")\n","    USE_FLASH_ATTENTION = True\n","except ImportError:\n","    print(\"FlashAttention is not installed\")\n","    USE_FLASH_ATTENTION = False"],"metadata":{"id":"WvvXJO_n0Awv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load dataset"],"metadata":{"id":"nZupSLJS0BSe"}},{"cell_type":"markdown","source":["We will load the [`GeometricPerception`](https://huggingface.co/datasets/euclid-multimodal/Geoperception) dataset, which provides images of various geometric diagrams, compiled from popular high-school textbooks, paired with question-answer pairs.\n","\n","We will use the original system prompt used during the model training."],"metadata":{"id":"ED23XrdOGk7_"}},{"cell_type":"code","source":["system_message = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\""],"metadata":{"id":"elHi1fV_Ggqj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For demo purposes, we will only train and evaluate on the Line Length Comparison task."],"metadata":{"id":"mR5SE4oLG_oR"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset_id = 'euclid-multimodal/Geoperception'\n","dataset = load_dataset(dataset_id)\n","\n","dataset_LineComparison = dataset['train'].filter(\n","    lambda x: x['predicate'] == 'LineComparison'\n",")\n","train_test = dataset_LineComparison.train_test_split(test_size=0.5, seed111)"],"metadata":{"id":"R2J2Ue7THGRu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_test"],"metadata":{"id":"zu-E7gEeH7cP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will format the dataset into a chatbot structure, with the system message, image, user query, and answer for each interaction."],"metadata":{"id":"BW6aq9HmH8ox"}},{"cell_type":"code","source":["def format_data(sample):\n","    return [\n","        {\n","            \"role\": \"system\",\n","            \"content\": [{'type': 'text', 'text': system_message}]\n","        },\n","        {\n","            'role': 'user',\n","            'content': [\n","                {'type': 'image', 'image': sample['image']},\n","                {'type': 'text', 'text': sample['question']}\n","            ]\n","        },\n","        {\n","            'role': 'assistant',\n","            'content': [{'type': 'text', 'text': sample['answer']}]\n","        }\n","    ]"],"metadata":{"id":"nZuDMzT5IBfC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = [foramt_data(x) for x in train_test['train']]\n","test_dataset = [foramt_data(x) for x in train_test['test']]"],"metadata":{"id":"c3LLVRs7IZvu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset[0]"],"metadata":{"id":"12B9ze_yIeRh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load model and check performance"],"metadata":{"id":"3wV6bfOxIfjL"}},{"cell_type":"code","source":["from transformers import AutoProcessor, AutoModelForVision2Seq\n","import torch\n","\n","model_id = 'ibm-granite/granite-vision-3.1-2b-preview'\n","processor = AutoProcessor.from_pretrained(model_id)\n","model = AutoModelForVision2Seq.from_pretrained(\n","    model_id,\n","    device_map='auto',\n","    torch_dtype=torch.bfloat16,\n","    _attn_implementation='flash_attention_2' if USE_FLASH_ATTENTION else None\n",")"],"metadata":{"id":"2uW3xLnHIg7_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# take a sample\n","test_idx = 0\n","sample = test_dataset[test_idx]\n","sample"],"metadata":{"id":"sK2IuVA7I5Dv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample[1]['content'][0]['image']"],"metadata":{"id":"R8kerEkvI8Pg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will create a method that takes the model, processor, and sample as inputs to generate the model's answer."],"metadata":{"id":"pfw5FqsQI_or"}},{"cell_type":"code","source":["def generate_text_from_sample(model, processor, sample, max_new_tokens=100, device='cuda'):\n","    text_input = processor.apply_chat_template(\n","        sample[:2], # use without the assistant response\n","        add_generation_prompt=True,\n","    )\n","\n","    image_inputs = []\n","    image = sample[1]['content'][0]['image']\n","    if image.mode != 'RGB':\n","        image = image.convert('RGB')\n","    image_inputs.append(image)\n","\n","    # Prepare the inputs for the model\n","    model_inputs = processor(\n","        text=text_input,\n","        images=image_inputs,\n","        return_tensors='pt',\n","    ).to(device)\n","\n","    # Generate output ids\n","    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n","\n","    # Trim the generated ids\n","    generated_ids_trimmed = [\n","        out_ids[len(in_ids) :]\n","        for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)\n","    ]\n","\n","    # Decode the output text\n","    output_text = processor.batch_decode(\n","        generated_ids_trimmed,\n","        skip_special_tokens=True,\n","        clean_up_tokenization_spaces=False\n","    )\n","\n","    return output_text[0]"],"metadata":{"id":"4UHhWRxSJFap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = generate_text_from_sample(model, processor, sample)\n","output"],"metadata":{"id":"YDdFaRSeJ499"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Remove model and clean GPU"],"metadata":{"id":"X2uCzFFXJ6jI"}},{"cell_type":"code","source":["import gc\n","import time\n","\n","\n","def clear_memory():\n","    # Delete variables if they exist in the current global scope\n","    if \"inputs\" in globals():\n","        del globals()[\"inputs\"]\n","    if \"model\" in globals():\n","        del globals()[\"model\"]\n","    if \"processor\" in globals():\n","        del globals()[\"processor\"]\n","    if \"trainer\" in globals():\n","        del globals()[\"trainer\"]\n","    if \"peft_model\" in globals():\n","        del globals()[\"peft_model\"]\n","    if \"bnb_config\" in globals():\n","        del globals()[\"bnb_config\"]\n","    time.sleep(2)\n","\n","    # Garbage collection and clearing CUDA memory\n","    gc.collect()\n","    time.sleep(2)\n","    torch.cuda.empty_cache()\n","    torch.cuda.synchronize()\n","    time.sleep(2)\n","    gc.collect()\n","    time.sleep(2)\n","\n","    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n","    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n","\n","\n","clear_memory()"],"metadata":{"id":"6Y7_IWFcJ8PD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Finetune the model using TRL"],"metadata":{"id":"F9GD3HVEJ-QW"}},{"cell_type":"markdown","source":["### Load the quantized model for training"],"metadata":{"id":"7u0m7YDqKAQS"}},{"cell_type":"code","source":["from transformers import BitsAndBytesConfig\n","\n","USE_QLORA = True\n","USE_LORA = True\n","\n","if USE_QLORA:\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_use_double_quant=True,\n","        bnb_4bit_quant_type='nf4',\n","        bnb_4bit_compute_dtype=torch.bfloat16,\n","        llm_int8_skip_modules=['vision_tower', 'lm_head'], # skip problematic modules\n","        llm_int8_enable_fp32_cpu_offload=True\n","    )\n","else:\n","    bnb_config = None\n","\n","\n","processor = AutoProcessor.from_pretrained(model_id)\n","model = AutoModelForVision2Seq.from_pretrained(\n","    model_id,\n","    device_map='auto',\n","    torch_dtype=torch.bfloat16,\n","    quantization_config=bnb_config,\n","    _attn_implementation='flash_attention_2' if USE_FLASH_ATTENTION else None\n",")"],"metadata":{"id":"AUTa5wc3J_wQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Set up Q-LoRA and SFTConfig"],"metadata":{"id":"-rWGSj_LKcW9"}},{"cell_type":"markdown","source":["QLoRA allows efficient fine-tuning of large models by reducing the memory footprint. Unlike traditional LoRA, which uses low-rank approximation, QLoRA further quantizes the LoRA adapter weights, leading to even lower memory usage and faster training.\n","\n","To boost efficiency, we can also leverage a paged optimizer or 8-bit optimizer during QLoRA implementation. This approach enhances memory efficiency and speeds up computations, making it ideal for optimizing our model without sacrificing performance."],"metadata":{"id":"YSsJ1L-qKhZz"}},{"cell_type":"code","source":["if USE_LORA:\n","    from peft import LoraConfig, get_peft_model\n","\n","    peft_config = LoraConfig(\n","        r=8,\n","        lora_alpha=8,\n","        lora_dropout=0.1,\n","        target_modules=[name for name, _ in model.named_modules() if 'language_model' in name and '_proj' in name],\n","        use_dora=True,\n","        init_lora_weights='gaussian'\n","    )\n","\n","    # apply peft model\n","    model.add_adapter(peft_config)\n","    model.enable_adapters()\n","    model = get_peft_model(model, peft_config)\n","\n","    model.print_trainable_parameters()\n","else:\n","    peft_config = None"],"metadata":{"id":"AuOvyZ9tKeiC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from trl import SFTConfig\n","\n","training_args = SFTConfig(\n","    output_dir='./checkpoints/geoperception',\n","    num_train_epochs=1,\n","    per_device_train_batch_size=8,\n","    gradient_accumulation_steps=2,\n","    warmup_steps=10,\n","    learning_rate=1e-4,\n","    weight_decay=0.01,\n","    loggint_steps=10,\n","    save_strategy='steps',\n","    save_steps=20,\n","    save_total_limit=1,\n","    optim='adamw_torch_fused',\n","    bf16=True,\n","    push_to_hub=False,\n","    report_to='none',\n","    remove_unused_columns=False,\n","    gradient_checkpointing=True,\n","    dataset_text_field='',\n","    dataset_kwargs={'skip_prepare_dataset': True}\n",")"],"metadata":{"id":"gS2lbPPYLBIl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train the model"],"metadata":{"id":"x1uW9YpKMfql"}},{"cell_type":"markdown","source":["We need a collator function to ensure that the data is correctly structured for the model during training. This function will handle the formatting and batching of our dataset inputs, ensuring the data is properly aligned for training."],"metadata":{"id":"uHV7fZkkMioJ"}},{"cell_type":"code","source":["def collate_fn(examples):\n","    texts = [\n","        processor.apply_chat_template(example, tokenize=False)\n","        for example in examples\n","    ]\n","\n","    image_inputs = []\n","    for example in examples:\n","        image = example[1]['content'][0]['image']\n","        if image.mode != 'RGB':\n","            image = image.convert('RGB')\n","        image_inputs.append(image)\n","\n","    batch = processor(\n","        text=texts,\n","        images=image_inputs,\n","        padding=True,\n","        return_tensors='pt'\n","    )\n","\n","    labels = batch['input_ids'].clone()\n","    assistant_token = processor.tokenizer(\n","        '<|assistant|>',\n","        return_tensors='pt'\n","    )['input_ids'][0]\n","    eos_token = processor.tokenizer(\n","        '<|end_of_text|>',\n","        return_tensors='pt'\n","    )['input_ids'][0]\n","\n","    for i in range(batch['input_ids'].shape[0]):\n","        apply_loss = False\n","        for j in range(batch['input_ids'].shape[1]):\n","            if not apply_loss:\n","                labels[i][j] = -100\n","            if (j >= len(assistant_token) + 1) and torch.all(\n","                batch['input_ids'][i][j + 1 - len(assistant_token) : j + 1] == assistant_token\n","            ):\n","                apply_loss = True\n","            if batch['input_ids'][i][j] == eos_token:\n","                apply_loss = False\n","\n","    batch['labels'] = labels\n","    return batch"],"metadata":{"id":"yqP3JT0OMhL6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from trl import SFTTrainer\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    data_collator=collate_fn,\n","    peft_config=peft_config,\n","    tokenizer=processor.tokenizer\n",")"],"metadata":{"id":"htIc_2pHOwSZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"UqnJ4hlWPMoY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.save_model(training_args.output_dir)"],"metadata":{"id":"u4T9ViCVPNp0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test and finetuned model"],"metadata":{"id":"NwNxndJAPQQS"}},{"cell_type":"code","source":["clear_memory()"],"metadata":{"id":"4SczW7XzPRnV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["processor = AutoProcessor.from_pretrained(model_id)\n","model = AutoModelForVision2Seq.from_pretrained(\n","    training_args.output_dir,\n","    device_map='auto',\n","    torch_dtype=torch.bfloat16,\n","    _attn_implementation='flash_attention_2' if USE_FLASH_ATTENTION else None\n",")\n","\n","if USE_LORA:\n","    from peft import PeftModel\n","    model = PeftModel.from_pretrained(model, training_args.output_dir)"],"metadata":{"id":"im3KwLRpPUGK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_idx = 0\n","sample = test_dataset[test_idx]\n","sample"],"metadata":{"id":"1Q2vWTUmPiyT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample[1]['content'][0]['image']"],"metadata":{"id":"9slhA-3tPmm6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = generate_text_from_sample(model, processor, sample)\n","output"],"metadata":{"id":"bg87-sGXPpKv"},"execution_count":null,"outputs":[]}]}