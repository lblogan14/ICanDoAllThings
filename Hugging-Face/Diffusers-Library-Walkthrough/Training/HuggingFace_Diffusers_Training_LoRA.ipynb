{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNeTirConSTm68sitXTxxKS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# LoRA"],"metadata":{"id":"bAWzsCLk3kG9"}},{"cell_type":"markdown","source":["**LoRA (Low-Rank Adaptation of Large Language Models)** is a popular and lightweight training technique that significantly reduces the number of trainable parameters. It works by inserting a smaller number of new weights into the model and only these are trained. This makes training with LoRA much faster, memory-efficient, and produces smaller model weights, which are easier to store and share. LoRA can also be combined with other training techniques like DreamBooth to speedup training.\n","\n","We will explore the [`train_text_to_image.py`](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py).\n","\n","As always, make sure to install the `diffusers` library from source:\n","```bash\n","git clone https://github.com/huggingface/diffusers\n","cd diffusers\n","pip install .\n","```\n","\n","We need to navigate to the following folder and install the corresponding dependencies:\n","```bash\n","cd examples/text_to_image\n","pip install -r requirements.txt\n","```\n","HuggingFace Accelerate is also helpful to train on multiple GPUs with mixed-precision.\n","```bash\n","pip install accelerate\n","```\n","\n","Now we can initialize a HuggingFace Accelerate environment\n","```bash\n","accelerate config\n","```\n","To set up a default Acclerate environment without choosing any configurations:\n","```bash\n","accelerate config default\n","```\n","Or if our environment does not support an interactive shell like a notebook, we can use:\n","```python\n","from accelerate.utils import write_basic_config\n","write_basic_config()\n","```"],"metadata":{"id":"d6lUL_kJ3oTD"}},{"cell_type":"markdown","source":["## Script parameters"],"metadata":{"id":"kmt_pk7F4p3M"}},{"cell_type":"markdown","source":["The training script provides many parameters to customize the training run. We can find all of the parameters and their descriptions in the `parse_args()` function.\n","\n","Many parameters are described in the **Text-to-image** training guide, so we only focus on the LoRA relevant parameters:\n","* `--rank`: the inner dimension of the low-rank matrices to train; a higher rank means more trainable parameters\n","* `--learning_rate`: the default learning rate is 1e-4, but with LoRA, we can use a higher learning rate."],"metadata":{"id":"G-yh0H_9yfbu"}},{"cell_type":"markdown","source":["## Training script"],"metadata":{"id":"JH18nTL_y61L"}},{"cell_type":"markdown","source":["The `main()` function contains the dataset preprocessing and training loop.\n","\n","A detailed walkthrough of the training script is in the **Text-to-image** training guide. Here we only take a look at the LoRA relevant parts."],"metadata":{"id":"Hc6Xdz0ky-ae"}},{"cell_type":"markdown","source":["##### UNet"],"metadata":{"id":"WuPCgjg5zP3Y"}},{"cell_type":"markdown","source":["Diffusers uses the `LoraConfig` from the `PEFT` library to set up the parameters of the LoRA adapter such as the rank, alpha, and which modules to insert the LoRA weights into. The adapter is added to the UNet, and only the LoRA layers are filtered for optimization in `lora_layers`:\n","```python\n","    unet_lora_config = LoraConfig(\n","        r=args.rank,\n","        lora_alpha=args.rank,\n","        init_lora_weights=\"gaussian\",\n","        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n","    )\n","\n","    # Move unet, vae and text_encoder to device and cast to weight_dtype\n","    unet.to(accelerator.device, dtype=weight_dtype)\n","    vae.to(accelerator.device, dtype=weight_dtype)\n","    text_encoder.to(accelerator.device, dtype=weight_dtype)\n","\n","    # Add adapter and make sure the trainable params are in float32.\n","    unet.add_adapter(unet_lora_config)\n","    if args.mixed_precision == \"fp16\":\n","        # only upcast trainable parameters (LoRA) into fp32\n","        cast_training_params(unet, dtype=torch.float32)\n","\n","    if args.enable_xformers_memory_efficient_attention:\n","        if is_xformers_available():\n","            import xformers\n","\n","            xformers_version = version.parse(xformers.__version__)\n","            if xformers_version == version.parse(\"0.0.16\"):\n","                logger.warning(\n","                    \"xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.\"\n","                )\n","            unet.enable_xformers_memory_efficient_attention()\n","        else:\n","            raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\n","\n","    lora_layers = filter(lambda p: p.requires_grad, unet.parameters())\n","```\n","The `optimizer` is intiialized with the `lora_layers` because these are the only weights that will be optimized:\n","```python\n","    optimizer = optimizer_cls(\n","        lora_layers,\n","        lr=args.learning_rate,\n","        betas=(args.adam_beta1, args.adam_beta2),\n","        weight_decay=args.adam_weight_decay,\n","        eps=args.adam_epsilon,\n","    )\n","```"],"metadata":{"id":"cE6qpwSMzRRy"}},{"cell_type":"markdown","source":["## Launch the script"],"metadata":{"id":"dL84pse1z16w"}},{"cell_type":"markdown","source":["We will train on the Naruto BLIP captions dataset to generate our own Naruto characters.\n","\n","The script will create and save the following files to our repository:\n","* saved model checkpoints\n","* `pytorch_lora_weights.safetensors` (the trained LoRA weights)\n","\n","```bash\n","export MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n","export OUTPUT_DIR=\"/sddata/finetune/lora/naruto\"\n","export HUB_MODEL_ID=\"naruto-lora\"\n","export DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n","\n","accelerate launch --mixed_precision=\"fp16\"  train_text_to_image_lora.py \\\n","  --pretrained_model_name_or_path=$MODEL_NAME \\\n","  --dataset_name=$DATASET_NAME \\\n","  --dataloader_num_workers=8 \\\n","  --resolution=512 \\\n","  --center_crop \\\n","  --random_flip \\\n","  --train_batch_size=1 \\\n","  --gradient_accumulation_steps=4 \\\n","  --max_train_steps=15000 \\\n","  --learning_rate=1e-04 \\\n","  --max_grad_norm=1 \\\n","  --lr_scheduler=\"cosine\" \\\n","  --lr_warmup_steps=0 \\\n","  --output_dir=${OUTPUT_DIR} \\\n","  --push_to_hub \\\n","  --hub_model_id=${HUB_MODEL_ID} \\\n","  --report_to=wandb \\\n","  --checkpointing_steps=500 \\\n","  --validation_prompt=\"A naruto with blue eyes.\" \\\n","  --seed=1337\n","```\n","\n","Once training is completed, we can use our LoRA for inference"],"metadata":{"id":"bVIPw95O1Iq9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"497rnybU3hI5"},"outputs":[],"source":["from diffusers import AutoPipelineForText2Image\n","import torch\n","\n","pipeline = AutoPipelineForText2Image.from_pretrained(\n","    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n","    torch_dtype=torch.float16\n",").to(\"cuda\")\n","pipeline.load_lora_weights(\n","    \"path/to/lora/model\",\n","    weight_name=\"pytorch_lora_weights.safetensors\"\n",")"]},{"cell_type":"code","source":["image = pipeline(\"A naruto with blue eyes\").images[0]\n","image"],"metadata":{"id":"9XodVcvx1o6A"},"execution_count":null,"outputs":[]}]}