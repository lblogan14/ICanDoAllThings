{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM5wiE34wP8SWrGvuimGajQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Using LLM-as-a-Judge for an Automated and Versatile Evaluation"],"metadata":{"id":"y3CEyOQkuzcb"}},{"cell_type":"markdown","source":["Evaluating LLMs is a difficult task. Given their broad capabilities, the tasks given to them often shoudl be judged on requirements that would be very broad, and loosely-defined.\n","\n","An assistant's answer to a question can be\n","* not grounded in context\n","* repetitive, repetitive, repetitive\n","* grammatically incorrect\n","* excessively lengthy and characterized by an overabundance of words, leading to a situation where the discourse or written content becomes overly detailed and protracted\n","* incoherent\n","* ...\n","\n","\n","Each of these is still hard to measure. Devising a rule-based program to assess the outputs is extremely challenging. Traditional evaluation metrics based on the similarity between outputs and reference answers (e.g., ROUGE, BLEU) are also ineffective for these issues.\n","\n","A powerful solution to assess outputs in a human way, without requiring costly human time, is LLM-as-a-judge, introduced in the paper [*Juding LLM-as-a-Judge with MT-Bench and Chatbot Arena*](https://huggingface.co/papers/2306.05685). The idea is to ask an LLM to do the grading for us."],"metadata":{"id":"Y0PVBA4-vbjl"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"Yt7eSaijxSe7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GKETTEfzusiM"},"outputs":[],"source":["!pip install -qU huggingface_hub datasets pandas tqdm"]},{"cell_type":"code","source":["import re\n","import pandas as pd\n","from tqdm.auto import tqdm\n","from datasets import load_dataset\n","from huggingface_hub import notebook_login, InferenceClient\n","\n","tqdm.pandas() # load tqdm's pandas support\n","pd.set_option('display.max_colwidth', None)"],"metadata":{"id":"yRbA-4wDxVyC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["notebook_login()"],"metadata":{"id":"i2BTu9bA6Mj3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["repo_id = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n","\n","llm_client = InferenceClient(\n","    model=repo_id,\n","    timeout=120\n",")\n","\n","# Test LLM client\n","llm_client.text_generation(\n","    prompt='How are you today?',\n","    max_new_tokens=20\n",")"],"metadata":{"id":"BK0MMmT76c9k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prepare the creation and evaluation of LLM judge"],"metadata":{"id":"jwrpUoxE6o51"}},{"cell_type":"markdown","source":["If we ask an LLM to answer open-ended questions, the challenge is that measuring the answer's quality is difficult, for example, an exact string match will flag too many correct but differently worded answers as false. In this case we can set up a LLM-as-a-judge.\n","\n","TO use a LLM-as-a-judge, we first need to evaluate how reliably it rates our model outputs.\n","\n","The first step is to create human evaluation dataset and it does not need to be large. 30+ examples should be enough to get a good idea of the performance. We can re-use this dataset everytime we want to test our LLM-as-a-judge.\n","\n","In this example, we will use the [`feedbackQA`](https://huggingface.co/datasets/McGill-NLP/feedbackQA), which contains 2 human evaluations and scores for each question/answer couple. Using a sample of 30 examples will be representative of what our small evaluation dataset could be."],"metadata":{"id":"ITaPjJzh652G"}},{"cell_type":"code","source":["ratings = load_dataset('MiGill-NLP/feedbackQA')['train']\n","ratings = pd.DataFrame(ratings)\n","\n","raings['review_1'] = ratings['feedback'].apply(lambda x: x['rating'][0])\n","ratings['explanation_1'] = ratings['feedback'].apply(lambda x: x['explanation'][0])\n","ratings['review_2'] = ratings['feedback'].apply(lambda x: x['rating'][1])\n","ratings['explanation_2'] = ratings['feedback'].apply(lambda x: x['explanation'][1])\n","\n","ratings = ratings.drop(columns=['feedback'])\n","\n","# Map scores to numeric values\n","conversion_dict = {'Excellent': 4, 'Acceptable': 3, 'Could be Improved': 2, 'Bad': 1}\n","ratings['review_1'] = ratings['review_1'].map(conversion_dict)\n","ratings['review_2'] = ratings['review_2'].map(conversion_dict)"],"metadata":{"id":"4Q_mTZ-Z6sRK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check a baseline for performance. In this dataset, it can be the agreement between the two human raters, as measured by the Pearson correlation of the scores they give."],"metadata":{"id":"kMOIwJKI8xsC"}},{"cell_type":"code","source":["print('Correlation between 2 human raters:')\n","print(f\"{ratings['score_1'].corr(ratings['score_2'], method='pearson'):.3f}\")"],"metadata":{"id":"E084ZiD-8mPK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If our human rating correlation is really bad, it probably means that the rating criteria are not clear enough.\n","\n","This means that our \"ground truth\" contains noise: we cannot expect any algorithmic evaluation to come that close to it.\n","\n","To reduce this noise, we can\n","* take the average score as our ground truth instead of any single score, we should even out some of the irregularities,\n","* only select the samples where the human reviewers are in agreement.\n","\n","In this example, we choose the second option and **only keep examples where the 2 human reviewers are in agreement**."],"metadata":{"id":"AD3bLfk389p8"}},{"cell_type":"code","source":["# sample examples\n","ratings_where_raters_agree = ratings.loc[ratings['score_1'] == ratings['score_2']]\n","examples = ratings_where_raters_agree.groupby('score_1').sample(10, random_state=111)\n","examples['human_score'] = examples['score_1']\n","\n","# visualize 1 sample for each score\n","display(examples.groupby('human_score').first())"],"metadata":{"id":"EYD-9mCd9f_l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create our LLM judge"],"metadata":{"id":"Lu0ROf8wRZ-X"}},{"cell_type":"markdown","source":["We will build our LLM judge with a basic prompt, containing:\n","* task description\n","* scale description: `minimum`, `maximum`, value types (`float` here)\n","* explanation of the output format\n","* a beginning of an answer, to take the LLM by the hand as far as we can"],"metadata":{"id":"2QWKdmyeRcS8"}},{"cell_type":"code","source":["JUDGE_PROMPT = \"\"\"\n","You will be given a user_question and system_answer couple.\n","Your task is to provide a 'total rating' scoring how well the system_answer answers the user concerns expressed in the user_question.\n","Give your answer as a float on a scale of 0 to 10, where 0 means that the system_answer is not helpful at all, and 10 means that the answer completely and helpfully addresses the question.\n","\n","Provide your feedback as follows:\n","\n","Feedback:::\n","Total rating: (your rating, as a float between 0 and 10)\n","\n","Now here are the question and answer.\n","\n","Question: {question}\n","Answer: {answer}\n","\n","Feedback:::\n","Total rating: \"\"\""],"metadata":{"id":"qQWwtNeNRbeR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["examples['llm_judge'] = examples.progress_apply(\n","    lambda x: llm_client.text_generation(\n","        prompt=JUDGE_PROMPT.format(question=x['question'], answer=x['answer']),\n","        max_new_tokens=1000\n","    ),\n","    axis=1\n",")"],"metadata":{"id":"mg-nmxujRr9j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_judge_score(answer: str, split_str: str = 'Total rating:') -> int:\n","    try:\n","        if split_str in answer:\n","            rating = answer.split(split_str)[1]\n","        else:\n","            rating = answer\n","\n","        digit_groups = [el.strip() for el in re.findall(r\"\\d+(?:\\.\\d+)?\", rating)]\n","        return float(digit_groups[0])\n","    except Exception as e:\n","        print(e)\n","        return None\n","\n","\n","examples['llm_judge_score'] = examples['llm_judge'].apply(extract_judge_score)\n","# Rescale the score given by the LLM on the same scale as the human score\n","examples['llm_judge_score'] = (examples['llm_judge_score'] / 10) + 1"],"metadata":{"id":"8c_lFowhR7Ry"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Correlation between LLM-as-a-judge and the human raters:\")\n","print(f\"{examples['llm_judge_score'].corr(examples['human_score'], method='pearson'):.3f}\")"],"metadata":{"id":"HPUyeEraq0-W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Improve the LLM judge"],"metadata":{"id":"0FQQk2sJq4d_"}},{"cell_type":"markdown","source":["LLMs are not good at evaluating outputs in continous ranges. The best practices to build a better prompt would be:\n","* **Leave more time for thought** by adding an `Evaluation` field before the final answer\n","* **Use a small integer scale** like 1-4 or 1-5 instead of a large float scale as we had previously\n","* **Provide an indicative scale for guidance**\n","* **Add a virtual reward to motivate the LLMs**"],"metadata":{"id":"ohau0fJeq89I"}},{"cell_type":"code","source":["IMPROVED_JUDGE_PROMPT = \"\"\"\n","You will be given a user_question and system_answer couple.\n","Your task is to provide a 'total rating' scoring how well the system_answer answers the user concerns expressed in the user_question.\n","Give your answer on a scale of 1 to 4, where 1 means that the system_answer is not helpful at all, and 4 means that the system_answer completely and helpfully addresses the user_question.\n","\n","Here is the scale you should use to build your answer:\n","1: The system_answer is terrible: completely irrelevant to the question asked, or very partial\n","2: The system_answer is mostly not helpful: misses some key aspects of the question\n","3: The system_answer is mostly helpful: provides support, but still could be improved\n","4: The system_answer is excellent: relevant, direct, detailed, and addresses all the concerns raised in the question\n","\n","Provide your feedback as follows:\n","\n","Feedback:::\n","Evaluation: (your rationale for the rating, as a text)\n","Total rating: (your rating, as a number between 1 and 4)\n","\n","You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n","\n","Now here are the question and answer.\n","\n","Question: {question}\n","Answer: {answer}\n","\n","Provide your feedback. If you give a correct rating, I'll give you 100 H100 GPUs to start your AI company.\n","Feedback:::\n","Evaluation: \"\"\""],"metadata":{"id":"dRai7pDgq6SF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["examples['llm_judge_improved'] = examples.progress_apply(\n","    lambda x: llm_client.text_generation(\n","        prompt=IMPROVED_JUDGE_PROMPT.format(question=x['question'], answer=x['answer']),\n","        max_new_tokens=500\n","    ),\n","    axis=1\n",")\n","\n","examples['llm_judge_improved_score'] = examples['llm_judge_improved'].apply(extract_judge_score)"],"metadata":{"id":"pnI9AY3BrZt0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Correlation between LLM-as-a-judge and the human raters:\")\n","print(f\"{examples['llm_judge_improved_score'].corr(examples['human_score'], method='pearson'):.3f}\")"],"metadata":{"id":"9p366ePHrqbK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Comparing with the result with the one in previous section.\n","\n","Now we need to display a few erros of our LLM judge to analyze them:"],"metadata":{"id":"l-hJ92oorreg"}},{"cell_type":"code","source":["errors = pd.concat([\n","    examples.loc[examples['llm_judge_improved_score'] > examples['human_score']].head(1),\n","    examples.loc[examples['llm_judge_improved_score'] < examples['human_score']].head(2),\n","])\n","\n","display(\n","    errors[\n","        ['question', 'answer', 'human_score', 'explanation_1', 'llm_judge_improved_score', 'llm_judge_improved']\n","    ]\n",")"],"metadata":{"id":"XiueXm0UrwX0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## How do we take our LLM judge even further?"],"metadata":{"id":"ljpuFc_usOOq"}},{"cell_type":"markdown","source":["Our human ground truth certainly has some noise, so agreement/correlation will never go up to 100% even with a perfect LLM judge.\n","\n","If we have access to a reference answer for each question, we should definitely give this to the LLM judge in its prompt to get better results.\n","\n","We can add some few-shot examples of questions and ground truth evaluations in the prompt to improve the results.\n","\n","When the judgement can be split into atomic criteria, we can also use an additive scale to further improve results:"],"metadata":{"id":"vNhng9nqsRJw"}},{"cell_type":"code","source":["IMPROVED_JUDGE_PROMPT = \"\"\"\n","You will be given a user_question and system_answer couple.\n","Your task is to provide a 'total rating' scoring how well the system_answer answers the user concerns expressed in the user_question.\n","Give your answer on a scale of 1 to 4, where 1 means that the system_answer is not helpful at all, and 4 means that the system_answer completely and helpfully addresses the user_question.\n","\n","Here is the scale you should use to build your answer:\n","1: The system_answer is terrible: completely irrelevant to the question asked, or very partial\n","2: The system_answer is mostly not helpful: misses some key aspects of the question\n","3: The system_answer is mostly helpful: provides support, but still could be improved\n","4: The system_answer is excellent: relevant, direct, detailed, and addresses all the concerns raised in the question\n","\n","- Award 1 point if the answer is related to the question.\n","- Give 1 additional point if the answer is clear and precise.\n","- Provide 1 further point if the answer is true.\n","- One final point should be awarded if the answer provides additional resources to support the user.\n","\n","Provide your feedback as follows:\n","\n","Feedback:::\n","Evaluation: (your rationale for the rating, as a text)\n","Total rating: (your rating, as a number between 1 and 4)\n","\n","You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n","\n","Now here are the question and answer.\n","\n","Question: {question}\n","Answer: {answer}\n","\n","Provide your feedback. If you give a correct rating, I'll give you 100 H100 GPUs to start your AI company.\n","Feedback:::\n","Evaluation: \"\"\""],"metadata":{"id":"RZJDRdoFsQn_"},"execution_count":null,"outputs":[]}]}