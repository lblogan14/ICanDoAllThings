{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPsKsrEm4yWVph85Y0x55Yy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -qU transformers datasets accelerate peft trl bitsandbytes wandb --progress-bar off"],"metadata":{"id":"VLWjtbUH_5bG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fine-tune Llama 3 with ORPO"],"metadata":{"id":"Qan4PQVI_y37"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SOuiukIs_tbw"},"outputs":[],"source":["import os\n","import gc\n","import torch\n","import wandb\n","from datasets import load_dataset\n","from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n","from transformers import AUtoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline\n","from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n","from google.colab import userdata\n","\n","# model\n","base_model = 'meta-llama/Meta-Llama-3-8B'\n","new_model = 'OrpoLlama-3-8B'\n","\n","# setups\n","wb_token = userdata.get('WB_TOKEN')\n","wandb.login(key=wb_token)\n","\n","# set torch dtype and attention implementation\n","if torch.cuda.get_device_capability()[0] >= 8:\n","    !pip install -qU flash-attn\n","    torch_dtype = torch.bfloat16\n","    attn_implementation = 'flash_attention_2'\n","else:\n","    torch_dtype = torch.float16\n","    attn_implementation = 'eager'"]},{"cell_type":"code","source":["# QLoRA config\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type='nf4',\n","    bnb_4bit_compute_dtype=torch_dtype,\n","    bnb_4bit_use_double_quant=True\n",")\n","\n","# LoRA config\n","peft_config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    lora_dropout=0.05,\n","    bias='none',\n","    task_type='CAUSAL_LM',\n","    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'down_proj', 'gate_proj']\n",")\n","\n","# load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(base_model)\n","# load model\n","model = AutoModelForCausalLM.from_pretrained(\n","    base_model,\n","    quantization_config=bnb_config,\n","    device_map='auto',\n","    attn_implementation=attn_implementation,\n",")\n","\n","model, tokenizer = setup_chat_format(model, tokenizer)\n","model = prepare_model_for_kbit_training(model)"],"metadata":{"id":"ze6EjVmGCv5V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load dataset"],"metadata":{"id":"jf3yewKVFcms"}},{"cell_type":"code","source":["# load dataset\n","dataset_name = 'mlabonne/orpo-dpo-mix-40k'\n","dataset = load_dataset(dataset_name, split='all')\n","dataset = dataset.shuffle(seed=111).select(range(1000)) # only use 1000 samples for demo purpose\n","\n","def format_chat_template(row):\n","    row['chosen'] = tokenizer.apply_chat_template(row['chosen'], tokenize=False)\n","    row['rejected'] = tokenizer.apply_chat_template(row['rejected'], tokenize=False)\n","    return row\n","\n","\n","dataset = dataset.map(\n","    format_chat_template,\n","    num_proc=os.cpu_count()\n",")\n","dataset = dataset.train_test_split(test_size=0.01)"],"metadata":{"id":"N2tUiI5iDh5E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train ORPO"],"metadata":{"id":"9XiN-CAcFeqR"}},{"cell_type":"code","source":["# ORPO config\n","orpo_args = ORPOConfig(\n","    learning_rate=8e-6,\n","    lr_scheduler_type='linear',\n","    max_length=1024,\n","    max_prompt_length=512,\n","    beta=0.1,\n","    per_device_train_batch_size=2,\n","    per_device_eval_batch_size=2,\n","    gradient_accumulation_steps=4,\n","    optim='paged_adamw_8bit',\n","    num_train_epochs=1,\n","    evaluation_strategy='steps',\n","    eval_steps=0.2, # 20% of the total trianing steps\n","    logging_steps=1,\n","    warmup_steps=10,\n","    report_to='wandb',\n","    output_dir='./results/'\n",")\n","\n","trainer = ORPOTrainer(\n","    model,\n","    args=orpo_args,\n","    train_dataset=dataset['train'],\n","    eval_dataset=dataset['test'],\n","    peft_config=peft_config,\n","    tokenizer=tokenizer\n",")"],"metadata":{"id":"rkGG3ATgECx0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trianer.train()\n","trainer.save_model(new_model)"],"metadata":{"id":"WPfe9mztFGvM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Inference"],"metadata":{"id":"1VeyJk9nFgdl"}},{"cell_type":"code","source":["# flush memory\n","del trainer, model\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"id":"LiJrIYNcFKM-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# reload tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(base_model)\n","fp16_model = AutoModelForCausalLM.from_pretrained(\n","    base_model,\n","    low_cpu_mem_usage=True,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    device_map='auto',\n",")\n","fp16_model, tokenizer = setup_chat_format(fp16_model, tokenizer)"],"metadata":{"id":"TsFjkMVEFY1S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merge adapter with base model\n","model = PeftModel.from_pretrained(fp16_model, new_model)\n","model = model.merge_and_unload()"],"metadata":{"id":"4XiVRTMVGKnc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.push_to_hub(new_model, use_temp_dir=False)\n","tokenizer.push_to_hub(new_model, use_temp_dir=False)"],"metadata":{"id":"pWqg-zX2GMig"},"execution_count":null,"outputs":[]}]}