{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMJJYnD8Ynth4tTySsw7le+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"kUmGZrJ9KbiU"},"outputs":[],"source":["!pip install -qU transformers accelerate flash_attn"]},{"cell_type":"markdown","source":["# Video-Text-to-Text"],"metadata":{"id":"zQP5OiJMKhvn"}},{"cell_type":"markdown","source":["**Video-text-to-text** models, also known as **video language models** or **vision language models with video input**, are language models that take a video input.\n","\n","These models have nearly the same architecture as **image-text-to-text** models except for some changes to accept video data, since video data is essentially image frames with temporal dependencies.\n","\n","Video-text-to-text models are often trained with all vision modalities. Each example may have videos, multiple videos, images, and multiple images. Some of these models can also take interleaved inputs.\n","\n","Types of video LMs:\n","* base models used for fine-tuning\n","* chat fine-tuned models for convservation\n","* instruction fine-tuned models\n","\n","We will focus on inference with an instruction-tuned model, `llava-hf/llava-interleave-qwen-7b-hf`, which can take in interleaved data.\n","\n","The term **\"interleave\"** in the model `llalva-interleave-qwen-7b-hf` refers to how the model integrates different types of tokens—typically visual (from an image encoder) and textual (language tokens)—by mixing them together within a single sequence.\n","\n","Instead of simply appending image tokens before or after the text, the model alternates (or “interleaves”) them. This design allows the model to align and fuse visual and textual information more closely. The interleaving can help the transformer capture fine-grained relationships between the image content and the corresponding text, potentially improving its ability to understand and generate responses based on multi-modal inputs.\n","\n","In essence, the \"interleave\" aspect denotes that the processing of multi-modal information is done in an integrated fashion, rather than handling each modality in isolation. This method has been adopted in several multi-modal models (like LLaVA) to enhance the synergy between visual cues and language understanding."],"metadata":{"id":"-0gF76hTKju4"}},{"cell_type":"code","source":["from transformers import LlavaProcessor, LlavaForConditionalGeneration\n","import torch\n","\n","model_id = 'llava-hf/llava-interleave-qwen-7b-hf'\n","\n","processor = LlavaProcessor.from_pretrained(model_id)\n","model = LlavaForConditionalGeneration.from_pretrained(\n","    model_id,\n","    torch_dtype=torch.float16\n",").to('cuda')"],"metadata":{"id":"6rEvd-b6KjQm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Some models directly consume the `<video>` token, and others accept `<image>` tokens equal to the number of sampled frames. This model handles videos in the latter use case:"],"metadata":{"id":"0d7LmcWTMgCb"}},{"cell_type":"code","source":["import uuid\n","import requests\n","import cv2\n","from PIL import Image\n","\n","\n","def replace_video_with_images(text, frames):\n","    return text.replace(\n","        '<video>'\n","        '<image>' * frames\n","    )\n","\n","\n","def sample_frames(url, num_frames):\n","    response = requests.get(url)\n","    path_id = str(uuid.uuid4())\n","\n","    path = f\"./{path_id}.mp4\"\n","\n","    with open(path, 'wb') as f:\n","        f.write(response.content)\n","\n","    video = cv2.VideoCapture(path)\n","    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n","    interval = total_frames // num_frames\n","\n","    frames = []\n","    for i in range(total_frames):\n","        ret, frame = video.read()\n","        pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n","        if not ret:\n","            continue\n","        if i % interval == 0:\n","            frames.append(pil_img)\n","\n","    video.release()\n","\n","    return frames[:num_frames]"],"metadata":{"id":"Bco1o7KFN_Nb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prepare some inputs\n","video_1 = \"https://huggingface.co/spaces/merve/llava-interleave/resolve/main/cats_1.mp4\"\n","video_2 = \"https://huggingface.co/spaces/merve/llava-interleave/resolve/main/cats_2.mp4\"\n","\n","video_1 = sample_frames(video_1, 6)\n","video_2 = sample_frames(video_2, 6)\n","\n","videos = video_1 + video_2\n","\n","videos"],"metadata":{"id":"XJkLGpEhPzGP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can preprocess the inputs.\n","\n","This model has a prompt template:\n","1. Put all the sampled frames into one list.\n","2. Add `assistant` at the end of the prompt to trigger the model to give answer.\n","3. Preprocess the prompt."],"metadata":{"id":"VGpXG7mbP3qc"}},{"cell_type":"code","source":["user_prompt = \"Are these two cats in these two videos doing the same thing?\"\n","\n","tokens = '<image>' * 12 # 8 frames in each video, w only insert 12 <image> tokens\n","prompt = \"<|im_start|>user\" + tokens + f\"\\n{user_prompt}<|im_end|><|im_start|>assistant\"\n","\n","inputs = processor(\n","    text=prompt,\n","    images=videos,\n","    return_tensors='pt'\n",").to(model.device, model.dtype)"],"metadata":{"id":"CEk799-bQJno"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can now `generate()` for inference:"],"metadata":{"id":"7IUj9hBhQlRI"}},{"cell_type":"code","source":["output = model.generate(\n","    **inputs,\n","    max_new_tokens=100,\n","    do_sample=False\n",")\n","\n","processor.decode(\n","    output[0][2:],\n","    skip_special_tokens=True\n",")[len(user_prompt)+10:]"],"metadata":{"id":"wTbzXAhlQvBn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model outputs the question in our input and answer, so we only take the text after the prompt and `assistant` part from the model output."],"metadata":{"id":"E2RnSALWQ4mB"}},{"cell_type":"code","source":[],"metadata":{"id":"-VzeM5rWQ-fU"},"execution_count":null,"outputs":[]}]}