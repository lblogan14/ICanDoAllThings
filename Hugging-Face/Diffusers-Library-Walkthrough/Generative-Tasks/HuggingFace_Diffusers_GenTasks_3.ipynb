{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6SgT_l0p2y8"
   },
   "outputs": [],
   "source": [
    "!pip install -qU diffusers accelerate transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGYrAY6fp5WS"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FT97w3-NqrTR"
   },
   "source": [
    "# Image-to-Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pgn9pNO0qvxQ"
   },
   "source": [
    "In addition o a text prompt, we can also pass an initial image as a starting point for the diffusion process. The initial image is encoded to latent space and noise is added to it.\n",
    "\n",
    "Then the latent diffusion model takes a prompt and the noisy latent image, predicts the added noise, and removes the predicted noise from the initial latent image to get the new latent image. Finally, a decoder decodes the new latent image back into an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gESG8tdXrKqZ"
   },
   "source": [
    "1. Load a checkpoint into the `AutoPipelineForImage2Image` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJYPY12jqsrh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
    "    'kandinsky-community/kandinsky-2-2-decoder',\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "\n",
    "pipeline.enable_model_cpu_offload()\n",
    "# Remove following line if xFormers is not installed or we have PyTorch 2.0 or higher installed\n",
    "pipeline.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nRMmoxPs2KZ"
   },
   "source": [
    "If we are using PyTorch 2.0, then we do not need to call `enable_xformers_memory_efficient_attention()` on our pipeline because it will already be using PyTorch 2.0 native `scaled_dot_product_attention`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSU2hy7ntChj"
   },
   "source": [
    "2. Load an image to pass to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-R41UFztB9y"
   },
   "outputs": [],
   "source": [
    "init_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfCK35nxtJU6"
   },
   "source": [
    "3. Pass a prompt and image to the pipeline to generate an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FgqXnhytMGf"
   },
   "outputs": [],
   "source": [
    "prompt = \"cat wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney, 8k\"\n",
    "\n",
    "image = pipeline(\n",
    "    prompt,\n",
    "    image=init_image,\n",
    ").images[0]\n",
    "\n",
    "make_image_grid([init_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpjbaDtvtzxT"
   },
   "source": [
    "## Popular models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrFe0rQTt2jW"
   },
   "source": [
    "### Stable Diffusion v1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4A99Vt6Ct1L3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
    "    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n",
    "    torch_dtype=torch.float16,\n",
    "    variant='fp16',\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()\n",
    "pipeline.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKFXTfZ7A8vE"
   },
   "outputs": [],
   "source": [
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\n",
    "init_image = load_image(url)\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "\n",
    "image = pipeline(\n",
    "    prompt,\n",
    "    image=init_image,\n",
    ").images[0]\n",
    "make_image_grid([init_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frSmqeGWBIcR"
   },
   "source": [
    "### Stable Diffusion XL (SDXL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMbSwCFYBT_z"
   },
   "source": [
    "SDXL uses a larger base model, and an additional refiner model to increase the quality of the base model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfHi4m4DBLx5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers.utils import make_image_grid, load_image\n",
    "\n",
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
    "    'stabilityai/stable-diffusion-xl-refiner-1.0',\n",
    "    torch_dtype=torch.float16,\n",
    "    variant='fp16',\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()\n",
    "pipeline.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxjHwgbDBju-"
   },
   "outputs": [],
   "source": [
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-sdxl-init.png\"\n",
    "init_image = load_image(url)\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "\n",
    "image = pipeline(\n",
    "    prompt,\n",
    "    image=init_image,\n",
    "    strength=0.5\n",
    ").images[0]\n",
    "\n",
    "make_image_grid([init_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYYSGfPCBsYn"
   },
   "source": [
    "### Kandinsky 2.2\n",
    "\n",
    "The Kandinsky model is different from the Stable Diffusion models because it uses an image prior model to create image embeddings. The embeddings help create a better alignment between text and images, allowing the latent diffusion model to generate better images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHbGdh0lBuIT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers.utils import make_image_grid, load_image\n",
    "\n",
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
    "    'kandinsky-community/kandinsky-2-2-decoder',\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()\n",
    "pipeline.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKC_SnMhCA44"
   },
   "outputs": [],
   "source": [
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\n",
    "init_image = load_image(url)\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "\n",
    "image = pipeline(\n",
    "    prompt,\n",
    "    image=init_image,\n",
    ").images[0]\n",
    "make_image_grid([init_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpKMVA8MCHYy"
   },
   "source": [
    "## Configure pipeline parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tb98PFAvCKRQ"
   },
   "source": [
    "### Strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fkp464bFCMPA"
   },
   "source": [
    "`strength` has a huge impact on our generated image. It determines how much the generated image resembles the initial image.\n",
    "* a higher `strength` value gives the model more \"creativity\" to generate an image that is different from the initial image; a `strength` value of 1.0 means the initial image is more or less ignored.\n",
    "* a lower `strength` value means the generated image is more similar to the initial image.\n",
    "\n",
    "The `strength` determines the number of noise steps to add. If the `num_inference_steps` is 50 and `strength` is 0.8, then this means adding 40 (50 * 0.8) steps of noise to the initial image and then denoising for 40 steps to get the newly generated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pdyH3j7FCJWC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
    "    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n",
    "    torch_dtype=torch.float16,\n",
    "    variant='fp16',\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()\n",
    "pipeline.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4N7I8K5rC5v2"
   },
   "outputs": [],
   "source": [
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\n",
    "init_image = load_image(url)\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "\n",
    "image = pipeline(\n",
    "    prompt,\n",
    "    image=init_image,\n",
    "    strength=0.8,\n",
    ").images[0]\n",
    "make_image_grid([init_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-w61zalDDxf"
   },
   "source": [
    "### Guidance scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZG6WRoVDGV_"
   },
   "source": [
    "The `guidance_scale` is used to control how closely aligned the generated image and the text prompt are.\n",
    "* a higher `guidance_scale` value means our generated image is more aligned with the prompt,\n",
    "* a lower `guidance_scale` value means our generated image has more space to deviate from the prompt.\n",
    "\n",
    "We can combine `guidance_scale` with `strength` for even more precise control over how expressive the model is.\n",
    "* a high `strength + guidance_scale` for maximum creativity\n",
    "* a low `strength + guidance_scale` to generate an image that resembles the initial image but is not as strictly bound to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXwoSRpuDE8D"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
    "    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n",
    "    torch_dtype=torch.float16,\n",
    "    variant='fp16',\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()\n",
    "pipeline.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5AwSusNGn0Q"
   },
   "outputs": [],
   "source": [
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\n",
    "init_image = load_image(url)\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "\n",
    "image = pipeline(\n",
    "    prompt,\n",
    "    image=init_image,\n",
    "    guidance_scale=8.0,\n",
    ").images[0]\n",
    "make_image_grid([init_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JQGKf9nHIFQ"
   },
   "outputs": [],
   "source": [
    "image = pipeline(\n",
    "    prompt,\n",
    "    image=init_image,\n",
    "    guidance_scale=0.1,\n",
    ").images[0]\n",
    "make_image_grid([init_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5plHBEESHPcW"
   },
   "outputs": [],
   "source": [
    "image = pipeline(\n",
    "    prompt,\n",
    "    image=init_image,\n",
    "    guidance_scale=5.0,\n",
    ").images[0]\n",
    "make_image_grid([init_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyU-NYE2HRAB"
   },
   "outputs": [],
   "source": [
    "image = pipeline(\n",
    "    prompt,\n",
    "    image=init_image,\n",
    "    guidance_scale=10.0,\n",
    ").images[0]\n",
    "make_image_grid([init_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgLRx_D6HSOt"
   },
   "outputs": [],
   "source": [
    "image = pipeline(\n",
    "    prompt,\n",
    "    image=init_image,\n",
    "    guidance_scale=8.0,\n",
    "    strength=0.5,\n",
    ").images[0]\n",
    "make_image_grid([init_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-HnXmMCHUp6"
   },
   "outputs": [],
   "source": [
    "image = pipeline(\n",
    "    prompt,\n",
    "    image=init_image,\n",
    "    guidance_scale=8.0,\n",
    "    strength=0.1,\n",
    ").images[0]\n",
    "make_image_grid([init_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSmF4th_HVo-"
   },
   "outputs": [],
   "source": [
    "image = pipeline(\n",
    "    prompt,\n",
    "    image=init_image,\n",
    "    guidance_scale=8.0,\n",
    "    strength=0.8,\n",
    ").images[0]\n",
    "make_image_grid([init_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyWovMhOHrLo"
   },
   "source": [
    "### Negative prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfXTqE2gHsft"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers.utils import make_image_grid, load_image\n",
    "\n",
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
    "    'stabilityai/stable-diffusion-xl-refiner-1.0',\n",
    "    torch_dtype=torch.float16,\n",
    "    variant='fp16',\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()\n",
    "pipeline.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUwGRW9GHxIl"
   },
   "outputs": [],
   "source": [
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\n",
    "init_image = load_image(url)\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "negative_prompt = \"ugly, deformed, disfigured, poor details, bad anatomy\"\n",
    "\n",
    "image = pipeline(\n",
    "    prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    image=init_image,\n",
    ").images[0]\n",
    "make_image_grid([init_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6iYG2GXIQvJ"
   },
   "source": [
    "## Chained image-to-image pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KYS9TxxIUL8"
   },
   "source": [
    "### Text-to-image-to-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUgi5ACgIY8B"
   },
   "source": [
    "Chaining a text-to-image and image-to-image pipeline allows us to generate an image from text and use the generated image as the initial image for the image-to-image pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBdAD2I4ITJl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForImage2Image, AutoPipelineForText2Image\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "pipeline = AutoPipelineForText2Image.from_pretrained(\n",
    "    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n",
    "    torch_dtype=torch.float16,\n",
    "    variant='fp16',\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()\n",
    "pipeline.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ld45hdlsJGSt"
   },
   "outputs": [],
   "source": [
    "text2image = pipeline(\n",
    "    \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    ").images[0]\n",
    "text2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O39kPVNyJLxS"
   },
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
    "    'kandinsky-community/kandinsky-2-2-decoder',\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()\n",
    "pipeline.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDzhO9cnJlY6"
   },
   "outputs": [],
   "source": [
    "image2image = pipeline(\n",
    "    \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\",\n",
    "    image=text2image,\n",
    ")\n",
    "make_image_grid([text2image, image2image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaHvyJSRKBd0"
   },
   "source": [
    "### Image-to-image-to-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lu1Q7S57KDNE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
    "    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n",
    "    torch_dtype=torch.float16,\n",
    "    variant='fp16',\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()\n",
    "pipeline.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HcwevEtd9pC"
   },
   "outputs": [],
   "source": [
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\n",
    "init_image = load_image(url)\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "\n",
    "image = pipeline(\n",
    "    prompt,\n",
    "    image=init_image,\n",
    "    output_type='latent',\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dt_cKmEeE0s"
   },
   "source": [
    "It is important to specify `output_type='latent'` in the pipeline to keep all the outputs in latent space to avoid an unnecessary decode-encode step. This only works if the chained pipelines are using the same VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7pdqWYzeRkU"
   },
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
    "    'ogkalu/Comic-Diffusion', # comic book art style\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()\n",
    "pipeline.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LA0Xm5GCeaCD"
   },
   "outputs": [],
   "source": [
    "# need to include the token `charliebo artstyle` in the prompt to use this checkpoint\n",
    "image = pipeline(\n",
    "    'Astronaut in a jungle, charliebo artstyle',\n",
    "    image=image,\n",
    "    output_type='latent',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUQMS5qTggsa"
   },
   "outputs": [],
   "source": [
    "# repeat one more time to generate the final image in a `pixel_artstyle`\n",
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
    "    'kohbanye/pixel-art-style',\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()\n",
    "pipeline.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLWHYIJoguID"
   },
   "outputs": [],
   "source": [
    "# need to include the token `pixelartstyle` in the prompt to use this checkpoint\n",
    "image = pipeline(\n",
    "    'Astronaut in a jungle, pixelartstyle',\n",
    "    image=image,\n",
    ").images[0]\n",
    "\n",
    "make_image_grid([init_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPkeeBA1g-pH"
   },
   "source": [
    "### Image-to-upscaler-to-super-resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljeO9aGJg56a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
    "    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n",
    "    torch_dtype=torch.float16,\n",
    "    variant='fp16',\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()\n",
    "pipeline.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCl7mgiAhGYI"
   },
   "outputs": [],
   "source": [
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\n",
    "init_image = load_image(url)\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "\n",
    "image_1 = pipeline(\n",
    "    prompt,\n",
    "    image=init_image,\n",
    "    output_type='latent',\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEdIr5CuhPeV"
   },
   "source": [
    "Make sure the `output_type='latent'` in the pipeline.\n",
    "\n",
    "Chain it to an upscaler pipeline to increase the image resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehnkPWt-hTlP"
   },
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionLatentUpscalePipeline\n",
    "\n",
    "upscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(\n",
    "    'stabilityai/sd-x2-latent-upscaler',\n",
    "    torch_dtype=torch.float16,\n",
    "    variant='fp16',\n",
    "    use_safetensors=True,\n",
    ")\n",
    "upscaler.enable_model_cpu_offload()\n",
    "upscaler.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ag0lHmXLhn6J"
   },
   "outputs": [],
   "source": [
    "image_2 = upscaler(\n",
    "    prompt,\n",
    "    image=image_1,\n",
    "    output_type='latent',\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ltcug7whu2I"
   },
   "source": [
    "Finally, chain it to a super-resolution pipeline to further enhance the resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDrNmmbphymP"
   },
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionUpscalePipeline\n",
    "\n",
    "super_res = StableDiffusionUpscalePipeline.from_pretrained(\n",
    "    'stabilityai/stable-diffusion-x4-upscaler',\n",
    "    torch_dtype=torch.float16,\n",
    "    variant='fp16',\n",
    "    use_safetensors=True,\n",
    ")\n",
    "super_res.enable_model_cpu_offload()\n",
    "super_res.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Ach5RKiiDGs"
   },
   "outputs": [],
   "source": [
    "image_3 = super_res(\n",
    "    prompt,\n",
    "    image=image_2,\n",
    ").images[0]\n",
    "\n",
    "make_image_grid(\n",
    "    [init_image, image_3.resize((512, 512))],\n",
    "    rows=1,\n",
    "    cols=2,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM+XZ/IqNuvmqSmIfbd6OQA",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
