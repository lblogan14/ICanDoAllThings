{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPFS542Hxzz794fIglL9EDn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Command Line Interfaces (CLIs)"],"metadata":{"id":"MsPXcCJ9FTqH"}},{"cell_type":"markdown","source":["We an use TRL to fine-tune our language model with **Supervised Fine-Tuning (SFT)** or **Direct Policy Optimization (DPO)** or even chat with our model using the TRL CLIs."],"metadata":{"id":"C2wRk1oRFXX6"}},{"cell_type":"markdown","source":["**Training commands**\n","* `trl dpo`: fine-tune an LLM with DPO\n","* `trl grpo`: fine-tune an LLM with GRPO\n","* `trl kto`: fine-tune an LLM with KTO\n","* `trl sft`: fine-tune an LLM with SFT\n","\n","**Other commands**\n","* `trl chat`: quickly spin up an LLM fine-tuned for chatting\n","* `trl env`: get the system information"],"metadata":{"id":"R1iSjGNPFlT-"}},{"cell_type":"markdown","source":["## Fine-tuning with the CLI"],"metadata":{"id":"tfQzrMSiF7JI"}},{"cell_type":"markdown","source":["We need to pick up a language model for text generation and a relevant dataset from HuggingFace Hub.\n","\n","Before using the `sft` or `dpo` commands, we need to run:\n","```bash\n","accelerate config\n","```\n","and pick up the right configuration for our training setup (single / multi-GPU, DeepSpeed, etc.). Make sure to complete all steps of `accelerate config` before running any CLI command.\n","\n","We also recommend passing a YAML config file to configure our training protocol. Below is a simple example of a YAML file that we can use for training our models with `trl sft` command:\n","```yaml\n","model_name_or_path: Qwen/Qwen2.5-0.5B\n","dataset_name: stanfordnlp/imdb\n","report_to: none\n","learning_rate: 0.0001\n","lr_scheduler_type: cosine\n","```\n","Save this file as `config.yaml` and we can get started immediately. We can also overwrite the arguments from the config file by explicitly passing them to the CLI, e.g., from the root folder:\n","```bash\n","trl sft --config path/to/configs/config.yaml --output_dir test-trl-cli --lr_scheduler_type cosine_with_restarts\n","```\n","which will force to use `cosine_with_restarts` for `lr_scheduler_type`."],"metadata":{"id":"PkDm00f3OVcZ"}},{"cell_type":"markdown","source":["### Supported Arguments"],"metadata":{"id":"KZmZL8awPqX1"}},{"cell_type":"markdown","source":["We can also set arguments from `transformers.TrainingArguments` for loading our model. This is all supported from the `trl.ModelConfig`."],"metadata":{"id":"XwhlsxkoPugg"}},{"cell_type":"markdown","source":["### Supervised Fine-Tuning (SFT)"],"metadata":{"id":"HEiwS_GnP-EM"}},{"cell_type":"markdown","source":["Follow the basic instructions above\n","```bash\n","trl stf --model_name_or_path facebook/opt-125m --dataset_name stanfordnlp/imdb --output_dir opt-sft-imdb\n","```\n","The SFT CLI is based on the `trl/scripts/sft.py` script."],"metadata":{"id":"htFDNevTQBVp"}},{"cell_type":"markdown","source":["### Direct Policy Optimization (DPO)"],"metadata":{"id":"PmwoMoL5SWmq"}},{"cell_type":"markdown","source":["To use the DPO CLI, we need to have a dataset in the TRL format such as\n","* TRL's [Anthropic HH dataset](https://huggingface.co/datasets/trl-internal-testing/hh-rlhf-helpful-base-trl-style)\n","* TRL's [OpenAI TLDR summarization dataset](https://huggingface.co/datasets/trl-internal-testing/tldr-preference-trl-style)\n","\n","These datasets always have at least three columns `prompt`, `chosen`, and `rejected`:\n","* `prompt` is a list of strings\n","* `chosen` is the chosen response in chat format\n","* `rejected` is the rejected response in chat format\n","\n","Follow the basic instruction\n","```bash\n","trl dpo --model_name_or_path facebook/opt-125m --output_dir trl-hh-rlhf --dataset_name trl-internal-testing/hh-rlhf-helpful-base-trl-style\n","```\n","The DPO CLI is based on the `trl/scripts/dpo.py` script."],"metadata":{"id":"ImQYUuBiScjg"}},{"cell_type":"markdown","source":["## Chat interface"],"metadata":{"id":"VPYY_N7JTQGL"}},{"cell_type":"markdown","source":["The chat CLI lets us quickly load the model and talk to it by running\n","```bash\n","trl chat --model_name_or_path Qwen/Qwen1.5-0.5B-Chat\n","```"],"metadata":{"id":"XxlXhtEMTSDy"}},{"cell_type":"markdown","source":["## Get the system information"],"metadata":{"id":"zxX06OE-TlQH"}},{"cell_type":"markdown","source":["We can get the system information by running\n","```bash\n","trl env\n","```\n","This will print out the system information including the GPU information, the CUDA version, the PyTorch version, the transformers version, and the TRL version, and any optional dependencies that are installed."],"metadata":{"id":"6ZNq8DwuTnac"}},{"cell_type":"markdown","source":["# Training Customization"],"metadata":{"id":"giyZOM5DT2rY"}},{"cell_type":"markdown","source":["## Train on multiple GPUs / nodes"],"metadata":{"id":"oF6PCOh9T7Rj"}},{"cell_type":"markdown","source":["The trainers in TRL use HuggingFace Accelerate to enalbe distributed training across multiple GPUs or nodes. To do so, we first need to create an Accelerate config file by running\n","```bash\n","accelerate config\n","```\n","and answering the questions according to our multi-gpu / multi-node setup. Then we can launch distributed training by running\n","```bash\n","accelerate launch our_script.py\n","```\n","We also provide config files in the [examples folder](https://github.com/huggingface/trl/tree/main/examples/accelerate_configs) that can be used as templates:\n","```bash\n","accelerate launch --config_file=examples/accelerate_configs/multi_gpu.yaml --num_processes {NUM_GPUS} path_to_script.py --all_arguments_of_the_script\n","```"],"metadata":{"id":"NExvGB05UBTQ"}},{"cell_type":"markdown","source":["### Distrubted training with DeepSpeed"],"metadata":{"id":"ouLKjXgxWZbZ"}},{"cell_type":"markdown","source":["All of the trainers in TRL can be run on multiple GPUs together with DeepSpeed ZeRO-{1,2,3} for efficient sharding of the optimizer states, gradients, and model weights:\n","```bash\n","accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero{1,2,3}.yaml --num_processes {NUM_GPUS} path_to_your_script.py --all_arguments_of_the_script\n","```\n","For ZeRO-3, a small tweak is needed to intialize our reward model on the correct device viad the `zero3_init_context_manager()` context manager. This is needed to avoid DeepSpeed hanging after a fixed number of training steps."],"metadata":{"id":"pvUblXhsWcWo"}},{"cell_type":"markdown","source":["## Use different optimizers and schedulers"],"metadata":{"id":"ZWIWLMr4XG4d"}},{"cell_type":"markdown","source":["By default, the `DPOTrainer` creates a `torch.optim.AdamW` optimizer. We can create and define a different optimizer and pass it to `DPOTrainer` as follows"],"metadata":{"id":"eJP2q_YZXJfB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"geIoH-ZMFIWl"},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from torch import optim\n","from trl import DPOConfig, DPOTrainer\n","\n","model_id = 'Qwen/Qwen2.5-0.5B-Instruct'\n","model = AutoModelForCausalLM.from_pretrained(model_id)\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","dataset = load_dataset('trl-lib/ultrafeedback_binarized', split='train')\n","training_args = DPOConfig(output_dir='Qwen2.5-0.5B-DPO')\n","\n","optimizer = optim.SGD(model.parameters(), lr=training_args.learning_rate)\n","\n","trainer = DPOTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset,\n","    tokenizer=tokenizer,\n","    optimizers=(optimizer, None), # no lr scheduler assigned\n",")\n","trainer.train()"]},{"cell_type":"markdown","source":["### Add a learning rate scheduler"],"metadata":{"id":"tG9qXX7cYBzN"}},{"cell_type":"markdown","source":["We can also play with our training by adding learning rate schedulers."],"metadata":{"id":"FKp0WuqLYERA"}},{"cell_type":"code","source":["from datasets import load_dataset\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from torch import optim\n","from trl import DPOConfig, DPOTrainer\n","\n","model_id = 'Qwen/Qwen2.5-0.5B-Instruct'\n","model = AutoModelForCausalLM.from_pretrained(model_id)\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","dataset = load_dataset('trl-lib/ultrafeedback_binarized', split='train')\n","training_args = DPOConfig(output_dir='Qwen2.5-0.5B-DPO')\n","\n","optimizer = optim.AdamW(model.parameters(), lr=training_args.learning_rate)\n","lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n","\n","trainer = DPOTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset,\n","    tokenizer=tokenizer,\n","    optimizers=(optimizer, lr_scheduler),\n",")\n","trainer.train()"],"metadata":{"id":"w6ZafxgEYD2T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Memory efficient fine-tuning by sharing layers"],"metadata":{"id":"jYkEAGhjZiUh"}},{"cell_type":"markdown","source":["Another tool we can use for more memory efficient fine-tuning is to share layers between the reference model and the model we want to train."],"metadata":{"id":"aVrp-MYMZlKB"}},{"cell_type":"code","source":["from datasets import load_dataset\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from trl import create_reference_model, DPOConfig, DPOTrainer\n","\n","model_id = 'Qwen/Qwen2.5-0.5B-Instruct'\n","model = AutoModelForCausalLM.from_pretrained(model_id)\n","ref_model = create_reference_model(model, num_shared_layers=6)\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","dataset = load_dataset('trl-lib/ultrafeedback_binarized', split='train[:1%]')\n","training_args = DPOConfig(output_dir='Qwen2.5-0.5B-DPO')\n","\n","trainer = DPOTrainer(\n","    model=model,\n","    ref_model=ref_model,\n","    args=training_args,\n","    train_dataset=dataset,\n","    tokenizer=tokenizer,\n",")\n","trainer.train()"],"metadata":{"id":"SEvdBrC2Zk0a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pass 8-bit reference models"],"metadata":{"id":"nSU1qbGkaJmp"}},{"cell_type":"markdown","source":["Since `trl` supports all keyword arguments when loading a model from `transformers` using `from_pretrained`, we can also leverage `load_in_8bit` from `transformers` for more memory efficient fine-tuning."],"metadata":{"id":"qhFhg5EXaL1R"}},{"cell_type":"code","source":["from datasets import load_dataset\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","from trl import DPOConfig, DPOTrainer\n","\n","model_id = 'Qwen/Qwen2.5-0.5B-Instruct'\n","model = AutoModelForCausalLM.from_pretrained(model_id)\n","quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n","ref_model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    quantization_config=quantization_config,\n",")\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","dataset = load_dataset('trl-lib/ultrafeedback_binarized', split='train')\n","training_args = DPOConfig(output_dir='Qwen2.5-0.5B-DPO')\n","\n","trainer = DPOTrainer(\n","    model=model,\n","    ref_model=ref_model,\n","    args=training_args,\n","    train_dataset=dataset,\n","    tokenizer=tokenizer,\n",")\n","trainer.train()"],"metadata":{"id":"VWhr_S4caLcO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Use the CUDA cache optimizer"],"metadata":{"id":"YleEIBoza5L3"}},{"cell_type":"markdown","source":["When training large models, we should better handle the CUDA cache by iteratively clearing it. To do so, we simply pass `optimize_cuda_cache=True` to `DPOConfig`:"],"metadata":{"id":"ZHcd7W5Wa7m5"}},{"cell_type":"code","source":["training_args = DPOConfig(\n","    output_dir=\"Qwen2.5-0.5B-DPO\",\n","    optimize_cuda_cache=True,\n",")"],"metadata":{"id":"Gn3W_s2Ta7Fq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Reducing Memory Usage"],"metadata":{"id":"yhA4bRvPbMCX"}},{"cell_type":"markdown","source":["not finished yet. continue watching on HuggingFace"],"metadata":{"id":"nnekQzuccjrr"}},{"cell_type":"markdown","source":["# Speeding Up Training"],"metadata":{"id":"JQXFSVuwcrj3"}},{"cell_type":"markdown","source":["not finished yet. continue watching on HuggingFace"],"metadata":{"id":"zN-IPzqocv1s"}},{"cell_type":"markdown","source":["# Using model after training"],"metadata":{"id":"__EDxpIvcwxy"}},{"cell_type":"markdown","source":["## Load and Generate"],"metadata":{"id":"K_Vy51lOc5ua"}},{"cell_type":"markdown","source":["If we have fine-tuned a model fully, meaning without the use of PEFT, we can simply load it like any other language model in `transformers`. The value head that was trained during the PPO training is no longer needed and it we load the model with the original transformer class it will be ignored."],"metadata":{"id":"qQ8DBB8PdZHI"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","model_name_or_path = \"kashif/stack-llama-2\" # path/to/our/model/or/name/on/hub\n","device = 'cpu'\n","\n","model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"],"metadata":{"id":"-3VxULrubN04"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = tokenizer.encode(\n","    'This movie was really',\n","    return_tensors='pt'\n",").to(device)\n","outputs = model.generate(inputs)\n","print(tokenizer.decode(outputs[0]))"],"metadata":{"id":"C47JOr-qd3ty"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Alternatively, we can also use the pipeline"],"metadata":{"id":"R-s4BLCgeCgs"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","model_name_or_path = 'kashif/stack-llama-2'\n","pipe = pipeline('text-generation', model=model_name_or_path)"],"metadata":{"id":"h0v-DXJ2eEzi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(pipe('This movie was really')[0]['generated_text'])"],"metadata":{"id":"dV16pLA1eNDU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Use PEFT Adapters"],"metadata":{"id":"xgWnNnEFeRmA"}},{"cell_type":"code","source":["from peft import PeftConfig, PeftModel\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","base_model_name = 'kashif/stack-llama-2'\n","adapter_model_name = 'path/to/my/adapter'\n","\n","model = AutoModelForCausalLM.from_pretrained(base_model_name)\n","model = PeftModel.from_pretrained(model, adapter_model_name)\n","tokenizer = AutoTokenizer.from_pretrained(base_model_name)"],"metadata":{"id":"cnw5G8CSeWG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = tokenizer.encode(\n","    'This movie was really',\n","    return_tensors='pt'\n",").to(device)\n","outputs = model.generate(inputs)\n","print(tokenizer.decode(outputs[0]))"],"metadata":{"id":"InMd11SyenvN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also merge the adapters into the base model so we can use the model like a normal transformers model:"],"metadata":{"id":"58kAg7dTeoZZ"}},{"cell_type":"code","source":["from peft import PeftConfig, PeftModel\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","base_model_name = 'kashif/stack-llama-2'\n","adapter_model_name = 'path/to/my/adapter'\n","\n","model = AutoModelForCausalLM.from_pretrained(base_model_name)\n","model = PeftModel.from_pretrained(model, adapter_model_name)\n","\n","model = model.merge_and_unload()\n","model.save_pretrained('merged_model')"],"metadata":{"id":"Fi7WR8eAetoF"},"execution_count":null,"outputs":[]}]}