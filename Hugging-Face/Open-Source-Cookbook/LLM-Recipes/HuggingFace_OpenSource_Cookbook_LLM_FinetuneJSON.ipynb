{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOxUF+VMkVjMBhmFRxEXdW1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Fine-tuning LLM to Generate Persian Product Catalogs in JSON Format"],"metadata":{"id":"34G8KKpbaOhY"}},{"cell_type":"markdown","source":["In this example, we will fine-tune a large language model with no added complexity. The model has been optimized for use on a customer-level GPU to generate Persian product catalogs and produce structured output in JSON format. It is effective for creating structured outputs from the unstructured titles and descriptions of products on e-commerce platform with user-generated content."],"metadata":{"id":"GNaFTuPpEkqp"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"3FaoKHqyFB7m"}},{"cell_type":"code","source":["!pip install -qU datasets bitsandbytes accelerate transformers peft trl vllm"],"metadata":{"id":"R7tgucehFRMu"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AvXgOcA8aIWW"},"outputs":[],"source":["import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForCausalLM,\n","    BitsAndBytesConfig,\n","    TrainingArguments\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"]},{"cell_type":"markdown","source":["## Set hyperparameters"],"metadata":{"id":"Jag3XW-nFX-7"}},{"cell_type":"code","source":["model_name = 'NousResearch/Llama-2-7b-chat-hf'\n","dataset_name = 'BaSalam/entity-attribute-dataset-GPT-3.5-generated-v1'\n","new_model = 'llama-persian-catalog-generator'"],"metadata":{"id":"aLg9Z2qSFZfL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LoRA parameters\n","lora_r = 64\n","lora_alpha = lora_r * 2\n","lora_dropout = 0.1\n","target_modules = ['q_proj', 'v_proj', 'k_proj']"],"metadata":{"id":"36sxmi1AFlX8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**LoRA (Low-Rank Adaptation)** stores changes in weights by constructing and adding a low-rank matrix to each model layer. This method opens only these layers for fine-tuning, without changing the original model weights or requiring lengthy training. The resulting weights are lightweight and can be produced multiple times, allowing for the fine-tuning of multiple tasks with an LLM loaded into RAM."],"metadata":{"id":"WYJ9bTqEFx1E"}},{"cell_type":"code","source":["# Q-LoRA parameters\n","load_in_4bit = True\n","bnb_4bit_compute_dtype = 'float16'\n","bnb_4bit_quant_type = 'nf4'\n","bnb_4bit_use_double_quant = False"],"metadata":{"id":"DZkW70AVF2SF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**QLoRA (Quantized Low-Rank Adaptation)** is an efficient fine-tuning approach that enables large language models to run on smaller GPUs by using 4-bit quantization. This method preserves the full performance of 16-bit fine-tuning while reducing memory usage, making it possible to fine-tune models with up to 65 billion parameters on a single 48GB GPU. QLoRA combines 4-bit NormalFloat data types, double quantization, and paged optimizers to manage memory efficiently. It allows fine-tuning of models with low-rank adapters, significantly enhancing accessibility for AI model development."],"metadata":{"id":"um1YGi8mF1QX"}},{"cell_type":"code","source":["# Training arguments\n","num_train_epochs = 1\n","fp16 = False\n","bf16 = False\n","per_device_train_batch_size = 4\n","gradient_accumulation_steps = 1\n","gradient_checkpointing = True\n","learning_rate = 0.00015\n","weight_decay = 0.01\n","optim = 'paged_adamw_32bit'\n","lr_scheduler_type = 'cosine'\n","max_steps = -1\n","warmup_ratio = 0.03\n","group_by_length = True\n","save_steps = 0\n","logging_steps = 25\n","\n","# SFT parameters\n","max_seq_pength = None\n","packing = False\n","device_map = {\"\": 0}\n","\n","# Dataset parameters\n","use_special_template = True\n","response_template = \" ### Answer:\"\n","instruction_prompt_template = '\"### Human:\"'\n","use_llama_like_model = True"],"metadata":{"id":"SrwqaNdKGGHz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model training"],"metadata":{"id":"IeqzVdBcGyo1"}},{"cell_type":"code","source":["# Load dataset\n","dataset = load_dataset(dataset_name, split='train')\n","\n","percent_of_train_dataset = 0.95\n","other_columns = [\n","    i for i in dataset.column_names if i not in ['instruction', 'output']\n","]\n","dataset = dataset.remove_columns(other_columns)\n","split_dataset = dataset.train_test_split(\n","    train_size=int(dataset.num_rows * percent_of_train_dataset),\n","    seed=111,\n","    shuffle=False\n",")\n","\n","train_dataset = split_dataset['train']\n","eval_dataset = split_dataset['test']\n","print(f\"Size of the train set: {len(train_dataset)}\")\n","print(f\"Size of the validation set: {len(eval_dataset)}\")"],"metadata":{"id":"vxibkdQbG0VE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load LoRA configuration\n","peft_config = LoraConfig(\n","    r=lora_r,\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    bias='none',\n","    task_type='CAUSAL_LM',\n","    target_modules=target_modules\n",")"],"metadata":{"id":"pSwas9qAHXEL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* `r`: The rank of the low-rank matrices used in LoRA. This parameter controls the dimensionality of the low-rank adaptation and directly impacts the model's capacity to adapt and the computational cost.\n","* `lora_alpha`: This parameter controls the scaling factor for the low-rank adaptation matrices. A higher alpha value can increase the model's capacity to learn new tasks.\n","* `lora_dropout`: The dropout rate for LoRA. This can help to prevent overfitting during fine-tuning. In this case, it's set to 0.1.\n","* `bias`: Specifies whether to add a bias term to the low-rank matrices. In this case, it's set to “none”, which means that no bias term will be added.\n","* `task_type`: Defines the type of task for which the model is being fine-tuned. Here, `“CAUSAL_LM”` indicates that the task is a causal language modeling task, which predicts the next word in a sequence.\n","* `target_modules`: Specifies the modules in the model to which LoRA will be applied. In this case, it's set to `[\"q_proj\", \"v_proj\", 'k_proj']`, which are the query, value, and key projection layers in the model's attention mechanism."],"metadata":{"id":"cPWKWNpxHiWn"}},{"cell_type":"code","source":["# Q-LoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=load_in_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=bnb_4bit_use_double_quant\n",")"],"metadata":{"id":"46s_BhmXH3Ad"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* `load_in_4bit`: A boolean that determines whether to load the model in 4-bit precision.\n","* `bnb_4bit_quant_type`: Specifies the type of 4-bit quantization to use. Here, it's set to 4-bit NormalFloat (NF4) quantization type, which is a new data type introduced in QLoRA. This type is information-theoretically optimal for normally distributed weights, providing an efficient way to quantize the model for fine-tuning.\n","* `bnb_4bit_compute_dtype`: Sets the data type used for computations involving the quantized model. In QLoRA, it's set to “float16”, which is commonly used for mixed-precision training to balance performance and precision.\n","* `bnb_4bit_use_double_quant`: This boolean parameter indicates whether to use double quantization. Setting it to False means that only single quantization will be used, which is typically faster but might be slightly less accurate.\n","\n","The reason why we have two data types (`quant_type` and `compute_type`) is that Q-LoRA employs two distinct data types: one for storing base model weights (in here 4-bit NormalFloat) and another for computational operations (16-bit). During the forward and backward passes, Q-LoRA dequantizes the weights from the storage format to the computational format. However, it only calculates gradients for the LoRA parameters, which utilize 16-bit bfloat. This approach ensures that weights are decompressed only when necessary, maintaining low memory usage throughout both training and inference phases."],"metadata":{"id":"8A-8eZVeIAsZ"}},{"cell_type":"code","source":["# Load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False"],"metadata":{"id":"fFgqmRZvH_0A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=new_model,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    gradient_checkpointing=gradient_checkpointing,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n",")"],"metadata":{"id":"q-5DcpT5ImFL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n","if not tokenizer.chat_template:\n","    tokenizer.chat_template = \"\"\"\n","        {% for message in messages %}\n","        {{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}\n","        {% endfor %}\"\"\""],"metadata":{"id":"GX4aqs8SInpc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def special_formatting_prompts(example):\n","    output_texts = []\n","    for i in range(len(example['instruction'])):\n","        text = f\"{instruction_prompt_template}{example['instruction'][i]}\\n{response_template} {example['output'][i]}\"\n","        output_texts.append(text)\n","\n","    return output_texts\n","\n","\n","def normal_formatting_prompts(example):\n","    output_texts = []\n","    for i in range(len(example['instruction'])):\n","        chat_temp = [\n","            {'role': 'system', 'content': example['instruction'][i]},\n","            {'role': 'assistant', 'content': example['output'][i]}\n","        ]\n","        text = tokenizer.apply_chat_template(chat_temp, tokenize=False)\n","        output_texts.append(text)\n","\n","    return output_texts"],"metadata":{"id":"bt6mcZaOLjZv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if use_special_template:\n","    formatting_func = special_formatting_prompts\n","\n","    if use_llama_like_model:\n","        response_template_ids = tokenizer.encode(\n","            response_template,\n","            add_special_tokens=False\n","        )[2:]\n","        collator = DataCollatorForCompletionOnlyLM(\n","            response_template=response_template_ids,\n","            tokenizer=tokenizer,\n","        )\n","    else:\n","        collator = DataCollatorForCompletionOnlyLM(\n","            response_template=response_template,\n","            tokenizer=tokenizer\n","        )\n","else:\n","    formatting_func = normal_formatting_prompts"],"metadata":{"id":"zpkJ97fkMofo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    peft_config=peft_config,\n","    formatting_func=formatting_func,\n","    data_collator=collator,\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")"],"metadata":{"id":"g0loJtVLNNUY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `SFTTrainer` is then instantiated to handle supervised fine-tuning (SFT) of the model.\n","* `formatting_func` - format training examples by combining instruction and response templates\n","* `packing` - disable packing multiple samples into one sequence"],"metadata":{"id":"fDsw2koTNQ81"}},{"cell_type":"code","source":["# Train model\n","trainer.train()\n","\n","# Save fine tuned Lora Adaptor\n","trainer.model.save_pretrained(new_model)"],"metadata":{"id":"Hvsvx32DNjNu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Inference"],"metadata":{"id":"-pwetYkfNmRf"}},{"cell_type":"code","source":["import torch\n","import gc\n","\n","\n","def clear_hardwares():\n","    torch.clear_autocast_cache()\n","    torch.cuda.ipc_collect()\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","\n","clear_hardwares()\n","clear_hardwares()"],"metadata":{"id":"HgfARNPjNo2L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate(model, prompt: str, kwargs):\n","    tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","    prompt_length = len(tokenized_prompt.get(\"input_ids\")[0])\n","\n","    with torch.cuda.amp.autocast():\n","        output_tokens = model.generate(**tokenized_prompt, **kwargs) if kwargs else model.generate(**tokenized_prompt)\n","        output = tokenizer.decode(output_tokens[0][prompt_length:], skip_special_tokens=True)\n","\n","    return output"],"metadata":{"id":"gJz9L1i7Nz_z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_model = AutoModelForCausalLM.from_pretrained(new_model, return_dict=True, device_map=\"auto\", token=\"\")\n","tokenizer = AutoTokenizer.from_pretrained(new_model, max_length=max_seq_length)\n","model = PeftModel.from_pretrained(base_model, new_model)\n","del base_model"],"metadata":{"id":"Ne-c5RuvN2la"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample = eval_dataset[0]\n","if use_special_template:\n","    prompt = f\"{instruction_prompt_template}{sample['instruction']}\\n{response_template}\"\n","else:\n","    chat_temp = [{\"role\": \"system\", \"content\": sample[\"instruction\"]}]\n","    prompt = tokenizer.apply_chat_template(chat_temp, tokenize=False, add_generation_prompt=True)"],"metadata":{"id":"jhNttylnN30O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gen_kwargs = {\"max_new_tokens\": 1024}\n","generated_texts = generate(model=model, prompt=prompt, kwargs=gen_kwargs)\n","print(generated_texts)"],"metadata":{"id":"8bKMBuP0N46y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Merge to base model"],"metadata":{"id":"D5WsWqDmN5px"}},{"cell_type":"code","source":["clear_hardwares()\n","merged_model = model.merge_and_unload()\n","clear_hardwares()\n","del model\n","adapter_model_name = \"<hf_account>/<desired_name>\"\n","merged_model.push_to_hub(adapter_model_name)"],"metadata":{"id":"-UNdjJc_N6pN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.push_to_hub(adapter_model_name)"],"metadata":{"id":"5XdomJt1OAF2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config = PeftConfig.from_pretrained(adapter_model_name)\n","model = AutoModelForCausalLM.from_pretrained(\n","    config.base_model_name_or_path,\n","    return_dict=True,\n","    load_in_8bit=True,\n","    device_map='auto'\n",")\n","tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n","\n","# Load the Lora model\n","model = PeftModel.from_pretrained(model, adapter_model_name)"],"metadata":{"id":"YcK_WOmWOCIX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fast inference with vllm"],"metadata":{"id":"oLpJNyEdOKEq"}},{"cell_type":"markdown","source":["The `vllm` library is one of the fastest inference engines for LLMs."],"metadata":{"id":"tG_uOl4sOL7W"}},{"cell_type":"code","source":["from vllm import LLM, SamplingParams\n","\n","prompt = \"\"\"### Question: here is a product title from a Iranian marketplace.\n","    Give me the Product Entity and Attributes of this product in Persian language.\n","    Give the output in this json format: {'attributes': {'attribute_name' : <attribute value>, ...}, 'product_entity': '<product entity>'}.\n","    Don't make assumptions about what values to plug into json. Just give Json not a single word more.\n","\n","    product title:\"\"\"\n","user_prompt_template = \"### Question: \"\n","response_template = \" ### Answer:\"\n","\n","llm = LLM(model=\"BaSalam/Llama2-7b-entity-attr-v1\", gpu_memory_utilization=0.9, trust_remote_code=True)\n","\n","product = \"مانتو اسپرت پانیذ قد جلوی کار حدودا 85 سانتی متر قد پشت کار حدودا 88 سانتی متر\"\n","sampling_params = SamplingParams(temperature=0.0, max_tokens=75)\n","prompt = f\"{user_prompt_template} {prompt}{product}\\n {response_template}\"\n","outputs = llm.generate(prompt, sampling_params)\n","\n","print(outputs[0].outputs[0].text)"],"metadata":{"id":"xtM50RAiOLgu"},"execution_count":null,"outputs":[]}]}