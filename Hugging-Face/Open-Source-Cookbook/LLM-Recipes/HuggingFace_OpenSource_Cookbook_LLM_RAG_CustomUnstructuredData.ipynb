{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyObDdALb5/kaN97s59lixxB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Building RAG with Custom Unstructured Data"],"metadata":{"id":"FO-ZTRp72cYQ"}},{"cell_type":"markdown","source":["Whether we are building our own RAG-based personal asistant, a pet project, or an enterprise RAG system, we will quickly discover that lots of important knowledge is stored in various formats like PDFs, emails, Markdown files, PPTs, HTML pages, Word documents, and so on.\n","\n","In this example, we will build a RAG system that incporates data from multiple data types. We will use the [`unstructured`](https://github.com/Unstructured-IO/unstructured) libray for data preprocessing, open-source models from HuggingFace Hub for embeddings and text generation, ChromaDB as a vector store, and LangChain for bringing everything together."],"metadata":{"id":"NQ1R6yiie9Wn"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"oDSd-R9yfo2P"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HYgw7UnN2U1I"},"outputs":[],"source":["!pip install -qU torch transformers accelerate bitsandbytes sentence-transformers unstructured[all-docs] langchain chromadb langchain_community"]},{"cell_type":"markdown","source":["Assuming that we want to build a RAG system that will help us manage pests in our garden. We will use diverse documents that cover the topic of integraed pest management (IPM): PDF, PPT, EPUB, HTML."],"metadata":{"id":"zTYKDHv4fu-B"}},{"cell_type":"code","source":["!mkdir -p \"./documents\"\n","!wget https://www.gov.nl.ca/ecc/files/env-protection-pesticides-business-manuals-applic-chapter7.pdf -O \"./documents/env-protection-pesticides-business-manuals-applic-chapter7.pdf\"\n","!wget https://ipm.ifas.ufl.edu/pdfs/Citrus_IPM_090913.pptx -O \"./documents/Citrus_IPM_090913.pptx\"\n","!wget https://www.gutenberg.org/ebooks/45957.epub3.images -O \"./documents/45957.epub\"\n","!wget https://blog.fifthroom.com/what-to-do-about-harmful-garden-and-plant-insects-and-pests.html -O \"./documents/what-to-do-about-harmful-garden-and-plant-insects-and-pests.html\""],"metadata":{"id":"Qz3yfVidgA-R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Unstructured data preprocessing"],"metadata":{"id":"BXGFlk_bgDRU"}},{"cell_type":"markdown","source":["We can use the `unstructured` library to preprocess documents one by one, and write our own script to walk through a directory, but it is easier to use a local source connector to ingest all documents in a given directory.\n","\n","The `unstructured` library can ingest documents from local directories, S3 buckets, blob storage, SFTP, and so on. In this example, we will use a local source connector. Optionally, we can also choose a destination connector for the processed documents - this could be MongDB, Pinecone, Weaviate, etc. Here, we will keep everything local."],"metadata":{"id":"gs0y4y2U3ITi"}},{"cell_type":"code","source":["import logging\n","\n","logger = logging.getLogger('unstructured.ingest')\n","logger.root.removeHandler(logger.root.hadlers[0])"],"metadata":{"id":"bdcGjHlMgE_s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import userdata\n","UNSTRUCTURED_API_KEY = userdata.get('UNSTRUCTURED_API_KEY')"],"metadata":{"id":"a_1rY8pw4rwm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","from unstructured.ingest.connector.local import SimpleLocalConfig\n","from unstructured.ingest.interfaces import PartitionConfig, ProcessorConfig, ReadConfig\n","from unstructured.ingest.runner import LocalRunner\n","\n","output_path = './local-ingest-output'\n","\n","runner = LocalRunner(\n","    processor_config=ProcessorConfig(\n","        verbose=True, # logs verbosity\n","        output_dir=output_path, # local directory to store outputs\n","        num_processes=2\n","    ),\n","    read_config=ReadConfig(),\n","    partition_config=PartitionConfig(\n","        partition_by_api=True,\n","        api_key=UNSTRUCTURED_API_KEY\n","    ),\n","    connector_config=SimpleLocalConfig(\n","        input_path='./documents',\n","        recursive=False, # get the documents recursively from given directory\n","    )\n",")\n","\n","runner.run()"],"metadata":{"id":"Zyvle7vD3zVQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* `ProcessorConfig` controls various aspects of the processing pipeline, including output locations, number of workers, error handling behavior, logging verbosity and more.\n","* `ReadConfig` customizes the data reading process for different scenarios, such as re-downloading data, preserving downloaded files, or limiting the number of documents processed.\n","* `PartitionConfig` determines if we partition the documents locally or via API. Here uses API which requries `UNSTRUCTURED_API_KEY`. If we remove these two parameters, the documents will be processed locally. If so, we may need to install `poppler` and `tesseract` if the documents require OCR and/or document understanding models.\n","* `SimpleLocalConfig` specifies where our original documents reside and whether we want to walk through the directory recursively."],"metadata":{"id":"J2LMZvfNVAT4"}},{"cell_type":"code","source":["from unstructured.staging.base import elements_from_json\n","\n","elements = []\n","\n","for filename in os.listdir(output_path):\n","    filepath = os.path.join(output_path, filename)\n","    elements.extend(elements_from_json(filepath))"],"metadata":{"id":"5uR-BiJ4WgDc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Chunking"],"metadata":{"id":"HTcO5bPxWqst"}},{"cell_type":"markdown","source":["The chunking methods in `unstructured` are slightly different from the chunking methods we used to apply, because the partitioning step has already divided an entire document into its structural elements: title, list items, tables, text, etc. By partitioning documents this way, we can avoid a situation where unrelated pieces of text end up in the same element, and then same chunk.\n","\n","When we chunk the document elements with `unstructured`, individual elements are already small so they will only be split if they exceed the desired maximum chunk size. We can also optionally choose to combine consecutive text elements such as list items, for example, that will together fit within chunk size limit."],"metadata":{"id":"xmLKel26Wu4U"}},{"cell_type":"code","source":["from unstructured.chunking.title import chunk_by_title\n","\n","chunked_elements = chunk_by_title(\n","    elements,\n","    max_characters=512, # max chunk size\n","    combine_text_under_n_chars=200, # combine consecutive elements that are too small\n",")"],"metadata":{"id":"M3UAsrLbXak2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now the chunks are ready for RAG. To use them with LangChain, we can convert `unstructured` elements to LangChain documents."],"metadata":{"id":"XkDEKe4mXngO"}},{"cell_type":"code","source":["from langchain_core.documents import Docuement\n","\n","documents = []\n","\n","for chunked_element in chunked_elements:\n","    metadata = chunked_element.metadata.to_dict()\n","    metadata['source'] = metadata['filename']\n","\n","    del metadata['languages']\n","\n","    documents.append(\n","        Document(\n","            page_content=chunked_element.text,\n","            metadata=metadata\n","        )\n","    )"],"metadata":{"id":"GUtjlMuUXtLg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Setting up the retriever"],"metadata":{"id":"ns-oEaeWX_wP"}},{"cell_type":"markdown","source":["We will use ChromaDB as a vector store and [`BAAI/bge-base-en-v1.5`](https://huggingface.co/BAAI/bge-base-en-v1.5) as an embeeding model."],"metadata":{"id":"guEKFQEQYCxQ"}},{"cell_type":"code","source":["from langchain_community.vectorstores import Chroma\n","from langchain.embeddings import HuggingFaceBgeEmbeddings\n","from langchain.vectorstores import utils as chromautils\n","\n","embedding_model = HuggingFaceBgeEmbeddings(model_name='BAAI/bge-base-en-v1.5')\n","\n","# ChromaDB does not support complex metadata, e.g., lists, so we drop it here.\n","docs = chromautils.filter_complex_metadata(documents)\n","vectorestore = Chroma.from_documents(docs, embedding_model)\n","retriever = vectorestore.as_retriever(\n","    search_type='similarity',\n","    search_kwargs={'k': 3}\n",")"],"metadata":{"id":"DW_XwAvwYA-5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## RAG with LangChain"],"metadata":{"id":"Bu9xGTUoYuNU"}},{"cell_type":"markdown","source":["In this example, we will use [`Llama-3-8B-Instruct`](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) with quantization."],"metadata":{"id":"LFiGA8oRYw_S"}},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","from langcahin.llms import HuggingFacePipeline\n","from langchain.chains import RetrievalQA\n","from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","import torch"],"metadata":{"id":"7IWyZjlqYvxd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type='nf4',\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config\n",")\n","\n","terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eos_id|>\")]"],"metadata":{"id":"ugvkrqijZJP4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_generation_pipeline = pipeline(\n","    model=model,\n","    tokenizer=tokenizer,\n","    task='text-generation',\n","    temperature=0.2,\n","    do_sample=True,\n","    repetition_penalty=1.1,\n","    return_full_text=False,\n","    max_new_tokens=256,\n","    eos_token_id=terminators\n",")\n","\n","llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"],"metadata":{"id":"b4yVqTvbZagx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt_template = \"\"\"\n","<|start_header_id|>user<|end_header_id|>\n","You are an assistant for answering questions using provided context.\n","You are given the extracted parts of a long document and a question. Provide a conversational answer.\n","If you don't know the answer, just say \"I do not know.\" Don't make up an answer.\n","Question: {question}\n","Context: {context}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\"\"\"\n","\n","prompt = PromptTemplate(\n","    input_variables=['context', 'question'],\n","    template=prompt_template\n",")\n","\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    retriever=retriever,\n","    chain_type_kwargs={'prompt': prompt}\n",")"],"metadata":{"id":"gQlVYmcfZuiu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Results"],"metadata":{"id":"jMS0EKLPaA3A"}},{"cell_type":"code","source":["question = 'Are aphids a pest?'\n","\n","qa_chain.invoke(question)['result']"],"metadata":{"id":"9myRwt0HaBhb"},"execution_count":null,"outputs":[]}]}