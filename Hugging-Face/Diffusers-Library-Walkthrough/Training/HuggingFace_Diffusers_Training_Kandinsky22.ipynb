{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPkD6uL55zufkduJ1sAUTnm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Kandinsky 2.2"],"metadata":{"id":"o2pMaXqc53F5"}},{"cell_type":"markdown","source":["**Kandinsky 2.2** is a multilingual text-to-image model capable of producing more photorealistic images. The model includes an image prior model for creating image embeddings from text prompts, and a decoder model that generates images based on the prior model's embeddings.\n","\n","In the Diffusers there are two separate scripts for Kandinsky 2.2, one for training the prior model and one for training the decoder model.\n","\n","We will explore the [`train_text_to_image_prior.py`](https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py) script to train the prior model and the [`train_text_to_image_decoder.py`](https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py) script to train the decoder.\n","\n","As always, make sure to install the `diffusers` library from source:\n","```bash\n","git clone https://github.com/huggingface/diffusers\n","cd diffusers\n","pip install .\n","```\n","\n","We need to navigate to the following folder and install the corresponding dependencies:\n","```bash\n","cd examples/kandinsky2_2/text_to_image\n","pip install -r requirements.txt\n","```\n","HuggingFace Accelerate is also helpful to train on multiple GPUs with mixed-precision.\n","```bash\n","pip install accelerate\n","```\n","\n","Now we can initialize a HuggingFace Accelerate environment\n","```bash\n","accelerate config\n","```\n","To set up a default Acclerate environment without choosing any configurations:\n","```bash\n","accelerate config default\n","```\n","Or if our environment does not support an interactive shell like a notebook, we can use:\n","```python\n","from accelerate.utils import write_basic_config\n","write_basic_config()\n","```"],"metadata":{"id":"hUxXUWZS55gd"}},{"cell_type":"markdown","source":["## Script parameters"],"metadata":{"id":"A1_Gx5YG7F2u"}},{"cell_type":"markdown","source":["The training script provides many parameters to customize the training run. We can find all of the parameters and their descriptions in the `parse_args()` function.\n","\n","To speed up training with mixed precision using the `fp16` format, add the `--mixed_precision` parameter to the training command:\n","```bash\n","accelerate launch train_text_to_image.py --mixed_precision=\"fp16\"\n","```\n","Most of the parameters are identical to the parameters in the **Text-to-image** training guide."],"metadata":{"id":"Wb43mmf-7OuO"}},{"cell_type":"markdown","source":["### Min-SNR weighting"],"metadata":{"id":"ukq07bJC7Zhh"}},{"cell_type":"markdown","source":["The `Min-SNR` weighting strategy can help with training by rebalancing the loss to achieve faster convergence. The training script supports predicting `epsilon` (noise) or `v_prediction`, but Min-SNR is compatible with both prediction types.\n","\n","We can add the `--snr_gamma` and set it to the recommended value of 5.0:\n","```bash\n","accelerate launch train_text_to_image.py --snr_gamma=5.0\n","```"],"metadata":{"id":"7MAJH4UV7d8D"}},{"cell_type":"markdown","source":["## Training script"],"metadata":{"id":"i7FNtWgv7fQF"}},{"cell_type":"markdown","source":["The training script is similar to other Text-to-image training, but it has been modified to support training the prior and decoder models."],"metadata":{"id":"9J7xBIl5744z"}},{"cell_type":"markdown","source":["##### prior model"],"metadata":{"id":"k_SrJLnG7zDJ"}},{"cell_type":"markdown","source":["The `main()` function contains the code for preparing the dataset and training the model.\n","\n","For the Kandinsky 2.2 prior model, the training script also loads a `CLIPImageProcessor` - in addition to a scheduler and a tokenizer - for preprocessing images and a `CLIPVisionModelWithProjection` model for encoding the images:\n","```python\n","    # Load scheduler, image_processor, tokenizer and models.\n","    noise_scheduler = DDPMScheduler(beta_schedule=\"squaredcos_cap_v2\", prediction_type=\"sample\")\n","    image_processor = CLIPImageProcessor.from_pretrained(\n","        args.pretrained_prior_model_name_or_path, subfolder=\"image_processor\"\n","    )\n","    tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_prior_model_name_or_path, subfolder=\"tokenizer\")\n","\n","    def deepspeed_zero_init_disabled_context_manager():\n","        \"\"\"\n","        returns either a context list that includes one that will disable zero.Init or an empty context list\n","        \"\"\"\n","        deepspeed_plugin = AcceleratorState().deepspeed_plugin if accelerate.state.is_initialized() else None\n","        if deepspeed_plugin is None:\n","            return []\n","\n","        return [deepspeed_plugin.zero3_init_context_manager(enable=False)]\n","\n","    weight_dtype = torch.float32\n","    if accelerator.mixed_precision == \"fp16\":\n","        weight_dtype = torch.float16\n","    elif accelerator.mixed_precision == \"bf16\":\n","        weight_dtype = torch.bfloat16\n","    with ContextManagers(deepspeed_zero_init_disabled_context_manager()):\n","        image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n","            args.pretrained_prior_model_name_or_path, subfolder=\"image_encoder\", torch_dtype=weight_dtype\n","        ).eval()\n","        text_encoder = CLIPTextModelWithProjection.from_pretrained(\n","            args.pretrained_prior_model_name_or_path, subfolder=\"text_encoder\", torch_dtype=weight_dtype\n","        ).eval()\n","```\n","Kandinsky uses a `PriorTransformer` to generate the image embeddings, so we want to set up the optimizer to learn the prior model's parameters:\n","```python\n","    prior = PriorTransformer.from_pretrained(args.pretrained_prior_model_name_or_path, subfolder=\"prior\")\n","\n","    # Freeze text_encoder and image_encoder\n","    text_encoder.requires_grad_(False)\n","    image_encoder.requires_grad_(False)\n","\n","    # Set prior to trainable.\n","    prior.train()\n","    .......\n","    if args.use_8bit_adam:\n","        try:\n","            import bitsandbytes as bnb\n","        except ImportError:\n","            raise ImportError(\n","                \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\n","            )\n","\n","        optimizer_cls = bnb.optim.AdamW8bit\n","    else:\n","        optimizer_cls = torch.optim.AdamW\n","    optimizer = optimizer_cls(\n","        prior.parameters(),\n","        lr=args.learning_rate,\n","        betas=(args.adam_beta1, args.adam_beta2),\n","        weight_decay=args.adam_weight_decay,\n","        eps=args.adam_epsilon,\n","    )\n","```\n","Next, the input captions are tokenized, and images are preprocessed by the `CLIPImageProcessor`:\n","```python\n","    # Preprocessing the datasets.\n","    # We need to tokenize input captions and transform the images.\n","    def tokenize_captions(examples, is_train=True):\n","        captions = []\n","        for caption in examples[caption_column]:\n","            if isinstance(caption, str):\n","                captions.append(caption)\n","            elif isinstance(caption, (list, np.ndarray)):\n","                # take a random caption if there are multiple\n","                captions.append(random.choice(caption) if is_train else caption[0])\n","            else:\n","                raise ValueError(\n","                    f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n","                )\n","        inputs = tokenizer(\n","            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n","        )\n","        text_input_ids = inputs.input_ids\n","        text_mask = inputs.attention_mask.bool()\n","        return text_input_ids, text_mask\n","\n","    def preprocess_train(examples):\n","        images = [image.convert(\"RGB\") for image in examples[image_column]]\n","        examples[\"clip_pixel_values\"] = image_processor(images, return_tensors=\"pt\").pixel_values\n","        examples[\"text_input_ids\"], examples[\"text_mask\"] = tokenize_captions(examples)\n","        return examples\n","```\n","Finally, the training loop converts the input images into latents, adds noise to the image embeddings, and makes a prediction."],"metadata":{"id":"anauQkyH71-n"}},{"cell_type":"markdown","source":["##### decoder model"],"metadata":{"id":"_YP4-KSN-vwP"}},{"cell_type":"markdown","source":["Same here, the `main()` function contains the code for preparing the dataset and training the model.\n","\n","Unlike the prior model, the decoder initializes a `VQModel` to decode the latents into images and it uses a `UNet2DConditionalModel`:\n","```python\n","    def deepspeed_zero_init_disabled_context_manager():\n","        \"\"\"\n","        returns either a context list that includes one that will disable zero.Init or an empty context list\n","        \"\"\"\n","        deepspeed_plugin = AcceleratorState().deepspeed_plugin if accelerate.state.is_initialized() else None\n","        if deepspeed_plugin is None:\n","            return []\n","\n","        return [deepspeed_plugin.zero3_init_context_manager(enable=False)]\n","\n","    weight_dtype = torch.float32\n","    if accelerator.mixed_precision == \"fp16\":\n","        weight_dtype = torch.float16\n","    elif accelerator.mixed_precision == \"bf16\":\n","        weight_dtype = torch.bfloat16\n","    with ContextManagers(deepspeed_zero_init_disabled_context_manager()):\n","        vae = VQModel.from_pretrained(\n","            args.pretrained_decoder_model_name_or_path, subfolder=\"movq\", torch_dtype=weight_dtype\n","        ).eval()\n","        image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n","            args.pretrained_prior_model_name_or_path, subfolder=\"image_encoder\", torch_dtype=weight_dtype\n","        ).eval()\n","    unet = UNet2DConditionModel.from_pretrained(args.pretrained_decoder_model_name_or_path, subfolder=\"unet\")\n","```"],"metadata":{"id":"I2rxiw4z-xTI"}},{"cell_type":"markdown","source":["Next, the script includes several image transforms and a `preprocessing` function for applying the transforms to the images and returning the pixel values:\n","```python\n","    # Preprocessing the datasets.\n","    # We need to tokenize inputs and targets.\n","    column_names = dataset[\"train\"].column_names\n","\n","    image_column = args.image_column\n","    if image_column not in column_names:\n","        raise ValueError(f\"--image_column' value '{args.image_column}' needs to be one of: {', '.join(column_names)}\")\n","\n","    def center_crop(image):\n","        width, height = image.size\n","        new_size = min(width, height)\n","        left = (width - new_size) / 2\n","        top = (height - new_size) / 2\n","        right = (width + new_size) / 2\n","        bottom = (height + new_size) / 2\n","        return image.crop((left, top, right, bottom))\n","\n","    def train_transforms(img):\n","        img = center_crop(img)\n","        img = img.resize((args.resolution, args.resolution), resample=Image.BICUBIC, reducing_gap=1)\n","        img = np.array(img).astype(np.float32) / 127.5 - 1\n","        img = torch.from_numpy(np.transpose(img, [2, 0, 1]))\n","        return img\n","\n","    def preprocess_train(examples):\n","        images = [image.convert(\"RGB\") for image in examples[image_column]]\n","        examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n","        examples[\"clip_pixel_values\"] = image_processor(images, return_tensors=\"pt\").pixel_values\n","        return examples\n","```\n","Lastly, the training loop handles converting the images to latents, adding noise, and predicting the noise residual."],"metadata":{"id":"PFTL2kOdpI4a"}},{"cell_type":"markdown","source":["## Launch the script"],"metadata":{"id":"cDdhHn3Jp0Mo"}},{"cell_type":"markdown","source":["Same as before, we will train our custom model on the Naruto BLIP captions dataset to generate our own Naruto characters."],"metadata":{"id":"1nBrCT3Jp2QC"}},{"cell_type":"markdown","source":["##### prior model"],"metadata":{"id":"fc1yPNtsqkaq"}},{"cell_type":"markdown","source":["```bash\n","export DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n","\n","accelerate launch --mixed_precision=\"fp16\" train_text_to_image_prior.py \\\n","  --dataset_name=$DATASET_NAME \\\n","  --resolution=768 \\\n","  --train_batch_size=1 \\\n","  --gradient_accumulation_steps=4 \\\n","  --max_train_steps=15000 \\\n","  --learning_rate=1e-05 \\\n","  --max_grad_norm=1 \\\n","  --checkpoints_total_limit=3 \\\n","  --lr_scheduler=\"constant\" \\\n","  --lr_warmup_steps=0 \\\n","  --validation_prompts=\"A robot naruto, 4k photo\" \\\n","  --report_to=\"wandb\" \\\n","  --push_to_hub \\\n","  --output_dir=\"kandi2-prior-naruto-model\"\n","```"],"metadata":{"id":"Nzvk_YhhqoSe"}},{"cell_type":"markdown","source":["Once training is finished,"],"metadata":{"id":"QdMYmimNsQKa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qcRwiCQV5yqB"},"outputs":[],"source":["from diffusers import AutoPipelineForText2Image, DiffusionPipeline\n","import torch\n","\n","prior_pipeline = DiffusionPipeline.from_pretrained(\n","    'path/to/our/custom/model',\n","    torch_dtype=torch.float16,\n",")\n","prior_components = {\n","    'prior_' + k: v for k,v in prior_pipeline.components.items()\n","}\n","\n","pipeline = AutoPipelineForText2Image.from_pretrained(\n","    'kandinsky-community/kandinsky-2-2-decoder',\n","    **prior_components,\n","    torch_dtype=torch.float16\n",")\n","pipe.enable_model_cpu_offload()"]},{"cell_type":"code","source":["prompt = 'a robot naruto, 4k photo'\n","\n","image = pipeline(prompt).images[0]\n","image"],"metadata":{"id":"PcDUEIAfswkw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### decoder model"],"metadata":{"id":"Be5RONWPs9gE"}},{"cell_type":"markdown","source":["```bash\n","export DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n","\n","accelerate launch --mixed_precision=\"fp16\"  train_text_to_image_decoder.py \\\n","  --dataset_name=$DATASET_NAME \\\n","  --resolution=768 \\\n","  --train_batch_size=1 \\\n","  --gradient_accumulation_steps=4 \\\n","  --gradient_checkpointing \\\n","  --max_train_steps=15000 \\\n","  --learning_rate=1e-05 \\\n","  --max_grad_norm=1 \\\n","  --checkpoints_total_limit=3 \\\n","  --lr_scheduler=\"constant\" \\\n","  --lr_warmup_steps=0 \\\n","  --validation_prompts=\"A robot naruto, 4k photo\" \\\n","  --report_to=\"wandb\" \\\n","  --push_to_hub \\\n","  --output_dir=\"kandi2-decoder-naruto-model\"\n","```"],"metadata":{"id":"LuLdHCD3s_7l"}},{"cell_type":"markdown","source":["Once training is finished,"],"metadata":{"id":"vuEEdmkYtZz6"}},{"cell_type":"code","source":["from diffusers import AutoPipelineForText2Image\n","import torch\n","\n","pipeline = AutoPipelineForText2Image.from_pretrained(\n","    'path/to/our/custom/model',\n","    torch_type=torch.float16,\n",")\n","pipe.enable_model_cpu_offload()"],"metadata":{"id":"WgTVXyRus-6I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For the decoder model, we can also perform inference from a saved checkpoint which can be useful for viewing intermediate results by loading the checkpoint into the UNet:"],"metadata":{"id":"xx4Os5JwtpSv"}},{"cell_type":"code","source":["from diffusers import AutoPipelineForText2Image, UNet2DConditionModel\n","\n","unet = UNet2DConditionModel.from_pretrained(\n","    'path/to/our/custom/model' + '/checkpoint-<N>/unet'\n",")\n","\n","pipeline = AutoPipelineForText2Image.from_pretrained(\n","    'kandinsky-community/kandinsky-2-2-decoder',\n","    unet=unet,\n","    torch_dtype=torch.float16\n",")\n","pipeline.enable_model_cpu_offload()"],"metadata":{"id":"lpeoNb1OtyRC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = pipeline(prompt=\"A robot naruto, 4k photo\").images[0]"],"metadata":{"id":"AJncMm6wuHFc"},"execution_count":null,"outputs":[]}]}