{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMgq6FhtGbNCITQWtAJtw5W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Structured Generation from Images or Documents Using Vision Language Models"],"metadata":{"id":"h46tK5njcWxL"}},{"cell_type":"markdown","source":["In this example, we will use the `SmolVLM-Instruct` model to extract structured information from documents. We will run the VLM using the Transformers library and the [`Outlines`](https://github.com/dottxt-ai/outlines) library, which facilitates structured generation based on limiting token sampling probabilities."],"metadata":{"id":"IQ3AHeLLccU6"}},{"cell_type":"markdown","source":["## Setups"],"metadata":{"id":"ArrDiqfgcvfW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7zFmPdE2cRqS"},"outputs":[],"source":["!pip install -qU accelerate outlines transformers torch flash-attn datasets sentencepiece"]},{"cell_type":"code","source":["import outlines\n","import torch\n","\n","from datasets import load_dataset\n","from outlines.models.transformers_vision import transformers_vision\n","from transformers import AutoProcessor, AutoModelForImageTextToText\n","from pydantic import BaseModel"],"metadata":{"id":"QHbHDNYrczp4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Initialize the model"],"metadata":{"id":"RcwBLCSfiIJD"}},{"cell_type":"markdown","source":["We start by initializing the [`HuggingFaceTB/SmolVLM-Instruct`](https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct).\n","\n","Outlines expects us to pass in a model class and processor class."],"metadata":{"id":"YwnGCZxhiLlw"}},{"cell_type":"code","source":["model_name = \"HuggingFaceTB/SmolVLM-Instruct\"\n","\n","def get_model_and_processor_class(model_name: str):\n","    processor = AutoProcessor.from_pretrained(model_name)\n","    model = AutoModelForImageTextToText.from_pretrained(model_name)\n","\n","    classes = mode.__class__, processor.__class__\n","    del model, processor\n","\n","    return classes"],"metadata":{"id":"jBhC8fSriJuv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_class, processor_class = get_model_and_processor_class(model_name)\n","\n","if torch.cuda.is_available():\n","    device = 'cuda'\n","elif torch.backends.mps.is_available():\n","    device = 'mps'\n","else:\n","    device = 'cpu'\n","\n","model = transformers_vision(\n","    model_name,\n","    model_class=model_class,\n","    device=device,\n","    model_kwargs={'torch_dtype': torch.bfloat16, 'device_map': 'auto'},\n","    processor_class=processor_class,\n","    processor_kwargs={'device': device},\n",")"],"metadata":{"id":"mlKttv-Wikt5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Structured generation"],"metadata":{"id":"_cGnvO5Ki83B"}},{"cell_type":"markdown","source":["We will define a function that defines how the output of our model will be structured. We will use the [`openbmb/RLAIF-V-Dataset`](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset) dataset, which contains a set of images along with questions and their chosen and rejected responses.\n","\n","We want to create additional text-image-to-text data on top of the images to get our own structured dataset, and potentially finetune our model on it. We will use the model to generate a caption, a question, and a simple quality tag for the image."],"metadata":{"id":"bwhdfUW_i-uI"}},{"cell_type":"code","source":["class ImageData(BaseModel):\n","    quality: str\n","    description: str\n","    question: str\n","\n","\n","structured_generator = outlines.generate.json(model, ImageData)"],"metadata":{"id":"g_35kafBi-ID"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we will define an extraction prompt:"],"metadata":{"id":"kNSXINxJjo1P"}},{"cell_type":"code","source":["prompt = \"\"\"\n","You are an image analysis assisant.\n","\n","Provide a quality tag, a description and a question.\n","\n","The quality can either be \"good\", \"okay\" or \"bad\".\n","The question should be concise and objective.\n","\n","Return your response as a valid JSON object.\n","\"\"\".strip()"],"metadata":{"id":"z4kr1jDpjrNL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load our dataset:"],"metadata":{"id":"M79CnwTbjxid"}},{"cell_type":"code","source":["dataset = load_dataset('openbmb/RLAIF-V-Dataset', split='train[:10]')\n","dataset"],"metadata":{"id":"JCoPUJ10jzAi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we will define a function that will extract the structured information from the image. We will format the prompt using the `apply_chat_template` method and pass it to the model along with the image after that."],"metadata":{"id":"3y00EhMgj4Uy"}},{"cell_type":"code","source":["def extract(row):\n","    messages = [\n","        {\n","            'role': 'user',\n","            'content': [\n","                {\n","                    'type': 'image'\n","                },\n","                {\n","                    'type': 'text',\n","                    'text': prompt\n","                }\n","            ]\n","        }\n","    ]\n","\n","    formatted_prompt = model.processor.apply_chat_template(\n","        messages,\n","        add_generation_prompt=True\n","    )\n","    result = structured_generator(formatted_prompt, [row['image']])\n","    row['synthetic_question'] = result.question\n","    row['synthetic_description'] = result.description\n","    row['synthetic_quality'] = result.quality\n","\n","    return row"],"metadata":{"id":"-iL0awCDkCCZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = dataset.map(lambda x: extract(x))\n","dataset"],"metadata":{"id":"uSVkDluykdQf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset[0]"],"metadata":{"id":"xx5EYdTqkic3"},"execution_count":null,"outputs":[]}]}